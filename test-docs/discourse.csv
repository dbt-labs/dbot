"topic_id","slug","raw","post_number"
18,"using-dbt-to-manage-user-defined-functions","**Note: This post uses Redshift SQL, however the same concept applies to most data warehouses. Check out the notes on BigQuery in the comments below.**

Our dbt project uses some user defined functions (UDFs).
In the past, I've created these UDFs in my SQL console as the superuser, however this isn't ideal as:
* The code was not surfaced as part of our dbt project, so it's unclear what the UDF does
* There was no version control on the UDFs if we needed to update them
* And we weren't maintaining separate development/production versions of the UDFs .

So I decided to put them into our dbt project, using this process:

1. Created a subdirectory, just to keep things tidy: `macros/udfs/`
2. Created a file for each udf, e.g. `macros/udfs/f_future_date.sql` (below). Note that each of my UDFs is prefixed with `f_`, as recommended in the [Redshift documentation](https://docs.aws.amazon.com/redshift/latest/dg/udf-naming-udfs.html).
```
{% macro create_f_future_date() %}
CREATE OR REPLACE FUNCTION {{target.schema}}.f_future_date()
RETURNS TIMESTAMP
IMMUTABLE AS $$
SELECT '2100-01-01'::TIMESTAMP;
$$ LANGUAGE sql
{% endmacro %}
```

4. Created a macro called `macros/create_udfs.sql` which calls each UDF macro. Note separation with `;s`. Also, note the inclusion of a `create schema` statement â€“ since the target schema is created after `on-run-start` hooks, and Iâ€™ve used the target schema in my UDFs, I have to create that schema if it doesnâ€™t exist.
```
{% macro create_udfs() %}

create schema if not exists {{target.schema}};

{{create_f_list_custom_keys()}};

{{create_f_count_custom_keys()}};

{{create_f_future_date()}}

{% endmacro %}
```
5. Added an `on-run-start` hook to my project
```
on-run-start:
    - '{{create_udfs()}}'
```
6. Updated the references to UDFs in my models to use the schema-versions, e.g.
```
SELECT
{{target.schema}}.F_FUTURE_DATE()
```


Other quirks I ran into when doing this were:
1. As the superuser, I had to grant permission to users to create python udfs:
```
-- as your superuser
GRANT USAGE ON LANGUAGE PLPYTHONU TO userA;
```
2. I _had_ to use schemas because users cannot edit each othersâ€™ UDFs (i.e. if `userA` creates the UDF, and `userB` runs the `CREATE OR REPLACE FUNCTION` statement, a ""must be owner of function"" error will occur). Using schemas means you are creating distinct UDFs* so wonâ€™t hit this issue. I would recommend using schemas anyway to maintain separate dev/production UDFs

\* Assuming each Redshift user profile used by dbt has a distinct target schema.",1
18,"using-dbt-to-manage-user-defined-functions","I like it!  I think I'll break up my GA post-processing code (which includes a BigQuery javascript UDF) to move the UDFs out like this to keep things appropriately modular.",2
18,"using-dbt-to-manage-user-defined-functions","@claire So, I think this might be a stupid question, but why use UDFs instead of macros within dbt?",3
18,"using-dbt-to-manage-user-defined-functions","Not a stupid question at all!
I use UDFs when I need to use python to solve a problem that SQL (or Jinja) can't.
For example, one of our data sources has values stored in JSON keys (ðŸ¤¦), like so:
| json | 
|------| 
| {""known_key"": true, ""custom text"": true, ""more custom text"": true}     | 
| {""known_key"": true, ""even more custom text"": true}     | 

I instead wanted my data like so:
|custom_keys|
|------|
|[""custom text"", ""more custom text""]|
|[""even more custom text""] |
I can't think of how Redshift SQL or Jinja would be able to do the above.
But it's an easy problem to solve using the json python package, so I wrapped it up in a UDF.",4
18,"using-dbt-to-manage-user-defined-functions","@claire Thanks so much for this post! Great tutorial. Depending on your Redshift config, you may also have to grant permissions to execute the functions you create. I do this with a macro in the post-hook:

    {% for u in user_list %}
      {% for s in schema_list %}
        grant execute on all functions in schema {{ s }} to {{ u }} ;
      {% endfor %}
    {% endfor %}",5
18,"using-dbt-to-manage-user-defined-functions","@claire Are you recreating your production schema every time? If not, I can see 2 issues with this approach.
1. If you change the number of arguments your UDF receives, you will end up with 2 functions with the same name but different arguments (redshift supports method overloading). Not sure this is always the wanted result.
2. If you change the returned type, I believe you will get an error when running the script.

Regards,
Zeev",6
18,"using-dbt-to-manage-user-defined-functions","Hey @zeevs
Yup, you're totally right! A significant change in the function is going to break the macro.

I think I played around with including a `DROP FUNCTION` statement in my macro, but if the arguments change, then the statement would return an error (since you'd be trying to DROP a function with different arguments to the one that currently exists).

So I chose that in that case I'd handle it manually. Not perfect, but also not a common scenario thankfully :slight_smile: 

I think best practice here is to drop your production schema regularly, but that's not something that I've implemented",7
18,"using-dbt-to-manage-user-defined-functions","I guess you could get the definition of all existing udfs from redshift's internal tables and drop all existing udfs, and then run clean create statements for what you need.",8
18,"using-dbt-to-manage-user-defined-functions","@claire, I adapted your instructions for BQ tonight and it seems one of the BigQuery Persistent UDF limitations is that you cannot define more than one `CREATE FUNCTION` per call. 

This is apparent even by using the BigQuery UI; BQ won't let you execute two CREATE FUNCTION statements in the same query unless they are set as TEMPORARY.

As a result, I don't think it's possible to use a `create_udfs()`-like function for BigQuery since all UDF creation will get compiled and executed in a single call. For example, my error back from `dbt run` is:

`Syntax error: Unexpected extra token CREATE at [7:1]`

I haven't tried it yet, but I suspect calling one macro for each array element of our `dbt_project.yml`'s `on-run-start` hook should solve this, since it will (from my understanding) execute each singular CREATE FUNCTION in it's own BQ request.

I will try it out tmrw!",9
18,"using-dbt-to-manage-user-defined-functions","@claire et al,

We are able to make BigQuery Persistent UDFs work by adapting your RedShift instructions above. It's not as modular as we like, but it's due to BigQuery limitations more than dbt ones.

For anyone visiting in the future, here is a contrived example that implements  `pi()` and `radian()` methods, and tucks them under a `f_math` dataset. 

Note that `pi()` simply returns a constant, but `radian()` method *uses* the `pi()` method. This has consequences on how you must call the `pi()` method inside of `radian()` (it must be project id qualified)

In our `dbt_project.yml` we have three `on-run-start` methods. One creates the schema (dataset in BigQuery) if it doesn't exist, the other two create the Persistant UDFs.

```
on-run-start:

  # MATH FUNCTIONS (order must be preserved!!)
  - '{{ create_schema(""f_math"") }}'
  - '{{ make_func_math_pi() }}'
  - '{{ make_func_math_radian() }}'
```

The `create_schema` macro is a simple wrapper around `adapter.create_schema` that has `target.project` hard-coded into the first argument. We could have just-as easily called `adapter.create_schema` directly from the `on-run-start` hook.

```
-- macros/create_schema.sql

{% macro create_schema (schema_name) %}

{% do adapter.create_schema(target.project, schema_name) %}

{% endmacro %}
```

Our `pi()` UDF looks like this:

```
-- macros/udfs/math/pi.sql

{% macro make_func_math_pi () %}

CREATE OR REPLACE FUNCTION f_math.pi() RETURNS FLOAT64 AS (3.141592653589793);

{% endmacro %}
```

and our `radian()` UDF:

```
-- macros/udfs/math/radian.sql

{% macro make_func_math_radian () %}

CREATE OR REPLACE FUNCTION f_math.radian() RETURNS FLOAT64 AS (`{{ target.project }}`.f_math.pi() / 180);

{% endmacro %}
```

**Note** the need to use \``{{ target.project }}`\` inside of the `radian()` UDF definition. Without this, the UDF creation will fail on `dbt run` because BigQuery requires that functions be fully-qualified up to the project id.

----

While this example works, things get hairy when these libraries grow. As such, we've reached out to the UDF PM to ask if the multiple CREATE Persistent limitations could be lifted.",10
18,"using-dbt-to-manage-user-defined-functions","in step 2, if the udf code does a create library in addition to the create function, then this might cause an issue, 
as create library statement in redshift seems not valid to run inside a transaction, I got this error when I tried `CREATE LIBRARY cannot run inside a transaction block`",11
18,"using-dbt-to-manage-user-defined-functions","Hmm, I haven't tried this, but it's possible that if you run a `commit;` just before your `create library` statement, you'll get it to work! Let me know how that goes!",12
18,"using-dbt-to-manage-user-defined-functions","Hey @dabit -- I got some more info internally on this, and my previous reply is not a good one! Running a `commit;` isn't a great idea because it might mess with other queries that dbt runs, where dbt is expecting a transaction to already be open.


Instead, you could run this statement as an `operation` ([docs](https://docs.getdbt.com/docs/using-operations)). So, before you `dbt run`, execute the following:
```
dbt run-operation create_udfs
```
You'll have to adjust your `create_udfs` macro to actually run each query -- the  `run_query`  macro will not begin a transaction automatically.
```
{% macro create_udfs() %}

{% do run_query('create or replace library my_library language plpythonu') %}

{% do run_query('create schema if not exists {{ target.schema }}') %};

{% do run_query(create_f_list_custom_keys()) %};

{% do run_query(create_f_count_custom_keys()) %};

{% do run_query(create_f_future_date() %}

{% endmacro %}

```
^ I haven't tested this code but it should be close to the thing that you want!",13
18,"using-dbt-to-manage-user-defined-functions","thanks @claire, I just moved from a pre hook to a run-operation and it's working :+1: 
kept my create_udfs macro as is (similar to the one from your initial post minus the create library statements) i.e without wrapping them in run_query",14
18,"using-dbt-to-manage-user-defined-functions","Hi @claire - One question I have about this is the schema.  If one has overridden the default (built-in) `macros/get_custom_schema.sql` with their own logic, how would they make that a run-operation like that above comport with the custom schema logic being used during regular model compilation?",15
18,"using-dbt-to-manage-user-defined-functions","Here's one way to do it that worked for me:

1. Define the functions in a macro, and inside the macro call `set_sql_header` like this:
```
{% macro define_bigquery_functions() %}
{% call set_sql_header(config) %}
create temporary function int(value any type)
returns int64 as (
  cast(cast(value as decimal) as int64)
);
{% endcall %}
{% endmacro %}
```

2. Define a shorthand macro that wraps `config` like this:

```
{% macro table() %}
{{ define_bigquery_functions() }}
{{ config(materialized='table', **kwargs) }}
{% endmacro %}
```

3. Now you can just invoke `table()` instead of `config(materialized='table')` at the top of your model files, and you'll have access to your temporary functions!

Advantages of this method:
  - You don't have to specify the schema name in front of every function call
  - You don't have to wait for an extra `on-run-start` operation to run before every `dbt run`
  - Actually reduces boilerplate instead of adding more of it
  - You'll never again wonder whether `config()` takes `materialize=` or `materialized=`

Disadvantages of this method:
  - You can't call these functions inside view definitions (BigQuery doesn't support temporary functions in views)",17
18,"using-dbt-to-manage-user-defined-functions","Hey, something that's taken me a while to figure out and probably worth adding to your post: we were getting errors in our build pipeline because we added a model and then referenced it in a UDF in the same pull request, which then triggered a build starting with `dbt seed`. When the `on-run-end` hook ran, it failed because it was referencing models that didn't exist.

I managed to resolve this by using the `WHICH` flag ([new in dbt 1.0](https://github.com/dbt-labs/dbt-core/pull/4004)) as follows:

```sql
{% macro create_udfs() %}
  {% if flags.WHICH == ""run"" %}
    {{ create_unbilled_volume_report() }}
  {% else %}
    select 1 as id
  {% endif %}
{% endmacro %}
```",18
18,"using-dbt-to-manage-user-defined-functions","

[quote=""jars, post:10, topic:18""]
While this example works, things get hairy when these libraries grow. As such, weâ€™ve reached out to the UDF PM to ask if the multiple CREATE Persistent limitations could be lifted.
[/quote]


Hi folks! - Working with my team to manage our UDFs and curious if anyone was able to make traction with getting the multiple CREATE persistent limitations lifted?",19
18,"using-dbt-to-manage-user-defined-functions","While following these instructions on dbt 1.2 with Snowflake, I was faced with the error [code]Snowflake adapter: got empty sql statement, moving on[/code] and was able to resolve it by calling [code]{% do run_query(create_udfs()) %}[/code] instead of just the macro.

Still, it seems a `run operation` may be preferred.",20
28,"use-more-than-one-key-as-incremental-unique-key","Can I use multiple keys as unique_key in config?
e.g. Something like this: `unique_key = ['date_sk', 'campaign_id', 'message_type', 'variation_name']`",1
28,"use-more-than-one-key-as-incremental-unique-key","Hey @nicor88! Great question. `unique_key` accepts any valid SQL. So, you could do: 

```
unique_key = 'field_1 || field_2 || field_3'
```

...or something similar. Generally, what we recommend is for all models to include a unique key in the contents of the model itself. This helps with uniqueness testing so that you can avoid fanout errors in your SQL. If there is no natural key in the underlying data and you need to create a surrogate key, I'd recommend using the [surrogate key macro](https://github.com/fishtown-analytics/dbt-utils/blob/master/macros/sql/surrogate_key.sql) in dbt-utils to create one. Then you can easily pass that field into the `unique_key` config.",2
28,"use-more-than-one-key-as-incremental-unique-key","As of dbt Core 1.1, a list of column values is supported, exactly in the way you described :tada:  

See the docs for more info: https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models#defining-a-uniqueness-constraint-optional",5
28,"use-more-than-one-key-as-incremental-unique-key","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
29,"pii-anonymization-and-dbt","Personally Identifiable Information (PII) is a real business concern with Data Warehouses. GDPR has more people talking about it now, but even without the new legislation it is something that needs careful consideration. 
Having a well structured Data Warehouse with exposed PII makes walking out the door with your entire customer email list as simple as ""SELECT * FROM customers""... and that's scary. 

Problem is, there are legit times you will need PII access. If an analyst is doing a study that needs components of the PII (like email provider), then a hashed string won't be very helpful. If you want to do 'fuzzy matching' when creating unified customer profiles, you need to start with raw PII. 

Here's how we tackle it at RevZilla:

1. Transform common PII values (first name, last name, email address) in an ephemeral model using a standardized macro. Simple transforms like trimming and down-casing allow us to compare apples to apples.

2. Materialize a hash salt under a _PRIVATE_schema, something like this:
        -- DB specific ways of random gen, I just did this as a semi-universal example.
        -- ideally you would do ascii lookups on the integers the rand returns. 
        SELECT RANDOM()::text AS salt LIMIT 1  

3. We materialize our production tables in a _PRIVATE_ schema, with the prefix PRIVATE on the table. I.e *ANALYSIS.PRIVATE.PRIVATE_USERS*. This schema is restricted and only our PII_GRANTED role can access it. 

4. We create a view of the table in the appropriate _business_ schema. The view should be sans prefix, I.e. *ANALYSIS.MARKETING.USERS*. This view includes a join to our SALT table. Then it is as a simple macro for each PII field that hashes the value with the salt:
    {%- macro anaon(col, salt) -%}
        MD5({{col}}, {{salt}})
    {%- endmacro -%}

And now the users have a safe, hashed value that they can aggregate on (because the hashes will match if the underlying values match).",1
29,"pii-anonymization-and-dbt","I love this. I think Snowflake makes a lot of this kind of stuff a bit easier than other warehouses--I find its permission model a bit easier to understand and we always end up implementing things in a much more locked-down way on Snowflake vs. Redshift. Adding this layer of PII anonymization that you've outlined is a really great improvement that we should be doing more often for our clients. Did you end up doing that right out of the gate or was it an enhancement you made to your warehouse along the way?",2
29,"pii-anonymization-and-dbt","This was definitely an iterative process. Initially we did what I'm sure a lot of shops do, and went very heavy-handed. There was to be **no** PII anywhere, ever, under any circumstances. Everything was obfuscated the second it hit the warehouse, no exceptions. What we found, of course, is that there are always use case exceptions; when business users had legit requirements for reporting that included PII we would send them back to the transactional system, totally undermining the point of BI. 
This 2 layer approach has worked really well so far. I've had to get creative as we move to marts - appending _private to mart schemas and creating the same 2 layers in a smaller context.",3
33,"table-field-syntax-best-practices-when-using-snowflake","Are there best practices as it relates to table / field capitalization when using Snowflake and DBT?  Iâ€™m coming from a postgres/redshift world where weâ€™d typically use snake case (e.g. `orders_items` and `product_id`), but Stitch seems to be pushing data into Snowflake as all caps tables/fields (e.g. `ORDERS` and `PRODUCT_ID`).  Iâ€™m trying to understand whether Snowflake prefers this format or Stitch is doing something odd.  What are others here doing syntax wise with Snowflake?  And, assuming we do use all uppercase tables/fields, how does that work with DBT, since it appears that tables get created based on file names?",1
33,"table-field-syntax-best-practices-when-using-snowflake","Good q. We've found Snowflake's behavior related to capitalization to be rather...cumbersome!...and have recently made some changes to dbt to deal with it more effectively.

Snowflake will by default _uppercase_ identifier names which are unquoted. This is different than most databases which _lowercase_ identifier names which are unquoted. Our 0.10.1 release makes it possible to control whether or not dbt quoted your identifier names. 

Docs:
- https://docs.snowflake.net/manuals/sql-reference/identifiers-syntax.html#identifier-syntax
- https://docs.getdbt.com/docs/configuring-quoting

After 0.10.1, I personally prefer to lower-case all of my Snowflake identifiers and configure dbt to _not_ quote them. I find this to be more readable, as dbt model names can be lower-cased, and SQL that references these models can also be lower-cased.",2
33,"table-field-syntax-best-practices-when-using-snowflake","A related note - in talking with the folks at Stitch, they decided not to quote the tables/fields when loading into Snowflake, since that would require people to have to quote within Snowflake on the query side.  That is, instead of loading data as `""order_items""` and then being forced to query by `select * from myschema.""order_items""`, they chose to load the data unquoted and let Snowflake upcase it, e.g. `order_items` => `ORDER_ITEMS`, which can then be accessed by doing `select * from myschema.order_items`

All in all, it's still all a bit confusing, but Tristan's advice is solid.  Just wanted to give some context on the Stitch side of it since that was in my initial question.",3
43,"using-macros-to-handle-concatenation-of-possibly-null-fields","Note: The DB in use here is BigQuery

I did a dumb thing 10 months ago.  I was converting a python ETL script into SQL, and one of the steps builds a nested address record.  Into this nested record goes the address from the doctor record, and related records from the address table, where the address in the address table does _not_ match the address in the doctor table.

I handled this with a `doctor_addresses` model file which filters the `doctor`+`address` dataset to only those where at least one of the address component fields didn't match the corresponding field on the `doctor` record.  Here's what I did:
```sql
select
  d.dr_id
  ,d.address
  ,d.address2
  ,d.city
  ,d.state
  ,d.zipcode
  ,d.country
  ,d.latitude
  ,d.longitude
from {{ref('doctor')}} d
union distinct
select
  d.dr_id
  ,a.line1 as address
  ,a.line2 as address2
  ,a.city
  ,a.state_region as state
  ,a.postal_code as zipcode
  ,a.country_code as country
  ,a.latitude
  ,a.longitude
from {{ref('doctor')}} d
inner join {{ref('address')}} a
  on  d.dr_id = a.dr_id
where concat(d.address, d.address2, d.city, d.state, d.zipcode, d.country, d.latitude, d.longitude)
  != concat(d.line1, d.line2, d.city, d.state_region, d.postal_code, d.country_code, d.latitude, d.longitude)
```

The dumb thing was not accounting for the possibility that one of more of the address component fields might be `null`, and when that happens, the `concat()` function returns `null`.  And of course doing a `!=` between `null` and anything (including `null`) returns `null`.  So when any of the considered columns in either table was `null`, the second query of the union did nothing.

How _embarrassing..._

To resolve this, I need to cast the values as strings, then replace `null`s with empty strings, then concatenate.  That where clause would look like this:
```sql
select ...
where concat(
  ifnull(safe_cast(d.address as string), ''),
  ifnull(safe_cast(d.address2 as string), ''),
  ifnull(safe_cast(d.city as string), ''),
  ifnull(safe_cast(d.state as string), ''),
  ifnull(safe_cast(d.zipcode as string), ''),
  ifnull(safe_cast(d.country as string), ''),
  ifnull(safe_cast(d.latitude as string), ''),
  ifnull(safe_cast(d.longitude as string), '')
  ) != concat(
  ifnull(safe_cast(d.line1 as string), ''),
  ifnull(safe_cast(d.line2 as string), ''),
  ifnull(safe_cast(d.city as string), ''),
  ifnull(safe_cast(d.state_region as string), ''),
  ifnull(safe_cast(d.postal_code as string), ''),
  ifnull(safe_cast(d.country_code as string), ''),
  ifnull(safe_cast(d.latitude as string), ''),
  ifnull(safe_cast(d.longitude as string), '')
  )
```

Yuck.  It's ugly to look at, and there are lots of opportunities for misplaced commas, quotes, and parens.

The `group_by()` macro in the [dbt macro documentation](https://docs.getdbt.com/docs/macros), however, gave me the idea to build those `concat()`s dynamically.  It iterates through a list of passed field names, generating the `concat()` statement on the fly.
```
{% macro safe_concat(field_list) %}
  {# Takes an input list and generates a concat() statement with each argument in the list safe_casted to a string and wrapped in an ifnull() #}
  concat({% for f in field_list %}
    ifnull(safe_cast({{ f }} as string), '')
    {% if not loop.last %}, {% endif %}
  {% endfor %})
{% endmacro %}
```

By using the macro, the `where` clause becomes quite approachable again:
```sql
select ...
where {{ safe_concat(['d.address','d.address2','d.city','d.state','d.zipcode','d.country','d.latitude','d.longitude']) }}
  != {{ safe_concat(['a.line1','a.line2','a.city','a.state_region','a.postal_code','a.country_code','a.latitude','a.longitude']) }}
```

The compiled SQL has a bunch of extraneous line breaks, as is typical with macro-generated code, but it's a small price to pay for the readability of both the model file and the macro code.",1
43,"using-macros-to-handle-concatenation-of-possibly-null-fields","Love this. As I've personally gotten a lot more comfortable with / in the habit of thinking in Jinja, I've started to see more and more opportunities like this arise. It's actually quite common to want to do the same thing to an entire list of fields, and it's much more concise / DRY to input them as a list and then loop through all of the elements in that list. I've done this with coalesces in the way that you show above, but also very frequently with `last_value()` window functions when I want to dedup some dataset.

The longer I work with Jinja + SQL, the more Jinja I find that I write :D Glad to see you're finding the same...",2
43,"using-macros-to-handle-concatenation-of-possibly-null-fields","As we continue to adapt into the dbt way of thinking, one of the things I and the analysts who use my DW struggle to wrap our heads around is how to have one source for truth for a lot of logic we use in our transformations.  User bucketing is one example.  Sure, we could have a model that has user and a column that contains the logic for the bucketing, but that's not intuitive for us yet, and having a bunch of models like that running around feels clunky.  But if they're macros, it's very intuitive, because we can think about it just like we'd think about a function in SQL or Python instead of thinking about it like a dataset.

It starts to break down the mental wall that SQL code isn't software, which is a great development for the space.",3
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","I'm struggling to have two different development ideologies coexist within dbt.  On the one hand is my team (BI) who is responsible for developing the core data model for the business.  Because those core components have broad impact across the business, development of those components needs to be pretty regimented.

On the other hand, there is a broad periphery of data models that need to be built and maintained by the business analyst group - they're assigned to specific business units, and given the pace of development in the business overall, they will need/want to iterate more quickly, and are more tolerant of flaws in their models.

So how should we structure dbt models to facilitate both ideals?  One thought I had was to have multiple repos (one for the core, and one or more for the data marts the BAs will create).  But it's not clear to me how execution would work in that sort of structure; would I include the data mart package(s) in the core package and `dbt run` from the core project?

Anyone else dealing with similar challenges?",1
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","And what about development in the data mart package(s)?  They'd need to include the core package, so that they could actually run their models to test them, and that would be a circular dependency and dbt just spins around and around forever on `dbt deps` in that case.

It would seem, therefore that the data mart models should refer to the actual database objects generated by my team's dbt repo (rather than `{{ref()}}`, and the data mart package(s) should be independently executed, but this seems antithetical to ""the dbt way"" of doing things.",2
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","Ooh! I've done similar things to what you're suggesting before. I like where your head's at.

When I develop open source packages, I build the ""library"" (the thing I want to share with other people) and then a ""testing project"" (the thing I actually run the code from). This is necessary because in the library, I actually can't specify things like `profile` and many variable names, because those things need to be configured by the core project that is including the library. So--I build the shared code in the library, and then write the specific code that allows me to test and run that code (profiles, variable names, etc) in the ""testing project"" and _run everything from there_. I never actually push the testing project to any git repos...it's just for testing. But I come away with code in a library that I can run pretty seamlessly. 

This translates to your situation pretty directly, even if it sounds a bit roundabout. Your analysts would be able to clone your ""core"" project but not push to it. They would have editor permissions on your ""marts"" project. They would edit the code in `marts`, which would get pulled in as a dependency in `core`, and they'd always run from `core` (which isn't a problem; they can get the code just not edit it). 

Again...I recognize that this seems roundabout but it does work quite nicely in practice. Please let me know if there's some part of this that I can make more clear.",3
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","Thanks @tristan , I think I'm getting the gist of what you're saying. By any chance is there any (dummy) github repo where one can see this kind of structure?",4
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","This isn't really something that gets checked into a package, it's more about a local workflow for how to go about developing packages... You can see packages that have been developed in this workflow in our git org; check out Stripe or Snowplow or Quickbooks or any of them.",5
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","Copy that, thanks.

So in your approach, wouldn't `core` get re-run each time?",6
50,"how-to-balance-the-need-for-controlled-development-of-core-models-with-rapid-development-of-peripheral-models","Assuming you do an entire `dbt run`, yep! You can of course just choose to run whatever subgraph you like though.",7
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","At RealSelf, people developing in DBT default to having their models built in user-specific sandbox schemas. My sandbox schemas are `dbt_md_base` for raw or lightly cleansed data, `dbt_md_work` for interstitial models, and `dbt_md` for final modeled tables.  These mirror the production `dbt_base`, `dbt_work`, and `dbt` schemas.

We use Looker for visualization, and one of the challenges we face is how to switch between sandbox schemas and production schemas so we can test views and explores before rolling them out.  If we hard-code the table reference to the sandbox schema during development, at some point, someone will forget to change it to the production schema before release.

@Tristan talks about a solution to that in some detail [here](https://blog.fishtownanalytics.com/how-to-integrate-dbt-and-looker-with-user-attributes-117fa48c1568).  I'm here to offer an improvement to his solution.  **Do** use user attributes as he suggests, but what if there was a way to set the user attributes once, and then never touch them again, but have Looker automatically switch between sandbox and production based on whether or not you're in dev mode?

Rejoice, fellow Lookers, for there is indeed such a way!

Looker has a not-well-advertised feature that allows you to dynamically insert text into the `sql:` option of a derived table definition, based on whether the user is in dev mode or not.

```text
-- if dev -- some text
-- if prod -- some other text
```

Conveniently, Liquid is parsed after this feature, so we can combine this with user attributes like so:
```text
derived_table: {
    sql:
      SELECT
        some_field
        ,some_other_field
      FROM
        -- if dev -- {{_user_attributes['sandbox_schema']}}.some_table
        -- if prod -- {{_user_attributes['dbt_schema']}}.some_table
      ;;
  }
```

In dev mode, therefore, a view using both of these columns would generate this SQL:
```sql
WITH some_view AS (SELECT
        some_field
        ,some_other_field
      FROM
         dbt_md.some_table
        -- if prod -- dbt.some_table
      )
SELECT 
	some_view.some_field  AS some_view_some_field
    ,some_view.some_other_field  AS some_view_some_other_field
FROM some_view
```

And outside dev mode, it would generate this:
```sql
WITH some_view AS (SELECT
        some_field
        some_other_field
      FROM
        -- if dev -- dbt_md.some_table
        dbt.some_table
      )
SELECT 
	some_view.some_field  AS some_view_some_field
    ,some_view.some_other_field  AS some_view_some_other_field
FROM some_view
```

VoilÃ !  You are now querying your sandbox when in dev mode, and production tables when not in dev mode!",1
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","@michael.dunn - this is so nifty! Thank you so much for sharing ðŸ™ŒðŸ¼",2
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","This is really a neat solution. I suppose it means that all of your looker views are generated from select statements, not `sql_table_name`s? 

That is some terrible syntax though!! Why didn't Looker just include the dev/prod checks as liquid?! Very strange.",3
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","Also, PS: I linked to this thread from the original post. Thanks again :)",4
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","Yeah, it seems like it'd be so simple to include a variable/function like `_user_attributes[]`, perhaps `_isdev()` that returns a boolean.  But I don't suppose it's a super common use case either.",5
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","Have we asked Looker if they can implement it as liquid? If you put something on their Discourse I'd be more than happy to pile on some support in the comments!",6
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","Looks like you can use `sql_table_name` with this syntax. For larger deployments we'd suggest running separate dev/prod instances of Looker ([link](https://discourse.looker.com/t/setting-up-development-qa-staging-instances/2702)).

    view: events {
      sql_table_name:
        -- if prod -- public.events
        -- if dev -- public.events2
    ;;

I've +1'd the Looker request to support this via a Liquid variable.",7
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","Nice thread. I've tried this and it works great for a Looker explore that is based on a single view (no additional joins to other views).

But for Looker explore that joins a couple of views, the 'conditional' statement don't seem to carry over. For example:

> view: some_view {
>   sql_table_name:
>   -- if dev -- dev.some_table
>   -- if prod -- prd.some_table ;;
> 
>   dimension: id {
>     type: number
>     sql: ${TABLE}.id ;;
>     primary_key: yes
>   }
> 
>   dimension: description {
>     type: string
>     sql: ${TABLE}.description ;;
>   }
> }

Joining the view above with another view via explore:

> explore: some_explore {
>   view_name: main_view
> 
>   join: some_view {
>     relationship: one_to_one
>     sql_on: ${main_view.id} = ${some_view.id} ;;
>   }
> }

SQL render output on Looker explorer Development mode:

> SELECT 
> 	some_view.description
> FROM prd.main_view
> LEFT JOIN  dev.some_view
>   -- if prod -- prd.some_view ON main_view.id = some_view.id 

the JOIN ON statement only applies to prd.some_view.

Has anybody experience this? Is there a solution this? This means that in order to avoid this issue all joins need to happen in dbt model. But, this means we cannot use Looker's explore modeling when using dbt and Looker together.",8
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","It's not even that it only applies to the prod version.  It's simpler than that.  They're just not inserting a line break between the `join` clause and the `on` clause, causing `on` to be part of the SQL comment.

Have you tried inserting an extra line break in your LookML before the `;;` in your `sql_table_name` attribute?",9
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","You're right! it works. I never thought of that. Thanks!",10
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","I want to put an emphasis on something important that was not specifically mentioned here, and which caused me some headaches until I figured it out.

It is super important that the double semicolons are on their own line, otherwise the ON clause on joins will be commented out and will therefore return duplicated rows.

So, this is bad:

```
view: events {
  sql_table_name:
    -- if prod -- public.events
    -- if dev -- public.events2 ;;
```

And this is good:

```
view: events {
  sql_table_name:
    -- if prod -- public.events
    -- if dev -- public.events2
;;
```",11
54,"looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas","This approach to implement separate environments is brilliant. Thank you a lot for this Michael and Tristan. 

I am wondering if this approach also can work with aggregate tables or persistent derived tables. After defining an aggregate table on an explore referencing views that are using the if dev / prod function and user attributes to switch between development and production as described above, I get the following warning:

*Persistent Native Derived Table for view ""<my_explore::my_aggregate_table>"" references user attributes. Persisting this table may result in unexpected behavior.*

Has somebody here come across the same issue? Is it somehow possible to user aggregate tables with this approach?",12
61,"writing-queries-in-a-bi-tool-vs-modeling-them-in-dbt","I recently asked [a question](https://getdbt.slack.com/archives/C0VLZPLAE/p1528487259000487) in Slack about best practices around modeling and would like to continue that discussion here because there are still some things I'm still unclear about.

To take a real-world example, I'm using Mode as a BI tool and I want to create a stacked bar chart showing how many recent upgrades there were by day for a certain segment of the users broken down by plan.

Here's what the query looks like if I write it directly in Mode:

```
select
  upgrade_date,
  plans.name as plan_name,
  count(*) as upgrades
from analytics.users 
join analytics.plans using (plan_id)
where
  upgrade_date >= current_date - interval '30 days' and
  vertical = 'work'
group by 1, 2
```

Alternatively, I can model this using dbt, something like this:

```
with recent_work_users as (

	select *
	from {{ ref('users') }}
	where
		vertical = 'work' and
		upgrade_date >= current_date - interval '30 days'

),

final as (

	select
	  upgrade_date,
	  plans.name as plan_name,
	  count(*) as upgrades
	from recent_work_users
	join analytics.plans using (plan_id)
	group by 1, 2

)

select * from final
```

And then in Mode simply run:

```
select * from analytics.recent_work_upgrades_by_plan
```

Which approach makes more sense - writing the query directly in Mode or modeling it in dbt? I could be wrong, but I feel like this isn't what I should be modeling in dbt, but writing the full query in Mode doesn't seem ideal either because it won't be version controlled, etc.

It may be that [analyses](https://docs.getdbt.com/docs/analyses) are the way to go, but I'm unclear about how to fit them into my workflow. For example, I typically will draft a query, run it, then repeat a few times until things are just right. Would I do that in Mode, then paste the final query into a new analysis file, replacing the view names with dbt references & converting it to use CTEs, compile it, then paste the compiled query back into Mode? I'm guessing there's a better way :).

It seems like there are a lot of ways this can be done - which approach would you all recommend?",1
61,"writing-queries-in-a-bi-tool-vs-modeling-them-in-dbt","The key here is _interactivity_. dbt is not interactive--it's a batch-based tool, so all decisions are made at coding-time, not analysis-time. It's very common that a user might want to change the `interval '30 days'` in your query above (if that's not relevant in your particular use case just imagine one of the many use cases where such a thing might be configurable). If you build this as a model in dbt, you prevent the user from interactively looking at the data.

The way we use dbt is to ""provide the best version of all nouns"" to the BI layer. If you have an object called `plans`, dbt's job is to provide the BI layer with the cleanest possible definition of a plan. That way, you can keep your queries in the BI layer clean and focused on actual analysis, not data munging!",2
61,"writing-queries-in-a-bi-tool-vs-modeling-them-in-dbt","Thank you for this.

My challenge with Mode (vs a tool like Looker) is that it's not very interactive. For example, I could be wrong, but I don't think there's a way to use the UI to say ""show me the last 30 days"". You can select a [range of dates](http://c.hlp.sc/1I171l3z2Q0A), but not an duration of time relative to today. So If I want a chart showing upgrades over the last 30 days or for just this month or whatever, it has to be in a query. And because there's no modeling layer, all of the joins and whatnot have to happen in a query as well.

It sounds like you would recommend just throwing my original query above in Mode and not having it in dbt or version control at all, correct?

Can you elaborate on how you use dbt analyses in your work? For example, if it doesn't make sense to use analyses for something like this, how else are dbt analyses used in practice?",3
61,"writing-queries-in-a-bi-tool-vs-modeling-them-in-dbt","You absolutely can build parameters in your Mode reports! This is extremely powerful and we do it a lot:

https://help.modeanalytics.com/articles/parameters/

In some ways Looker's parameters are more powerful but in others Mode's are *much* more powerful. This is specifically due to the fact that you can program *anything in a mode report* using Liquid. We've used this to do some pretty impressive stuff in the past and it allows for quite a bit of interactivity with the user. The answer to your short question of ""should I just put all of this stuff in Mode"" is, IMO, ""yes"" :) 

We don't actively make use of the ""analysis"" folder in dbt in any meaningful way. At some point we'd love to more directly integrate dbt's compilation of jinja with other tools (like Mode) but we're not there. This feature is a bit of a stub at the moment...use at your own risk.",4
61,"writing-queries-in-a-bi-tool-vs-modeling-them-in-dbt","I'm on Mode's free plan so apparently no go on using parameters for now, but that's very good to know about. Thank you for the info on analyses as well. ðŸ™Œ",5
62,"choosing-a-data-warehouse","I've been reading up on the differences between BigQuery and Redshift. I'm curious how you all would decide which is appropriate for an organization that's just getting set up with a data warehouse. Also, when would Snowflake be appropriate?",1
62,"choosing-a-data-warehouse","There are a few key factors:
### 1. Where is your existing infrastructure?
If your business has everything in AWS, it can simplify your life to just use Redshift.  If you're planning to go the full Google Analytics 360, you might prefer BigQuery for its native data flow from GA to BQ.

### 2. How much do you like managing infrastructure/how much infrastructure flexibility do you want?
BigQuery abstracts out all hardware considerations.  Just load the data and go.  But the Google Cloud ecosystem isn't as robust as AWS, so there isn't as much flexibility with respect to using the platform to move the pieces around versus having to write scripts to do that.  With Redshift, it's relatively seamless to move data from Redshift to EMR, or to use Kinesis to stream data into your DB, but then you have to manage the cluster yourself.

### 3. How much data?
If you don't have a ton of data, a pay-as-you-query setup like BigQuery might save you money (but might cost you more money longer term if and as your data volume grows), whereas there is a bit of a cost floor for Redshift.

---
I'm not a fan of the variable cost model of BigQuery, but I don't want to spend time managing infrastructure, and my company's got the full GA360 package, so BigQuery makes the most sense for us.

Regarding snowflake, I don't think there's a time where it's ""too early"" to consider it.  If you're at a volume now where Postgres doesn't cut it as a data warehouse, and you're going to go to an MPP system like Redshift or BigQuery, then you should be considering Snowflake _now_.  I implore you to decide which system you want 5 years from now and go with that now.  Migration can be painful, and should be avoided whenever possible by choosing wisely up front.

Good luck!",2
62,"choosing-a-data-warehouse","Thank you for the thoughtful response @michael.dunn - very helpful.",3
62,"choosing-a-data-warehouse","At Fishtown Analytics, we've worked with Postgres, Redshift, Snowflake, and BigQuery. Ultimately, each database has its own strengths and weaknesses.

@michael.dunn makes a great point above regarding your existing infrastructure. If you're on GCP, BigQuery probably makes a lot of sense. The same goes for AWS/Redshift. I don't think this is a hard-and-fast rule though, as Snowflake isn't offered by a particular cloud provider, and most off-the-shelf ETL services can pipe data across clouds.

My main insight here is that these databases are more similar than they are different, and all (except Postgres) are acceptable choices for a first data warehouse. I think some choices are better than others, but ultimately, there's a good reason for using each of them. The primary differences between these databases occur at an architectural level, and a summary of each of these is given below:

### Postgres
Postgres is a relational database, meaning that it stores _rows_ of data. It is fantastic for transactional application workloads, like selecting a single record from a `users` table, or updating the status of a couple of `orders`. 

Analytical queries tend to look very different than these ""transactional"" queries. Instead, you might do a `count(distinct)` on a whole column of data with no filters applied, or join four tables together and group by half a dozen fields, aggregating the rest. Postgres is not designed to handle queries like this, and you'll probably find that queries take forever to return unless indexes are liberally added to the columns that you join on.

To Postgres's credit, it will serve you well for analytical queries until your tables contain millions of records. My thinking here is generally that your data _will_ grow in volume. You can either migrate to a more capable analytical warehouse early (when it's not strictly necessary yet), and provide yourself a whole lot of runway. Or, you can wait until your data (and team!) outgrows Postgres, and you are _forced_ to migrate. I personally would prefer to be in the former situation, as migrating warehouses is very possible, but can be very not fun.

### Redshift
Amazon Redshift is a [columnar](https://en.wikipedia.org/wiki/Column-oriented_DBMS) database that is based on Postgres. Postgres and Redshift feel pretty similar - they both use the same protocol, and share (mostly) identical syntaxes.

Redshift stores data in columns (not rows), meaning that a given query will only read the columns required for its execution. This means that operations like [full table scans](https://en.wikipedia.org/wiki/Full_table_scan) are not only doable, but totally reasonable. You would probably have a bad time doing a full table scan on a large Postgres table.

A Redshift database is a _cluster_ of worker database nodes. These nodes are able to split up a query, execute its constituent parts in parallel, then combine the results. As a result, you can horizontally scale performance by adding nodes to your cluster. If you check out the [Redshift pricing page](https://aws.amazon.com/redshift/pricing/) you'll see that adding a node will give you more processing power, as well as more storage.

This is sort of a problem though: sometimes you need a lot of storage space, but you don't need the extra processing power. The converse may also be true. This coupling of ""storage"" and ""compute"" is one of the main drawbacks of Redshift compared to the other databases listed below. As data teams push the limits of Redshift, they find themselves ""resizing"" their cluster to add another node every couple of months. This can be a painful and expensive process, and it gets pretty old pretty quickly.

Redshift comes with a bunch of configuration to optimize query performance like [sort keys](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html) and [dist keys](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html). You can also configure [compression](https://docs.aws.amazon.com/redshift/latest/dg/t_Compressing_data_on_disk.html) for your columns, or perform table maintenance tasks like [vacuum](https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html) and [analyze](https://docs.aws.amazon.com/redshift/latest/dg/r_ANALYZE.html). It ends up being close to a full-time job to keep Redshift humming along, and honestly, I think that time could be better spent _actually_ doing analytics. The databases below don't have as many bells and whistles, but you end up not really missing them. Maintaining Redshift can be a real chore.

Redshift is pretty easy to set up and is integrated deeply into the AWS ecosystem. You can natively pipe in data from S3, Kinesis, GLUE, etc. If using these tools is high on your priority list, then Redshift might be a good choice for your organization.

### BigQuery
BigQuery is Google's take on a distributed analytical database. Whereas in Redshift you might have six or eight compute nodes, BigQuery will throws _hundreds or thousands_ of nodes at you query. Further, storage on BigQuery is effectively infinite, and you just pay for how much data you load into and query in the warehouse. This means that truly insane amounts of data can be queried with pretty astounding query latency. [This video](https://www.youtube.com/watch?v=UueWySREWvk) is the best resource I've found on BigQuery under the hood; definitely give it a quick look if you're interested in using BQ.

So, BigQuery can process really big datasets really quickly, but it of course comes with some caveats. BigQuery is really, very bad at doing joins. The recommended way to to handle one-to-many relationships is through [repeated records](https://cloud.google.com/bigquery/docs/nested-repeated). Repeated records are a radically new data storage pattern that Google just decided to make up. These repeated records are incredibly useful and they work very well, but you'll need to internalize this feature of BigQuery to make effective use of its power.

I have a pretty good intuition for SQL and how databases work, but really, it just doesn't translate to BigQuery. I think that BQ is a great choice for a database, as long as the folks in your organization are willing to rewire their brains to take advantage of its features and access patterns.

BigQuery has some annoying artificial limitations. [Date partitioning](https://cloud.google.com/bigquery/docs/partitioned-tables) is crucial for performant querying, but only something like 2,300 date partitions can be created at a given time. My bet is that limitations like these will become more flexible (or disappear) over time, but keep in mind that GMail was in Beta for five years, and Google's gonna do Google things.


### Snowflake
Snowflake is situated as a sort of happy medium between Redshift and BigQuery. It generally looks and acts how you would expect a database to behave. If you have existing intuition around analytics, it will probably translate to Snowflake pretty readily. Check out [this article](https://medium.com/@jthandy/how-compatible-are-redshift-and-snowflakes-sql-syntaxes-c2103a43ae84) for more information on migrating from Redshift to Snowflake.

Snowflake is more similar in architecture to Redshift than BigQuery. Crucially though, its storage is decoupled from its compute. With Snowflake you pay for 1) storage space used and 2) amount of time spent querying data. Snowflake also has a notion of a ""logical warehouse"" which is the ""compute"" aspect of the database. These warehouses can be scaled up or down to deliver different grades of performance. You can also configure the number of compute nodes to parallelize query execution. These warehouses can be configured to ""pause"" when you're not using them. As a result, you can have a super beefy warehouse for BI queries that's only running when people are using your BI tools, while your background batch jobs can use cheaper hardware.

Snowflake doesn't have all the performance optimization bells and whistles of Redshift, but you end up not really needing or missing them. Snowflake's query optimizer is substantially better than Redshift's, and I find myself not really thinking about performance considerations when writing SQL for Snowflake.

### Final thoughts
I prefer using Snowflake and BigQuery over Redshift because I can mostly just write the logic I want to implement without regard for how the database will execute the query behind the scenes. On Redshift, you have to think about column compression, sort keys, dist keys, disk utilization, or vacuum/analyze statements. `ERROR: Disk Full` is a thing you have to deal with on Redshift, but that's just not a class of error that can occur on BigQuery or Snowflake.

It is absolutely possible to migrate warehouses, but it can definitely be a big pain. The best advice I can give is: conduct analytics in a way that preserves your optionality. Avoid writing custom scripts to pipe data into Redshift; instead, use a tool like [Singer](https://singer.io/) that will pipe data to any of the warehouses listed above. Be judicious when using non-standard features of a warehouse (like Postgres GIS plugins, or BigQuery javascript UDFs). To be sure, you _should_ use your warehouse to its fullest potential, but definitely be mindful of how these decisions impact your switching cost. Today, Redshift feels old and crufty, but it was the best-in-class analytical warehouse only 6 years ago. This industry moves fast!

The good news is that technology in the data space [is becoming more modular](https://blog.modeanalytics.com/designing-analytics-stacks-like-software/), and databases are continuing to coalesce around a standard-ish version of SQL. If you're just getting started with a warehouse, really any of Redshift, Snowflake, or BigQuery are likely appropriate. I think that choosing Snowflake or BigQuery is probably a more forward-thinking decision, and they're my preference absent any other business-specific considerations.

This is just one analyst's opinion though. Here's some further reading:
- https://www.periscopedata.com/blog/interactive-analytics-redshift-bigquery-snowflake
- https://fivetran.com/blog/warehouse-benchmark
- https://www.blendo.co/blog/amazon-redshift-vs-google-bigquery/
- https://blog.panoply.io/a-full-comparison-of-redshift-and-bigquery",4
62,"choosing-a-data-warehouse","I'm curious to here more from Big Query users about the difficulty of joins.  From the benchmark posts above, it seems like this isn't that big of an issue, but @drew seems to suggest otherwise.",5
62,"choosing-a-data-warehouse","[quote=""drew, post:4, topic:62""]
The recommended way to to handle one-to-many relationships is through repeated records. 

I think that BQ is a great choice for a database, as long as the folks in your organization are willing to rewire their brains to take advantage of its features and access patterns.
[/quote]

Is it fair to say that this mainly applies at some level of scale where performance issues come into play? For example, when I write one-to-many queries in BigQuery, I write them without giving any thought to performance and have never noticed any significant issues. For a simple example, this query takes about 4 seconds:

```
select com_id, count(tag_id) as tags
from mysql_app.app_companies 
left join mysql_app.app_companies_tags on tag_com_id = com_id
group by 1
```

However, it may simply be because we're working with millions of rows in these tables, not billions.

If that's the case, how would you decide when it's worth taking time to optimize for performance, either in BigQuery specifically, or any of these data warehouses?",6
80,"removing-jinja-comments-from-compiled-sql","Putting this here. To prevent DBT from materializing comments, wrap them in Jinja comments.

```
{# I can write stuff here all day, and it won't show up in compiled sql #}

---- however this stuff will be all over the compiled code
```",1
80,"removing-jinja-comments-from-compiled-sql","I'm a fan of putting block sql comments within block jinja comments, so syntax highlighters don't freak out:
```
{# /* super sneaky comments */ #}
```",2
80,"removing-jinja-comments-from-compiled-sql","I do this too @michael.dunn! I think it's a really nice little trick",3
81,"using-window-functions-in-incremental-tables","**TL;DR**: Should I just create views on top of incremental tables? My gut says probably.

We have a number of incremental tables that we refer to as 'daily timeseries' tables. They reflect the state of a record at the end of each day. 

dbt is great for these materialising these tables, except in a few use-cases where we need to use window functions that evaluate the whole table. Because we only load yesterday and today's data in each incremental load, the window functions would only evaluate the values for those two days instead of the whole table.

We have started building views on top of the incremental models to calculate the window functions, which is working okay, but I wanted to see if there was a preferable way of dealing with this that others have found? I don't really have an issue with the view method, except that it appears to make some queries against the final output a bit slower, even when the window function isn't being used. (For the most part, we get around this issue by dynamically changing which model is queried in Looker using the `in_query` liquid variable in LookML.)

Has anyone else found other ways of dealing with this? Thanks!",1
81,"using-window-functions-in-incremental-tables","Heh. You're diving into the deep end here. 

Incremental models in dbt are *wonderful* and many times they just work, and make incremental builds very straightforward. There are two specific cases when they don't work great, though: `distinct` and window functions. Both of these require more context than just recent data to appropriately calculate their values.

There are two different ways to deal with this, and each of them take a bit more work than what would typically be involved in building a dbt model:

1. **Incremental model >> view or table.**
Put the stuff that can be incrementalized into an incremental model and then layer another model on top of it for distincts or window functions. Sounds like this is what you're doing today. I'd recommend putting a table on top of it instead of a view so that you can limit the complexity of downstream queries...the extra storage is cheap and the strategy you outline just adds complexity.
2. **Write custom logic to combine results**
It's also possible to query the existing table and get the current value of, for example, a row number. Then, you can add that row number value to the incremental data to get the true value for incremental rows. This strategy allows you to incremental-ize the whole transformation, but can require meaningfully more code. When you're transforming datasets of many terabytes, this type of strategy is very important because strategy #1 just isn't really feasible. Use specific database features like sort key or date partitioning to get the current values without performing full table scans.

It's possible to go deep down a rabbit hole with #2, creating intermediate tables storing all kinds of bookmark values that essentially just allow your incremental jobs to work. It's all about a performance / code complexity tradeoff though. It sounds like your current approach is working out ok.",2
81,"using-window-functions-in-incremental-tables",":+1: Thanks @tristan. That makes sense. Do you have any publicly available examples of #2? We will definitely stick with #1 for the moment, and take your suggestion about materialisation, but want to understand what that next step looks like.",3
81,"using-window-functions-in-incremental-tables","I actually don't think we do. The only times we've had to do that are on very specific client applications, and that code doesn't lend itself well to being open sourced. Sorry about that!",4
81,"using-window-functions-in-incremental-tables","We did this by creating an `updated_key` column to use for the incremental `sql_where` value, which I think is what Tristan is saying in #2. In our case, we are just using `lead()` to grab information from the record immediately after the current row:

```sql
-- Check if there is another bid after this, if not, then this is the winning bid
lead(bids.bid_id, 1) OVER (partition by bids.item_id order by bids.bid_id) IS NULL AS is_winning_bid,
```

In this example, `is_winning_bid` is dependent on whether another bid exists after this one for the item. With an incremental model only built from `bid_id` or `updated_at`, this would fail to update if a new high bid is created after the table is built. So, we check for both bids in our `sql_where` key:

```sql
-- This field is used to let dbt know what records need to be updated since the model last ran. Usually,
-- we would just use an updated_at timestamp, but this model is also dependent on the bid placed after the current
-- bid, so we need to use the greatest of the bid_id from both bids.
greatest(bids.bid_id, lead(bids.bid_id, 1) OVER (partition by bids.item_id, bids.bidder_id order by bids.bid_id)) AS data_updated_key,
```
And then our incremental config:
```
{{
  config(
    materialized = 'incremental',
    sql_where = 'data_updated_key > (select max(data_updated_key) from {{ this }})',
    unique_key = 'bid_id'
  )
}}
```
So now our `data_updated_key` will be the newest of both bids in our window function. This is a simple example as we are only looking at one additional row using the `lead()` function. As Tristan mentioned, this could get considerably more complex if you have multiple different window function columns you need to consider for.",5
81,"using-window-functions-in-incremental-tables","I used tristan's #2 approach at one of our web analytics data pipeline, for our daily incremental update. We actually implemented the rabbit role with #2 that @tristan talked about. I found that additional bookmark table make the whole logic easier to understand, maintain and debug, though it requires a bit more code complexity.",6
81,"using-window-functions-in-incremental-tables","Interesting! I've just completed the [dbt Fundamentals](https://courses.getdbt.com/courses/fundamentals) course (thanks for the awesome course!) and trying to evaluate how it suits our needs.

---


There's an ever-growing list of raw immutable events:

- `timestamp` - date and time of an event

- `id` - agent ID

- `is_online` - true if an agent got online, false if went offline

Milestone no.1 is to calculate the number of unique agent IDs that were online (even for a moment) on a given day, i.e.:

- `day` - date

- `num_online_ids` - number of unique agent IDs that were online (even for a moment)

Milestone no.2 is to do the same on a rolling 28-day basis:

- `day` - date

- `num_online_ids_28d` - number of unique agent IDs that were online in the previous 28 days

> Incremental models in dbt are *wonderful* and many times they just work, and make incremental builds very straightforward. There are two specific cases when they donâ€™t work great, though: `distinct` and window functions. Both of these require more context than just recent data to appropriately calculate their values.

Sounds like it might be a tough nut to crack, because right away I can see `distinct` and window functions coming into play.

I was thinking maybe aggregate periods when agents were online into non-overlapping timestamp ranges:

- `id` - agent ID

- `start_ts` - date and time when an agent got online

- `end_ts` - date and time when an agent went offline

Then explode those into ID-day pairs leveraging something like [`dbt_utils.data_spine`](https://github.com/dbt-labs/dbt-utils#date_spine-source):

- `id` - agent ID

- `day` - date when an agent was online

But I realize the SQL will be getting very hairy pretty quickly.

Have there been any new developments/considerations/breakthroughs in this area in the past couple of years @tristan ?

Being completely new to dbt, I'm afraid of missing something obvious or already implemented/packaged.

Thanks in advance!!",7
82,"do-you-test-ephemeral-models","We test all (probably really most) of our views and tables that sit in our analytics schema, which is all the data that is deemed to be ready for use.

We currently don't have much testing on downstream models that aren't materialised, particularly 'base' ephemeral models that have no joins and effectively just cleans up the individual raw tables. We've started doing it, which has massively increased the number of tests of project has (>1000), and I wanted to know what people's views on this are.

It's starting to take much longer for the tests to run as well. We currently run tests after each production refresh of the tables, which may not be necessary. 

Questions:

* Do you test all your ephemeral/down-stream models?
* Do people have a specific setup with all of this? 
* Is there a best practice?",1
82,"do-you-test-ephemeral-models","I like testing everything, including ephemerals. It's a handy development tool. It does take quite some time to run all tests though.",2
82,"do-you-test-ephemeral-models","Agreed, we like testing ephemeral models. Thinking behind this:

1. We create ephemeral models for other downstream models to use. 
2. The downstream models are invariably making a bunch of assumptions about the data coming out of the ephemeral models.
3. It can be hard or even impossible to reliably test all of the assumptions made by the ephemeral models indirectly in the downstream models. Worst-case scenario is that an assumption is violated, but all of the downstream models and tests function properly and incorrect data gets used in analytics and then incorrect decisions are made based on these incorrect analytics.
4. Therefore we should test as many assumptions as we can about ephemeral models in an automated fashion.

Yes, it does cause more tests to run which can cause a slowdown. One solution could be tagging or only running subsets of tests at various times.

Also, from a dbt perspective it seems like not such a great idea to being attached to models being ephemeral. The whole point is to easily be able to flip between model types - ephemeral, view, and table as performance or other needs dictate. If we design something with the hardcoded assumption that it will always and only be ephemeral we're probably building in some technical debt or bugs that will pop up in the future.",3
86,"is-dbt-an-etl-tool","I am new to dbt and would like to know if dbt is an ETL tool? 
1) I am trying to create models where my source table is in postgres but I would like my target model table to be created in Redshift can I do that using dbt?
2) Another use case both source and target tables are in redshift same warehouse but different host name and different schema and database can I create models where source and target are in dfferent database in redshift?",1
86,"is-dbt-an-etl-tool","I don't believe dbt can handle either of those use cases. It's the ""T"" in ETL. For ""EL,"" AWS has DMS to replicate from Postgres to Redshift (doesn't work if you have jsonb columns), or Stitch Data can do the same.",2
86,"is-dbt-an-etl-tool","Thanks for your reply so its basically you can just build models on the same warehouse and same database.
what if I need to move my old model table to new database do I have to rebuild all again?",3
86,"is-dbt-an-etl-tool","I think your options are to rebuild or if it's an incremental model you could unload and copy. I don't believe redshift supports creating a table from a different database but I'm not 100% sure about that.",4
86,"is-dbt-an-etl-tool","Thankyou! I guess it doesn't will have to use unload and copy into s3.",5
86,"is-dbt-an-etl-tool","Hey Pallavi! dbt doesn't do any sort of data extraction or loading -- there are many other tools that excel at this task like Singer/Stitch, Fivetran, AWS DMS, etc. For loading data from Postgres into Redshift, I think a tool like Singer or AWS DMS might be your best bet.

So, rather than extraction and loading, dbt exists to transform data once it's _already loaded_ into your warehouse. Using dbt, you can select data from your source schemas, then transform that data in SQL, writing the resulting data out into a different ""analytics"" schema. You can find a little bit more about this workflow in the [dbt documentation](https://docs.getdbt.com/docs/overview#section-how-does-dbt-work-).

You mentioned:
> can I create models where source and target are in dfferent database in redshift?

The answer is unfortunately ""no"" -- Redshift doesn't allow you to transfer data between different ""logical"" databases in a single warehouse. I'd recommend using schemas inside of a single ""logical"" database to separate your source data and your dbt models.

Hope this helps!",6
86,"is-dbt-an-etl-tool","Thanks alot for detailed explanation.
This makes so much sense!!",7
86,"is-dbt-an-etl-tool","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",8
92,"running-dbt-in-kubernetes","Hey there! We've been working with `dbt` and using Sinter for quite a while now. We felt that we needed a bit more control and decided to give a try to running and scheduling `dbt` in one of our Kubernetes clusters. In this post Iâ€™d love to share how we did it.

## Running `dbt` on Kubernetes

The main objective is to replace Sinter scheduled runs with a custom environment based on Kubernetes Cronjobs. This gives some flexibility but it also takes some time to setup and maintain :wrench: .

### 1. Creating the Docker Image

Lets assume a `dbt` project with the [default folder structure](https://github.com/fishtown-analytics/dbt-starter-project) is in place. Running that project with `dbt` using Docker is as simple as writing a similar Dockerfile to this one, then building(`docker build -t your-dbt-image-name:tag .`) and running it.

```Dockerfile
FROM davidgasquez/dbt:0.10.0

COPY your-dbt-folder /dbt

CMD [""dbt"", ""run"", ""--profiles-dir"", ""profile""]
```

The image in which is based is [dbt-docker](https://github.com/davidgasquez/dbt-docker). You can see a complete example in the [kubedbt](https://github.com/davidgasquez/kubedbt) repository. 

The `CMD` step is using a flag to specify the source of the [profiles file](https://github.com/davidgasquez/kubedbt/blob/master/project-test/profile/profiles.yml). This is really helpful when running inside Docker as you can pass the secrets through the environment! That meand that youâ€™ll need to setup a few environment variables in a `.env` file. 

If everything is setup properly you should be able to run `docker run --env-file .env -it --rm your-dbt-image-name:tag` locally.

### 2. Setting up the Cronjob

Next step is telling Kubernetes to run that command periodically. To do that, simply create a new Cronjob resource YAML file (cronjob.yaml) pointing to the Docker Image you created and pushed to a registry. 
```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: your-dbt-project
spec:
  schedule: ""0 16 * * *""
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: your-dbt-project
            image: username/your-dbt-image-name:tag
            env:
              - name: REDSHIFT_DB_NAME
                valueFrom:
                  secretKeyRef:
                    name: redshift
                    key: database
              ...
              ...
            imagePullPolicy: Always
          restartPolicy: OnFailure
```

 If you don't have the secrets created in the cluster, you'll also need to create the proper resource.

Running `kubectl apply -f cronjob.yaml` will create a Cronjob in Kuberetes. That cronjob will take care of running `dbt` once a day ([exactly at 16:00](http://www.adminschoice.com/crontab-quick-reference)).

There are a couple of interesting options to keep in mind:
- `restartPolicy`: This will make Kubernetes restart the job (that runs all the models) in case of unhandled errors. You can read more about `dbt` [exit codes in the official docs](https://docs.getdbt.com/v0.8/reference#exit-codes).
- `imagePullPolicy`: With this setting, Kubernetes will always pull the image from the container registry and ignore the cached one. This makes deploying as easy as pushing the Docker image to the registry!

### 3. Adding CI/CD

This step is optional but recommended unless you want to push the image manually each time. There are numerous CI/CD vendors so I'll skip the implementation details. These are the things it should take care after each change to the repository `master` branch.:

1. Build a new Docker image with the same name and tag
2. Upload the image to the registry (overwriting the existing one)
3. *(Optional)* Test the models
4. *(Optional)* Update the Cronjob resource in Kubernetes. This is helpful when changing the schedule or base image

With the previous setup, at any point in time, youâ€™ll only have a Docker Image for each `dbt` project. Rolling back can be done just reverting the code changes that triggers the continuous integration pipeline.

### 4. Monitoring and Alerting

As with CI/CD, there are a vast number of vendors and each one might need a custom implementation. That said, there's one thing almost all of them will need. **Structured logging**.

Calling `dbt` from the command line prints out some custom messages. There are really handy for the human eye but not that useful for machines to parse. Luckily, `dbt` has an internal API we can use. Once we're able to write our own logs we'll hook to any monitoring or alerting vendor.

This is an example showing how a simple script (`run.py`) could look if you want to write custom logs using the internal `dbt` API:

```
import dbt
import dbt.main
import dbt.logger
import logging
import json

logger = logging.getLogger(""dbt"")
logger.setLevel(logging.FATAL)

logging.info(""DBT Run Started"")

results, success = dbt.main.handle_and_check([""run"", ""--profiles-dir"", ""profile""])

for res in results:
    name = res.node.get(""alias"")
    if res.errored:
        error = res.error
        logging.error(f""Model {name} errored! Error: {error}"")
    else:
        time = int(res.execution_time)
        logging.info(f""Model {name} completed in {time} seconds"")

logging.info(""DBT Run Completed!"")
```

After that, youâ€™ll need to change the Dockerfile and execute `CMD [""python"", ""run.py""]` instead of just `dbt run`.

-----

That's all! If you have any feedback or question please feel free to reach me. :smile:",1
113,"faq-cleaning-up-removed-models-from-your-production-schema","If you remove a model from your dbt project, dbt does not automatically drop the relation (table/view) from your schema. This means that users can end up with extra objects in their production schema, which can reduce the reliability of your prod data.

As a result, we've seen a few questions about how to clean up these objects in your production schema on Slack.

**Option 1: Periodically drop and rebuild the entire schema.**
dbt is built on the assumption that everything can be rebuilt at any given time, so this is a really easy way to remove objects.
However, it's not for the faint of heart, and may cause some downtime

**Option 2: Query the information schema to find extra objects in prod.**
I have the following query in my `analysis` directory. When run against my database, it finds objects (tables, views, functions) that exist in a prod schema but does not exist in the related dev schema. 
Note that this works for me because I routinely drop my dev database so don't have any extra objects in it. This query has been tested against Redshift and Postgres.

```sql
-- set pairs of schemas up as a cte for maximum flexibility
with dbt_schemas as (
  select
  'analytics' as prod_schema,
  'analytics_claire' as dev_schema
  union all
  select
  'foo' as prod_schema,
  'foo_claire' as dev_schema
)

, objects as (
  select
  lower(coalesce(nullif(table_type, 'BASE TABLE'), 'table')) as obj_type,
  table_schema as obj_schema,
  table_name as obj_name
  from information_schema.tables

  union all

  select
  'function' as obj_type,
  specific_schema as obj_schema,
  routine_name as obj_name
  from information_schema.routines
  where routine_type = 'FUNCTION'
)
, prod_objects as (
  select
  objects.*
  , dbt_schemas.dev_schema as expected_dev_schema
  from objects
  inner join dbt_schemas on dbt_schemas.prod_schema = objects.obj_schema
)

, prod_only_objects as (
  select prod_objects.*
  from prod_objects
  left join objects on objects.obj_schema = prod_objects.expected_dev_schema
    and objects.obj_name = prod_objects.obj_name
    and objects.obj_type = prod_objects.obj_type
  where objects.obj_name is null
)
select
  '-- drop ' ||  obj_type || ' if exists ""' || obj_schema || '"".""' || obj_name || '"" cascade;' as drop_statement
from prod_only_objects

```
The query returns a set of sql statements that can then be copied to a sql console and executed to drop the extra objects.

Ideally I'd return the first part of the query as a dbt [statement](https://docs.getdbt.com/reference#statement) and output the drop statements themselves to a sql file in `compiled/analysis/`. However, analyses are built when you execute `dbt compile`, and dbt statements are only executed on `run` (rather than `compile`). As such, it's not possible to return the results of the dbt statement to your Jinja context.

**Option 3: ???**
There's probably also a third way to do this which would involve iterating through your dbt project to find the expected objects, and comparing it to the state of your prod schemas. Let me know if anyone investigates this option!",1
113,"faq-cleaning-up-removed-models-from-your-production-schema","A similar approach to option 2 is to query the information schema for objects that haven't been updated recently. We run dbt 3x a day so these queries returns objects that should be dropped (we use snowflake)

```sql
SELECT *
FROM information_schema.tables
WHERE last_altered < DATEADD('hours', -8, CURRENT_TIMESTAMP);
```

```sql
SELECT *
FROM information_schema.views
WHERE last_altered < DATEADD('hours', -8, CURRENT_TIMESTAMP);
```",2
113,"faq-cleaning-up-removed-models-from-your-production-schema","**Option 3??!!**

Decided to tackle this as a bunch of refactoring recently resulted in a lot of models being renamed. Thankfully these days there is `run-operation`.

```sql
{% macro delete_outdated_tables(schema) %} 
  {% if (schema is not string and schema is not iterable) or schema is mapping or schema|length <= 0 %}
    {% do exceptions.raise_compiler_error('""schema"" must be a string or a list') %}
  {% endif %}

  {% call statement('get_outdated_tables', fetch_result=True) %}
    select current.schema_name,
           current.ref_name,
           current.ref_type
    from (
      select schemaname as schema_name, 
             tablename  as ref_name, 
             'table'    as ref_type
      from pg_catalog.pg_tables pt 
      where schemaname in (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )
      union all
      select schemaname as schema_name, 
             viewname   as ref_name, 
             'view'     as ref_type
      from pg_catalog.pg_views
        where schemaname in (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )) as current
    left join (values
      {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                    + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list %} 
        ('{{node.schema}}', '{{node.name}}'){% if not loop.last %},{% endif %}
      {%- endfor %}
    ) as desired (schema_name, ref_name) on desired.schema_name = current.schema_name
                                        and desired.ref_name    = current.ref_name
    where desired.ref_name is null
  {% endcall %}

  {%- for to_delete in load_result('get_outdated_tables')['data'] %} 
    {% call statement() -%}
      {% do log('dropping ' ~ to_delete[2] ~ ' ""' ~ to_delete[0] ~ '.' ~ to_delete[1], info=true) %}
      drop {{ to_delete[2] }} if exists ""{{ to_delete[0] }}"".""{{ to_delete[1] }}"" cascade;
    {%- endcall %}
  {%- endfor %}

{% endmacro %}
```

To use the macro run the following, substituting your schemas:

```bash
dbt run-operation delete_outdated_tables --args ""{schema: [foo, bar, baz]}""
```

**What does it do?**
Queries the information schema to find all tables and views (within the specified schemas), removes any from the list that exist in the current graph, and attempts to delete the rest.  Also handles seeds.

**Limitations**
Doesn't handle sources. Ephemeral models may prevent something from being deleted.  Currently only works for postgres (and maybe redshift).",3
113,"faq-cleaning-up-removed-models-from-your-production-schema","To find disabled models you can use `jq`:

```
cat target/manifest.json | jq -r '.nodes | to_entries | map(select((.value.config.enabled|not) and (.value.resource_type == ""model""))) | map(""DELETE FROM "" + .value.database + ""."" + .value.schema + ""."" + .value.alias + "";"") | join(""\n"")'
```",4
113,"faq-cleaning-up-removed-models-from-your-production-schema","Thanks @elexisvenator for this! I was able to modify your solution to make it work for Redshift. I also added a ""dry_run"" parameter to the macro as a precaution as well. I thought I'd share my solution here since I benefited from your work. Thanks!

```
{% macro delete_orphaned_tables(schema, dry_run=False) %}
  {% if (schema is not string and schema is not iterable) or schema is mapping or schema|length <= 0 %}
    {% do exceptions.raise_compiler_error('""schema"" must be a string or a list') %}
  {% endif %}

  {% call statement('get_orphaned_tables', fetch_result=True) %}
    SELECT current.schema_name,
           current.ref_name,
           current.ref_type
    FROM (
      SELECT schemaname AS schema_name,
             tablename  AS ref_name,
             'table'    AS ref_type
      FROM pg_catalog.pg_tables pt
      WHERE schemaname IN (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )
      UNION ALL
      SELECT schemaname AS schema_name,
             viewname   AS ref_name,
             'view'     AS ref_type
      FROM pg_catalog.pg_views
        WHERE schemaname IN (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )) AS current
    LEFT JOIN (
      {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                    + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list %}
        SELECT
        '{{node.schema}}' AS schema_name
         ,'{{node.name}}' AS ref_name
        {% if not loop.last %} UNION ALL {% endif %}
      {%- endfor %}
    ) AS desired on desired.schema_name = current.schema_name
                and desired.ref_name    = current.ref_name
    WHERE desired.ref_name is null
  {% endcall %}
  {% set result = load_result('get_orphaned_tables')['data'] %}
  {% if result %}
      {%- for to_delete in result %}
        {% call statement() -%}
            {% if dry_run %}
                {% do log('To be dropped: ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) %}
                SELECT
                    '{{ to_delete[2] }}'
                    , '{{ to_delete[0] }}'
                    , '{{ to_delete[1] }}';
            {% else %}
                {% do log('Dropping ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) %}
                DROP {{ to_delete[2] }} IF EXISTS ""{{ to_delete[0] }}"".""{{ to_delete[1] }}"" CASCADE;
                {% do log('Dropped ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) %}
            {% endif %}
        {%- endcall %}
      {%- endfor %}
  {% else %}
    {% do log('No orphan tables to clean.', True) %}
  {% endif %}
{% endmacro %}
```",5
113,"faq-cleaning-up-removed-models-from-your-production-schema","I also had to change `'{{ node.schema }}'` to `'{{ node.config.schema }}'` when running in our production environment. I'm not entirely sure why, but my guess was it had to do with dbt version. Either way, I hope someone finds this useful.",6
113,"faq-cleaning-up-removed-models-from-your-production-schema","I ended up actually not being able to execute the drop commands on a non-dry run because of some whitespace that was creating silent errors in Redshift because of my jinja templating. I also modified things to utilize `run_query()` instead of call statements to make it more readable and easily executable at dbt's suggestion. Here's the final product:

```
{% macro delete_orphaned_tables(schema, dry_run=True) %}
  {% if (schema is not string and schema is not iterable) or schema is mapping or schema|length <= 0 %}
    {% do exceptions.raise_compiler_error('""schema"" must be a string or a list') %}
  {% endif %}

  {% set query %}
    SELECT current.schema_name,
           current.ref_name,
           current.ref_type
    FROM (
      SELECT schemaname AS schema_name,
             tablename  AS ref_name,
             'table'    AS ref_type
      FROM pg_catalog.pg_tables pt
      WHERE schemaname IN (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )
      UNION ALL
      SELECT schemaname AS schema_name,
             viewname   AS ref_name,
             'view'     AS ref_type
      FROM pg_catalog.pg_views
        WHERE schemaname IN (
        {%- if schema is iterable and (var is not string and var is not mapping) -%}
          {%- for s in schema -%}
            '{{ s }}'{% if not loop.last %},{% endif %}
          {%- endfor -%}
        {%- elif schema is string -%}
          '{{ schema }}'
        {%- endif -%}
      )) AS current
    LEFT JOIN (
      {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                    + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list %}
        SELECT
        '{{node.config.schema}}' AS schema_name
         ,'{{node.name}}' AS ref_name
        {% if not loop.last %} UNION ALL {% endif %}
      {%- endfor %}
    ) AS desired on desired.schema_name = current.schema_name
                and desired.ref_name    = current.ref_name
    WHERE desired.ref_name is null
  {% endset %}
  {%- set result = run_query(query) -%}
  {% if result %}
      {%- for to_delete in result -%}
        {%- if dry_run -%}
            {%- do log('To be dropped: ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) -%}
        {%- else -%}
            {%- do log('Dropping ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) -%}
            {% set drop_command = 'DROP ' ~ to_delete[2] ~ ' IF EXISTS ' ~ to_delete[0] ~ '.' ~ to_delete[1] ~ ' CASCADE;' %}
            {% do run_query(drop_command) %}
            {%- do log('Dropped ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) -%}
        {%- endif -%}
      {%- endfor -%}
  {% else %}
    {% do log('No orphan tables to clean.', True) %}
  {% endif %}
{% endmacro %}
```",7
113,"faq-cleaning-up-removed-models-from-your-production-schema","I just had to do a similar process, with a focus on dropping source tables that we had added to our sync job in the past but weren't using. 

I approached this with a combo of the [Jinja graph variable](https://docs.getdbt.com/reference/dbt-jinja-functions/graph#accessing-sources), the information_schema and the venerable spreadsheet. 

To build a list of sources we _did_ use, I used the compiled results of this snippet: 
```
{% for node in graph.sources.values() -%}
  select '{{ node.schema }}' as s_name, '{{ node.name }}' as t_name
  {%- if not loop.last %} union all {% endif %}
{% endfor %}
```

And pasted their results into this query: 

```
with defined_sources as (
    select 'users' as s_name, 'users' as t_name union all
    select 'users' as s_name, 'contactroles' as t_name union all
    ...
    select 'ft_hubspot' as s_name, 'ticket' as t_name
),
     
relevant_tables as (
    select 
        table_schema as s_name, 
        table_name as t_name
    from information_schema.tables
    where table_schema not ilike 'dev_%' --our dbt dev schemas
    and table_schema not ilike 'analytics%' --our output schemas
    and table_schema not in ('pg_catalog', 'information_schema', 'admin', 'public', 'census', 'inbound__timesheets_and_quality_control') --Redshift internal tables and tables built by using get_relations_by_pattern instead of being defined
)

select * from relevant_tables
except
select * from defined_sources
```

This gave me all the tables in the warehouse that weren't defined in dbt, so we weren't using and could consider dropping to save disk usage and reduce load on the source databases.

To focus on the biggest tables first, I wanted to combine it with the metadata from `SVV_TABLE_INFO`: 
```
select
   schema as s_name,
   ""table"" as t_name,
   size as size_mb,
   tbl_rows as total_rows
from SVV_TABLE_INFO
```

However, I found that [information_schema queries always run against the leader node](https://stackoverflow.com/a/57327730/14007029) which meant I couldn't join the results of this query with my first query. Instead, I dumped the results to a spreadsheet and worked it out there.",8
113,"faq-cleaning-up-removed-models-from-your-production-schema","I also adapted @elexisvenator 's code to work with Snowflake and dbt 1.1.0 - figured I would add my adaptation to the discussion for any future lost souls! Some notable changes I made to that source:

* Re-set the schema variable to always be a list to prevent duplicate logic checking the schema type repeatedly
* Always use uppercase schemas to match Snowflake consistently
* Join the Snowflake information_schema data to values for both the current dbt node's schema and the provided target schemas, to handle cases where a user is in a target environment whose schema differs from the desired schema
* Insert an exception to exit the script if a BRONZE schema is about to be dropped, to prevent dropping raw source data in an automated CI/CD job (assumption being that dropping calculated silver/gold views and tables is less risky, since we can always recreate them from the raw bronze data instead)

```
{% macro delete_outdated_tables(schema) %} 
  {% if (schema is not string and schema is not iterable) or schema is mapping or schema|length <= 0 %}
    {% do exceptions.raise_compiler_error('""schema"" must be a string or a list') %}
  {% endif %}
  {% if schema is string %}
    {% set schema = [schema] %}
  {% endif %}

  {% call statement('get_outdated_tables', fetch_result=True) %}
    select c.schema_name,
           c.ref_name,
           c.ref_type
    from (
        select table_schema as schema_name, 
           table_name  as ref_name, 
             'table'    as ref_type
      from information_schema.tables 
      where table_schema in (
        {%- for s in schema -%}
        UPPER('{{ s }}'){% if not loop.last %},{% endif %}
        {%- endfor -%}
      )
    union all
    select table_schema as schema_name, 
           table_name  as ref_name, 
             'view'     as ref_type
      from information_schema.views
        where table_schema in (
        {%- for s in schema -%}
        UPPER('{{ s }}'){% if not loop.last %},{% endif %}
        {%- endfor -%}
      )) as c
    left join (values
      {%- for node in graph['nodes'].values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                    + graph['nodes'].values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list %} 
        {% for s in schema %}
            (UPPER('{{ s }}'), UPPER('{{node.name}}')),
        {% endfor %}
        (UPPER('{{node.schema}}'), UPPER('{{node.name}}')){% if not loop.last %},{% endif %}
      {%- endfor %}
    ) as desired (schema_name, ref_name) on desired.schema_name = c.schema_name
                                        and desired.ref_name    = c.ref_name
    where desired.ref_name is null
  {% endcall %}

  {%- for to_delete in load_result('get_outdated_tables')['data'] %} 
    {% set fqn = target.database + '.' + to_delete[0] + '.' + to_delete[1] %}
    {% if 'BRONZE' in fqn %}
      {% do exceptions.raise_compiler_error('Was asked to drop a bronze table, will not proceed. Table: ' + fqn) %}
    {% endif %}
    {% call statement() -%}
      {% do log('dropping ' ~ to_delete[2] ~ ': ' ~ fqn, info=true) %}
      drop {{ to_delete[2] }} if exists {{ fqn }} cascade;
    {%- endcall %}
  {%- endfor %}

{% endmacro %}
```",9
113,"faq-cleaning-up-removed-models-from-your-production-schema","hey, does this work also for simple incremental runs or do you need to fully refresh those tables in order to enable Snowflake to pick that up in the 'last_altered' field?",10
113,"faq-cleaning-up-removed-models-from-your-production-schema","[quote=""ff9991, post:10, topic:113""]
do you need to fully refresh those tables in order to enable Snowflake to pick that up in the â€˜last_alteredâ€™
[/quote]

[snowflake's docs](https://docs.snowflake.com/en/sql-reference/info-schema/tables.html) say it is 
>Date and time when the table was last altered by a DDL or DML operation

So any normal insert will also work",11
113,"faq-cleaning-up-removed-models-from-your-production-schema","A post was split to a new topic: [How do I temporarily make an archive copy of a schema?](/t/how-do-i-temporarily-make-an-archive-copy-of-a-schema/5051)",12
113,"faq-cleaning-up-removed-models-from-your-production-schema","For what it's worth a colleague and I took the above and developed it a little - to iterate over the available schemas automatically (so no need to pass in as an arg), and to provide a bit clearer output on what is happening.  This is for Snowflake.

```
--instructions
--
--If you want to simply see which tables/views exist in Snowflake, that are not in the dbt graph:
--dbt run-operation delete_orphaned_tables --args ""{dry_run: True}""
--
--if you want to actually drop those tables/views:
--dbt run-operation delete_orphaned_tables --args ""{dry_run: False}""
--
--Note by default this macro will look in the database specified in your default target, as defined in your profiles.yml.
--you can be explicit about the database, by specifying a different target (add --target argument), 
--but you will need to set up targets per database, i.e. not just dev/prod, but for example:
--  dev
--  dev-raw
--  dev-analytics
--  prod
--
--so then you might run for example:
--dbt run-operation delete_orphaned_tables --args ""{dry_run: True}"" --target dev-analytics

{% macro delete_orphaned_tables(dry_run=False) %}
  
  {% do log("""", True) %} 
  {% do log(""Searching for orphaned tables/views..."", True) %}
  {% do log(""Using target profile: "" ~ target.name ~ "" (database: "" ~ target.database ~ "")."", True) %} 

  {% set schema_query %}
      SELECT distinct table_schema
      from (
        SELECT distinct table_schema
        FROM information_schema.tables
        UNION ALL
        SELECT distinct table_schema
        FROM information_schema.views
      ) u
      where table_schema <> 'INFORMATION_SCHEMA'
  {% endset %}

  {#
    {% do log(schema_query, True) %} 
  #}
  {%- set result = run_query(schema_query) -%}
  {% if result %}
    {%- for row in result -%}
      {% set schema = row[0] %}
      
      {% do log("""", True) %} 
      {% do log(""schema: "" ~ schema, True) %} 
    
      {% set query %}
        SELECT UPPER(c.schema_name) AS schema_name,
                UPPER(c.ref_name) AS ref_name,
                UPPER(c.ref_type) AS ref_type
        FROM (
          SELECT table_schema AS schema_name,
                  table_name  AS ref_name,
                  'table'    AS ref_type
          FROM information_schema.tables pt
          WHERE table_schema = '{{ schema }}'      
          AND TABLE_TYPE = 'BASE TABLE'
          UNION ALL
          SELECT table_schema AS schema_name,
                  table_name   AS ref_name,
                  'view'     AS ref_type
          FROM information_schema.views
            WHERE table_schema = '{{ schema }}'
                ) AS c
          LEFT JOIN (
            {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                          + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list %}
              SELECT
              upper('{{node.config.schema}}') AS schema_name
                ,upper('{{node.name}}') AS ref_name
              {% if not loop.last %} UNION ALL {% endif %}
            {%- endfor %}
          ) AS desired on desired.schema_name = c.schema_name
                      and desired.ref_name    = c.ref_name
          WHERE desired.ref_name is null
      {% endset %}

      {#
      {% do log(query, True) %} 
      #}
      
      {%- set result = run_query(query) -%}
      {% if result %}
          {%- for to_delete in result -%}
            {%- if dry_run -%}
                {%- do log('To be dropped: ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) -%}
            {%- else -%}
                {% set drop_command = 'DROP ' ~ to_delete[2] ~ ' IF EXISTS ' ~ to_delete[0] ~ '.' ~ to_delete[1] ~ ' CASCADE;' %}
                {% do run_query(drop_command) %}
                {%- do log('Dropped ' ~ to_delete[2] ~ ' ' ~ to_delete[0] ~ '.' ~ to_delete[1], True) -%}
            {%- endif -%}
          {%- endfor -%}
      {% else %}
        {% do log('No orphan tables to clean.', True) %}
      {% endif %}

    {%- endfor -%}
  {% endif %}

{% endmacro %}
```",13
121,"publishing-dbt-docs-to-netlify","**What is DBT Docs?**
DBT Docs is the fantastic new feature which enables the autogeneration of documentation of your models and enables you to create descriptions of fields and tables among its many neat new features.
It is available from v0.11 and can be generated with the command `dbt docs generate`.

To serve these docs locally, you can run the command: `dbt docs serve`.

Now this is great when you want to view this documentation on your own machine, but that isn't very scalable when you're in a distributed company of any size. An alternative is to host it on an ec2 instance with nginx or apache but this would be overkill for a static webpage.

Enter Netlify, a largely free static site web hosting solution: 
https://www.netlify.com/

In order to deploy your documentation site, you'll need to
- create a netlify account
- run dbt docs generate from within your dbt folder
- drag the ""target"" folder from dbt into netlify where it says to ""drag and drop your site folder here""
- this will create a manual site and viola you are done!

If you would like to password protect this site (recommended if it contains sensitive information about your companies data warehouse schemas), you can get that on a teams based plan which is $9 per user per month - here's some docs on the feature:
https://www.netlify.com/docs/visitor-access-control/

**Advanced topic**
Integrating this into your CI/CD pipeline, you can auto-deploy after `dbt run` and `dbt docs generate` to Netlify using the following CLI tool:
https://github.com/netlify/netlifyctl",1
121,"publishing-dbt-docs-to-netlify","Thanks @peter_hanssens - this is great! Just updated this section of the docs to link here :slight_smile:
https://docs.getdbt.com/v0.11/docs/documentation-website#section-hosting-the-documentation-site",2
121,"publishing-dbt-docs-to-netlify","Thanks for sharing this option, looks like Netlify have OAuth in addition to password. SSO would be nicer that password but requires some extra js, which means extending the dbt docs in some way. I can't think of a way other than forking the dbt docs repo and adding to the inline js in the index.html file.",3
122,"understanding-hash-joins-in-redshift-query-plan","Curiosity question: Can someone explain why Redshift goes through the step of hashing the join columns in some joins? I'm not sure I computationally understand what happens there and would be interested in knowing more. Couldn't find anything good online.",1
122,"understanding-hash-joins-in-redshift-query-plan","Hey @dylanbaker  - this is a great question!

Hash joins can be used when thereâ€™s a join on an _equality_, eg:

```
select ...
from table_a
join table_b on table_a.id = table_b.id
```

it does _not_ work for _inequality_ joins like:

```
select ...
from table_a
join table_b on table_a.id > table_b.id
```

To understand why this is the case[1], you need to understand Hash Maps.

### Hash Maps
A [Hash Map](https://en.wikipedia.org/wiki/Hash_table) is a [data structure](https://en.wikipedia.org/wiki/Data_structure) that supports constant-time lookups, meaning: computers can figure out if a _key_ is present in a Hash Map in a single operation! This is not true of all data structures, as we'll see below.


### Detour: Arrays
Another type of data structure is an Array. You can generally think of Arrays as ""lists"", but my computer science professors would be mad if I told you the two terms were interchangeable.

Arrays are indeed used to maintain an ordered collection of things. Computers can return the _element_ at a specific _index_ in an Array in a single step. Imagine you have an Array like this:

```
[""Apple"", ""Banana"", ""Pear"", ""Orange"", ""Grapefruit""]
```

Each _element_ in the Array has an _index_:

| index | element |
|-|-|
|0|Apple|
|1|Banana|
|2|Pear|
|3|Orange|
|4|Grapefruit|

Given this Array, a computer can tell you that the 3rd element in the Array (index=2) is `Pear` in a single operation. It's harder for computers to answer other types of questions with Arrays. A question like
> is there an element named `Falafel`  in there?

will require the computer to look at every single element in the Array. You may be able to look up the element at an index in a single operation, but finding an element by its value requires one operation for each element! That means that an Array with `N` items requires `N` operations to find an element, so we call searching Arrays for elements a  ""Linear"" operation. If the Array doubles in size, then it will require twice as many operations to find an element.

### Detour: Sorted Arrays

Bear with me here - I know this question is about Hash Maps! 

We just saw that finding an element in an Array is a ""linear"" operation, but this isn't always the case. Instead, imagine that we sorted our Array of fruit. Sorted alphabetically, it would look like:

| index | element |
|-|-|
|0|Apple|
|1|Banana|
|2|Grapefruit|
|3|Orange|
|4|Pear|

If you know that the values are sorted, then you can start in the middle of the Array and repeatedly bisect it until you find (or donâ€™t find) the value youâ€™re looking for. A search for ""Orange"" would look like:

1. Start in the middle
  --> index=2, element=Grapefruit

2. ""Orange"" comes after ""Grapefruit"" alphabetically, so we know that if Orange is in the Array, it's in the second half of the list.

3. Pick a new index between the current index (2) and the last index in the Array (4)
  --> index=3, element=Orange. Found it! 

In this example, we only needed to check two elements instead of four! This algorithm is called ""Binary Search"", and it's a great way to find elements in a sorted Array. Since you can cut the search space in half with each guess, the number of guesses you need grows â€œlogarithmicallyâ€ with the size of the Array. Therefore, doubling the size of the Array only requires one extra guess! Weâ€™d call this a `log n` search function, since the number of guesses you need to make is about `log_2(N)`, where N is the size of the Array. This is super handy if you have thousands or millions of items in an Array, and you need to find one in particular.

The big idea here is that by spending a little extra time sorting the list, we can vastly improve the performance of our search functions. So, onto Hash Maps....

### Hash Maps (for real this time)

Let's imagine that we again have a series of Fruit:

```
[""Apple"", ""Banana"", ""Pear"", ""Orange"", ""Grapefruit""]
```

Recall: Arrays can tell us the _element_ found at an _index_ in a single operation. Hash Maps are a way of exploiting this property of Arrays to find an element by it's _value_ in a single operation. To do this, we need to convert the _value_ of an element into its _index_. We can do that using a [hash function](https://en.wikipedia.org/wiki/Hash_function).

You might be familiar with hash functions like MD5, but thatâ€™s only one type of hash function. A different type of hash function might accept a string (like ""Apple"") and return an integer between 0 and 100. This function might work by assigning a number to each letter of the alphabet, then doing some clever math to produce a value for the input string.

A hash function will always produce the same value for a given input, so if Apple is hashed to 75 once, it will _always_ be hashed to 75 using the same algorithm.

With our hash function, we can produce numbers between 0 and 100 for different types of fruits. In the above example, that might look like:

| element | `hash(element)` |
|-|-|
|Apple|28|
|Banana|52|
|Grapefruit|81|
|Orange|7|
|Pear|12|

Next, we can create an Array with 100 elements, initialized to be totally empty. Further, we can place each element at the index indicated by the hash of it's value. That would look like:

| index | element |
|-|-|
|0|NULL|
|1|NULL|
|...|...|
|7|Orange|
|...|...|
|12|Pear|
|...|...|
|28|Apple|
|...|...|
|52|Banana|
|...|...|
|81|Grapefruit|
|...|...|
|98|NULL|
|99|NULL|

In the above example, the _index_ of each _element_ in the Array is determined by hashing it's _value_. Just as we saw in the Sorted Array example, building this Hash Map takes time, but it pays dividends! With this data structure, we can determine if elements are present in the Hash Map in a single operation[2]! To determine if an element is present in a Hash Map:
1. Calculate the hash of the element you want to find to produce an index
2. Get the element at that index

If the element is present, then :boom:, you found it! If there's nothing there, then you know the element isn't present in the Hash Map. A couple of quick examples:

1. Is Banana present?
  - hash(Banana) = 52
  - lookup(52) = Banana, present!

2. Is Falafel present?
  - hash(Falafel) = 40
  - lookup(40) = NULL, not present!

### But... databases?

Pulling this back to the database world: Building a Hash Map takes some time, but then you can do a _constant time lookup_ for every comparison in the join! I think databases typically hash the smaller of the two tables in a join, then they iterate through the bigger table, hashing each value and consulting the Hash Map to determine if the row should come through the join or not. This type of join becomes effectively ""linear"" in complexity with the size of the bigger table, which is pretty good!

If the database couldn't do a Hash Join, it would instead need to do a ""Nested Loop Join"". This would require the database to check every value in the left table against every value in the right table. The complexity of a Nested Loop Join would be ""quadratic"", in that you need to do about N*N (or NÂ²) different operations to process the join. Not great! Nested Loop Joins don't hold up when you're joining million-row tables together -- your database might end up needing to complete trillions of operations to execute that join! Compare that to a logarithmic algorithm, where log_2(1000000) is close to 20 :slight_smile: 

So, this touches on some other topics that I won't dig into here, but that are definitely deserving of future posts:
1. We saw that sorted datasets support faster lookups, which should give you a sort of intuition for Sort Keys on Redshift, for instance.
2. Databases like Redshift use ""statistics"" to determine which of the tables in a join is ""smaller"". That's part of why it's so important to run `analyze` periodically!
3. In some cases, spending time pre-processing a dataset can pay dividends. While building a Hash Map is time consuming, a Hash Join will be faster than a Nested Loop Join for any moderately sized dataset.
4. There's no such thing as a free lunch! A Hash Map requires memory space, which you're trading in exchange for performance. It's hard to have it both ways, and generally optimizing for one will require sacrificing the other. Tradeoffs!


This ended up being more of a computer science flavored answer then a database-specific one, but I think it's super important to build intuition like this. There are heaps (:wink:) of other data structures and algorithms out there that Databases make use of, and a basic understanding of the science can really help you reason about things like database performance and optimizer decisions.

---

[1] I didn't really spend time explaining this, but inequality joins don't work with Hash Maps because you're not checking for item presence in a list, you're instead computing an expression. While we can hash a date like `2018-01-01`, we definitely can't hash an expression like `date >= 2018-01-01`. These queries will probably be executed using Nest Loop Joins.

[2] I said ""single operation"", but ""hash"" and ""find by index"" are two operations! When we talk about algorithm complexity, we frame it in regards to the size of the dataset. More precisely than ""a single operation"", you would say that the search occurs ""in constant time"". The amount of time/steps required to find an element in a Hash Map does not change with the size of the dataset! 

---

**Some further reading:**
https://en.wikipedia.org/wiki/Binary_search_algorithm
https://en.wikipedia.org/wiki/Hash_table
https://en.wikipedia.org/wiki/Hash_join
https://en.wikipedia.org/wiki/Nested_loop_join
https://en.wikipedia.org/wiki/Big_O_notation
https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming",2
122,"understanding-hash-joins-in-redshift-query-plan","<img src='//cdck-file-uploads-global.s3.dualstack.us-west-2.amazonaws.com/business7/uploads/getdbt/original/1X/f6962abbecac664ced6c2b133695bbd5c1f0d189.gif'>",3
122,"understanding-hash-joins-in-redshift-query-plan","This is awesome. Makes complete sense. Thanks!

(Has prompted another question relating to the efficient way of joining on inequalities. I'm going to ask that in another post.)

https://discourse.getdbt.com/t/how-to-create-efficient-timeseries-models-or-how-to-join-efficiently-on-inequalities/123",4
123,"how-to-create-efficient-timeseries-models-or-how-to-join-efficiently-on-inequalities","Somewhat inspired by @drew's great answer to [this question](https://discourse.getdbt.com/t/understanding-hash-joins-in-redshift-query-plan/122/5) about hash tables:

**What we currently do**

We need to report on the state of various records as of the end of every day, based on [Type 4](https://en.wikipedia.org/wiki/Slowly_changing_dimension#Type_4:_add_history_table) history tables created by our Django application. For example, this could be the facility limits of each of our borrowers on a daily basis.

Currently, our models take the following steps:

1. Build a daily timeseries model. We build this as a utility table in our production schema as we use it in multiple places.

1. Join on to each day all records from the history table that occured prior to or on that date.

1. `row_number()` for each record/day combo to understand which is the most recent record to that day.

1. Filter on `row_number_field = 1`.

This typically looks like this:
```
with timeseries as (

    select * from {{ ref('timeseries') }}

), history as (

    select * from {{ ref('facility_history') }}

), joined as (

    select 
        t.date_day,
        h.*,
        row_number() over (partition by t.date_day, h.id order by h.action_date desc) 
              as row_number_field
    from timeseries t
    inner join history h
        on h.action_date <= t.date_day

)

select *
from joined
where row_number_field = 1
```
_where_ action_date _is the date of the change to the record_.

This works fine and we build all of these tables incrementally. However, the initial builds take forever and even some of the incremental loads are less efficient than I'd like because of the â€œNested Loop Joinâ€.

**My question**

Is there a more efficient way to do this without joining on the inequality? We have instances where the join criteria are similar but more complicated where this becomes more of an issue.

One solution that came to mind was leaving as is for the initial load (or exploring @claire's new split up incremental materialisation) but for each incremental load, which replaces the current day's value until the day is over, we simply insert the last value from the history table. Has anyone implemented anything like that?",1
123,"how-to-create-efficient-timeseries-models-or-how-to-join-efficiently-on-inequalities","@dylanbaker I don't know all of the context around this, but I think I would just write the query a little differently... 

```
with timeseries as (

    select * from {{ ref('timeseries') }}

), history as (

    select * from {{ ref('facility_history') }}

), history_agg as (

    select 
        h.*,
        h.action_date::date as date_day
        row_number() over (partition by h.id order by h.action_date desc) 
          as row_number_field
    from history

),

history_daily as (

    select * from history_agg where row_number_field = 1

),

joined as (

    select 
      t.date_day,
      h.* --modify to select everything but date_day here
    from timeseries t
    left join history_daily h 
      on t.date_day = h.date_day 
      

)

select * from joined
``` 

The core intuition here is that you want to aggregate prior to joining--you previously were joining and aggregating within the same step, which caused the database's intermediate result to be much bigger than you actually needed. Find the most current row for `history`, then left join it back to `timeseries` on a shared key. 

If what you actually need is to join a single daily record from `history_daily` to **multiple** records in `timeseries` because you don't always have a record for every day, then use a lag window function to create a `from_date` and `to_date` so that you can do **two inequalities instead of one**. There is no way to get away from the nested loop join in this condition but you want to limit the total # of rows returned. Currently the fact that you're only using a single inequality can cause a massive fanout. I think. I frequently need to write date inequality joins just like you're doing, and never do so without including both a greater than and a less than.",2
123,"how-to-create-efficient-timeseries-models-or-how-to-join-efficiently-on-inequalities","[quote=""tristan, post:2, topic:123""]
```
select 
    h.*, 
    h.action_date::date as date_day row_number() over (partition by h.id order by h.action_date desc) 
        as row_number_field 
from history
[/quote]

In `history_agg` above, are we trying to pull the last record of each day for each `id`? I think we also therefore need to partition by `h.action_date::date`? I may have misunderstood the change.

Otherwise, that all completely makes sense. We don't have records for every day in the history table so, as you suggested, I think a lead window function will work well. :+1:",3
123,"how-to-create-efficient-timeseries-models-or-how-to-join-efficiently-on-inequalities","@dylanbaker

I had a similar situation where I was trying to get the most recent value for a particular primary key. I normally use the approach you and @tristan describe but in this particular case, Redshift performed better by doing a self-join with the most recent record. It looks something like this:

```
with history as (

    select * from {{ ref('facility_history') }}

), history_id_x_max_date as (

    select
        id,
        max(action_date) as action_date
    from history
    group by 1

),

new_history as (

    select *,
           row_number() over (partition by id order by action_date desc)
              as row_number_field
    from history
    join history_id_x_max_date using (id, action_date)

),

history_daily as (

    select * from new_history where row_number_field = 1

)

....
```
I am not sure why but after adding this self-join the query performed better than using the row_number() function directly against the unfiltered table.

Of course, if your data only has one record per id/date, you would not need the row_number() in this case.

I'm curious to learn if my situation was unique or if this helps in other cases.",4
132,"how-do-you-run-models-in-a-path","https://docs.getdbt.com/reference#run states

> The  `--models`  flag accepts one or more arguments. Each argument can be one of:
> 3. a path hierarchy to a models directory

> Examples:
> dbt run --models path.to.my.models     # runs all models in a specific directory
> dbt run --models path.to.models.*   # select all models in path/to/models

For example, my project has a structure like:
- models/base/
- models/blue/
- models/green/
- models/red/

And I only want to build the models in the models/red/ folder. How would I do that?

Am I misunderstanding the documentation?",1
132,"how-do-you-run-models-in-a-path","Hey @jon-rtr - I totally agree that this syntax is confusing! In this example, you could run the models in `models/red/` with:

```
dbt run --models red.*
```

The `.*` is actually superfluous in most cases, but I like to add it to make things explicit.

The `--models` flag works this way to support selecting:
1) models by name
2) directories
3) packages

Since it supports selecting three different types of things, the syntax is necessarily a little weird. Hope this helps!",2
132,"how-do-you-run-models-in-a-path","2 posts were split to a new topic: [Select models from multiple directories](/t/select-models-from-multiple-directories/4959)",8
132,"how-do-you-run-models-in-a-path","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
133,"faq-handling-timezone-conversion-and-timestamp-truncation-in-data-warehouses","A common question we see in the dbt community is around how people are handling timestamps in their data warehouse â€“ more specifically:
* whether people store timestamps in UTC or in a timezone, or both, and
* whether people perform timestamp truncations in their warehouse

The orthodox, Kimball-style, advice on the subject is to store timestamps as both UTC and local, and store date and time as separate fields. However, when working in a modern data stack (as described [here](https://blog.fishtownanalytics.com/what-are-the-steps-tools-in-setting-up-a-modern-saas-based-bi-infrastructure-281e0860f9a9), and [here](https://medium.com/@foundinblank/whats-a-data-stack-7c96f7a15fe8)), that advice feels a bit outdated for a few reasons:
* Data warehouses have advanced so that you do not need to be concerned about the computational burden of performing timestamp truncations and/or conversions at query time
* Often, users interacting with your models will do so through a BI tool rather than raw SQL, and any BI tool worth its salt handles timestamp truncation and timezone conversions easily.
* This approach does not necessarily prioritize the computational burden (i.e. brain time :wink:) of analysts using the warehouse. Adding extra fields, especially for time parts, can bloat models, degrading the information architecture. Building those fields in the first place can lead to extra code, when it's likely your BI tool can do it for you.

So, if you're using a modern data stack, and not strictly married to Kimball style modeling, how should you handle your timestamps? Well, like a lot of  things, it depends on your situation. Below is some consolidated advice based on what we've seen in the amazing [dbt Slack](http://ac-slackin.herokuapp.com/)Â¹.

### If you are consistently reporting from one timezone:
- Store timestamps in UTC
- Perform timezone conversion in your BI tool
- Perform timestamp truncation in your BI tool 

This represents the simplest approach, and for many organizations, is the best option. Use this approach until it doesn't work.


### If you are rolling up your data by a date part (e.g. `daily signups`) OR using a timestamp in a join to another model:
- If you are doing timestamp conversions and/or truncations in your select statements when aggregating data in your modelsÂ², Ã  la: 
```
select
date_trunc('day', convert_timezone('EST', created_at)) as date_est
, count(*) as number_of_signups
from users
```
- Or if you are doing timestamp conversions and/or truncations in your join conditions when joining to a table at a different grain:
```
select
orders.id
, calendar.business_day
from orders
left join calendar on calendar.date_est = date_trunc('day', convert_timezone('EST', orders.created_at))
```
- You might consider doing this calculation in an upstream model, so that your models look like:
```
select
created_date_est
, count(*) as number_of_signups
from users
```
```
select
orders.id
, calendar.business_day
from orders
left join calendar on calendar.date_est = orders.created_date_est
```
- If implementing this approach, it's likely a good idea to still provide UTC timestamps alongside the target timezone timestamps in your warehouse.

### If you have different timezones for different records within the same model OR the local context is important:
- It may be useful to store timestamps in both UTC and the local timezone. Each timestamp should be suffixed `_utc` and `_local` respectively.
- Additionally, it may be useful to store the local timezone (e.g. `'EST'`) for each record in a `local_timezone` field.
- You may be able to use your BI tool to use the `local_timezone` field and the UTC timestamp to convert timestamps to the correct timezone â€“ I recently [wrote](https://discourse.looker.com/t/using-looker-parameters-for-dynamic-timezones/9060) about how I approached this using Looker. For me, this negated the need to store the local timestamp.

### Generally:
- Timezones are _hard_. There's no single best approach, so choose one that makes sense for your organization.
- Consistency is key â€“ whatever approach you do choose, use it consistently throughout all your models.
- If you are using timezones other than UTC (or even if you are using UTC but want to be explicit), _suffix_ (as opposed to prefix) your timestamps with the timezones. For example, the fields `created_at_utc` and `created_at_est` are preferable to `utc_created_at` and `est_created_at`. Most BI tools order things alphabetically so using suffixes keeps the two fields together in a list, improving your information architecture.

----

Â¹A massive thanks to @TimothyBurke, @ted.conbeer, Leon T, Harlan H and @mplovepop (and others I've lost to the Slack ether!) for contributing to the discussion.

Â²Generally consider whether you should be doing this pre-aggregation in your warehouse as opposed to in your BI tool. Pre-aggregating often limits the ability to explore the data, but is required in cases, for example rolling up daily revenue to compare actuals to sales targets, where sales targets are defined at a daily level.",1
141,"publishing-dbt-docs-from-a-docker-container","First, allow me to acknowledge that this is overkill for most.  I want my DBT docs to be accessible to everyone in my business, but not to everyone else.  I could do this with [Sinter](https://sinterdata.com) of course, but 300 employees @ $20 per seat per month is a bit too rich for my blood.

@peter_hanssens wrote a great piece [here](https://discourse.getdbt.com/t/publishing-dbt-docs-to-netlify/121) about how to use Netlify to serve your docs.  But to password protect that content runs you $9 per user per month.  Less spendy than the native Sinter option, but still more than I want to spend.

My team already uses docker for a ton of stuff, and we already have infrastructure in AWS that runs docker containers, so I decided to go that route.

### Dockerfile
```bash
FROM python:3.6

ARG user=someUser
ARG organization=yourGitHubOrg
ARG repo=yourDBTRepo

ARG homedir=/home/${user}
COPY entrypoint.sh ${homedir}

# Non-root group & user creation
RUN groupadd -r ${user}
RUN useradd -r -m -g ${user} ${user}
RUN mkdir ${homedir}/.ssh

# Git
ENV REMOTE_REPO git@github.com:${organization}/${repo}.git
ENV REPO_DIR ${homedir}/${repo}
RUN apt-get update && apt-get install -y git
COPY id_rsa ${homedir}/.ssh/
RUN ssh-keyscan github.com >> ${homedir}/.ssh/known_hosts

# BigQuery
ENV GOOGLE_APPLICATION_CREDENTIALS ${homedir}/service_account.json
COPY service_account.json ${homedir}

# Permissions!
RUN chmod 0700 ${homedir}/.ssh
RUN chmod 0600 ${homedir}/.ssh/id_rsa
RUN chmod 0644 ${homedir}/.ssh/known_hosts
RUN chmod 0755 ${homedir}/entrypoint.sh
RUN chown -R ${user}:${user} ${homedir}/.ssh

# DBT!
RUN pip install dbt==0.11.1

# Prep for container execution
USER ${user}
WORKDIR ${homedir}
ENTRYPOINT [""/bin/bash"", ""entrypoint.sh""]
```
_Note: You may need to tweak this to enable a connection to your data warehouse as specified in your `profiles.yml` file._

### entrypoint.sh
```bash
#!/usr/bin/env bash
git clone $REMOTE_REPO
cd $REPO_DIR
dbt deps --profiles-dir .
dbt docs generate --target prod --profiles-dir .
dbt docs serve --profiles-dir . > /dev/null 2>&1 &
while [ True ]
do
    sleep 600
    if [ `git rev-parse --short HEAD` != `git rev-parse --short origin/master` ]; then
        git fetch --all
        git reset --hard origin/master 
        dbt deps --profiles-dir .
        dbt docs generate --target prod --profiles-dir .
    fi
done
```
_Note: I have my `profiles.yml` file in the root of my repo._

This will clone your dbt repo, install dependencies, generate docs, start the webserver in the background, then enter a loop where every 10 minutes it will check if there's been a change to the code on github, and if so pull down the new code and regenerate the docs files.

Build the image from a location where your authentication files are present, so they can be copied into your image.  I have a little EC2 instance for just this purpose, so I don't have service account credentials proliferating onto a bunch of engineer laptops.  From the desired folder, I run `docker build --pull -t dbtdocs .`

I can deploy that image to ECR if I want, but I run it from the image build machine (it's idle most of the time anyway; might as well give it something else to do!) To start serving the site, I run `docker run -d -p 8080:8080 --restart unless-stopped dbtdocs`

Assuming your network settings allow it, now you can hit port 8080 on that machine and your docs site should be visible.

I went a step further, however, to SSL-ify the connection.  I use a EC2 HTTPS load balancer (because it makes it easy to deal with certificates and whatnot) listening on port 443 (as expected) and it routes all traffic to the docker host's 8080, where the docker host will then route it to the docker container's 8080, where `dbt docs generate` is serving the content.  My security group settings in AWS disallow anything except 443 from outside (and only allowing 443 from office IP addresses), but allow 8080 on the internal network.",1
141,"publishing-dbt-docs-from-a-docker-container","Some known weaknesses:
* I have to maintain the hardware for this.
* The first time through doing the load balancer and security group config was a bit hairy - I've done it a few times now, so it's not a big deal now, but that first time wasn't trivial.  A better engineer than me would make a CloudFormation template for this.
* Expects that I have a subdomain that I can point to the load balancer (since the subdomain is what the SSL cert is applied to). I _do_ but that's not a given for some organizations.
* It doesn't do user auth, relying on the doubleplusungood assumption that anyone on my office wifi is authorized to see the docs.  I hope to add an oAuth portal at some point.  Until then, I just have to be mindful of what I put in the docs.",2
141,"publishing-dbt-docs-from-a-docker-container","Thanks for sharing this. Very helpful to see how you serve and then loop over generating the files, nice.

The `service_account.json` is used for accessing Bigquery,  I'm wondering if there is an alternative to baking the secret into the container. I think one of these GCP options would let you apply service account permission to the container without requiring the secret to be in the container as a file, https://cloud.google.com/container-options/ also, the DW is in GCP so why not host the docs there. 

I'm also thinking about authentication, you're already using a load balancer; it is possible to set up the load balancer to do auth with Cognito e.g. https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html#cognito-requirements
Cognito supports a bunch of different SSO integrations, would be worth looking at.",3
141,"publishing-dbt-docs-from-a-docker-container","Just wanted to chip in here and describe my train of thoughts and solution.

In the docs section, you can see [4 possibilities](https://docs.getdbt.com/docs/documentation-website#section-deploying-the-documentation-site), presumably ranked by the authors' preference.
My personal ranking is :

1. **dbt cloud** or **netlify**: fully managed CI/CD option but you have to pay at one point or another
1. ""spin up a **webserver** like nginx"": in other words deploy the static website the way you would deploy any other static website (think sites generated by frameworks like jekyll or hugo)
1. **S3 bucket**

In a case where pricing is an issue and engineering resources/knowledge are available, *I even would swap 2 for 1*.

In the situation you want to achieve 2, you need 3 things:

* a docker image that can build and serve the static website
* a CI that can build the docker image on any new commit and push it to a repository
* an ""environment"" in which to deploy your docker image: whether it is a server on which you pull the image, or kubernetes if you have a cluster setup

Let's assume you have the last 2 steps ready from your other production services, and let's focus on the first step: **build and serve** the static website.

The best way I've found to do this is to leverage [docker multistage builds](https://docs.docker.com/develop/develop-images/multistage-build/). In other words, you first build and then serve. The image to build your website is a python based image with dbt installed, that will run `dbt docs generate`. The image to serve your website will be a web-server based image (in my case I use nginx) that copies over the static files generated in the previous step and serves them.

For the Dockerfile itself, there are a few specificities to each project to I can't share a turnkey Dockerfile, but here is the structure:
```bash
# build site
FROM python:3.7-slim as dbt_site_builder

# here add the environment variables you need and will pass via --build-arg 
ARG ...  

# install dbt
ENV DBT_VERSION=0.14.2
RUN apt-get update -y && \
    apt-get install --no-install-recommends -y -q \
    git libpq-dev python-dev && \
    pip install dbt==${DBT_VERSION}

# Set environment variables and working directory
ENV DBT_DIR /source
WORKDIR $DBT_DIR
ENV DBT_PROFILES_DIR $DBT_DIR
COPY . .

# pull dbt dependencies
RUN dbt deps

# build static pages
RUN dbt docs generate

# serve site
FROM nginx:stable-alpine

# here copy any nginx related files you might need for your deployment, for example nginx.conf
ADD ...

COPY --from=dbt_site_builder /source/target/index.html /source/target/manifest.json /source/target/catalog.json /source/target/run_results.json /usr/share/nginx/html/

EXPOSE 80
```
You can then : 
```bash
docker build -t dbt-docs --build-arg MY-ENV-VARIABLE  .
docker run -p 8080:80 dbt-docs
open http://localhost:8080
``` 

Hope this helps",4
141,"publishing-dbt-docs-from-a-docker-container","Love it!  Proper CD is far superior to the hack-job I did in the original post!",5
149,"how-do-you-test-your-data","I'm curious to hear about approaches to testing data to ensure quality. Many of us in the data world are not trained software engineers and aren't as familiar with things like CI/CD, unit or integration tests, separate testing environments, or containers.

If you use any creative dbt testing approaches, continuous integration/deployment, automation of SQL tests through a workflow tool like Airflow, or otherwise, I would love to hear about it! How do you test?",1
149,"how-do-you-test-your-data","This is a really good question, and something that we've been asking ourselves more recently. We add the typical unique/not null test constraints, and some relationship tests between our models. A more recent ""test type"" we've incorporated are regression tests. This is especially useful for us when we find a bug in our code with unintended consequences. Before attempting to ""fix"" the issue we write a test that catches the bug and then attempt to fix the code, which is similar to test-driven development.",2
149,"how-do-you-test-your-data","Hey @joshtemple - we use testing in a few different capacities. 

1. During development

When creating or modifying a table, I like to add tests for some invariants, like the uniqueness or non-nullness of a given column. This helps me gain confidence that the code which I am writing works as intended. @kriselle makes a great point about TDD above too. For well-defined models, I like to write my tests _first_, then build a model around the tests. This flow helps reduce the friction around merging PRs... more on that below.

2. During PRs

We've built build-on-PR functionality into [sinter](sinterdata.com). Whenever a Pull Request is made against our internal-analytics repo, Sinter will kick off a full run & test of our dbt project in a scratch schema. If any of the models or tests fail, then Sinter puts a big red X on the Pull Request. If everything works as intended, then Sinter shows a nice green check mark. The contextualized test success/failures help guide code review, and acts as a last check before code gets merged into production. In practice, it looks like this:

![46%20AM|690x335](upload://iM2ORcOrg121Bsa7nZr5Pmy9N9D.png) 

3. In production

We also use Sinter to run our models on a scheduled basis. A typical deployment of dbt includes running:
```
dbt seed
dbt run
dbt test
```

So, after every invocation of `dbt run`, we also run all of the tests in the project. If any of these steps fail, we get email notifications from Sinter. This helps alerts us when bad/invalid data has arrived from a data source, or when some logic in a model doesn't hold up over time.

There's lots more to say about testing, deployment, CI, etc. I know many dbt users are doing pretty interesting/complex things. You should cross-post this to #general in Slack :)",3
149,"how-do-you-test-your-data","I would like to echo Drew's sentiments above. I would like to add two thoughts.

**During Development**

Before working on an existing model, I like to take a couple of key metrics about it- things like row count, sum of key columns (e.g. revenue), and count of distinct values in other columns. If the number of rows in the beginning and the end of the development process don't match, that's fine, but I need to be able to justify the change. 

**After Development**
_This is not yet something we've implemented, but it's something we hope to implement soon._

 I think a summary metrics table is a good idea. Pick 10 or so key business metrics from a bunch of different data sources and decide on some tolerance for change. If yesterday's values are outside of that tolerance for change, have a test fail. A little bit of noise (too rapid growth? slow holiday weekend? etc) is better than a problem with data that your end-user discovers first.

I've spent a lot of time lately thinking about _what CI looks like for data_. Thanks for starting the conversation.",4
149,"how-do-you-test-your-data","Thank you all for your input!

We have a similar setup where we use GitLab CI to create and test the entire data model in a build schema before merging into master. We also trigger our GitLab pipeline for daily runs after our EL jobs finish, which executes `dbt seed/run/test` in a build schema, then `dbt seed/run` in production if the tests pass.

**This is helpful if issues with newly loaded source data cause a test to fail. The side effect of this is that if a single test fails, we don't update our production schema at all, since we don't have a good way to pass/fail specific models and promote the results to production.** I would be interested to hear if anyone else has thought of a better way to do that.

@emilie, I like your idea of checking a few ""health"" metrics during and after development. At a previous company, we had a daily email that displayed the difference in record count between today and yesterday for each table. Even that simple count helped us catch a lot of issues.",5
149,"how-do-you-test-your-data","@joshtemple â€“ you might be interested in checking out [this issue](https://github.com/fishtown-analytics/dbt/issues/1054), there's a fair bit of discussion about how dbt might handle this in the future! Feel free to contribute to the discussion there as well :)",6
149,"how-do-you-test-your-data","I've thought about this a lot too and I've come to the conclusion that testing is a data analysis problem....something we are all familiar with!  To that end, I like to think about instrumentation, storage and analysis as discrete problems with testing. Something gets the data, the data is stored and then you figure out what it means. Keeping it modular let's us evolve each piece as we need to. So that noise you referenced earlier....over time our analysis should learn to deal with that",7
149,"how-do-you-test-your-data","Hey folks :wave: - I recently wrote a post on this topic sharing one approach I've used in the past. May be of interest to this group: https://ianwhitestone.work/testing-sql/",8
149,"how-do-you-test-your-data","@joshtemple  - Can you please share more details on the below?

We have a similar setup where we use GitLab CI to create and test the entire data model in a build schema before merging into master. We also trigger our GitLab pipeline for daily runs after our EL jobs finish, which executes `dbt seed/run/test` in a build schema, then `dbt seed/run` in production if the tests pass.",9
155,"ctes-are-passthroughs-some-research","Hi all! I recently did some research on a question that comes up when I talk to people about SQL and optimizers.

At Fishtown Analytics, we have a [style guide](https://github.com/fishtown-analytics/corp/blob/master/dbt_coding_conventions.md) for how we write SQL. Part of that style guide is using CTEs as though they're `import` statements. We start each model by ""importing"" each upstream relation that's going to be used. In practice, that looks like this:

```
with customers as (
  select * from {{ref('fct_customers')}}
),
orders as (
  select * from {{ref('fct_orders')}}
)
...
```
This functions a lot like your imports at the top of a Python script (or any programming language):
```
import pandas as pd
import numpy as np
...
```

This achieves a couple of nice things. First, it makes it very easy to see what the dependencies are for a given model when you first open up its file. Second, it gives you an easy mechanism to consistently alias all input tablesâ€”this mechanism ends up being much cleaner than typing out and then aliasing tables individually within each individual CTE. Overall, we've found that this pattern leads to significantly more readable code.

The pattern causes consternation for folks who are familiar with database optimizers on older databases. Lots of CTEs, and lots of `select *`, neither of which is a good idea according to conventional wisdom. I've always believed that this is no longer true with modern analytic databases (BQ, Snowflake, Redshift), but I hadn't done the detailed work to validate that. Until now.

I wrote two versions of a query. The first selects directly from a raw `order_items` table and does a simple aggregation on it. Here's what that looks like with the associated Redshift explain plan:

![49%20PM|690x98](upload://59u42BVFZlJYc7P9Cg6QUVAIZLX.png) 

The second passes the source data through *ten CTEs* before finally doing an aggregation on it. Here's what each looks like (along with the associated Redshift explain plan:

![51%20PM|690x98](upload://fs9v9o5oKALZHLtX4wQrBmRDcUj.png) 

The thing you'll note about these two explain plans is that _they are identical_. Redshift doesn't care if you pass the source data through 10 ""obfuscating"" CTEs, each of which select every single column in the underlying table. It ""optimizes through"" the CTEs and understands what the user is actually asking for: a simple count distinct on top of a single column. **The other columns in the table don't get used in the plan at all.**

OK, great. Redshift treats CTEs as pass-throughs. Let's prove that the same is true of Bigquery and Snowflake. BigQuery first. Here are the two explain plans (first a direct query, second with 10 passthrough CTEs):

![48%20PM|690x166](upload://i2NJms29YG9D8PoQqnd4wKPwNeV.png) 

![58%20PM|690x167](upload://nRLmm2YU5b7BCOEzjwPYq6inZTw.png) 

Because BQ is a shared resource, two queries' execution will never be exactly identical. These are as close to identical execution plans as you can really get. On to Snowflake:

![55%20PM|612x500](upload://3GX6iVdpFniykqJrtQmTFW4hgoE.png) 

![17%20PM|603x500](upload://poSVR9B8VupCBIkMM5RfXOBoKHp.png) 

This one is slightly more perplexing--the second query (with the CTEs) is actually faster! That doesn't make a ton of sense to me, but I also ran this query second and my guess is that I wasn't able to fully invalidate the cache for the second run. More importantly though, the explain plan is identical across both runs--Snowflake treats the queries as identical from an optimizer perspective.

## Summary
All modern analytical database optimizers appear to treat our ""import statement"" CTEs as pass-throughs. These CTEs have no impact on performance whatsoever, and just act in the way that we want: as a great tool to clean up our code but not ultimately to change the explain plan.",1
155,"ctes-are-passthroughs-some-research","While CTEs function as passthroughs in these examples, on Redshift, at least, there is a limit at which too many CTEs cause a degradation in performance. See this [article](http://www.47lining.com/index.php/2016/10/06/improving-redshift-query-performance-by-reducing-query-steps/).",2
155,"ctes-are-passthroughs-some-research","Nice link, thanks for sharing! I ultimately don't think that this plays into the overall ""CTEs as imports"" pattern I describe above, as that article was mostly warning against excessively complicated code primarily written by automated tools. Very good to know though.",3
155,"ctes-are-passthroughs-some-research","I'm just leaving this waypoint here for those on Postgres - there is no pass through of CTE logic on that platform.",4
155,"ctes-are-passthroughs-some-research","That's absolutely correct--Postgres treats CTEs as optimization boundaries at the moment.",5
155,"ctes-are-passthroughs-some-research","hi Tristan, thanks for the research on CTE, i was wondering if CTE are still optimization boundaries for newer versions of postgres?",6
155,"ctes-are-passthroughs-some-research","I don't actually know!! If you look into it I'd love to hear what you find...",7
155,"ctes-are-passthroughs-some-research","Not for a while now, see: https://www.postgresql.org/docs/current/queries-with.html",8
155,"ctes-are-passthroughs-some-research","[quote=""marcelvv, post:4, topic:155, full:true""]
Iâ€™m just leaving this waypoint here for those on Postgres - there is no pass through of CTE logic on that platform.
[/quote]

As @wrb points out, it looks like Postgres 12+ have resolved the issue with

> Automatic (but overridable) inlining of [common table expressions](https://www.postgresql.org/docs/12/queries-with.html) (CTEs)
>
> https://www.postgresql.org/docs/release/12.0/",9
155,"ctes-are-passthroughs-some-research","While this may be true, for longer graphs/chains of CTEs, there comes a point at which the BigQuery planner throws up its hands and says, ""Enough is too much""  and throws this error:

![image|690x53](upload://ySIhK5zVt2rv8fxf3bWNNNKyhRQ.png)

While I have been able to clear this through some optimization and simplification of queries, most often, the only way to get the pipeline to run is to materialize one or more of the intermediate views as tables.

Has anyone else run into this?  Or is there something peculiar about my environment/data?",10
155,"ctes-are-passthroughs-some-research","I'm not sure if this is compatible with dbt models (I haven't tried), but an solution I use elsewhere is to replace CTEs with temporary tables. I don't think it's something peculiar with your environment; I've run into similar issues multiple times with a not-so-complex database.",11
155,"ctes-are-passthroughs-some-research","yeah, we had the same problem and had to replace some ephemeral models to tables too :/",12
155,"ctes-are-passthroughs-some-research","Just posting this article below by database developer Dominik Golebiewski at Atheon Analytics, which confirms with Snowflake support that - at least in some circumstances - CTEs are optimisation fences in Snowflake. 

I guess some questions are:

* Whether this is a performance regression since the 2018 analysis by OP
* What are the circumstances (e.g. table size, number of refs to imported model) under which it is an optimisation fence
* What is the table size where for the typical non latency sensitive incremental model it's worth refactoring CTE code (e.g. 1 billion rows per dbt run)

https://medium.com/@AtheonAnalytics/snowflake-query-optimiser-unoptimised-cf0223bdd136
https://getdbt.slack.com/archives/CJN7XRF1B/p1634116586013000",13
155,"ctes-are-passthroughs-some-research","I have faced the same issue with some SaaS ETL tools which produce code. I guess it does not matter whether  a human is writing a code (CTE bases, or long chain of CTEs) or an ETL tool is producing one, it is always a good practice to materialize at some point if you such error messages.",14
159,"how-to-create-a-series-of-python-udfs-for-parsing-user-agents-in-redshift","What follows is a step-by-step guide to creating a series of Python UDFs on Redshift for parsing user agents with Python's [`user-agents`](https://github.com/selwin/python-user-agents) module. While our ultimate goal will be efficiently parsing user agents in Redshift, the first few steps are generalizable to building custom UDFs with any custom Python module (though your results may vary based on the complexities of the module you're trying to use!).

## 1. Package up your python dependencies

When adding custom Python modules to your Redshift cluster, you need to first consider how Python modules work, and how they might possibly work on your cluster. Most modules rely on a series of other modules, which are loaded at runtime by looking at your machine's `PATH`. In most cases, `pip` handles this dependency resolution without us having to worry about it. But since we can't just open up our cluster's terminal and type `pip install user-agents`, we have to do something a bit more complicated.  

We'll follow AWS Lab's [PipLibraryInstaller](https://github.com/aws-samples/amazon-redshift-udfs/tree/master/bin/PipLibraryInstaller) guide, though we won't use their provided script so as to encourage greater understanding of what's going on under the hood. We'll walk through each step of the process. While you're encouraged to try and abstract these steps out into your own script, my personal experience has shown that different modules require different installation strategies. I also doubt you'll ever do this more than a handful of times per cluster, so it's not clear what you're saving by automating this process (unless you're in the business of managing clusters for other people, then by all means, automate!).

First, we need to build a `wheel` of `user-agents`. A [`wheel`](https://fileinfo.com/extension/whl) file is the standard built-package format used for python and is essentially just a `zip` file of the module's code and metadata. To do this let's run the following commands:

```bash
mkdir -p tmp/
pip wheel user-agents --no-cache-dir --wheel-dir tmp/user-agents/
```
We should now be able to inspect the specified `wheel-dir` and see the following:
```bash
ls tmp/user-agents/
>>> ua_parser-0.8.0-py2.py3-none-any.whl user_agents-1.1.0-py3-none-any.whl
```
**NOTE**: As you can see, `pip wheel` generated two files when packaging up `user-agents`, one for the module itself, and one for  `ua_parser`, the module it depends on. 

To add these modules to Redshift, we need to upload both files to S3 using the [`awscli`](https://docs.aws.amazon.com/cli/latest/reference/), which you can install via `pip install --user awscli`.

```bash
aws s3 cp tmp/ua-parser/ua_parser-0.8.0-py2.py3-none-any.whl \ 
 s3://my-bucket/ua-parser.zip
aws s3 cp tmp/ua-parser/user_agents-1.1.0-py3-none-any.whl \ 
 s3://my-bucket/user-agents.zip
```
**NOTE**: The above commands might be slightly different depending on the version of `user-agents` you install. 

## 2. Add your modules to Redshift 

After uploading your module to S3, we can now add it to our Redshift cluster by executing the following SQL statements:

```sql
CREATE LIBRARY ua_parser
LANGUAGE plpythonu
from 's3://my-bucket/ua-parser.zip'
WITH CREDENTIALS AS 'aws_access_key_id={my_key};aws_secret_access_key={my_secret}'

CREATE LIBRARY user_agents
LANGUAGE plpythonu
from 's3://my-bucket/user_agents.zip'
WITH CREDENTIALS AS 'aws_access_key_id={my_key};aws_secret_access_key={my_secret}'
```
**NOTE**: Replace `{my_key}` and `{my_secret}` with your AWS credentials. You can also use an `iam_role` here. 

## 3. Write your UDFs

Next, we'll attempt to test our module by writing our UDFs. We'll follow Amazon's [best practices](https://docs.aws.amazon.com/redshift/latest/dg/udf-naming-udfs.html) for naming UDFs by placing them in a custom schema and prefixing the function name with `f_`:

```sql
CREATE SCHEMA udf;

CREATE OR REPLACE FUNCTION udf.f_ua_is_bot(ua VARCHAR(MAX)) RETURNS boolean IMMUTABLE as $$
  if ua is None or ua == '': return False
  from user_agents import parse; return parse(ua).is_bot
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_browser(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).browser.family
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_browser_version(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).browser.version_string
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_device_brand(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).device.brand
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_device_family(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).device.family
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_device_model(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).device.model
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_os_family(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).os.family
$$ LANGUAGE plpythonu;

CREATE OR REPLACE FUNCTION udf.f_ua_os_version(ua VARCHAR(MAX)) RETURNS VARCHAR(MAX) IMMUTABLE as $$
  if ua is None or ua == '': return None
  from user_agents import parse; return parse(ua).os.version_string
$$ LANGUAGE plpythonu;
```

## 3. Testing your UDFs

You should now be able to use these UDFs in queries to parse user agent strings:

```
SELECT udf.f_ua_is_bot('Twitterbot');
>>> True
SELECT udf.f_ua_is_bot('Mozilla/1234');
>>> False
```

## 4. Optimizing your use of UDFs

One thing to know about UDFs is that they're extremely slow. Think about what's happening under the hood: Your database is basically piping a string out to another process and capturing the result. This will not be anywhere near as fast as a normal query. As a result you need to thoroughly optimize your user agent parsing queries to _minimize the number of rows it operates on_. Here are a few strategies:

### Only parse distinct user agents.

Instead of naively running your UDF on every user agent string in your data warehouse, build a lookup table of unique user agents to user agent metadata, then join against this table when you want to enrich your user agent strings with additional metadata. You can also hash the user-agent string in order to speed up future joins on this table:

```sql
strtol(left(func_sha1(user_agent), 15), 16)::bigint
```

### Don't re-parse user agent strings

Instead of re-building this table everyday from scratch, identify the new user-agent strings that show up every day and add these to your lookup table.

## 5. Managing UDF permissions

Follow [this guide](https://docs.aws.amazon.com/redshift/latest/dg/udf-security-and-privileges.html) to allow other DB users to access your UDFs.

## 6. Integrating with DBT

Follow @claire's excellent guide for integrating UDFs into your DBT workflow: https://discourse.getdbt.com/t/using-dbt-to-manage-user-defined-functions-redshift/18",1
181,"examples-of-custom-schema-tests","I'm really diving into data testing at the moment (thanks to @joshtemple's [Medium post](https://medium.com/@josh.temple/automated-testing-in-the-modern-data-warehouse-d5a251a866af)!). By default, dbt comes with 4 schema tests: not null, unique, relationships, and accepted values. Also, dbt allows for [custom schema test](https://docs.getdbt.com/docs/custom-schema-tests) which lets you define additional schema tests. 

**I'm interested in how others have defined custom schema tests for their data models.** It'd be great to have several examples here to inspire the rest of us in brainstorming data tests that fit our data needs. 

For example, I work with North American phone numbers a lot, so I have a custom schema tests to check that phone number fields always contain ten-digit number (TDN) strings: 

```
{# Verifies all elements in field are TDNs. If validation_errors returns >0 rows, test fails. #}

{% macro test_is_tdn(model, column_name) %}

with validation as (
  select
    {{ column_name }} as tdn_field
  from {{ model }}
),

validation_errors as (
  select
    tdn_field
  from validation
  where tdn_field not rlike '^\\d{10}$'
)

select count(*)
from validation_errors

{% endmacro %}

```


What are some custom data schema tests you've developed?",1
181,"examples-of-custom-schema-tests","I have two simple modifications on the standard schema tests: `not_null_where` and `unique_together`:

```
{% macro test_not_null_where(model, field, condition) %}

  with exceptions as (
    select
      count(*)

    from
      {{ model }}

    where
      {{ field }} is null
      and {{ condition }}

  )

  select * from exceptions

{% endmacro %}
```
```
{% macro test_unique_together(model, fields) %}

  with exceptions as (
    select
      {% for field in fields %}
        {{ field }}{% if not loop.last %},{% endif %}
      {% endfor %}

    from
      {{ model }}

    {{ dbt_utils.group_by(n=fields|length) }}

    having
      count(*) > 1
  )

  select count(*) from exceptions

{% endmacro %}
```
You use the following syntax to call these tests (v1 `schema.yml`):
```
not_null_where:
  - {field: 'field_1', condition: ""another_field = 'something'""}
  - {field: 'field_1', condition: ""another_field > 0""}
  - {field: 'field_1', condition: ""not(field_2 = 'a' and field_3 = 'b')""}
unique_together:
  - {fields: ['field_1', 'field_2']}
```",2
181,"examples-of-custom-schema-tests","We have built a few custom schema tests for ourselves at PageUp, which can be viewed [here](https://github.com/PageUpPeopleOrg/pageup-dbt-utils/tree/master/macros/schema_tests).  Some of these are specific to our use cases but are interesting nonetheless.

* **Equality for range** test that two tables have the same data in them, for the rows in the given id range. This is useful for testing against a controlled data set, as it allows you to test specific use cases on a subset of data while having other data in the model that may be used by other tests.
* **Has timestamp** and **Has version** are tests that ensure that certain design patterns we have are enforced. For example, we have (had) a rule that every denormalised table or foreign key in a model should have its own timestamp column, which is then used for incremental updates later. This test ensures that a given column has a matching timestamp column, and that the timestamp column has a value in it whenever it needs to.
* **Null when parent column null** This is a junk-data/model design test. It ensures that when you denormalise a table, its columns are either all populated or (in the event that the foreign key was null) all not populated.",3
181,"examples-of-custom-schema-tests","We put all of the custom schema tests we find useful in [dbt-utils](https://github.com/fishtown-analytics/dbt-utils#schema-tests).

One pattern which is super simple but really useful to implement is the use of the `expression_is_true` test, so you can define your own logic without having to write the sql for a test. I used this _a lot_ in financial models when I had to validate that subtotal columns were equal to a total, that financial values were positive, etc. etc., so my schema.yml would look like:
```
version: 2

models:
  - name: payments
    tests:
      - dbt_utils.expression_is_true:
          expression: ""inbound_amount = payout_amount + refund_amount""
      - dbt_utils.expression_is_true:
          expression: ""inbound_amount = inbound_cash_amount + inbound_credit_amount""
```",4
181,"examples-of-custom-schema-tests","We wanted to be able to test that numbers (float or non-float) were between a certain set of values. So we did this:

    {% macro test_is_between(model, column_name, bottom_number, top_number) %}

    with validation as (

        select
            {{ column_name }} as field_to_test

        from {{ model }}

    ),

    validation_errors as (

        select
            field_to_test

        from validation
        where field_to_test > {{ top_number }} or field_to_test < {{ bottom_number }}

    )

    select count(*)
    from validation_errors

    {% endmacro %}

And here is an example of a test:

      - name: flight_end_long
        tests:
          - is_between: {'bottom_number' : -180, 'top_number' : 180}",5
181,"examples-of-custom-schema-tests","I'm using `is_between` macro now, thanks! For schema.yml v2, use: 

```
        - name: flight_end_long
          tests:
              - is_between:
                  bottom_number: -180
                  top_number: 180
```",6
181,"examples-of-custom-schema-tests","Hey ted,
Quick question:
If I want to set ""condition"" as an optional argument and set a default value for it, then can I do it like this:

```
{% macro test_not_null_where(model, field, condition=""1=1"", condition2=""1=1"") %}

  with exceptions as (
    select
      count(*)

    from
      {{ model }}

    where
      {{ field }} is null
      and {{ condition }}
      and {{ condition2 }}

  )

  select * from exceptions

{% endmacro %}
```
Also, can I use it in a **test** block instead of a **macro** block ?",7
181,"examples-of-custom-schema-tests","Our team needed to delineate between warning and error thresholds for a number of tests, including dbt built ins like unique and not_null, based on a configurable threshold. For the built ins, we overrode the core implementation, adding support for thresholds and severity.  Example implementation for our overridden implementation of unique:

```
{% test unique(model, column_name, severity_type= 'warn', percent_threshold=0.0) %}
-- overriden unique test allowing for a threshold of non-unique values
-- calculation of threshold is based on the total number of values with more than one row / total number of distinct values.
{% if severity_type == 'warn' %}
    {{ config(tags=[""critical""], severity = severity_type, warn_if = '>0') }}
{%- else -%}
    {{ config(tags=[""critical""], severity = severity_type, error_if = '>0') }}
{% endif %}

with row_count as (
    select
        count(1) as total_rows
    from
        {{ model }}
),
total_distinct_values as (
    select
        count(distinct {{ column_name }}) as distinct_vals_count
    from
        {{ model }}
    where
        {{ column_name }} is not null
)
, non_unique_values as (
    select
        {{ column_name }} as unique_field,
        count(*) as n_records
    from
        {{ model }}
    where
        {{ column_name }} is not null
    group by
        {{ column_name }}
    having count(*) > 1
)
, error_count as (
    select
        count(*) as n_errors
    from
        non_unique_values
)
, combined as (
    select
        case
            when distinct_vals_count > 0 then
                case
                    when cast(n_errors as decimal)/distinct_vals_count <= {{ percent_threshold }} then 0
                    else cast(n_errors as decimal)/distinct_vals_count
                end
            else 0
        end as result
    from
        error_count
        cross join total_distinct_values
)
select
  result as percentage_duplicate_values
  , v.*
from
  combined a
  cross join non_unique_values v
where a.result <> 0

{% endtest %}
```

In order to support different thresholds with different errors, we actually have to apply two differently configured unique tests to the same column in the yaml config, such as 

```
    - name: SOME_FIELD
      data_type: varchar
      description: A field that is supposed to have unique values, but we know there are a small number of cases where there are non-unique values that are outside of our team's control. We want to be aware those exists, but only want to error if we see a lot of cases.
      tests:
        - unique:
            severity_type: 'warn'
            percent_threshold: 0.000001
        - unique:
            severity_type: 'error'
            percent_threshold: 0.0001

```",8
181,"examples-of-custom-schema-tests","My post is over 4 years old. I'd recommend using the built-in `not_null` test, which now supports the `where` config (like all generic tests). You would use it like so:

```yml
models:
  - name: my_model
    columns:
      - name: my_not_null_col
        tests:
          - not_null:
              config:
                where: >
                  foo == 1
                  and bar == 2
```",9
193,"validating-the-sql-in-a-redshift-late-binding-view","## Why Redshift's late-binding views are great
Redshift's late-binding views ""unbinds"" a view from the data it selects from. In practice, this means that if upstream views or tables are dropped with a `cascade` qualifier, the late-binding view does _not_ get dropped as well.

Using late-binding views in a production deployment of dbt can vastly improve the availability of data in the warehouse, especially for models that are materialized as late-binding views and are queried by end-users, since they wonâ€™t be dropped when upstream models are updated.

For more information on late-binding views, see the [docs](https://docs.getdbt.com/docs/warehouse-specific-configurations#section-late-binding-views) and this [blogpost](https://blog.fishtownanalytics.com/using-redshifts-late-binding-views-with-dbt-e735d80f9cfc).

## Why Redshift's late-binding views are not so great
One downside of using late-binding views is that if the SQL in your view is invalid, it may only be picked up when a query that returns results from this view is executed â€“ while Redshift will detect syntax errors in your SQL straight away, if you happen to use an incorrect column name or table name in your query, or forget to give a calculated column an alias, it will still create the view for you anyway!

For models in your dbt project that have downstream models materialized as tables, this error will be raised during the execution of the downstream model, making it harder to diagnose where your bad SQL exists. And if your late-binding view is exposed to an end user, you may not realize you have bad SQL until someone queries it!

## Validating the SQL in your late-binding view
Fortunately, you can leverage post-hooks and macros so check whether a late-binding views contain valid SQL by selecting a single row from your model. Note that this _will_ slow down your dev runs of dbt, so use it with caution!

With the following macro, if youâ€™re in dev, and your model is a late-binding view, a query that selects a row from the view will be executed.
```sql
-- macros/test_late_binding_view_in_dev.sql
{% macro test_late_binding_view_in_dev() -%}

    {%- set is_late_binding_view=
        model['config']['materialized'] == 'view'
        and model['config']['bind'] == False
    -%}

    {%- set is_dev = (target.name == 'dev') -%}

    {%- if is_dev and is_late_binding_view -%}

        select * from {{ this }} limit 1

    {%- else -%}

        select 1 as test

    {%- endif -%}

{%- endmacro %}
```
By calling this as a post hook for each model, this macro will catch any bad SQL before moving on to the next model, raising an error for the model with the invalid SQL, and skipping downstream models.
```yaml
models:
  bind: false
  post-hook:
    - ""{{ jaffle_shop.test_late_binding_view_in_dev() }}""
```

Notes:
* I have decided to only test this when in dev, to reduce my prod run time.
* I namespaced my macro in the post-hook definition so that this macro works for my installed packages
* As an alternative solution, I also investigated setting the `bind` parameter based on the target so views would only be late-binding in prod. I couldn't get that appraoch to work due to the way dbt parses the `dbt_project.yml` file.",1
211,"structure-snowflake-database-schema","Hey! Are there any best practices on Snowflake on how to structure your database / schema? In conventional databases, you're not really able to query data from multiple databases. Usually, we've been having one schema per data sources (raw) and one schema for cleaned data sources (etl).

I was wondering what's the common approach on Snowflake to structure those datasets?

Using Segment data as an example, I'm thinking of two main ways to approach this:

> a)
> Database: segment_data
> Schema: raw_website_data
> Schema: raw_mobile_data
> 
> Schema: etl_website_data
> Schema: etl_mobile_data

> b)
> Database: raw_segment_data
> Schema: website_data
> Schema: mobile_data
> 
> Database: etl_segment_data
> Schema: website_data
> Schema: mobile_data

What are your thoughts?",1
211,"structure-snowflake-database-schema","I do not know what's ""common"", but we decided on a few things when setting ours up:

1. We use databases to separate user role permissions (`raw` for loaders to load raw data, `dbt_dev` for analysts to build development dbt models, and `analytics` for our production models). 

2. The `raw` database houses our raw data as ingested by connectors. Connectors use the `loader` role, which is the only role with write access to this db, and it does not have access to any other db. The schemas in this database are named with a `{source}_{connector}` convention (eg `salesforce_stitch`). The purpose here is twofold, 1) It makes it easy to identify which connector is being used to ingest the data, and 2) It removes any naming conflict awkwardness when changing a source connector (eg if moving to Fivetran we would just create `salesforce_fivetran`):

`raw.salesforce_stitch.leads`
`raw.delighted_stitch.responses`
`raw.paycom_airflow.employees`, etc.

3. The `dbt_dev` database is for (surprise) our analysts to develop dbt models. Each analyst has a namespaced default schema (eg `dbt_dev.dpearce`). Analysts use the `transformer` role, which has read access to all dbs but can only write to the `dbt_dev` database.

`dbt_dev.dpearce`
`dbt_dev.dpearce_base`, etc.

4. Finally, the `analytics` database is for our production models. We separate our base tables and transformed models into different schemas (`analytics.base` and `analytics.analytics`). Our `base` models use a naming convention of `{source_table}`. Only our Sinter account can write to this database. Our BI tool (Looker) is read-only and only has access to the `analytics.analytics` schema. So:

`analytics.base.freshdesk_agents`
`analytics.base.paycom_employees`
`analytics.base.salesforce_users`

`analytics.analytics.employees`, etc.",2
211,"structure-snowflake-database-schema","After debating the idea in my head and reading further documentation on warehouses (effectively database), wouldn't it be more manageable to have one data source being reflected as one warehouse?

Let's say that we have 3 ""connections"": Segment, Stitch, Luigi.

We would then have

`segment.source_name.table_name`
`stitch.source_name.table_name`
`luigi.source_name.table_name`

Then we could scale up or down these warehouses individually! Let's imagine that we have an influx of Frontend events and we need to scale up the Segment database for better processing, then it would only affect that source of data instead of potentially scaling up for every data sources in the example of the ""raw"" database containing everything.

Have you hit issues or slowdown by having all the ""connector"" syncs into the same database? @dapearce",3
211,"structure-snowflake-database-schema","""Warehouses"" and ""databases"" are not the same thing within Snowflake. A database in Snowflake really just represents a storage partition, while warehouses are compute resources. 

You can have multiple warehouses processing data in the same ""database"" concurrently. For example, you can have a ""segment"" warehouse writing to the ""raw"" database at the same time a ""stitch"" warehouse is writing to the ""raw"" database. I am not aware of any scaling considerations with ""databases"" within Snowflake. My understanding is that a database is just an organizational partition on top of the storage to help with stuff like permissions. Your scaling would be handled with warehouses.",4
211,"structure-snowflake-database-schema","You are totally right, I understood the concept wrong and wrongly jump to assumptions. Thanks for your input, it helped clarify it!",5
211,"structure-snowflake-database-schema","I love this question @cab â€“ thanks so much for posting it to Discourse. I've included some advice for folks that aren't on Snowflake too.

I totally agree with @dapearce's reply!

We often leverage the `raw.<source>.*` pattern on Snowflake (often, all our data sources are loaded by the same tool, but I can see the utility in including `<connector>`!). Note for warehouses where you can't do cross-database queries (read: Redshift), I recommend using a `raw_<source>.*` pattern to get a similar result.

In terms of databases/schemas for objects built by dbt, in snowflake we often use an `analytics.<mart>.*` pattern. On Redshift, I like to use a pattern like `zone_<mart>.*` so the schemas get ordered nicely in your warehouse, but that's my own preference! 

Personally, I'd steer clear of using separate databases for each source (or collection of sources) in Snowflake, it feels like it would add too much complexity to the design without much benefit.",6
211,"structure-snowflake-database-schema","Thanks for doing this.

I'm curious about Warehouse naming and convention. Perhaps a new thread required (please let me know), but if not:
I've setup two warehouses `etl_wh` and `query_wh` per [this article](https://medium.com/hashmapinc/snowflakes-cloud-data-warehouse-what-i-learned-and-why-i-m-rethinking-the-data-warehouse-75a5daad271c), and I've setup all ETL tools + dbt to have permissions on `etl_wh` and I'll setup users and BI tools to use `query_wh`. 

Will this scale? How are are you thinking about warehouses. There is very little in the public domain about this type of stuff, so very useful to hear your thoughts.",7
211,"structure-snowflake-database-schema","@matt we have been incrementally switching over from Redshift and have thus far gotten by with a single small warehouse for everything. However, we are basically at our limit there now and will need to break it out soon. So my first-hand experience is so far limited but my understanding of warehouses is you want to consider a few things:

1. The biggest cost with Snowflake is warehouse usage, which is dependent on a combination of the size of the warehouse and how many minutes it runs.

2. The bigger the warehouse, the faster things run. So it's very possible a big query can be cheaper to run on a large warehouse than a small, because although a large warehouse costs more per minute to run, the query takes less time to run than on a small warehouse.

It's recommended to [configure a warehouse to auto-suspend](https://docs.snowflake.net/manuals/user-guide/warehouses-overview.html#auto-suspension-and-auto-resumption). This will pause the warehouse after a specified period of inactivity (thus stop the billing).

For us, because of how Stitch loads data it is running a small warehouse almost 24/7, but not using the full capacity of the warehouse. So we have so far been able to get by with our dbt modeling and BI tool on the same warehouse without too much issue (thus minimizing our billing costs). However, we moved over some of our data science jobs and are pretty close to separating things out now.

What's ""best"" is likely different for a lot of organizations. For example, if you have some data science models that run once a week that use a lot of compute, but infrequently, you may assign them their own XL warehouse, which you only get billed for when they run. If you have a lot of BI users and want to ensure they never compete with ingestion or data science computing you could give the BI tool it's own warehouse.

However, if you give Stitch it's own warehouse, and Segment it's own warehouse, and Looker it's own warehouse, and dbt it's own warehouse, and Airflow it's own warehouse, and the most any of these ever use is 20% of the compute capacity then you've probably over-optimized and are paying more than you need to.

With that in mind, I think warehouse naming conventions are probably dependent on what you are assigning them to. I think what you have makes sense if you just want to make sure BI users aren't impacted by ETL compute, and you can get by with a single warehouse for ETL compute. Warehouses are also pretty easy to create and delete and reassign so I think the consequences of warehouse naming conventions aren't the same as with a database.",8
211,"structure-snowflake-database-schema","Perfect, thank you, that makes sense.

[quote=""dapearce, post:8, topic:211""]
most any of these ever use is 20% of the compute capacity then youâ€™ve probably over-optimized and are paying more than you need to
[/quote]

I'm not sure I understand the billing well enough, but is it not the case that compute resources are strictly pay-per-use, so that even if each tool had it's own warehouse, you would only pay for the use of each warehouse, and not the fact that they exist? (assuming parity of warehouse size).",9
211,"structure-snowflake-database-schema","@matt, correct, you only pay for the time that you use each warehouse, not that they exist. So you can have as many warehouses as you want, and if they aren't running you won't be billed anything.

The consideration comes with when and how much they are being utilized. For example, let's consider your data loading usage (Stitch) and your dbt usage (Sinter or Airflow). 

You can give each of them their own small warehouse to ensure they don't compete with each other. Or you can put them on the same warehouse. Which to choose?

Let's say they both are scheduled to run at the top of the hour. Stitch runs for 15 minutes and dbt runs for 10 minutes. If you put them on separate warehouses you will be billed:

stitch_wh: 15 mins X 24 hours = 360 mins/day
sinter_wh: 10 mins X 24 hours = 240 mins/day
Total: 600 mins/day

However, do they need to be on separate warehouses? If they each only use 20% compute, then you could put them on the same warehouse and they would still only use 40% of the compute (meaning you could probably put your BI tool on there too).

In this scenario, they both still run at the top of the hour, and both still take the same amount of time to complete, but you are only running one warehouse and since they run concurrently you are only billed for the max 15/mins per hour that Stitch uses (Sinter's 10mins overlaps with this), so:

shared_wh: 15 mins * 24 hours = total 360 mins/day (much cheaper)

Of course, if Sinter's run is scheduled for half-past the hour, because you want to run it AFTER your Stitch load, then they would not run concurrently, and your billed minutes would be the same with one warehouse as it is for two (in this case it's really no benefit either way, since they wouldn't compete even on a shared warehouse).

So when scaling, yes, warehouse strategy can become complicated. You may need to consider the number of concurrent queries, the amount of compute needed, the priority of the job or the importance of how fast it's completed, etc.

If you have a simple setup my recommendation is just put it all on a small warehouse. Make sure each piece of the system uses its own Snowflake user, then once you start to hit performance issues you can dig into usage per user and consider moving one or more users to separate warehouses.",10
211,"structure-snowflake-database-schema","@dapearce thank you, that is a great explanation, and certainly new insight for me. I'll stick to a single warehouse for now, as reducing cost is initial priority.",11
211,"structure-snowflake-database-schema","This is a really useful thread.

Please note that this exists:

Jeremy Cohen @ Fishtown Analytics,  [How we configure Snowflake](https://blog.fishtownanalytics.com/how-we-configure-snowflake-fc13f1eb36c4)",12
211,"structure-snowflake-database-schema","For those of you who use separate databases like @dapearce describes, could you post how you setup your profiles.yml and dbt_project.yml to connect to the different ""databases""? Or whatever device you use to source data from one database/schema and create artifacts in another database/schema?",13
211,"structure-snowflake-database-schema","@dapearce Late comment but I was able to reduce my Stitch related snowflake spend by about 80% by scheduling my pipelines to be active at the same intervals. Being able to choose run time for your pipelines was a feature Stitch introduced some time last summer.",14
211,"structure-snowflake-database-schema","I like the notion of separate databases for raw, ETL, analytics. However, how do you deal with the orthogonal dimension of dev/UAT/prod?",15
211,"structure-snowflake-database-schema","[quote=""crholliday, post:13, topic:211""]
For those of you who use separate databases like @dapearce describes, could you post how you setup your profiles.yml and dbt_project.yml to connect to the different â€œdatabasesâ€?
[/quote]

@crholliday you don't need to do anything in this files to allow dbt to connect to different databases. You just need to make sure the dbt user has permissions to access the different databases. You just query them by including the database in the selector:
```
SELECT * FROM database1.schema.table

LEFT JOIN database2.schema.table USING (join_key)
```
If you are talking about ensuring dbt builds to the correct database (developers build to `dbt_dev` but production builds to `analytics`), then you just specify that in the `database` parameter of the `profiles.yml`. Here's what our `profiles.yml` template looks like:
```
snowflake:
  outputs:
    # This is the dev profile. Configure it by adding
    # your Snowflake username and password, and setting a
    # default dev schema for your dbt models.
    dev:
      type: snowflake
      threads: 1
      account: company.us-east-1
      user:     # snowflake username
      password:  # snowflake password
      role: TRANSFORMER
      database: DBT_DEV
      warehouse: SOLO_WH
      schema:  # {firstinitiallastname} ## Schema your dev models will build to (dpearce)

  target: dev   # Default target is dev unless changed at run time
```
We use dbt Cloud to build our production models, so our configuration there is to build to our `analytics` database.

[quote=""gordonwong, post:14, topic:211""]
Late comment but I was able to reduce my Stitch related snowflake spend by about 80% by scheduling my pipelines to be active at the same intervals.
[/quote]

@gordonwong That's awesome

[quote=""gordonwong, post:15, topic:211, full:true""]
I like the notion of separate databases for raw, ETL, analytics. However, how do you deal with the orthogonal dimension of dev/UAT/prod?
[/quote]

We use `dbt_dev` for analysts building dbt models locally, and `raw_dev` for data engineers developing custom ingesters. We don't have much UAT other than analysts testing their own stuff. We do use dbt Cloud and it runs our dbt tests as part of a CI process anytime a PR is created or updated (it's configured to build to a `continuous_integration` database).",16
211,"structure-snowflake-database-schema","David, thanks for sharing your modeling outline. Iâ€™m doing some more reading on modeling methods/standards and was curious if your above description has evolved over the last year.

Specifically, Iâ€™m curious if you (or anyone) explored an architecture like `analytics.base_{source}.{table}` , or possibly a three-db approach of `raw` , `staging` , and `analytics`? i.e. something that maintains schemas per source, even for base tables.

I keep thinking that having **all** base models end up in the `analytics.base` schema will create lots of clutter â€” am I missing something, some trade off or unexpected advantages? It seems the standard is to go with a two-DB (`raw` & `analytics` approach based on [1]), as you've outlined, so I'd love to hear comments on why going beyond this is not recommended.

I found an PR [2] that introduces the ability to route models to a specific database, so having the ability to read from `raw` and then output base models to `base.{source}.{table}` while having all analytics-related models go to an `analytics` DB might not be too hard to manage.

Would love to hear thoughts on this or any reference to other useful examples. Thanks in advance

[1] [https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355](https://slack-redir.net/link?url=https%3A%2F%2Fdiscourse.getdbt.com%2Ft%2Fhow-we-structure-our-dbt-projects%2F355)

[2] [https://github.com/fishtown-analytics/dbt/issues/1183](https://github.com/fishtown-analytics/dbt/issues/1183)",17
211,"structure-snowflake-database-schema","Hey Kevin, when we designed our architecture it was not possible with dbt to build to multiple databases. I haven't followed dbt updates closely over the last ~6 months so this is actually news to me that it's possible now. 

At that time, the primary reason we put base and analytic models in a single schema was to simplify permissions. If we had analytic models in multiple schemas, and base models in multiple schemas (within the same database), then anytime we want to add new schemas, or change permissions, it would be difficult to manage. 

Of course, with the ability to configure dbt to build to multiple databases, permissions can be maintained at the database level. This can certainly provide for an additional level of organization.

Since I have not tried this in practice, I may be overlooking something, but to answer your question no I don't see any immediate concerns or tradeoffs with this approach. I think it's particularly appealing in a *very* large project. With that said, we have ~250 models in our project and have not found our current setup to be cumbersome or difficult to manage. We are very rarely browsing through the list of tables directly in Snowflake, and when we do, our base tables are prefixed with the source, so they are all grouped together and sorted alphabetically.",18
211,"structure-snowflake-database-schema","Hey Kevin

I've been trying to get started with dbt using the ""multiple databases"" approach in Snowflake, and have struggled to understand the PR[2] that you referenced. Moreover, Snowflake is about to introduce ""Organization accounts"" (similar to AWS Organizations), so we'll have the option of using multiple Snowflake accounts going forward.

David previously kindly provided an example of his `profiles.yml` file.  Kevin, could you share what a ""multiple databases"" `profiles.yml` should look like, and how to use the multiple database connections in your models?

Beyond that, I wonder if @claire or another dbt team member might share some additional insight on this topic.  As you noted in your post, Kevin, the current dbt docs seem to focus on working with multiple schemas in a single database like `analytics`. 

I appreciate any help that folks could offer here. Thanks!
Kyle",19
211,"structure-snowflake-database-schema","Hi Claire, thanks for this post. We're setting up dbt on Synapse, hence the non-Snowflake perspective was quite useful. Would be awesome to get your thoughts on the following:
* Would raw_<source> and zone_<mart> be schemas be within the same database? If yes, then is the rationale to simplify cross-db queries? 
* Would you create a separate db/schema for dbt_dev vs prod or just use the 'analytics' db/schema for both?

We're new to dbt and in the process of figuring out the basics, like the best way to structuring it along with the underlying data warehouse. Thanks a lot!",20
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","> Is Kimball dimensional modeling still relevant in a modern data warehouse?

We see this discussion _a lot_ on [dbt Slack](slack.getdbt.com), so I'd love to move it to a Discourse post! Comment below with your thoughts!",1
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I have a lot of thoughts from my anecdotal experience and knowledge of redshift internals, but would love to rigorously test them. If you or your company would like to sponsor an analysis of performance using snowflake / star / denormalized across the big 3 warehouse solutions, get at me (I don't want to have to cover the redshift / bigquery / snowflake compute costs on my own).",2
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","Hey, so I guess I'd like to give some context and get a feel for what other people are doing.  These are the ""denominations"" of data people that I know about. Feel free to add.
* Inmon - Bill Inmon
* Data Vault 2.0 - Dan Linstedt
* Kimball - Ralph Kimball
* Functional Kimball - [Maxime Beauchemin](https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)
* Agile Kimball - [Lawrence Corr](http://www.decisionone.co.uk/training/)
* Denormalised Tables

My org is looking at a green fields implementation so, I'm pretty attracted to Maximes Functional Kimball approach, but I hear lots of merit in just making wide tables.

The challenge with wide data is that its challenging to contextualise a 500 column wide table properly and there is no recognised ""denomination"" that I can train my analysts in a consistent fashion and recruit already skilled applicants.  I don't want to advertise for a ""data munger""...",3
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I'll add [Anchor Modeling](http://www.anchormodeling.com/) proposed by [Lars RÃ¶nnbÃ¤ck](https://www.researchgate.net/profile/Lars_Roennbaeck) to this as well for a method of Temporal Dimensional Modeling.",4
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","So in Kimball modeling there were some assumptions made in1996 that just aren't true in 2019:

1) Databases are slow and expensive 
2) SQL language is limited
3) You can never join fact tables because of one to many or many to one joins 
4) Businesses are slow to change

A lot has happened:

1) databases are now fast and cheap
2) SQL has evolved to include many more date functions and window functions that cut down on ETL work required
3) Looker solved this with Symmetric Aggregates 
4) Business doesn't wait for you to finish your perfect DWH

So I respect the work of Kimball and there are some great tips and techniques in the DWH Toolkit that I use everyday. But I see too many ""over engineered"" data warehouses that don't need to be that complex and don't encompass new things happening in the business.",5
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","The ""core"" of Kimball to me is based on a few ideas:

1. Users will need to query data in any number of different ways, but fully normalized data models are way too complex for non-developers to deal with. So we need a simpler way to model data for analytics.
2. We need to think about dimensions in a conformed way. if you are slicing a bunch of facts by, say, customer, it needs to be the same, common list of customers no matter how many different things you slice it by. Otherwise users across the business eventually start getting inconsistent results and then everybody loses faith in the accuracy of the data in the warehouse. And no, denormalizing all of your dimensions down into every fact is not a good solution when you have 30 facts and 30 dimensions in a DW of respectable size.
3. We need to model data in such a way that when we add new data it doesn't break existing reports and analytics. This means making sure we think thoroughly about the grain of all things ahead of time. If we change the existing columns or the grain, especially the grain, it's going to break a lot of existing stuff. Well, maybe you can change things if you work in a small shop, but in an Enterprise Data Warehouse with hundreds or thousands of users, getting everybody to change their existing analytics takes literally years.
4. But, after #3 - we are also going to constantly find new data and reports. So we have to make sure that we have modeled in a way that allows us to add new data.
5. We need a consistent conceptual way to get people to think about events we analyze (facts) versus the things we are analyzing them by so that we aren't reinventing the wheel every time we go to build data models. I can tell everytime I see a DW or analytics solution built by somebody who doesn't understand this and doesn't follow some approach (whether Kimball or Inmon) because it's invariably some crazy denormalization approach that is internally inconsistent and impossible to explain in an organized way. Or even worse they've just imported all the fully normalized datasets from the source and point the reports right at the source tables.
6. Users will invariably want to use multiple different BI and query tools to access the same dataset. And you can try to herd them in a certain direction (like Looker) but you'll never get them all. So the DW should be built so that, to the best extent possible, users can still query with the tool of their choice. Which means not relying on BI tools, like Looker, to perform major further transformations of the data in the BI layer. Minor transformations, documentation, labels, etc.? Sure.

Database performance is much better now than it was when Kimball was written , but Kimball's model of conformed dimensional design was not primarily concerned about database performance. I will admit it is helpful in managing performance and he discusses performance and storage space, but that was a secondary benefit and regardless of that the primary use cases for it are still relevant.",6
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","Hereâ€™s my chaos modeling special blueprint:

1. Raw - Ingest as raw with Fivetran / Stitch / Matillion / Custom Stages, making sure to have an ingestion time and pulling out expected predicate columns if theyâ€™re packed in a variant. Strongly prefer append-only ingestion when possible.
2. Stage - All tables named `STG_<SOURCE_TABLE_NAME>` with the most basic cleaning - timestamp conversion, upper/lowercasing of strings, null fills, maybe some basic mapping (like mapping AWS Account Id values to human-readable names using a case statement). These are often views.
3. Analytics - Working from the end of the pipeline, identify use case / pain points and determine the data required for this. Anything a BI tool will hit should be materialized as a table and should have human-readable names with no prefix. Schemas and databases at this level should also be human-readable with schemas split by business function or access level
4. Curated - Iteratively apply DRY concepts to pull useful models out of individual end-table computations and into a curated layer (prepended with `CUR_<TABLE_NAME>`). These are a mix of views and tables and should reference only other curated tables and stage tables. Once you do this a few times, some `CURATED` tables become obvious to you when doing step 3 so you donâ€™t have to go down the path of repeating yourself a ton then reworking backward.

At no point do I normalize incoming sources simply to conform to a formal data model, particularly when I will need to denormalize them by the end of the pipeline.

This approach assumes a smaller data team and a very broad community of mostly BI-focused access. It takes a centralized authority view of the world and doesnâ€™t account for citizen data scientists. This is an approach designed primarily for enterprise clients new to the cloud.

I think formal modeling is very useful, especially in an OLTP environment, but I find that too many large companies that Iâ€™ve worked with have gotten obsessed with formal modeling and have completely neglected end-user needs. It has largely been a theatrical exercise that gives the impression of adding value, but at the end of the day weâ€™d spend all our time accommodating some arbitrary modeling needs and end up with no real usage of the warehouse.

I definitely donâ€™t have anything against modeling, I would just prefer to focus efforts on understanding users and delivering value early and intentionally leveraging technical debt in the warehouse in a greenfield cloud data warehouse migration. If we feel the pain of the technical debt, itâ€™ll be due to increases in usage and requirement (ostensibly due to users gaining value) so we can address modeling needs after weâ€™ve delivered value (and using dbt makes it easy to fix this debt). If we donâ€™t feel the pain, then we avoided a costly modeling engagement and can spend all our cycles iterating on use cases to find ways to deliver real value.

I feel like Iâ€™m probably in the minority and Iâ€™m definitely over biased towards the kinds of projects / companies Iâ€™ve worked with.",7
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I prefer a combination of two approaches: 

1. Some consistent implementation of a normalized model, e.g. any of the ones discussed above. 
2. Flat, denormalized tables built on top which contain the most commonly used facts and dimensions. 

In terms of ownership: 1. is managed by data engineering or more engineering focused analysts, and 2. is managed by a committee of analysts or by a senior analyst. The flat table should aim to cover 80% of use cases, or common queries, for example users, orders, key events etc.. 

For the remaining 20% of use cases, or for new pieces of data and analysis, then the analyst needs to query from tables built in 1. Once an edge case becomes a common query, the facts and dimensions should be added into the flat table.",8
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","That's a very interesting approach. For approach 2, presumably you still have keys to the dimensions from which those attributes are pulled to produce the flat tables?",9
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","Hi there, I've been a Kimball DW Architect for a while now and have consistently built denormalized dimensional models as described in the books, without a normalized underlying layer like Inmon suggests.  I'm trying to figure if or out how creating Kimball models is different in DBT than it is in the Microsoft world.  Any thoughts?

One implication of ColumnStore indexing to me is that the cost of degenerate dimensions is reduced, which means you might consider using more of them.  If you took this to the extreme it would result in just one big table (OBT), but everyone I've spoken to has seen that this hits a wall at scale.",10
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I am probably totally naÃ¯ve about the difference - but I can't see how dbt or MS or any other for that matter, has any bearing on wide table design. Now there are cost differences - based on what some people have mentioned in the Slack community, is that BigQuery has some wierd cost associated to this.

I dont know about this wall, but there are some practical implications for the extreme, where EVERYTHING is in one table... for one, you'll never find anything and there will be many ambiguous columns.",11
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I think Kimball might not suit mobile app/games for the following reasons:

1 - Needs very fast implementation. Analysts are usually into the field as soon as data regarding engagement is out. A full DWH, even a MVP (say 5-7 dim tables and maybe 2-3 fact tables) would wait for tons of data and would take many weeks for development. It's just not practical.

2 - Columnar database welcomes wide table and hate a lot of joins. Kimball naturally needs a lot of joins for one analysis. Again the DBAs won't be very happy.

I think the most practical method is:
Step 1 - Gather enough requirements (of the analysis) BEFORE the developers start developing the feature

Step 2 - Communicate the requirement (transferred to fields needed so that developers can better understand) to developers and make sure the data contains some connection fields.

Step 3 - When the raw data is out, just import into a raw table with very little transformation

Step 4 - Look at the requirements again, build up a DWH that results in a wide table that analysts don't need to join (or very few joins, <= 3 joins should be good). It's definitely not gonna be Kimball, but each wide table is related to an analysis requirement.

Step 5 - Done and profit. No need to use keys,. Got a data issue? If it's less than say 2% just fix and ignore. Move fast and as long as it doesn't derail it's OK. Worst case we can still fall back to the raw table and rebuild everything.

OK this also has some implicit requirements on the setup of the teams:

1 - The data team (analysts and developers and DBA) should be grouped together and the head should at least be a Director so that he can fight the developers and business sides on equal footing.

2 - Training of any member in the team should include training on all three fields -- analysis, data developer including DE and DWH dev, admin with the right weights. So if you are a data developer, you still get exposed to analysis and admin but 60% of the training goes to developing.

3 - The data developer squadron needs to sync with app developer or project managers on a per Sprint basis. This is to make sure that nothing important is neglected. Before the meeting the data developers should also make sure they consume the requirements taken from the analysts and the suggestions taken from DBA.

4 - You can remove DBA team if data developer does the job.",12
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","It is a great question and one question people should ask when entering data analytics. I'll answer by first providing a question, why do any data modelling at all?
It is an enterprise question, and when you think of the word enterprise you think of large banks or car manufacturers but in fact an enterprise question relates to another discipline, enterprise architecture. The first block of enterprise architecture is business architecture. Here's what I know of business architecture:
It defines business capabilities - these are the building blocks of any business, and the collection of which helps fuel the business strategy (example capabilities are attached)
![typical industry-related business capabilities industry|690x300](upload://90zE0eNTjrlEinzBJIlrNaBTsW4.png)

Each capability may have one or many overlapping business processes, start to finish these are things like onboarding prospects, hardening applications to accounts, processes to deploy one or many accounts for a customer, relating to products and so on. All of these fall under capabilities supporting a strategy right?

Each business process in turn have overlapping business rules, the automated procedures to get work done - unit of work. There is always a data model, Kimball models serve the analytic questions by dividing dimensions and high churn metrics / measures about those measures. The things we care about in the business, products, accounts, customers, clients, plane plants, factories etc. The business objects. Absolutely Kimball has its place! So too does Data Vault 2.0! 
As the the enterprise continues to flex and grow it needs to adapt without refactoring and applying DV2.0 achieves that, Kimball rests on top of DV2.0 to deliver business focussed answers that the DV2.0 stores, DV2.0 recognizes the business objects and business processes, whereas Kimball delivers the analytics needed. They live together. See: [DV2 learning is like learning how to Brew!](https://patrickcuba.medium.com/learning-data-vault-is-like-learning-how-to-make-beer-fff2f7f4d3b)

Where you might be thinking of in terms of other data requirements is real-time data modelling, or no modelling at all! Let me explain, 
Mobile apps needs very fast access to analytical data - explore Kashlev Data Modeller (url: [Portal](http://kdm-portal.weebly.com/)), this takes the data mart and deploys all results to a query-driven data table approach, still the data came from an effort to denormalise the data in the first place!
Data Lake landed data, well to be honest, how did it get there? All data needs data modelling -- data modelling describes the business, after all, it is the third block in enterprise architecture - data architecture, without a business architecture you cannot have a data architecture, what are you modelling?
As for Anchor... it's a cute modelling technique but ask yourself... if you told your Database Administrator you're expecting to manage every column as its own table (6NF) what do you think his/her reaction would be? It may be logically efficient (no nulls) but it is not physically efficient on relational data stores.",13
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","What I'm thinking is that I 100% agree that data should be modelled under business requirements. What I would argue is that we should directly build models that business users can use, instead of dividing them into dim/fact.

Take mobile gaming as an example. Typical data analytics require a lot of slicing on one measure, say profit. Not only that analysts want to slice against common ""dimensions"" such as date/platform/geo, they also want to slice against a lot of other measures such as balance/player behavior/etc.

Now that with the benefit of columnar database, I really don't see the benefit of Kimball here. Analysts don't care how the data is layered out, what they care is to dump the data into a reporting tool or a mathematics model, and the least thing they want is to take care of the joins themselves. With Kimball, they are going to need at least 5-10 joins just for one analysis, because Kimball breaks everything apart.

My proposal is simple. You don't care about business meaning of your data, because frankly business owners NEVER use database, it's the business ANALYSTS who use database. The ""business meaning"" is in fact the final requirement that business analysts ask for. What do they ask for? They want a measure and a bunch of slicers, so why not give them that?

I forgot whether Kimball mentioned that Data modeler should work closely with developers (that give them raw data), but from top of my head I think he emphasizes more about working closer to the clients. I think working with developers is at least of equal importance because you want to make sure that the raw data is of the form you can transform easily.

So here is my method:

Step 1: BI developer (data modeller) meet with analysts to discuss requirement. At the end of the meeting they need to have a table with all fields needed mapped out. And in fact this table is the end product, not just a helper on the road.

Step 2: You meet with upstream developers and make sure they can give you these data. It is not necessarily that they give you the data in one piece, because developers also have frameworks and guidelines to follow, and giving you the data in one piece may damage those guidelines. But you need to make sure that they give you some linking fields to connect the dots.

Step 3: You import each piece to a different raw table with minimum transformation.

Step 4: You transform the raw tables into one single wide table that match exactly the one you got in Step 1. Profit.

OK one major caveat of this method is: If the analysts suddenly change requirements, then you have to expose the raw tables to them, which is uglier than what Kimball will expose. However I'd argue that for analysts, they should be able to figure the business information out from the raw table. After all they are not business owners. Business owners only know business, but analysts need to know how to slice/segment business, and even the raw data should more or less give them that.

In conclusion, I really don't see how Kimball offers any additional value here. If you break up your data to dim/fact tables, the analysts STILL WANT the wide table, but in the case of Kimball, they have to build it up by themselves, or you have to write a view for them. So essentially you add an additional unnecessary layer into the picture, and waste a ton of time building up a nicely looking palace.",14
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","Different models and model types for different purposes, dims and facts are very much relevant but maybe not for your use case",15
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I am curious if there are any good references for how organizations have maybe blended modeling traditional kimball dims and facts with more of a hypercube/OBT approach.",16
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","Popular BI tools like Looker and Tableau still relate to data sets using concepts of facts and dimensions even if the data aren't strictly modeled as a star schema. I think the logical concepts are still relevant and even helpful for ad-hoc data exploration.
 
Since raw data nearly always needs some measure of transformation to be useful, choosing a modeling design pattern (like star schemas) can help guide the process. As the data modeler, you're free to extend or hybridize the pattern, but it's nice to have a starting point.",17
225,"is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse","I have been designing and building dimensional data warehouses (DWs) for a few years now and I believe this methodology gives us simple rules for 'judicious denormalization' which creates business friendly tables.

I agree with Josh's points here. I just want to add that the idea of joining fact tables together makes my skin crawl, if they're truly the same grain, then widen your table, if not, then you're risking gaps and duplicates that analysts will be annoyed by (and payback is a coming). For similar reasons I'm not a fan of ODS (Inmon) DWs, those subject area specific tables are great until they need to be expanded, combined and compared (and inevitably, the grain changes).

I would like to comment on Linstedt's Data Vault 2.0 (DV), it was very popular about 5-10 years ago (least in Australia), it seemed like everyone was decommissioning their Kimball DWs for DV. At the time, it made sense, DWs and SANs were slow, poorly designed and their DBAs stubborn (or cash poor). Overnight fact/dim loads were running well into the morning if not the afternoon. DV's methodology of parallel loading hubs and spokes from multiple sources seemed like the obvious next evolution. I worked on 2 sites that implemented DV and they were both a mess (technically and culturally). User queries or building cubes from DV tables was difficult, leading to analysts and BI devs were building ETLs on ETLs; data governance and linage suffered immeasurably. Not sure how it all turned out on those sites (I didn't stick around), needless to say, cloud computing meant the Kimball DW load issues dissipated, and hopefully so did DV.",18
234,"questions-thoughts-on-staging-versus-analytics-areas-in-warehouse","Hello All -

Question how everyone handles their schema management for staging data versus transformations done on staging data.

It's a good rule of thumb to not let users access the staging data, because as you build a warehouse you often need to constantly change how the staging and transformation process works, and if users write analytics/reports directly against the staging data the analytics will end up breaking as you change the staging and loading processes. And if users can somehow find and access data they will write reports against it one way or another.

To enable this, in a ""traditional"" DW, I am accustomed to having a separate schema or even database for the staging data versus the end-user accessible transformed data area and using the schema as a global way to ensure that the groups/roles to which end-users are assigning are blocked from the staging area. 

We are trying to figure out how to handle this with dbt. With our current warehouse project in Snowflake, we have a ""staging"" schema and an ""analytics"" schema. We have multiple analysts editing and working on dbt code and data transformations. We also want to keep all of the dbt transformations in one dbt project to have one DAG that makes understanding the data workflow simpler for all concerned rather than splitting ""staging"" processing into one dbt project and ""analytics"" processing into another.

The problem that we are encountering is that in order to allow analysts to be working on different code branches and data transformation flows independently, we want to use dbt's capabilities to allow them to just rename the schema and dbt should automatically create their own independent schemas. However, this doesn't work so well with the staging schema because we only want to maintain one copy of that, and some of the dbt tasks involve creating additional worktables and views in the staging schema. We also have some parts of the process that go staging -> analytics -> staging -> analytics because of the complexity of some transformations.

How do others handle this? Do you just put everything in one schema and try to be careful at a granular level about granting end-users access to only particular tables/views? Does everybody get their own copy of two schemas? Or are we just over-complicating all of this?

Thanks in advance for any thoughts!",1
234,"questions-thoughts-on-staging-versus-analytics-areas-in-warehouse","Hey @josh! Good question!

Can you elaborate on the difference between ""staging"" and ""analytics"" schemas for your team? In particular, you mentioned:

>  However, this doesnâ€™t work so well with the staging schema because we only want to maintain one copy of that

In general, the dbt workflow supports each analyst having their own copies of all of the models in the project. If each analysts shares a single staging schema, I can imagine things getting pretty hairy pretty quickly!

If you haven't already, check out the docs on [custom schemas](https://docs.getdbt.com/docs/using-custom-schemas). By default, these will add a suffix to your ""base"" schema name. That will let you create schemas in development like:
```
- dbt_josh
- dbt_josh_staging
```

dbt will handle the materialization logic for your models, so you can definitely have models that weave between staging and ""analytics"" tables. 

With custom schemas, you can override the schema name logic on an environment-specific basis. This functionality lets you eg. create all of your models in `dbt_josh` in dev, but split out the schemas into `analytics` and `analytics_staging` in prod. Check out the docs on doing that [here](https://docs.getdbt.com/docs/using-custom-schemas#section-altering-schema-override-behavior-advanced-) and let me know if you have any questions!",2
234,"questions-thoughts-on-staging-versus-analytics-areas-in-warehouse","Hi @drew thanks for the response!

Sounds like you are suggesting each analyst having their own separate staging schema as well. In our scenario this means that dbt would need to be creating and building two schemas per analyst, not just one, as we need dbt to be able to create and modify many of the objects in the staging schema as well as the analytics schema.

We tried this a bit early on but seemed to get a lot of errors and had difficulty with doing it. If we collapse everything into one schema, then we run into the issue of greater risk of end-users seeing data that they should not. 

How do you suggest handling scenarios where the analysts are doing a bunch of staging transformations, involving both source data and intermediate steps (whether views or actual, materialized tables), but where the end results of the transformations should be a queryable, cleaned up model of data for usage by another separate set of end-users who should not be able to see the intermediate steps?",3
234,"questions-thoughts-on-staging-versus-analytics-areas-in-warehouse","So, this isn't a super unusual or uncommon use case -- this is exactly why we build custom schemas! I'd do exactly the thing you're describing: materialize your production models into the standard prod schema (eg. `analytics`), and materialize all of your intermediate transformations into some scratch schema (like `analytics_staging` or similar).

You can permission the schemas differently s.t. only analysts have access to the staging schema, but all relevant users have access to the production schema. 

So, I'd totally recommend just using custom schemas here!  You mentioned:

> We tried this a bit early on but seemed to get a lot of errors and had difficulty with doing it.

Can you elaborate on what kinds of errors you saw or where the difficulty came in? Happy to help you get it sorted if you want to give it another try",4
274,"what-alternatives-are-there-to-dbt","Hi,

(I also asked this question in the Slack channel.)

I realise that dbt is very new, unique idea -- but what would you say are the most similar products, or alternatives to dbt today? What is the main competition?",1
274,"what-alternatives-are-there-to-dbt","Stephen Levin has written the best post about this so far. I don't 100% agree with every line of it but I think on the whole it is quite fair:

https://www.stephenlevin.co/data-modeling-layer-startup-analytics-dbt-vs-matillion-vs-lookml/",2
274,"what-alternatives-are-there-to-dbt","Def if you use gcp you might look into something new soon .... they acquired something like dbt recently ... but I cannot remember the name",3
274,"what-alternatives-are-there-to-dbt","https://dataform.co/blog/dataform-is-joining-google-cloud

https://cloud.google.com/dataform

https://github.com/Dataform-co/Dataform

and 
https://rdrn.dev/dataform-and-dbt/

good dbt will have another  competitor - to get better ...",4
274,"what-alternatives-are-there-to-dbt","There is probably someone better placed to tell this history more accurately than me, but I will give it a go (note that it's also in one of the linked posts, but I think it's worth surfacing more obviously in this thread).

Dataform was originally built **on top of** dbt.  My understanding is that they got to market with a dbt Cloud  like offering before (then) Fishtown Analytics did (and FWIW, their early cloud version was called 'sinter').

Eventually dataform decided to replace dbt underneath with their own tool, which leads to what you find now. A very dbt-ish experience from a totally separate platform (although with distinct differences, like javascript rather than Python-based templating)

> good dbt will have another competitor - to get better

This will be interesting to see.  I'm all for competition in the space, but it seems like Dataform is a long way behind in terms of marketshare (if GitHub stars/folks is any indication. BigQuery is experiencing some major growth, and I bet there's folks who (assuming they integrate dataform well into the GCP ecosystem) would love to keep things all-in-one.

One last thought, I can't quite help but think of Looker, whom Google also acquired.  They're still an outstanding tool but QC and service seem to have gone backwards since the acquisition. It may be telling for Dataform's future...",5
287,"tips-and-tricks-about-working-with-dbt","Working with dbt for a while, one can start to develop workflows that are really useful. It would be great to collect these somewhere, so that beginner dbt-ers can benefit from those experiences. 

Here is my tip: 

* When I was really into R, I would use the beepr package all the time because then I could switch onto a new task without losing site of my priority. Many moons ago, I asked about a built-in dbt beep and Drew taught me the `dbt run .. && say beep`. 

What is something you have in your workflow that might make it easier for someone to work in/around dbt?",1
287,"tips-and-tricks-about-working-with-dbt","Nice! The one trick I really like involves using some bash magic to run all of the changed files on a branch.

Snippet:
```
dbt run --models $(git diff --name-only | grep '\.sql$' | awk -F '/' '{ print $NF }' | sed 's/\.sql$/+/g')
```

You can save this in your .bashrc with a function, eg:
```
function dbt_run_changed() {
    children=$1
    models=$(git diff --name-only | grep '\.sql$' | awk -F '/' '{ print $NF }' | sed ""s/\.sql$/${children}/g"" | tr '\n' ' ')
    echo ""Running models: ${models}""
    dbt run --models $models
}
```

This function takes an optional argument, `+`, that will also run the children of the changed models!

Usage:
```
$ dbt_run_changed
$ dbt_run_changed +
```",2
287,"tips-and-tricks-about-working-with-dbt","Here's my hot tip!

I'm a huge fan of keeping my directories organized in a tree structure. Since I do client work, my tree structure ends up looking like:
```
.
â””â”€â”€ fishtown
    â”œâ”€â”€ client
    â”‚   â”œâ”€â”€ stark-industries
    â”‚   â”‚   â”œâ”€â”€ stark-industries-dbt
    â”‚   â”‚   â””â”€â”€ stark-industries-lookml
    â”‚   â””â”€â”€ wayne-enterprises
    â”‚       â””â”€â”€ wayne-enterprises-dbt
    â”œâ”€â”€ dbt
    â””â”€â”€ packages
        â”œâ”€â”€ segment
        â””â”€â”€ utils
```
This can be annoying to `cd` into the right folder, so I use the [goto](https://github.com/iridakos/goto) utility to set up aliases for my folders.
Then I can do things like:
```bash
$ goto stark
$ pwd
./fishtown/client/stark-industries/stark-industries-dbt
```

I also recently set up a bash alias to open GitHub for my current directory (not tested on GitLab! Sorry @emilie) â€“ instructions [here](https://gist.github.com/igrigorik/6666860).",3
287,"tips-and-tricks-about-working-with-dbt","Oh and another one...

Sometimes if my `dbt run` is resulting in an error, I like to cycle my `logs/dbt.log` file before trying to run it again, to make debugging easier.

I have a pretty rudimentary bash function that does this for me:
```bash
function cycle_logs() {
  suffix=$(date '+%Y-%m-%dT%H:%M:%S')
  mv -v logs/dbt.log logs/dbt.log.${suffix}
}
```",4
287,"tips-and-tricks-about-working-with-dbt","I just set up goto! It's great! +1 this tip!",5
287,"tips-and-tricks-about-working-with-dbt","Drew, do you think this approach could be used in a CI framework to only build/test/deploy only the models changed by the commits being merged into master?

I'm imagining diffing the feature branch against master and only running those models (with @ and +) in production. Could speed up build/test/deploy times a *lot* by leaving out the models that aren't affected.",6
287,"tips-and-tricks-about-working-with-dbt","Cool idea! This would definitely be a good starting point, but it won't hold up for changes to macros or `dbt_project.yml`, for instance. dbt doesn't currently provide a rock-solid way of understanding which models are impacted by changes to a given set of _files_, but that's certainly something it could do some day!",7
287,"tips-and-tricks-about-working-with-dbt","Here's a basic run-and-test function I've got in my `~/.bash_profile`:

```
dbtrt() {
    echo ""Running and testing: $@""
    dbt run -m ""$@"" && dbt test -m ""$@""
}
```",8
287,"tips-and-tricks-about-working-with-dbt","Here's another bash function:
```
# find and open in emacs
find_and_open() {
    find . -type f -iname ""*$1*"" | xargs emacsclient -nw -a ''
}
```
This can be configured for other editors too. E.g. For Sublime just change the ending to `... sublime -w`",9
294,"should-i-move-my-ctes-into-separate-models","Every so often, a question like this comes up on dbt slack:
> ""If I write a query using CTEs, would you recommend having the CTEs as a separate sql files as `ephemeral` and then reference those or keep the CTE with a with statement in the same file? the compile result will be the same - but what are the pros / cons of the two approaches?""

**Pros of splitting CTEs into separate models:**
* Your CTE becomes a separate model, which you can then test (including ephemeral models!). This is often useful for testing uniqueness.
* If your models are materialized as tables or views, you can more easily debug since you can select directly from them in your SQL client.
* You can reuse logic in multiple downstream models (in which case it may be a good idea to materialize your model as a table)

**Cons of splitting CTEs into separate models:**
* More files!
* If your models are materialized as tables or views, you'll also have more relations in your data warehouse, which can make your warehouse seem cluttered.

**My recommendation:**
Like a lot of things in programming, it's about finding a balance! And normally I find that balance by going too far in one direction (e.g. no CTEs as separate models), and then overcorrecting in the other (e.g. _all_ CTEs as separate models), and then continuing to correct until I find a happy medium.

My rules of thumb are:
* If your model has multiple CTEs (and hundreds of lines of code) it may be worth breaking it into separate models
* If you are _aggregating_ in a CTE, split it into a separate model. For example, in this query, the `customer_orders` CTE aggregates, and I would normally split this into a separate model.
```
with customers as (
    select * from {{ ref('stg_customers') }}
),

orders as (
    select * from {{ ref('stg_orders') }}
),

-- I'd pull the customer_orders cte into a separate model
customer_orders as (
    select
       customer_id,

        -- aggregates
        min(order_date) as first_order,
        max(order_date) as most_recent_order,
        sum(amount) as lifetime_value,
        count(*) as number_of_orders
    from orders
    group by 1
),

final as (
    select
        *
    from customers
    left join customer_orders using (customer_id)
)

select * from final
```
* If it's a CTE that will not get re-used, it may be a good idea to keep it in the same model.
* Use the ephemeral materialization for lightweight transformations, e.g. renaming/casting to the right data type.
* If you are concerned about the number of relations that then get materialized in your warehouse, consider using [custom schemas](https://docs.getdbt.com/docs/using-custom-schemas) to put them in a separate ""staging"" schema.
* Use subdirectories within your `models/` directory to group together intermediate models to reduce the clutter within your dbt project. This also makes it easier to configure custom schemas from you `dbt_project.yml` file. For example a nested structure might look like:
```
models/
â””â”€â”€ marts/
    â””â”€â”€ core/
        â”œâ”€â”€ fct_customers.sql
        â”œâ”€â”€ fct_orders.sql
        â””â”€â”€ intermediate/
            â”œâ”€â”€ customer_orders.sql
            â”œâ”€â”€ customer_payments.sql
            â””â”€â”€ order_payments.sql

```",1
303,"on-the-limits-of-incrementality","I've been doing a lot of work recently on projects that have large data volumes (1TB++), and this has caused me to think a lot harder about incremental models than I ever have had to previously. I wanted to catalog some of this knowledge for others, and would love to learn if others have developed approaches that I have not. 

## The Big Easy
The easiest use case to create an incremental model is when your underlying data comes in as an immutable event stream and your transformation on top of it does not require any windowing and does not have any late-arriving facts. Imagine an `events` table that you want to aggregate into a table `daily_user_events` that has one record per user per day. This is a simple group by with a `count(*)`.

In this case, you simply select new events that are greater than or equal to the max day already in the table. Here's what the code looks like:

```
with events as (
  select * from {{ref('events')}}
  {% if is_incremental() %}
    where event_timestamp >= (select max(event_timestamp)::date from {{this}})
  {% endif %}
)
--rest of model...
```
This will grab new events and transform them. Assuming you set your underlying table up correctly using a sort key or equivalent on your warehouse of choice, this will be _very_ fast. I have in practice seen models using this strategy take 20 seconds to process where a full refresh takes 1500 seconds, for a performance improvement of 98%+. YMMV, obviously, but my point is that this incrementality scenario is the best possible case if your underlying data and required transformation supports it.

## Late-Arriving Facts
Extend the above scenario to _late-arriving facts_â€”records that arrive in our source data table out of order. This is a common occurrence with event data, especially when that event data is collected on a mobile device. When mobile devices cannot connect to the internet they typically save events that they can't send in a queue, and then batch send them all later when they reconnect. 

This scenario causes problems for our above approach, because the `event_timestamp` will be in the past and therefore we will never pick these records up to be transformed. There are two ways to deal with this: _close enough & performant_ or _always correct & slow_.

### Close Enough & Performant
```
with events as (
  select * from {{ref('events')}}
  {% if is_incremental() %}
    where event_timestamp >= (
      select dateadd(day, -3, max(event_timestamp)::date) from {{this}}
      )
  {% endif %}
)
--rest of model...
```
This approach establishes a ""window"" of recent data to always re-transform every time the model runs. In the code above, that window is three days. Essentially, late-arriving facts have three days to arrive, after which point they are no longer eligible to be incorporated into our metrics (until, at least, we do a full-refresh).

This strategy is just what it seems: _close enough_. If you require perfect accuracy on every transformation, this isn't a good strategy. But in many cases the performance-for-accuracy tradeoff makes sense given that we're often talking about behavioral data that needs to be directionally correct, not 100% correct.

This strategy performs _almost_ as well as the base case. It's still an extremely limited table scan because of the `where` condition built on top of the sort key (or equivalent). If you can get away with this in your scenario, I highly recommend it.

### Always Correct & Slow
```
with events as (
  select * from {{ref('events')}}
  {% if is_incremental() %}
    where session_id in (
      select session_id from {{ref('table'}}
      where event_timestamp >= 
        (select dateadd(day, -3, max(event_timestamp)::date) from {{this}})
      )
  {% endif %}
)
--rest of model...
```

Using this approach we're grabbing _all_ events associated with a given session if we've seen _any_ events associated with that session since our most recent run. This will produce a completely accurate result every time the transformation is run because there's no possibility of ""missing"" any events: if any event associated with a session arrives lateâ€”even if it's a year late!â€”all of the events from that session will be selected and re-transformed. 

However, it's _much_ slower. The innermost subquery is still quite fast, and pulling all `session_id`s is quite fast, but grabbing all events from `{{ref('events')}}` that have a particular `session_id` is very slow because it's not using a sort key, and, really, cannot. Whereas the earlier approach could result in a 95% performance increase vs. a full-refresh, this approach is often (in my experience) closer to only a 50%. It's still better than doing a full-refresh, but it's not fast. Use this approach sparingly.

## Slowly-Changing Dimensions
If your transformation includes slowly changing dimensions, this will have meaningful implications for your incrementalization strategy. Here's the scenario. Imagine you have an table called `streams` that represents user's streaming songs on Spotify. And one of the fields on that table is `artist_name`. Well, from time-to-time, artists change their name (think Prince). When a name change for an artist happens, all prior streams look like they're associated with a completely different artist, unless you explicitly handle this in your modeling. There are two ways to handle this: _dimension tables_ and `custom incrementality logic`.

### Dimension tables
This is the best argument I've ever seen for creating star schemas instead of wide fact tables. If your fact table has an `artist_id` instead of an `artist_name` column, this entire problem goes away. You move some of the performance cost to analysis time because you're doing the fact >> dimension join at analysis time, but as long as the cardinality of your dimensions is less than a million rows you're generally going to be OK. On Redshift, make sure to set your dist style as `all` for the dimensions.

### Custom incrementality logic
```
with streams as (
  select * from {{ref('events')}}

  {% if is_incremental() %}

    where event_timestamp >= (select max(event_timestamp)::date from {{this}})

    or artist_name in (
      --subquery that gets distinct artist names 
      --that have been updated since last run
    )
  {% endif %}
)
--rest of model...
```
This approach has the same performance drawbacks as the _always correct & slow_ approach above: we're no longer exclusively using a sort key (or equivalent) to grab new records and so the table scan will take much longer. The only reason you would generally prefer this approach over building dimension tables is if the cardinality of your dimension tables is high enough that the analysis-time join would take untenably long. This happens in some rare cases, but it's typically rare.

## Final Thoughts
Incrementality is _hard_. It's easy to screw up, there are a surprising number of situations that arise requiring different design decisions, and it's fairly hard to test. I'm hopeful that this download gives you a bit of a head start (or a sanity check) as you're thinking through these issues on your own. 

Also: I freely admit that _I am not an expert on this topic_â€”there are real data engineers out there who manage many-TB datasets who have far more experience than I do. As a result I'm sure that there's plenty that I'm missing in this discussion. One of the patterns that I realize is common in the Airflow / Hive world that is not idiomatic to dbt is building tables explicitly partition-by-partition and running jobs explicitly for a given day. I do not fully understand if there are benefits to this approach that dbt does not take into account, or whether this approach is simply an artifact of those particular technologies and not necessarily desirable. 

Would love any and all thoughts. Thanks for sticking with me :)",1
303,"on-the-limits-of-incrementality","Couple of thoughts to add to this great post!

Even the Big Easy can get Pretty Hairy if you're combining multiple staged models into a fact table where each staged model could be quite large and potentially benefit from incrementality.
For example, your fact represents shipped orders (fct_shipments) based on a staged or base model of shipment data and you'd like to include several fields from a separate staged or base model for orders.
Your first instinct might be to select from both staged model incrementally. 
However, this will quickly lead to out of sync fact tables if you're not careful.

Let's say orders can ship 1 or more, or several days, after the order has been placed. Let's further say that to keep things simple and performant, we want build incremental logic based on dates (to support partition pruning etc) and not orders ids.
So, you might start with something like this:
```
{{
    config(
        materialized = 'incremental',
        unique_key = 'ship_date'
    )
}}
with shipments as (
  select * from {{ ref('shipments') }}
  {% if is_incremental() %}
    where ship_date >= (
      select dateadd(day, -3, current_date)
      )
  {% endif %}
),
orders as (
  select * from {{ ref('orders') }}
  {% if is_incremental() %}
    where order_date >= (
      select dateadd(day, -3, current_date)
      )
  {% endif %}
),
shipped_orders as (
-- inner join the two models
...
)
```
In this case, you'd **only** be selecting orders that were placed _**and**_ shipped in the last 3 days.
Also, because we're going to be deleting target data based on ship date, we'd be wiping out all shipments for a particular ship date, only to be replacing it with potentially partial shipment data for that date.

So, depending on your business model, you may have to either make window for your incremental date logic much _wider_, or forgo that part altogether and get all orders regardless of date, and then hope the inner join to shipments for the last 3 days will get you some benefits from the query optimizer. 

Hope that made sense...!",2
303,"on-the-limits-of-incrementality","Thank you @tristan, it's a great post!

There's something I don't quite follow. I've implemented *Close Enough & Performant* in the past but I had to delete rows that are being re-transformed. Getting that code to work in DBT is definitely not pretty.

In the example you're showing here, can you clarify how do you effectively overwrite rows without having to scan the whole destination table?",3
303,"on-the-limits-of-incrementality","This is a *great* question. I can tell you what I know. Curious to hear your thoughts.

dbt handles the deletion for you automatically, and it does it using query that scans on the unique ID that you specify in your model config. That requires a scan on the ID field, you're absolutely right. In practice, this ends up working quite well on Redshift and Snowflake--it performs well in most cases. On Bigquery we've seen this scan end up costing a lot--for some reason it seems to trigger a scan of the entire destination table rather than just the single ID column. I don't totally understand why that ends up being.

I wouldn't recommend writing custom logic to implement this functionality in dbt--if you're attempting to write hooks or something similar that do this stuff for you you're really going to be fighting the framework instead of working with it. If you want dbt to handle the delete part of the materialization differently, you should override the materialization in your local project and change the implementation.",4
303,"on-the-limits-of-incrementality","@tristan -- Great post, thank you.

One additional thing that I would like to see further discussed here. You mentioned that incrementality is good except in cases that involve windowing. However, a classic case involving windowing is the need to deduplicate incoming data. I'll usually write a window function to do this, partitioning over the common fields in the incoming data that would have the same values.

When calling APIs or getting any sort of event stream data, normally an ETL architect has two choices. 

1. At-least once: If there is any problem with an API call, batch, workflow, or anything along the way, the given data or set of data will be reprocessed and in some cases may be re-sent to the target (which is likely your raw/staging area in the database which dbt will then consult as Sources.) Meaning that incoming data may be duplicated on import but this is an acceptable price to pay for ensuring that incoming data is for _sure_ (for some practical and limited definition of _sure_) going to arrive even if there is a problem along the way. Usually used in distributed systems where having fully reliable and incoming data is more of a priority than performance and overhead of extra validation and deduplication of data at the target.
2. At-most once: Meaning that incoming data may be dropped along the way if there is a problem somewhere. Hopefully in practice this happens rarely but still possible. Usually used in systems where directional accuracy is more important than 100% reliability and the team wants to avoid the extra complexity, delay, and performance overhead involved in issue detection and deduplication.

More info on at-least-once versus at-most once here: https://bravenewgeek.com/you-cannot-have-exactly-once-delivery/

In practice, I have almost always used ""at-least-once"" semantics. Getting users to agree to ""at-most-once"" is hard because their very logical question in response will be ""Tell me how much data I am going to lose and when?"" to which there is no great response. You can't know that until you actually get all of the data, validate it, and thoroughly verify against the source where the data is coming from, and by the time you have done that you have probably already built an at-least-once system.

So, if we are often building systems that are at-least-once, and therefore requiring incoming data to be deduplicated, and the easiest way to do this is windowing, doesn't that mean that **in most cases this whole discussion about performant incrementality is moot because we have to do windowing?**

If others are mostly building at-most-once systems where windowing and/or deduplication is not required then I would be interested to hear about it. My experience is generally ""at-least-once"" and requiring windowing.",5
303,"on-the-limits-of-incrementality","[quote=""tristan, post:1, topic:303""]
One of the patterns that I realize is common in the Airflow / Hive world that is not idiomatic to dbt is building tables explicitly partition-by-partition and running jobs explicitly for a given day. I do not fully understand if there are benefits to this approach that dbt does not take into account, or whether this approach is simply an artifact of those particular technologies and not necessarily desirable.
[/quote]

I've always thought that partitioning should be a feature of `dbt`.  The nice thing about it is that you can compose models that are idempotent, which is preferable to an incremental strategy imho (for reasons explained below). 

We use the following pattern with a lot of models at my job (it's the main reason why I put some time into adding the `alias` feature). It directly violates the `dbt` maxim of ""one model, one table"" but I'll try and explain how it addresses the issues you outline in your post.

### events_monthly.sql
```
-- set runtime variables
{% set year = var('year', run_started_at.strftime('%Y')) %}
{% set month = var('month', run_started_at.strftime('%m')) %}

{{
  config({
    'alias': '_tracking_' + year|string + '_' + month|string,
    'schema': 'events'
  })
}}

SELECT 
  * 
FROM events
WHERE
    date_part('month', ""time"") = {{ month }}
    AND date_part('year', ""time"") = {{ year }} 
```
### events.sql

```
{{
  config({
    'alias': 'tracking',
    'schema':'events',
    'materialized':'view'
  })
}}

-- set upstream dependency on monthly model
--  {{ ref('events_monthly') }}
{{ union_by_prefix('events', '_tracking_') }}

```

Here, we have two models: `events_monthly.sql` which builds a table for a discrete monthly partition and `events.sql`, which builds a view of all tables that match a common prefix. The heavy lifting is done by a macro, `union_by_prefix`, which queries the database to get a list of all tables with a common pattern (in this case, `_tracking_`) and `UNIONS` them together.

By default, the monthly model will build the current month's table, but you can also pass in `vars` at runtime to build a previous month's table. This is nice because you can address the challenge of ""Late-Arriving Facts"" by also regularly building models from previous months. This is an ""eventually consistent"" approach. Performance of the monthly models can be tuned by increasing / decreasing the size of the partitions (years, quarters, weeks, days, hours, minutes, etc). Previous partitions can be programmatically generated using this script I wrote: https://gist.github.com/abelsonlive/16611a745cace973a0c9a6f3b2b6000b

Downstream, other models only reference `events.sql` such that, to the analysts that consume these models, there's really only one `events` model. However, the challenge with this approach is isolating production and development environments. Ideally, an analyst shouldn't need to build all monthly partitions in order to work with the `events` model. For that reason, we hardcode the production schema in `events.sql`. This means that when you call `{{ ref('events') }}` it will only build a view of the monthly partitions in your development schema. Obviously, this is not ideal, but we've thoroughly explained this behavior to all `dbt` users, so they're aware of what's going on. 

How could we bring this strategy into the `dbt` paradigm without breaking some of its core tenets?",6
303,"on-the-limits-of-incrementality","Thanks for the quick reply @tristan.
As you guessed, instead of declaring a `unique_key` on the model, I used a prehook with a `DELETE` statement in order to get a ""truly"" incremental update where the table is not scanned.
Maybe this scan isn't as expensive as I thought (we're on Redshift), so I'll try removing that custom logic to see how well it does.
And writing a custom materialization strategy sounds a bit scary but I'll try to give it a shot next time I have to do something like this.",7
303,"on-the-limits-of-incrementality","ðŸ¤¯ðŸ¤¯ðŸ¤¯

@abelsonlive Thanks for your detailed explanation here! I really like this a lot--it's really the first time I've gotten my head around the benefits here. Things I'm curious about:

- if you're locating all of this data into separate tables, how are you performing windowing across all of the tables? for instance, how would you create a `user_session_number` field?
- how does the optimizer treat the unioned view? does it understand how to only look at some of the underlying tables depending on the date range? assuming that all the underlying tables are partitioned appropriately, my guess is that the optimizer would be OK here but I'm curious what you've experienced.
- is the mechanism you're describing just the best way to do this thing on Redshift or do you think it might be the best way to do the thing you're describing across each warehouse?

I don't have any answers for you in terms of how to make this more dbt-idiomatic. I could imagine ways of achieving that, but honestly this is _currently_ more of a personal curiosity than a burning need from within the community. We just aren't hearing from a lot of people that this kind of functionality is a priority for them right now. If there are others out there for whom this is a big problem, I'd love to hear from you!

Thanks again for taking the time to lay all this out. I may want to follow up with you offline at some point to discuss.",8
303,"on-the-limits-of-incrementality","I concur with @abelsonlive re partitions. 

This article lays out the benefits of this whole notion of partitions and idempotent pipelines: â€œFunctional Data Engineering â€” a modern paradigm for batch data processingâ€ by Maxime Beauchemin https://link.medium.com/q4dWH2pFdY",9
303,"on-the-limits-of-incrementality","My apologies on reviving an old topic, but I wanted to share my learnings about optimizing BigQuery incremental partitioned tables - specifically about @tristan's comment where BigQuery ends up costing a lot since it scans the entire destination table. For us, this resulted in incremental runs still being faster, but costing more than generating the table from scratch since we were billed not only for reading the incremental data, but re-reading the entire destination table.

While there's an idiomatic pattern to add predicate filters on the partitioned column of the tables you're reading from in dbt, there is no such way to do the same for the destination table you're merging into. When dbt writes the merge statement, it follows the typical pattern:

```
merge {target}
using {source}
on {target}.id = {source}.id
when not matched then update ...
when not matched then insert ...
```

The issue is BigQuery cannot perform partition pruning on the destination table since it needs to scan the entire table for matching ids. If for example you partition on `created_at`, and can guarantee that you only need to update matching data from the past 3 days, you can change the statement into this:

```
merge {target}
using {source}
on {target}.created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 3 DAY)
  and {target}.id = {source}.id
when not matched then update ...
when not matched then insert ...
```

This will give you the runtime and cost savings you would expect. For us, this was the difference of a single table being billed at 34GB vs 600MB. As we applied this strategy to dozens of our incremental tables, the total cost savings were much more.

You can have dbt do this by patching your merge function in your project. Create a `merge.sql` file in your macros folder with the following contents:

```
{#
  Fork of common_get_merge_sql from the dbt source code that adds ""destination_predicate_filter"" config support.
  This optimizes billing for incremental tables so we can limit the upsert searching logic to a few recent partitions.
  This is the difference of doing a fully billed table scan of the destination table, vs only being billed for reading
  a few days of the destination table.
#}
{% macro bigquery__get_merge_sql(target, source, unique_key, dest_columns) %}
    {%- set dest_cols_csv =  get_quoted_csv(dest_columns | map(attribute=""name"")) -%}
    {%- set destination_predicate_filter = config.get('destination_predicate_filter') -%}

    merge into {{ target }} as DBT_INTERNAL_DEST
    using {{ source }} as DBT_INTERNAL_SOURCE

    {% if unique_key %}
      on
        {% if destination_predicate_filter %} DBT_INTERNAL_DEST.{{destination_predicate_filter}} and {% endif %}
        DBT_INTERNAL_SOURCE.{{ unique_key }} = DBT_INTERNAL_DEST.{{ unique_key }}
    {% else %}
        on FALSE
    {% endif %}

    {% if unique_key %}
    when matched then update set
        {% for column in dest_columns -%}
            {{ adapter.quote(column.name) }} = DBT_INTERNAL_SOURCE.{{ adapter.quote(column.name) }}
            {%- if not loop.last %}, {%- endif %}
        {%- endfor %}
    {% endif %}

    when not matched then insert
        ({{ dest_cols_csv }})
    values
        ({{ dest_cols_csv }})

{%- endmacro %}
```

And then in your incremental models, you can update your config accordingly:
```
{{
  config(
    materialized = ""incremental"",
    partition_by = ""date(created_at)"",
    unique_key = ""id"",
    destination_predicate_filter = ""created_at >= timestamp_sub(current_timestamp(), interval 3 day)""
  )
}}
```

I feel like the API I proposed above leaves something to be desired so I didn't open a pull request, but if there's interest I can submit one. Either way though, hopefully this helps other teams trying to optimize their BigQuery bill with incremental tables.",10
303,"on-the-limits-of-incrementality","@joe this is so great!! Thanks for contributing this back to the community.",11
303,"on-the-limits-of-incrementality","@joe Very nice! Your proposal is similar to an initial approach we developed internally when we ran our first big incremental models on BigQuery. It's simple, elegant, and definitely cost-effectiveâ€”runs that had been billing us $50-a-pop fell to less than $1.

There's been some great back-and-forth from members of the community over the past several months on how to generalize this solution. It's culminated in a big proposed change to BigQuery's incremental materialization and `partition_by` config, and it's forthcoming in the next minor version of dbt. Check out the [PR](https://github.com/fishtown-analytics/dbt/pull/1971).

**In summary:** dbt will use BigQuery's new [scripting](https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting) feature to create a temp table of new records, grab all partition values from the temp table, and merge into the existing table _on those partitions only_. There's support for all partition column types (`date`, `timestamp`, and the new `integer` range), and we don't need to add any configs or hard-code the merge window.

I plan to write more about this when we release it in dbt!",12
303,"on-the-limits-of-incrementality","[quote=""tristan, post:1, topic:303""]
`event_timestamp `
[/quote]

Hi @tristan
Just a small contribution on how we do it.

We have snowflake and we load events using snowpipe and apart from maintaining  event_timestamp (time when the event occured) we also preserve â€œevent_load_timestampâ€ ( time when the event was loaded into dw) and base our incremental models on event_load_timestamp instead.
So this covers for any late arriving events as well without putting any additional window as such.

Hope it makes sense.

/Manish",13
303,"on-the-limits-of-incrementality","You could also use modelling approach that tracks records and their place that can be used to help correct time lines when data does arrive out of sequence. 

https://datavaultalliance.com/news/solving-the-time-crime-of-back-dated-data-in-your-data-vault/",14
303,"on-the-limits-of-incrementality","[quote=""tristan, post:1, topic:303""]
```
select session_id from {{ref('table'}}
      where event_timestamp >= 
        (select dateadd(day, -3, max(event_timestamp)::date) from {{this}})
      )
```
[/quote]

Regarding Always Correct & Slow approach for the late arriving fact, I found it's contradictory between the above code and "" Using this approach weâ€™re grabbing *all* events associated with a given session if weâ€™ve seen *any* events associated with that session since our most recent run."" 
From my understanding, the above code says grabbing all session_id from previous 3 days from the max event_timestamp.",15
303,"on-the-limits-of-incrementality","This is very interesting, I also think partitions should be part of dbt core. When looking at multiple ways to view data , partitions play an important role.. for eg., in Kafka you could partition the same topic multiple ways depending on how you join your topics downstream.. interesting part is that partitions define scaling, giving power to the end user.. if partition is by year making query runs slow, then i partition by month and re-run. If i partition by any other entity than time, your scaling is different",16
303,"on-the-limits-of-incrementality","Hello,
Sorry for reviving this old interesting thread.

Why relying on a business timestamp to manage incremental? 
If we are relying on an mechanical timestamp, like the loading timestamp , then even late arrival events will be taken and applied correctly during the incremental transformation.


```sql
{{
    config(
        materialized = 'incremental',
        unique_key = 'event_id'
    )
}}
with events as (
 -- event_timestamp will always be < loading_timestamp
  select event_timestamp, loading_timestamp , * from {{ref('events')}}
  {% if is_incremental() %}
    where loading_timestamp >= (select max(loading_timestamp)::date from {{this}})
  {% endif %}
),

{{ deduplicate(""partition by event_id"", ""order by event_timestamp desc, loading_timestamp desc"" ),

--rest of model...
```",17
315,"does-dbt-add-primary-foreign-key-constraints","Hi there,

I'm a newbie to this tool. With what i've read and tested so-far I was under the false impression of what the 'relationships' method of the testing feature does.

I've set relationships between tables and when I run the test command those tests are successful (i.e PASS). However, for those tables in the data warehouse i'm developing as of now, there are no PK/FK constraints - I have defined that in postgres myself or figured out where to define that within the models schema.

 - What does the dbt-tool actually check for in the case of 'relationships'?
 - Where would I define the PK/FK constraints within the model? Have not seen documentation about this, possibly missed that part.

Short snippet of the schema.yml:
version: 2

models:
  - name: customers
    description: A table containing all customers
    columns:
        - name: uid
          description: This is a unique identifier in the customer
          tests:
              # - unique
              - not_null
        - name: suppliers
          description: This is a unique identifier in the supplier table
          tests:
              - not_null
              - relationships:
                  to: ref('customers')
                  field: uid


Thanks in advance!",1
315,"does-dbt-add-primary-foreign-key-constraints","Hey @oferk83, welcome!

dbt is typically used with analytical data warehouses like Redshift, Snowflake, and BigQuery. In these databases, column constraints are usually either unsupported or unenforced by the database. For this reason, dbt has the ability to assert data constraints without actually materializing them in the database.

When you run `dbt test`, dbt will generate SQL queries that assert the validity of the tests that you've defined for your models. You can check out the definition of the [relationships](https://github.com/fishtown-analytics/dbt/blob/dev/wilt-chamberlain/core/dbt/include/global_project/macros/schema_tests/relationships.sql) schema test in the dbt repo. The `unique`, `not_null`, and other tests are all defined similarly.

With this paradigm, you can also test other attributes of your models that can't be expressed as column constraints in your database. Check out some examples of those in the [dbt-utils](https://github.com/fishtown-analytics/dbt-utils#schema-tests) repo.",2
315,"does-dbt-add-primary-foreign-key-constraints","I'm not sure about BigQuery, but for both Snowflake and Redshift if you do define a constraint not only will it not be enforced by the database, but queries may return incorrect results if the underlying data violates the logical rule defined by the constraint.

So it can be risky defining constraints in the database if there is ever a possibility of the data being incorrect. Yes, you would catch it after the fact if a test was defined in dbt, but a business user might have run a query in the interim before you are able to remedy the data problem and gotten incorrect results.

So we have tended to shy away from defining constraints in Snowflake and Redshift when working with them. The risks outweighed the benefits for now, although if things somehow were to change we would revisit the question.",4
315,"does-dbt-add-primary-foreign-key-constraints","Thanks @josh - really good points! 

We don't have any plans to support adding column constraints natively in dbt, but some folks have made this work using post-hooks to add constraints. This might look like:

```
{{
  config(
    post_hook='create index if not exists ""{{ this.name }}__index_on_COL_NAME"" on {{ this }} (""COL_NAME"")'
  )
}}
```

We have a fledgling postgres package that implements these macros, which might be helpful here: https://github.com/fishtown-analytics/postgres

I haven't used these macros before, but might be a good starting point!",5
315,"does-dbt-add-primary-foreign-key-constraints","I didn't see anyone mention it here, but the main case for primary/foreign key constraints in data warehouses is data discoverability.

Two main use cases on the top of my mind:

1. Navigating related data and auto-completing SQL queries become a lot better, in DataGrip at least. These Database clients use the Primary Key and Foreign Key information to auto complete join conditions when writing SQL queries. When you query data, you can use the context menu to select data that match a foreign key column from a query result, for example.

2. Metabase automatically fills in Table metadata for you if foreign and primary keys information is available, it fills out metadata about the tables. If you have that information in place, when you use Metabase query builder, it's able to show to the end user the fields of the joined table so they can be filtered upon.

My thoughts:

Someone wanting to take advantage of that would have to use post hooks to successfully implement that. It's a little bit clunky, but works.

Since what I talked about is mostly about documenting the relationship between models, I think the sensible place for it to be would be in the YAML files, and then we should have an option some how to generate the ""alter table"" statements to make those happen.

@drew Question to you: do we have access to the data in the YAML files when executing post-hooks? I thinking it would be relatively easy to create a macro that you invoke from a post-hook to parse the list of columns and generate those exact ""alter table"" statements.",11
315,"does-dbt-add-primary-foreign-key-constraints","Hey @thalesmello - I really like the way you're thinking about this! I don't believe the yaml file contents are actively available in the hook context, but this is definitely a workflow I'd like to enable.

Presumably we'd want to do something like the following:
1. add a `primary_key` test which effectively just implements `unique` + `not_null`
2. provide some knowledge of the existing tests for a model in its compilation context
3. make it easy to distinguish between views/tables in these hooks: we definitely won't want to add these constraints on views!

Do you buy all of this?",12
315,"does-dbt-add-primary-foreign-key-constraints","I do agree with the things you pointed out.

Regarding your third point, that's something I worked around by checking

{% if this.materialized in ['table', 'incremental'] %}

When I was adding indices in our old Postgres warehouse.",13
315,"does-dbt-add-primary-foreign-key-constraints","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",14
325,"building-a-calendar-table-using-dbt","At GitLab, our data team loves dbt, not just for the power of the tool, but also for the power of the community. All too often, I find myself coming up with a hacky macro only to find that there's already a solution for that in dbt-utils, a dbt package with utility functions that can be used across dbt projects. As @tmurphy wrote in [4 Examples of the power of open source analytics](https://about.gitlab.com/2019/04/15/open-source-analytics/), ""the actual code that you use for analytics isn't your company's competitive advantage"". In fact, most of us are doing very similar analyses- the retention framework is the same, whether you're doing it in customer counts, dollar-weighted, or users in your SaaS product.

A couple of months ago, GitLab rolled out changes to our fiscal year, shifting from the traditional calendar year (January to December) to a new fiscal calendar which starts on February 1 and runs through January 31. As our [writing guidelines](https://about.gitlab.com/handbook/communication/#writing-style-guidelines) outline, all references to quarters and years now *mean* fiscal quarter and fiscal year. All new metrics, not only going forward, but also historically, would need to be updated to fit our new fiscal model. This meant our existing date table, used for easily mapping dates to a myriad of values for analysis, was no longer going to be enough.

## Gathering Requirements

Before just replacing our table, I started by understanding what values the table *actually* needed to have. We used to have ISO dates that were never used. Rather than just try to replicate what existed, I started by understanding what columns we actually needed to have in the table. Which fiscal values and calendar values did we need to support? Where was the last iteration of this table being used in our existing dbt models that I needed to worry about breaking or, at least, updating? Empowered with my list of requirements, I was ready to get starting.

## Not reinventing the wheel

The most important part of a date table is the full series of dates that becomes the core of everything else. Rather than generate it myself, I relied on dbt util's [date spine macro](https://github.com/fishtown-analytics/dbt-utils#date_spine-source), which I've used with great success before. I chose the starting date to be something well before our analysis began and ended it 40 years after today. We do use this table to do future analysis, but I thought 40 years from today would be further than our Finance team is doing any planning.

## Customizing for our use-case

Once the date spine was in place as the core of the analysis, customize the date table to our needs was easy, as I was comfortable with our business logic.  

Our fiscal year definition, for example, is:

>        CASE WHEN month_actual < 2
>             THEN year_actual
>             ELSE (year_actual+1) END AS fiscal_year,

March 2019 is FY2020, and January 2020 is FY2020. January keeps its calendar year as its fiscal year, but for every other month, the fiscal year is one more than the calendar year.

I tested this by picking out a number of dates and writing out what the appropriate value for each of those dates should be *before* comparing the results. They weren't perfect and it took a couple of tries to get it all right, but eventually we were there. We considered adding testing to the date table to be sure that we weren't missing anything, but after concluding that any custom data tests would just spot-check what I manually checked we decided not to.

## Documenting anyway

While this date model may appear to be self-documenting, we wanted to make it easier for our less technical users to understand what they were looking at and what column would hold which pieces of information. In our `schema.yml` file, we define what the column holds and an example of what the value would be for a given date.


If you've got anything other than a traditional calendar as your fiscal year, I hope this can inspire your approach to a date model.

Our dbt docs where you can see more about our date model: https://gitlab-data.gitlab.io/analytics/dbt/snowflake/#!/model/model.gitlab_snowflake.date_details

All of our dbt models (and the git history for each of them!) is available online [in our analytics project](https://gitlab.com/gitlab-data/analytics/tree/master/transform/snowflake-dbt).",1
328,"why-cant-i-use-dbt-seed-for-large-csv-files","Hi,

I have a few rather large but static files that I would like to upload using dbt seed.

More precisely, they are files of legacy data from systems not active anymore, and while it's well possible to host them elsewhere, or write a script to upload them, or any of a myriad of other options, dbt seed just... seems so perfect to use at first glance.

Well, dbt seed seems to be an easy and quick way to get the data where it needs to be, except that dbt seed doesn't seem to like large data files... It starts and says ""Running with dbt=0.13.0"" and nothing else happens*.

So... I suppose my question is why does it not work file files in excess of a few MB? (the largest file in question is 33MB in my case)

*I might add that
1) if I am having some patience, it loads eventually (and once it starts the first model, it loads quickly, too)
2) At least twice I had had some patience, but not enough, and CTRL+c'd... except that my action appeared to trigger the start of dbt seed uploading the data? Might have been coincidences, but the timing was spot on in both those cases...",1
328,"why-cant-i-use-dbt-seed-for-large-csv-files","What database platform are you using? In Snowflake we actually put these files in S3, point an external stage to them, and write code to create the stage for each dbt run and literally query the stage and import the records. Works in 10 or 20 seconds for 50 or 100k records. You do lose the ability to version the files in git directly integrated with dbt, but versioning csv files that are that large is a bit odd anyway.",2
328,"why-cant-i-use-dbt-seed-for-large-csv-files","Hey @Bijan! This is a really good question, and one that comes up frequently. Let me provide some background on the `dbt seed` command which I hope will help answer your question.

### Mapping models

In the early days of dbt, we created ""mapping"" models that were used in the data modeling process. These models looked like:

```sql
-- models/mapping/country_codes.sql

{{ config(materialized='table') }}

select 'DE' as country_code, 'Germany' as country union all
select 'FR' as country_code, 'France' as country union all
select 'BE' as country_code, 'Belgium' as country union all
....
```

In downstream models, we joined to this model in order to map a country code to a country name. Obviously this wasn't ideal, so we created `dbt seed` to help store (and version control) these mapping datasets in csv files instead of sql files. In order for this to work, we needed to make a general-purpose csv loader, but we definitely stopped short of building a full ETL tool.

The `dbt seed` command works by
1. Inferring the schema of your CSV file
2. Creating an empty table in the database
3. Loading data into this table using an `insert into ...` statement

This process works well for *mapping datasets* -- a few hundred rows with a couple of string columns is no problem! Things work less-well when you have complex datatypes (dates, booleans, mixed types, etc). Beyond data type issues, most analytical databases just aren't good at inserting data with `insert into...` statements.

Snowflake, BigQuery, and Redshift all recommend loading data from a blob filestore (like S3, GCS, etc). The problems that you're seeing are a direct result of things like table locks and query compilation, both of which can be avoided by using native database data loading functionality, or some ETL tool that is purpose-built for solving this type of problem.

### Should I load this csv with dbt seed?

In [the docs](https://docs.getdbt.com/reference#seed) we note:
>  `dbt seed`  loads data from csv files into your data warehouse. Because these csv files are located in your dbt repository, they are version controlled and code reviewable. Thus,  `dbt seed`  is appropriate for loading static data which changes infrequently.

I think the version control aspect is important here -- if you'd version control the file, then it's probably appropriate to use `dbt seed` to load it into your warehouse. If the dataset doesn't make sense to version control, then you're probably better off using some other tool!

If you're looking for a rule of thumb, I'd say a csv file that's up to ~1k lines long and less than a few kilobytes is probably a good candidate for use with the `dbt seed` command.

### Other resources for loading CSVs into a warehouse
- [Stitch S3 CSV](https://www.stitchdata.com/docs/integrations/databases/amazon-s3-csv)
- [Fivetran CSV Uploader](https://fivetran.com/docs/files/browser-upload)
- [Redshift COPY INTO](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html)
- [Snowflake Stages](https://docs.snowflake.net/manuals/user-guide/data-load-s3-create-stage.html)
- [BigQuery Loading Data](https://cloud.google.com/bigquery/docs/loading-data)

If I missed any good resources, please link them below!",3
328,"why-cant-i-use-dbt-seed-for-large-csv-files","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
331,"best-practices-on-deploying-bi-reporting-tools","Does anyone have guidelines/ best practices / resources around ""deployment""  of reports via Mode/ Looker/ Periscope/ etc? 

Mostly curious how teams are validating changes and testing, especially those that might be client-facing (white labeled, for instance). There is a sentiment that we shouldn't expect analysts and other parties who might be editing the reports to follow a traditional dev lifecycle (which I get to some degree), especially considering the fast iterations of reports that are possible with these tools.

It seems like this is a problem across various vendors, and editing a report on the fly has a wide range of potential down-stream effects (bad queries, bad visualizations, etc, etc). 

How are teams currently working to address this issue?",1
331,"best-practices-on-deploying-bi-reporting-tools","This is something we've thought a lot about and unfortunately no vendor that I know of currently supports the solution we want. (We here being the GitLab data team).

We're heavily biased towards a git-based workflow. My ideal world would have the entire BI tool based around git branches with a production branch being the main one that people consume. Ideally, any edits would happen on a branch specific to a user (or a named feature) branch and once an editor is happy with the changes they can submit for their branch to be merged back into production. This would trigger a review process from our team where we would check for all the things you'd want to check when reviewing a dashboard.

We're currently on Periscope and I believe something like this is on their roadmap. Mode has suggested they'd be looking at this sort of workflow as well. We used Looker previously and their git integration does allow for branching and merge requests to happen, but not for any of the actual dashboards. We felt this was pretty good though since as long as the explores were set up well then we would feel confident that folks wouldn't get bad results. 

Our current process now requires some education and training for people to submit issues to have their dashboard reviewed. (We have an issue template [here](https://gitlab.com/gitlab-data/analytics/blob/master/.gitlab/issue_templates/Periscope%20Dashboard%20Review.md).) It works well for us, but that's partially because we already have a strong culture of code review at GitLab.

Hope this helps!",2
331,"best-practices-on-deploying-bi-reporting-tools","The separation between dev and prod environments was actually one of the main reasons we switched from Mode to Looker. Allowing each analyst to work on their own branch and confirming things don't break prior to merging has helped us mitigate a lot of issues around broken reports. The current workflow looks something like:

1. Build and test dbt models locally
2. Change references to tables in Looker to point to your local schema
3. Run the Content Validator in Looker to ensure that everything still works. This is great for catching other reports that depend on the table you just changed.
4. Change references back to prod schema, and deploy to master

This workflow is still a little clunky, but it's worked pretty well for us so far. There are ways to programatically manage the schema reference switching between dev and prod, outlined in another discourse post [here](https://discourse.getdbt.com/t/looker-user-attributes-and-if-dev-to-auto-switch-between-dev-prod-schemas/54), but that didn't work too great for us.

We also recently added a script that regularly validates all our user-facing content via the Looker API, and posts any errors in our #data-qa slack channel so we can quickly fix. Getting that in place has helped catch issues that slip through the cracks when deploying.

Hope that helps! We are definitely figuring out the best process for this ourselves, so I'm very curious to see how others have approached this problem.",3
331,"best-practices-on-deploying-bi-reporting-tools","@arvindr â€“ I would love to see that script! You should totally consider open sourcing it :) 

@JoeNaso â€“ re: your original question:

First of all, I have been in this situation more times than I wish to admit. The first thing that I recommend is creating a culture that helps you debug ""broken"" or ""wrong"" dashboards quickly, to help you understand _what exactly has changed_!  We have [a guide on reporting bugs](https://github.com/fishtown-analytics/corp/blob/master/Writing%20bug%20reports%20in%20analytics.md) as well as an [issue template](https://github.com/fishtown-analytics/dbt-init/tree/master/starter-project/.github) that we share with any BI users that are reporting issues.

^That's obviously a reactive strategy, and long-term it's better to be proactive, so my $0.02AUD on this:

I think there's two types of changes that can impact a dashboard:
1. The data model that the dashboard selects from has changed
2. The dashboard structure has changed

These are pretty distinct problems, so I'll address them separately.

**1. The data model that the dashboard selects from has changed**
From a process perspective, here are some suggestions:
  * Implement a [PR template](https://help.github.com/en/articles/creating-a-pull-request-template-for-your-repository) that prompts dbt users to indicate if any existing models are changing â€“ you can see our standard template [here](https://github.com/fishtown-analytics/dbt-init/blob/master/starter-project/.github/pull_request_template.md#changes-to-existing-models). This small change can have big impacts.
  * Check the ""before and after"" of any major dashboards â€“ no matter which BI tool we are using, we find a way to parameterize the schema we select from, so that we can easily do this.
  * Write some audit macros to validate changes! I have some macros that compare columns across two relations, and often use a version of the [equality](https://github.com/fishtown-analytics/dbt-utils#equality-source) macro to do record-by-record validation. Pro-tip: write the audit query _before_ you make your changes; you'll thank yourself at the end :relieved:. Also check out [dbt-helper](https://github.com/mikekaminsky/dbt-helper)'s `compare` command â€“ I'm pretty sure you can use this to detect model name changes, and it can probably be extended to also compare column changes (I'm sure @michael-kaminsky would accept a PR!). 
  * If your data model changes affect an end user, they should be involved in the PR process as a reviewer!
  * If your BI tool has a git integration, every dbt PR should have a complementary PR on the repo containing your BI code (or you could even have them both in the same repo! I haven't done this, but it's not a terrible idea!)
  


**2. The dashboard structure has changed**
i.e. the fields the dashboard selects, the visualizations it uses, or the filters it applies.

The answer to this one ultimately depends on the BI tool you're using, and I think the above responses do a really good job at covering this, so I won't repeat what they've said.

One point I'd like to make here is that sometimes dashboards change/mistakes creep in because the data model an analyst is querying isn't intuitive/well documented. Strong data model design, and good documentation can go a long way on this front, which of course, dbt helps a lot with.

A few extra points if you do use Looker:
* Looker offers LookML dashboards that are written in code, so can also be version controlled. I've seen some orgs have ""Source of Truth"" dashboards in LookML, and all non-Source of Truth dashboards as user-defined dashboards
* Alternatively, you can use Looker's permissions to ensure that only authorized users are allowed to edit a user-defined dashboard.
* I find that Looker can abstract away the git workflow too much â€“ as an analyst whose first introduction to git was via Looker, I definitely learned bad git habits. I think it's worth spending the time to go through basic git workflows with your analysts (especially if you want to include them in the PR process, as outlined above), and (as you might expect) we've open sourced our git guide [here](https://github.com/fishtown-analytics/corp/blob/master/git-guide.md).",4
340,"pulling-in-slow-changing-manually-entered-data","So we have a need to capture occasional data points - for now, future sales and marketing events - so we can incorporate those in our forecasting models. 

This sort of data doesn't get updated often, but when it does, the sales & marketing teams need a way to enter their event info somewhere (e.g., ""Marketing Event, June 27, 2019, Los Angeles, CA"") so I can pull that into our data warehouse (Snowflake). 

We use Stitch for data loading. I thought about using a Google Form, and then pulling via [Stitch's hacky Google Sheets integration](https://www.stitchdata.com/blog/how-to-replicate-google-sheets-to-your-data-warehouse/), but I've tried that before and it didn't work very well. 

I think a solution involving S3 + Snowflake/Snowpipe could work, but I'm not seeing Google Forms-like products that would save or automatically export data to S3. Any ideas/suggestions?

_(Also - We're also implementing Salesforce sometime in the near future so maybe they could enter stuff in a Salesforce object or create a calendar event or something, and that would be easier to pull in via the Stitch+Salesforce integration. But we need a solution in the meantime, and I imagine this is a problem most data teams have!)_",1
340,"pulling-in-slow-changing-manually-entered-data","We've used Stitch's Google Sheets integration dozens of times across dozens of clients--it's a core part of our workflow! We've found it to be quite reliable. What problems are you running into with it?",2
340,"pulling-in-slow-changing-manually-entered-data","We have a custom Google Sheet / csv uploader that works with Snowflake. Docs are here https://gitlab.com/gitlab-data/analytics/tree/master/extract/sheetload",3
340,"pulling-in-slow-changing-manually-entered-data","INTERESTING! ðŸ‘€ Maybe I did something wrong. It's been a while since I last tried this, so let me check my notes about what happened... 

Okay, it had something to do with unique keys - I needed to combine them using a md5 function, and it kept hanging and then creating multiple rows. [I filed an issue about it](https://github.com/stitchdata/google-sheets-integration/issues/12) and found an alternative function, but I don't think I ever got it to work well. This was the last error msg I got from Stitch re this integration: 

```
NULL result in a non-nullable column
  File '229649-2644895918347462805.tab.gz', line 1, character 1
  Row 1, column ""STAGING_SHEET1_13_LOADER_SNOW2_11350_F3EF50D4_3F16_4CD1_ACA9_E860AC2D56AB""[""VI_NUMBER__ST"":20]
```

I'm thinking it's the spreadsheet that was the problem, so I'll give this another shot with a cleaner spreadsheet.",4
340,"pulling-in-slow-changing-manually-entered-data","Also, @tristan, I wasn't sure if the sync, once set up, is continuous or if I have to manually kick off a sync job by going to the Stitch menu in Google Sheets and selecting ""Sync with Stitch."" Would you know?

Also [filed an issue](https://github.com/stitchdata/google-sheets-integration/issues/13) about that but no response.",5
340,"pulling-in-slow-changing-manually-entered-data","You have to manually click ""sync with stitch"" when you want to push new data. I actually prefer that process, because I as the owner of the spreadsheet want to control when data in that spreadsheet gets pushed over to my warehouse. There are many intermediate states where I wouldn't want that data pushed over! But you can't, for example, create automatically updating spreadsheet functions and just have them constantly pulled in once a day.",6
340,"pulling-in-slow-changing-manually-entered-data","[quote=""tristan, post:6, topic:340""]
automatically updating spreadsheet functions and just have them constantly pulled in once a day
[/quote]

That's what I was hoping for, but I can live with manual syncing. :) I've set up a test and it is syncing correctly to Stitch. Thanks!! 

I am also looking for suggestions about snazzy online forms that snazzily save results in S3, that'd be good to know...",7
340,"pulling-in-slow-changing-manually-entered-data","We use Fivetran's GSheets connector for this and it works amazingly well. Totally worth the $3,000/year connector fee in my opinion.",8
340,"pulling-in-slow-changing-manually-entered-data","@tristan, it's working really well (which confirms my suspicions that when I tried to set it up last fall, it was on a legacy, badly-formatted spreadsheet). I notice to be safest it's best to filter by rows containing the max _SDC_RECEIVED_AT or _SDC_SEQUENCE value present in the table. Is that how you set it up as well?",9
340,"pulling-in-slow-changing-manually-entered-data","If you're on BQ, that'll be how Stitch sends the data. stitch treats BQ as a write-only data store for all integrations as far as I know.",10
340,"pulling-in-slow-changing-manually-entered-data","We use Zapier to push new or updated spreadsheets within a Google Drive folder to S3. By using a template with strict data validation, this seemed to be the best way to get data into our DW. We then use a Dataiku job to merge the docs from S3 and pipe the output into an S3 destination, which is set up as an external table in Snowflake.

It has a slightly higher setup cost but is fully automated and lets our sales team take ownership of their data (if they want to update a file, they just edit it and then it'll come through via Zapier and replace the old version). This is also handy since we can't deal with just a few sources - we have new files being added that need to automatically come through.",11
340,"pulling-in-slow-changing-manually-entered-data","Hello, [dan](https://discourse.getdbt.com/u/dan)

Yes, You're right I agree with you - *(Also - Weâ€™re also implementing [Salesforce](https://mindmajix.com/salesforce-tutorial) sometime in the near future so maybe they could enter stuff in a Salesforce object or create a calendar event or something, and that would be easier to pull in via the Stitch+Salesforce integration. But we need a solution in the meantime, and I imagine this is a problem most data teams have!)*. Can you please explain how may this help us?

Regards,
Srija.",12
360,"how-to-set-up-a-google-sheet-as-a-reliable-data-source","_This post is adapted from a more in-depth Medium article I wrote [here](https://towardsdatascience.com/google-sheet-data-warehouse-c22bb2cce4b0)._

I occasionally need to grant a non-technical colleague the ability to input information into our data warehouse on an ad-hoc basis. For example, our customer service team at [Milk Bar](https://milkbarstore.com/) maintains a list of special wedding cake orders in Google Sheets that we need to collect data from for downstream calculations.

This is a tricky problem: I'm not a web developer, and standing up and maintaining a web form or a database for something like this is overkill. I'm also not interested in paying for a separate product to do this for me. What if we could sync their Google Sheet directly to a table in our data warehouse?

I know what you're thinking. What about data governance? Whatâ€™s to stop someone from accidentally deleting a column or entering data of the wrong type? How do I know that my colleagues are entering data according to my expectations?

In my experience, with the right guardrails and protections, Google Sheets can be a reliable and flexible component of your data stack. Here are my tips for setting one up properly as a data source.

## Securing your Google Sheet

### Permissions

Start by locking down your sheet so that it can only the right people can view and edit it. I like to check the Advanced option ""Prevent editors from changing access or adding new people"" to prevent inadvertent link sharing by my end users.

### Protected sheets and ranges

You can also lock down tabs or ranges within your sheets so that only you or authorized users can edit them. You can even set an option to warn users that editing a cell in a specific range may have consequences and they should proceed with caution.

You might have some sheets that contain necessary information for formulas or calculations that donâ€™t need to be edited by any of your users. If thatâ€™s the case, protect these sheets and ranges so only you can modify them.

### Data validation

Data validation (accessed via Data > Data Validation) is the most important requirement for Google Sheets usage in the data stack. With data validation, you can set conditions on ranges within your sheet that must be fulfilled for data entry to be accepted. **Data validation is how you can essentially enforce schema control on a Google Sheet.**

For instance, let's say you have a currency column that must have a positive value. With data validation, Google Sheets will reject any negative numbers entered into the validated range.

There are a number of useful data validation criteria to choose from:

* ***List from a range.*** This criterion requires any entry in our cell range to match the value of a cell in a validation range. We often use it to force the user to choose from a list of predefined values or categories. As a handy bonus, you can add a drop-down list to each cell in your range so your user can pick a valid value directly. I like to keep a separate, fully protected sheet (only I can edit it) for my validation range.
* ***List of items.*** Similar to List from a range, except you provide the valid values directly as a comma-separated list. This is great for validating entries against a short list of static values.
* ***Number.*** This criterion is used to check that entries fall within a numeric range. We can prevent negative numbers or force entry to be less than a known maximum.
* ***Text.*** We can test entries to see if they contain or do not contain certain sub-strings. Perhaps you want to make sure your users are entering a proper prefix when they copy over invoice numbers. There are also a couple of useful options at the bottom of the drop-down: is valid email and is valid URL. These are both common validation needs that would require regex or something similarly complex to confirm.
* ***Date.*** We can validate that entries are valid dates and fall within a required range. Because of the multitude of date formats, dates are notoriously messy. This validation criterion is a vital if your users are entering dates anywhere in your sheet.
* ***Custom formula is.*** This is the real game-changer for data validation. You can write any custom formula that returns a True or False value and Google will validate entries against it. You can implement some really complex validation logic this way. For example, use the `REGEXPMATCH` formula to confirm that entries match a more complicated set of string logic. Google Sheets also doesnâ€™t allow you to apply multiple validation criteria to the same cell, so this option allows us to combine criteria into a single formula.
* ***Checkbox.*** You can replace the cell(s) in your cell range with a checkbox that returns a custom value when it is checked and unchecked. True and False will probably account for 99% of the use cases here, but itâ€™s nice to have the flexibility to represent something else if you want to.

Once youâ€™ve thoroughly validated the data in your Google Sheet, your sheet will be just as good as a custom web form, if not better. You can be very confident in the quality of your data.

### Optional: Using an IMPORTRANGE intermediate sheet

Instead of syncing directly from the user-accessed sheet to the data warehouse, I like to set up an intermediate import sheet that only I can edit. This intermediate sheet provides me with a bit more control. I configure this sheet to copy all the data from the user-accessed sheet using the `IMPORTRANGE` [formula](https://support.google.com/docs/answer/3093340?hl=en). This sheet is where I can make any necessary adjustments before I pass it to my data warehouse.

* For example, I might only import a specific range of the user-accessed sheet, so errant data outside of that range doesnâ€™t break anything downstream.
* I could also union two user-accessed sheets (maybe there are multiple sheets, one for each year) into a single sheet with a [vertical array](https://stackoverflow.com/q/31657470/3524951).
* I can even use the `QUERY` [formula](https://support.google.com/docs/answer/3093343?hl=en) to filter the rows in the user-accessed sheet to remove blanks.

### Alternative: Use a Google Form + Google Sheet instead

If users donâ€™t need to edit data after they enter it, you can set up an append-only system using a Google Form. Google Forms has a native integration with Google Sheets, which makes this setup dead simple for append-only needs. Google Forms will append a row for each response to your form in a timestamped Google Sheet. If you add questions, the sheet will expand to match your form.

Ensure clean data by choosing the appropriate question types for your form.

## Syncing to the data warehouse

Loading a Google Sheet into a data warehouse is a common problem, so there are a lot of potential solutions.

If youâ€™re a Google BigQuery user, the easiest approach, *by far* , is to set up the Google Sheet as an external table. BigQueryâ€™s documentation [describes how to do this](https://cloud.google.com/bigquery/external-data-drive) in more detail, but all you need to do is define a schema for the table and you can query the first tab of your sheet directly from BigQuery.

If youâ€™re using Snowflake, Redshift, or another data warehouse, you can take your pick from these other options.

* Most ***data loading services*** have an integration that handles loading Google Sheets. See [Fivetranâ€™s documentation](https://fivetran.com/docs/files/google-sheets) and [Aloomaâ€™s](https://www.alooma.com/docs/connect-your-data/google-drive-and-sheets/google-sheets-integration).
* [***Stitchâ€™s Google Sheets integration.***](https://www.stitchdata.com/blog/how-to-replicate-google-sheets-to-your-data-warehouse/) Data loading service Stitch has a guide on how to set up some Apps Script code in Google Sheets that will send data to Stitchâ€™s Import API. Stitch will then load that data into your data warehouse of choice. With this approach, you have to sync the data manually by clicking â€œsync to Stitch,â€ which may be a blessing or a curse depending on your needs. Note that you will have to sign up with Stitch if youâ€™re not already a customer, but syncing a few sheets isnâ€™t likely to exceed their 5 million rows/month free tier.
* [***sheetload.***](https://gitlab.com/gitlab-data/analytics/tree/master/extract/sheetload) This is a Python tool created by the data team at [GitLab](https://gitlab.com/) to load local CSVs and Google Sheets into a data warehouse. At the time of writing, the tool supports Postgres and Snowflake.
* [***Roâ€™s Google Sheet integration.***](https://github.com/Ro-Data/Ro-Create-Table-From-Sheet) The data team at [Ro](https://ro.co) open-sourced their custom approach to loading Google Sheets data into Snowflake. Note that their approach truncates and replaces the data table on every load, so it wonâ€™t preserve deleted or modified rows.
* You could also look into rolling your own modular approach with [Zapier](https://zapier.com/), webhooks, and a serverless function like AWS Lambda. You could trigger your Zap on row creation in your sheet which would send the data, via a webhook, to a serverless function set up to receive the payload as an endpoint. This approach becomes more challenging if you need to capture updates and deletions.

Do you use Google Sheets in your data stack? If you do, add a comment with your approach and any tips or pitfalls you've encountered!",1
360,"how-to-set-up-a-google-sheet-as-a-reliable-data-source","If you're importing from Google Sheets from Google Drive with BigQuery and using an automatic schema, is there any process where you can have the schema automatically update, or even get updated at all manually?

We were using a Google Sheet as a source, and business shuffled columns around and added some new ones, which broke all of the reports. Of course, we can lock this down and use a new sheet, but then all of the reports would have to be updated. It'd be ideal if you could reorder columns or add new ones without disruptions as long as the header names stayed the same, but it seems like the schema is locked after you create the table. Is that right?

One possibility might be to create a view that can just be updated, acting like a symlink. Then all reports can use that view, so we can update in one place.",3
360,"how-to-set-up-a-google-sheet-as-a-reliable-data-source","Great writeup. For the data entry interface it might be interesting to use something like [Glide](https://www.glideapps.com/)â€”although Glide is actually optimized for mobile.",4
364,"currency-conversions","I am trying to understand the best practices for handling currency conversion rates

- [ ] How do you handle currency conversions?
- [ ] For reporting, do you lock the currency conversion rates by day/month/quarter?
- [ ] If the rates are not locked and timestamped, do you always use the current market rate and apply it to all the metrics I.e. even the historical records?
- [ ] Do you lock the currency rates after the Finance team books the numbers for the month?

I intend to create 4 columns -
```Currency code,
metric in local currency, 
currency conversion rate (at that point in time),
metric converted to corporate currency```

I want to know how people in this community are handling it.",1
364,"currency-conversions","At a Fintech I worked for we used the ECB rates to set daily Exchange Rates (not just in BI but actually in the production environment as well): https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html

Corporate currency was Euro, so that made sense. I suppose you can use them for any currency that is offered, but there is probably a similar source for other central banks/currencies. The ECB exposes its currency rates in an XML format* as well if you want to write the ETL yourself, but there is a python library somewhere that you can invoke to get them without hussle.

Thus, you have a daily rate. Of course you also need to have a logic that decides which date to use for any particular row of your data.

*For the XML, go to a particular currency and you find a link in the bottom right corner ""Alternative data format, XML (SDMX-ML) data file""",2
364,"currency-conversions","@Bijan I can start storing currency rates per day, however, I am not sure which rates should be used for month-end.quarter-end or year-end reporting.",3
364,"currency-conversions","When you report facts as of end of periods, I would use the rate at the last date of the period.",4
364,"currency-conversions","In my opinion, rate of the last date of the period must be used for month-end reporting. 

For quarter/year end reporting, all the metrics must be derived/aggregated from month-end metrics.  

With this approach, the currency fluctuations won't drastically impact quarter/year end reporting.",5
364,"currency-conversions","

# Which exchange rate do you use? Day/month/quarter?
* It depends. If these reports are being used by your finance team, _ultimately it is up to the finance team_ to decide which exchange rate to use, and for the data team to implement it. That's because currency conversions have a real impact on their reports! For example, in Australia, it's regulation that accountants use the month-end currency exchange rate for financial reporting.
* If your finance team truly doesn't have strong opinions on this, my preference is to use a daily exchange rate.


# How do you model this?
## Source data
We generate a table of exchange rates in our data warehouse. Currently, it's a (sub-optimal) process of building a Google Sheet that uses the `googlefinance` function to grab exchange rates, and loading the Google Sheet into our warehouse.

Ideally, I'd like to use a Singer tap on Stitch for this, but there currently isn't a reliable tap to do this.

## Build a model of exchange rates
I would build a model with one record per currency, per day, with the fields:
  * date
  * source_currency: ISO currency code, e.g. AUD, GBP, USD
  * target_currency: ISO currency code, e.g. AUD, GBP, USD
  * rate: exchange rate to convert 1 unit of the source currency to the target currency

Any business logic on which exchange rate to use should be included in this model.


## Perform the conversion
I would convert prices at the lowest granularity possible (i.e. on the `orders` table, rather than a rollup of `daily_revenue`).
For each model with multiple currencies, I tend to store:
```txt
price_local
currency # the ISO code for the currency, e.g. AUD, GBP, USD
price_usd # or whatever the target exchange rate is
```
This is what the SQL would look like to convert to USD. Note that I've done it in a way that can be extended if you wish to convert to other currencies too.
```
with orders as (
    select * from {{ ref('orders') }}
),

exchange_rates as (
    select * from {{ ref('exchange_rates') }}
),

select
    orders.order_id,
    orders.price as price_local,
    orders.currency,
    orders.price * usd_exchange_rates.rate as price_usd
    
from orders

left join exchange_rates as usd_exchange_rates
    on usd_exchange_rates.target_currency = 'USD'
    and date_trunc('day', orders.ordered_at) = usd_exchange_rates.date
    and orders.currency = usd_exchange_rates.source_currency

```
Things to be mindful of:
* The timezone for conversion - it's up to your business to decide the logic here
* Accidentally using the inverse rate (I've done this more times than I care to admit)

# How do you handle currencies in your BI tool?
One thing to be really conscious of is that you want to **prevent your users from trying to aggregate the `price_local` field**. It makes _no sense_ to average an order of $20USD and another order of $15GBP!

To do this, I recommend ensuring that currency is added to any BI queries that reference the `price_local` field â€“ on Looker it might look like adding `currency` as a [required field](https://docs.looker.com/reference/field-params/required_fields) to any of your measures that reference `price_local`.

On other BI tools where users can query the underlying data model without the guard rails of LookML, the only way to truly prevent this from happening is to remove the `price_local` field and instead create a column for each currency, filling the columns with NULL when the record is not in that currency, e.g.:
```sql
select
  case when currency = 'USD' then price_local as price_local_usd,
  case when currency = 'GBP' then price_local as price_local_gbp,
  case when currency = 'AUD' then price_local as price_local_aud,
  ...
```

^ This approach isn't very sustainable as your company grows though. 


Advanced things you can do in Looker:
* If converting to multiple target currencies, you can do some trickery with using [parameters](https://docs.looker.com/reference/field-params/parameter) that are added as an [always_filter](https://docs.looker.com/reference/explore-params/always_filter), so users can switch their reports between USD, GBP etc, without having to choose different measures. Kind of related is [this article](https://discourse.looker.com/t/using-looker-parameters-for-dynamic-timezones/9060) I wrote a while back on a similar pattern for timezone conversion.
* You can use custom formatting to get the right currency symbol for a price field, as described in [this article](https://discourse.looker.com/t/creating-a-formatted-money-value-with-a-dynamic-currency-symbol/1204)",6
364,"currency-conversions","Thank you so much @claire. This is very helpful, I will review it with the finance team and ask for their opinion.",7
364,"currency-conversions","@claire I am attempting to join similar data to your example, but there are cases where the system (Netsuite / Xignite) we are pulling rates from do not have a rate for every date. Is there a way to make that join grab the latest rate if the exchange rate doesn't exist for that specific date?",8
364,"currency-conversions","Hey @ryanonymous :wave:

There's two options here:

**:one: Create a `valid_to` column in your `exchange_rates` model, and then use an inequality join in `orders`.** 
To create the `valid_to` column, you need to check the _next_ record, which you can do by using the `lead` function. 
Depending on your SQL dialect, your exchange rate model might look like:
```sql
select
  source_currency,
  target_currency,
  rate,
  date as valid_from, 

  -- use a window function to find the next record
  lead(date) over (
    partition by source_currency, target_currency
    order by date
  ) as next_record,
  
  -- coalesce this with today's date so your most recent record has a valid_to date
  coalesce(
    next_record,
    current_date::date
  )
from exchange_rates
```

Then in your `orders` model, use something like:
```sql
select
   ...
from orders

left join exchange_rates as usd_exchange_rates
    on usd_exchange_rates.target_currency = 'USD'
    and orders.currency = usd_exchange_rates.source_currency
    -- order happens on or after valid_from
    and orders.ordered_at >= usd_exchange_rates.valid_from
    -- order happens before valid_to
    and orders.ordered_at < usd_exchange_rates.valid_to
```

**:two: Use an ""all days"" table to get your  `exchange_rates` model to have one record per day.**
We call this ""date spining"", and you can see an article about it over here :point_right:   https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265",9
402,"building-dbt-models-to-be-compatible-with-multiple-data-warehouses","Our Company uses both Redshift and BigQuery for our Datawarehousing needs. Due to internal contracts we need to maintain the similar models within both environments. 

We understand the platform differences with data modeling, but if we needed to keep a single dbt model for both database platforms what would be the best options.

Here is a link to the slack thread where this was discussed.
https://getdbt.slack.com/archives/C0VLZPLAE/p1559582239052500",1
402,"building-dbt-models-to-be-compatible-with-multiple-data-warehouses","## The best solution is no solution..
Even though this is not applicable to your use case, it needs to be said here (in case others are looking at this):
The best way to solve this problem is to avoid it! _Unless_ you are trying to maintain parity across both warehouses, **I strongly recommend using two separate dbt projects.**

Further, I don't recommend maintaining parity across two warehouses â€“ ""source of truth"" becomes very messy quickly!

We sometimes see this need arise when someone is maintaining a legacy warehouse while transitioning to a new one. In this case, I'd also recommend _not_ trying to share models across two warehouses, and instead trying to move your dbt project over in one go!

But, since it can't be avoided in this case, it's worth sharing some wisdom we've collected when building for compatibility across multiple warehouses :) . This advice will also likely be useful for anyone trying to build [packages](https://docs.getdbt.com/docs/building-packages)!

##  Challenges when modeling on different warehouses
You'll likely encounter the following challenges:
1. The SQL syntaxes vary slightly across warehouses
2. Your data may be loaded in a different structure
3. A performant model design varies across warehouses
4. Performance levers in dbt vary across warehouses

## 1.  The SQL syntaxes vary slightly across warehouses
### Why is this an issue?
Some of the non-ANSI functions vary across warehouses.
For example, on Redshift, to calculate the difference between two dates, you might do:
```
datediff('day', first_date, second_date)
```
But on BigQuery, you need to use a slightly different function, _and_ reverse the order of your args :exploding_head:
```
datetime_diff(second_date, first_date, 'day')
```
And on postgres this function doesn't exist at all. Bonkers!

### Using dbt-utils / adapter macros for cross-database SQL functions
Fortunately, you can use [macros](https://docs.getdbt.com/docs/macros) to handle the logic of which SQL function you should use. Instead of writing the SQL, you can call a macro which returns the correct SQL for your warehouse, like this:
```
{{ datediff(""first_date"", ""second_date"", ""'day'"") }}
```
^ This assumes that you have a `datediff` macro that would return the correct SQL for your warehouse.

Even more fortunately, we've written a whole bunch of cross-database compatible macros in our [dbt-utils](https://hub.getdbt.com/fishtown-analytics/dbt_utils/latest/) package! (Just quietly, we think this is the best repository of knowledge around the syntactical differences between warehouses out there.)

You can leverage this work by [installing](https://docs.getdbt.com/docs/package-management) dbt-utils as a package, and then using the utils functions whenever you have SQL that needs to differ across your warehouses.

If there is a specific SQL function that we haven't written a macro for, we welcome PRs! Or you can also add your own macro to your project, following the `adapter_macro` pattern we use in `dbt-utils`.

One thing to note though is that there's a trade-off to be aware of here: using macros can reduce the readability of your models, and quoting strings correctly can be confusing (see above), so be mindful of using them too much.

## 2. Your data may be loaded in a different structure
### Why is this an issue?
If you use off-the-shelf products (Stitch, Fivetran) to load data into your warehouse, these services may load the data slightly differently. I have more experience with Stitch, so I'll speak to the differences they have across warehouses as an example:
* Stitch loads data into BQ in an ""append only"" fashion
* Stitch unpacks/flattens JSON objects on Redshift (since Redshift doesn't handle JSON well) but may not on other warehouses
* Timestamp types might differ across warehouses


### Using Jinja directly in your model
If a column name, or type, varies slightly across your two warehouses, I'd handle this with an `if` statement in your model:
```sql
select
{% if target.type == 'redshift' %}
    my_redshift_column as my_column,
{% elif target.type == 'bigquery' %}
    my_bq_column as my_column,
{% else %}
    {{ exceptions.raise_compiler_error(target.type ~"" not supported in this project"") }}
{% endif %}
    my_other_column

from my_table
```

On the Stitch front, @dylanbaker wrote some macros to handle the different way Stitch loads data in each warehouse, check them out [here](https://github.com/dbanalyticsco/facebook-ads/tree/master/macros/stitch). 

### Using separate models with different `enabled` options
If there is a huge structural difference between the data that gets loaded into each of your warehouses, it _may_ be worth splitting the SQL for the models into _two separate_ files.

A handy tip here is that you _can_ have two models with the same name, so long as only one of them is enabled. A pattern weâ€™ve started using lately is to use a jinja variable to determine whether a model is enabled or not, as seen [here](https://github.com/fishtown-analytics/facebook-ads/blob/master/models/router/fivetran/fb_ad_creatives.sql#L3).

You might create a directory of ""staging"" models for each warehouse (see [this article](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355) for more details on how we structure projects):
```
â”œâ”€â”€ dbt_project.yml
â””â”€â”€ models
    â””â”€â”€ staging
        â””â”€â”€ braintree
            â”œâ”€â”€ bigquery
            |   â””â”€â”€ stg_braintree__customers.sql
            â””â”€â”€ redshift
                â””â”€â”€ stg_braintree__customers.sql

```
Then in each model define the following [1]:
```
{{
    config(
        enabled = target.type == 'redshift'
    )
}}
```
### Using adapter macros (not recommended)
Most of our packages use a pattern where each model calls a macro, and then the macro uses the `adapter` pattern to choose which SQL to use (for example, our [Snowplow](https://github.com/fishtown-analytics/snowplow/tree/master/macros/adapters) package).

However, we've found that this _severely limits_ the readability of your code, so I'd advise against following this pattern. We are probably going to move away from it in future iterations of these packages!


## 3. A performant model design varies across warehouses
Credit to @josh for pointing this out.

### Why is this an issue?
Different warehouses favor different model designs â€“ for example, BigQuery pushes in favor of denormalization, so you should leverage nested objects rather than joins. Redshift doesn't easily unnest objects, so they should be flattened first.

If you want to maintain parity between your two warehouses, you'll have to choose which warehouse you'll design for and make it the first class citizen, and then just squish your other warehouse to work with that data structure.

## 4. Performance levers in dbt vary across warehouses
### Why is this an issue?
The way to make models performant varies:
* Redshift uses sort and dist keys on tables
* BigQuery uses date partitioning
* Snowflake works the way you expect without any additional configuration

As such, the configs you supply to your dbt model will differ. For example, a Redshift model might have:
```
{{
    config(
        materialized='table',
        sort='created_at',
        dist='user_id',
    )
}}
```

Whereas a BigQuery model might have:
```
{{
    config(
        materialized='table',
        partition_by='date(created_at)',
        cluster_by='user_id'
    )
}}
```

### Supplying all the configs to shared models
Since dbt doesn't mind if you supply a `config` that it doesn't recognize, I would just add _all the things_  (i.e. performance configs):
```
{{
    config(
        materialized='table',
        sort='created_at',
        dist='user_id',
        partition_by='date(created_at)',
        cluster_by='user_id'
    )
}}
```



----
[1] In an ideal world, you would be able to enable your projects in your `dbt_project.yml` file:
```models:
  my_project:
    staging:
      braintree:
        redshift:
          enabled: ""target.type == 'redshift'""
        bigquery:
          enabled: ""target.type == 'bigquery'""
```
However, because when dbt parses the `dbt_project.yml` file it doesn't yet know which profile to use (since the profile info is stored in that file!), it also doesn't know about your `target.type` so won't be able to parse this correctly.

There's a related [github issue](https://github.com/fishtown-analytics/dbt/issues/1255), feel free to :+1: it!",2
402,"building-dbt-models-to-be-compatible-with-multiple-data-warehouses","Use a pre dev branch when you create a pr and commit that commit will be executed against 2 small dev project one for bq and another for rs.... in case of one of the 2 breaks you need to refactor that code and move it to a macro to handle both specific cases with specific syntax ... in this way you can in theory keep all the code in 1 prj",3
410,"sharing-my-sql-style-guide","Hey all - I put together a style guide outlining the preferences I've adopted over the years for how to format SQL queries. I [shared it in dbt Slack](https://getdbt.slack.com/archives/C0VLNUUTZ/p1560449579075400) and Claire recommended I share here as well.

You can check it out on GitHub here: [Mazur's SQL Style Guide](https://github.com/mattm/sql-style-guide).

Feedback welcome on any of it, especially parts where you've adopted different conventions. Cheers!",1
410,"sharing-my-sql-style-guide","Very nice style guide. Since you asked for feedback, I'll share mine. 

I prefer ""<>"" over ""!="" for not equals since the former is ANSI SQL-92 and latter is not. It is also very easy to type.

I ""group by numbers"" all the time. I find it very convenient and I have not heard a good reason (yet) as to why I should not do this. :)  I always follow ""Grouping columns should go first"" which I think makes this approach ok.

What is the reasoning for "" Commas should be at the the end of lines""? I think there are pros/cons for either approach on this one; I used to put commas at the beginning, but lately have been putting them at the end. 

For anything noted as ""bad"", it may be good to explain why. Some were obvious to me, some were not.

Thanks for sharing! Nice work.",2
410,"sharing-my-sql-style-guide","Hey @davemasino, thanks so much for this thoughtful feedback.

Regarding <> vs !=, I didn't realize the former was a ANSI SQL-92 standard. TIL. I probably won't update change the guide it simply because I don't think it really matters one way or the other, and I just prefer <> is all.

I hear you on grouping by numbers too. I go back and forth on it, as the commit history of the guide would show for anyone who ever checks it out :). Might wind up going back to ""numbers are fine"" in time, we shall see.

Regarding commas at the beginning, it just looks bad to me. Obviously that's just a personal preference though. The main argument for having them at the beginning is that it makes diffs cleaner, but that's doesn't outweigh the ugliness of it IMO.

Copy on elaborating on the ""bad"" conventions when I have more than just a personal preference for it. 

If you think of anything else that would be worth adding or changing I'd love to chat about it more. 

Cheers!

Matt",3
410,"sharing-my-sql-style-guide","You are welcome, @mhmazur ! Will do for sure.

Dave",4
410,"sharing-my-sql-style-guide","In defense of leading commas:

in general, the most important columns come first. They're the ones least likely to change. The last column is generally much more likely to be modified than the first!

Do we care?

Well, if code is write-once read-many, then no. But if code is write-many read-many, then at a small aesthetic cost, it becomes possible to simplify git diffs. The vast majority of changes to a set of expressions will not affect the first column. 

So your diff when adding or removing a column at the end of a sql file will be exactly one line - it will reflect the action that actually occurred (a line was added, a line was removed) as opposed to two or more lines, which imply that a line was changed, when in reality a comma was added and then a line was added -- the real change was the new column, or the removed column. 

It's a small thing, but for me, keeping diffs as clean as possible trumps the aesthetic value of trailing commas.",5
410,"sharing-my-sql-style-guide","Great response. However, I would beg to differ and say the aesthetic value of trialing commas only applies to past-scenarios where Text Editors didn't offer a way to quickly resolve trailing commas. (e.g. VS Code and another editor offer multiline edits or some other macro/shortcut to make it easier for trailing commas to be edited).

But in regards to your aesthetic portion, I would say that trailing commas are great because they visually end the feature/column call, and logically proceed into the next call or line.

However, I know it's a touchy topic--I once had this conversation once with someone--it got interesting haha",6
410,"sharing-my-sql-style-guide","I love your style guide (maybe because it's very close to how I write SQL haha)! Do you have thoughts on whether it is ever appropriate to split out function calls with several arguments over multiple lines? 

I find myself doing this every now and then, especially with date/timestamp manipulation functions, or if I want to aggregate the result of a `case...when`. Perhaps your ""avoid function-wrapped arguments"" requirement could be interpreted as arguing that I should separate ""creating"" my arguments (using a `case...when` in this case) from the function call?",7
410,"sharing-my-sql-style-guide","No worries, I hear you there. I have a hate/love/hate affair with feature rich IDEs, and so my choice to use only basic autocomplete (and maybe some linting in python...maybe) strongly colors my opinion. For my development style I am definitely less likely to make errors if I use leading commas, but I understand that the vast majority of people these days prefer feature rich tools.

super-clean git diffs are still my main defense of leading commas, though. no tool will get you around that =D",8
410,"sharing-my-sql-style-guide","Hereâ€™s a [fun article](https://hackernoon.com/winning-arguments-with-data-leading-with-commas-in-sql-672b3b81eac9) around the leading/trailing comma debate that uses a ton of Github data to try and determine some measure of â€œsuccessâ€ of projects that adhere to leading/trailing/both. Itâ€™s a little dated - I wonder if we re-ran the queries in there if those trends still hold?",9
410,"sharing-my-sql-style-guide","Awesome guide - mostly because it is close to what many data analyst unconsciously end up doing without following formal rules. A nice balance between readability, speed, consistency.",10
429,"the-difference-between-users-groups-and-roles-on-postgres-redshift-and-snowflake","Every so often, I fall down a rabbit hole â€“ this time it happened to be on users, groups, and roles across Postgres, Redshift, and Snowflake.

I don't know if anyone will find this _useful_, but I definitely found it _interesting_ so thought it worth writing up what I found!

## Postgres
We'll start here, because Postgres generally sets the standard for relational databases.

In Postgres 9, **users** and **groups** were replaced with **roles** (Postgres have been very kind, and have maintained `user` and `group` as aliases for `role` for backwards compatibility). The difference between users and roles? Simply put, users can now be considered as roles with the ability to login.

* Membership: Any role can be a member of another role, so you can create a hierarchical structure with as many tiers as you'd like.
* Ownership: Any role can own relations.
* Privileges: Any role can have privileges granted to it.
* Inheritance: Up to you! Roles can be configured such that any members automatically inherit the privileges of the role, or can be configured to force a member to explicitly change their role in order to use those privileges.

## Redshift
Redshift forked from Postgres somewhere around version 8. As such, roles look more like an old-school Postgres database rather than the role-based world of Postgres 9 â€“ on Redshift, **users** and **groups** are separate entities.
* Membership: Only a user can be a member of groups, i.e. a group cannot be a member of other groups, so you can only have a two-tiered structure.
* Ownership: Users, rather than groups, own relations.
* Privileges: Both users *and* groups can have privileges granted to them.
* Inheritance: Users automatically inherit all privileges of any groups they are a member of.

## Snowflake
Snowflake looks almost like the Postgres 9 view of the world, but abstracts **users** into a separate entity from **roles**. Users are just the login credentials, while roles look more like Postgres 9 roles.

* Membership: Roles can be granted to users and to other roles, so you can create a hierarchical structure with as many tiers as you'd like.
* Ownership: Roles, rather than users, own objects.
* Privileges: Roles, rather than users, have privileges granted to them.
* Inheritance: Users have to explicitly change their role in order to inherit the privileges granted to that role. Each user can have a default role.


## Why does this matter?
This is good information to have if you're used to working with one data warehouse and are switching to another (maybe you're migrating warehouses, or maybe you're changing jobs!). It also has implications for the best practices when setting up users and/or groups and/or roles in your warehouse â€“ we've got a whole article on this coming out :soon:, stay tuned!",1
445,"how-to-not-lose-your-mind-when-auditing-data","TL:DR; We've got a new package, [audit_helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/) to help with data audits.

#  Auditing data: my least favorite task in analytics
As a data analyst, there's one task that used to instill more dread in me than any other â€“ *auditing data* :weary:. This task generally pops up in one of two situations:
- I'm refactoring the SQL of an existing dbt model and want to make sure I haven't introduced a regression
- I'm creating dbt models that replace a data transformation that's happening somewhere outside of dbt (and sometimes not in SQL!), and need to show my stakeholder that my new model is a like-for-like replacement.

Sure, you get to feel a bit like a detective, but in reality, I felt this dread for two reasons:
1. The SQL involved in auditing data is often *harder* than the SQL for building models â€“ writing a query to compare two relations record-by-record uses `intersect` and `except`, a query to compare relation structures requires querying `information_schema.columns` (unless you're using a Redshift [late binding view](https://discourse.getdbt.com/t/validating-the-sql-in-a-redshift-late-binding-view/193), in which case you also need to  `select pg_get_late_binding_view_cols()` :man_facepalming:) .
2. And it's also *tedious*, bordering on frustrating â€“ if your audit doesn't match up, it can be really hard to track down exactly why that is â€“ finding the exact records that don't match can be difficult, and finding the SQL that has introduced the regression can be even more so! (Especially if you leave it to the end.)

But it's also a _really important_ skill. When I've opened up a PR that has a ""validation query"", it demonstrates to my reviewer (and to me!) that my work is rock solid, meaning we can get the work merged in more quickly. Here's a screenshot from a recent PR that I opened:
![53%20AM|690x239](upload://dEOFLeq63lNF3Y67ytdXaOl18YD.png) 
(I was also able to demonstrate _exactly why_ those 45 records didn't match.)

# Zen and the art of data auditing
I recently changed my workflow around data audits, borrowing one of the best practices from software engineering development â€“ test driven development.

When refactoring an existing dbt model, rather than making all my changes, and then checking _at the end_ that my models matched up, I did the following:
1. Write an audit query (my test!) *first.*
2. Run a development version of your model without making any changes, and check that the dev and prod versions of your model match up (they should! You haven't made any changes yet!)
3. Refactor a small piece of the model.
4. Run the model.
5. Run the audit query.
6. If the query doesn't pass, debug. Once it passes, commit.
7. Repeat steps 3 through 6.

I felt Zen as I did this, I worked quickly, and I got the tricky part (writing the audit query) out of the way first! This tackled the ""tedious"" aspect of the problem, but that audit query was still more difficult than it needed to be!

# Introducing the `audit_helper` package
I've packaged up my most used audit queries into a new dbt package, [audit_helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/) (new to dbt packages? Check the docs [here](https://docs.getdbt.com/docs/package-management)!).

There's two macros in here that are super useful:
- `compare_relations`: This macro generates SQL that can be used to do a row-by-row validation of two relations.
- `compare_queries`: This macro generates SQL that can be used to do a row-by-row validation of two _queries_ â€“ sometimes when refactoring, I change a column name, or intentionally change the values in the column (e.g. coalescing NULLs with 0s). By comparing the results of two queries, I can take these changes into account!

Importantly, both of these macros are written in a way that means they can be reused across warehouses and projects, and the SQL in them is written in a way that helps you debug! ðŸ§˜â€â™€ï¸ðŸ•‰

In the past, we've written lots of macros that are intended to become part of your project; whether they are used in models, custom schema tests, hooks, or operations, they end up living in the version controlled code. Here, we're writing a _new kind_ of macro; a macro that you should use while developing, but not past that. And the cool thing here is that we recently released a great interface for these class of macros â€“ [dbt Develop](https://docs.getdbt.com/docs/cloud-developer-ide).

Here's a quick demo of how you can use it in dbt Develop!
https://imgur.com/5USpaGL

In this video, you can see:
1. My project has the `audit_helper` package in the `packages.yml` file.
2. I used the `audit_helper.compare_relations` macro to generate a query to compare an existing table, with a model (yup! I could `ref`!).
```
{% set old_etl_relation=adapter.get_relation(
      database=target.database,
      schema=""old_etl_schema"",
      identifier=""stg_jaffle_shop__orders"" )%}

{% set dbt_relation=ref('stg_jaffle_shop__orders') %}

{{ audit_helper.compare_relations(
    a_relation=old_etl_relation,
    b_relation=dbt_relation,
    exclude_columns=[""loaded_at""],
    primary_key=""order_id""
) }} 
```
3. Two of my records didn't match perfectly:

| in_a  | in_b  | count | 
|-------|-------|-------| 
| true  | true  | 97    | 
| true  | false | 2     | 
| false | true  | 2     | 

4. Fortunately I could copy/paste/uncomment the compiled SQL to find those exact records that didn't match to help me debug!

| order_id | customer_id | order_date | status    | in_a  | in_b  | 
|----------|-------------|------------|-----------|-------|-------| 
| 44       | 66          | 2018-02-16 | completed | true  | false | 
| 44       | 66          | 2018-02-17 | completed | false | true  | 
| 56       | 79          | 2018-02-28 | returned  | true  | false | 
| 56       | 79          | 2018-02-28 | completed | false | true  | 



# Planned changes
This package is very much a `v0.0.1` â€“ we intend to polish up the existing macros, and to add to it over time, creating macros that [compare the columns of two relations](https://github.com/fishtown-analytics/dbt-audit-helper/pull/5), the relations within a schema, and more! If you've got an audit query that you find useful, PRs are very welcome :smiley:

Happy auditing!",1
445,"how-to-not-lose-your-mind-when-auditing-data","Love this, thanks to the contributors!",2
452,"passing-a-source-table-to-a-macro","Maybe I am missing the obvious, but I have a macro which can do a select out of multiple source tables, so I need to pass the table name.  I am confused on what type parameter to pass to the macro so I can use it in my FROM statement.  The source table has been defined in a .yml by name and identifier only.

The from statement would look like this if a source statement is used in the macro.

    FROM {{ source( 'source_name_in_yml', {{table_name}} ) }}

This doesn't work.

Is there a way to set the source table in the yml to make this easier?  Or do I need to define a variable?  Or both?

Thanks",1
452,"passing-a-source-table-to-a-macro","Hey @cjremley :wave:

So, in Jinja, once you're already inside curly brackets `{{ }}`, you don't need to use them again.

Try something like:
```
FROM {{ source( 'source_name_in_yml', table_name ) }}
``` 

If that doesn't fix it, let me know! If you can also share the full macro code (i.e. everything from `{% macro my_macro(table_name) %}` to `{% endmacro %}`, and a snippet of how you're calling the macro (i.e. `{{ my_macro('my_table') }}` ), I'll likely be able to help out more easily here.",2
452,"passing-a-source-table-to-a-macro","Thanks for the quick answer Claire.  That worked.  I hadn't figured that nuance in Jinja out yet.",3
452,"passing-a-source-table-to-a-macro","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
455,"how-to-get-started-with-data-testing","The ability to test data in dbt is something that dbt power users love and yet, overall, it's an underused piece of functionality -- out of the 684 dbt projects active last week, only 36% ran a test. 

I'd like to compile expertise from this group into an article that helps analysts think about how to use tests to improve the quality of their data.

A few questions for the testers in the community:
1. What are some tests you currently run in dbt?
2. Why are these tests important for you and your team?
3. What advice do you have for an analyst who isn't currently testing their data but wants to get started?
4. And, just because we're curious, what's your current ratio of tests to models?",1
455,"how-to-get-started-with-data-testing","Across several projects, we run tests on:
- sources: mostly simply uniqueness, not_null schema tests
- staging models: occasionally to make sure transformations didn't create duplicates etc
- dim/fact models: this is where test the core of the business logic, e.g. relationship tests, expression, recency tests etc

We also have a series of custom tests that test conditions across models. 
For example, one project checks to make sure all orders contain at least one item from a certain product category. Another tests whether we've received a reasonable amount (above some threshold) of Fedex tracking data for our shipments (there is a time delay). 

In our largest project, we currently have 315 models to 191 tests, where all dim/fact models are covered by at least one test.",2
455,"how-to-get-started-with-data-testing","We use tests extensively and they have enabled us to dramatically improve the quality of our transformations.

For several years prior to using dbt, we did all of our transformations in (untested) Looker PDTs. Eventually, we started having problems as our engineering team grew and our product development velocity increased. Since we adopted dbt in 2018, we've used it to **validate our assumptions**. IMO Josh Temple's [Automated testing the modern data warehouse](https://medium.com/@josh.temple/automated-testing-in-the-modern-data-warehouse-d5a251a866af) is a must read.

I'd highly recommend testing to analysts that haven't gotten started yet. We got by without them for awhile, but in the end I wish we started sooner. Trust in analytics can be fickle and finding out about issues before your end users report them makes a big difference.

We use all of the built-in tests (uniqueness, not null, accepted values, and relationships), have created some macros for repeated patterns (unique_where, not_null_where, etc.) along with custom data tests for business logic.

Our project has 430 models and 2287 tests (admittedly this may be a little excessive, but it's easy to add/remove them).

To truly reap the benefit of tests and ensure data quality, some activities at the periphery of the tests are also required. I highly recommend:

* Monitoring the health of your data warehouse (i.e. disk usage) and that dbt is actually running on the schedule it's supposed to, within expected run-times, and alerting as necessary
* Routing test failures to a tracking tool (i.e. Rollbar) rather than Slack or email so you're not interrupted unless it's truly urgent. We're excited to use the new error levels functionality to further segment test failures here.
* Systematically addressing patterns of false positives (i.e. noisy tests) by pruning and updating tests
* Systematically addressing false negatives (i.e. data quality issues not caught by tests) through something analogous to postmortems",3
455,"how-to-get-started-with-data-testing","> What are some tests you currently run in dbt?

We run all the common dbt tests (unique, not null, relation, etc). Our most valuable ones though are our custom tests. These typically validate more complicated assumptions about our models. For some of these, they also store in code some aspect of an upstream business process. Our zuora tests (https://gitlab.com/gitlab-data/analytics/tree/master/transform/snowflake-dbt/tests/zuora) are our most fleshed out custom tests. We [have one](https://gitlab.com/gitlab-data/analytics/blob/master/transform/snowflake-dbt/tests/zuora/zuora_account_has_crm_id.sql) that validates that every new subscription is linked to a Salesforce account. If this fails, it flags us to make and issue for the Finance team to fix it. We also validate the [lineage of subscriptions](https://gitlab.com/gitlab-data/analytics/blob/master/transform/snowflake-dbt/tests/zuora/zuora_assert_no_circular_linkages.sql) so that renewals can't be in the past. These are *very* valuable for data integrity purposes.

> Why are these tests important for you and your team?

It gives us confidence to move quicker. We trust that our data is correct _and_ we trust that we can make changes without breaking things because we have the proper checks in place. It removes some of the cognitive load on the analyst because they don't have to self-check all of these assumptions - they can look at a test and know ""Oh, every value in this column is unique"" and they can move forward with those assumptions instead of constantly second-guessing. 

> What advice do you have for an analyst who isnâ€™t currently testing their data but wants to get started?

Don't think you have to do a big refactor to get going. Two is better than one and one is better than none. If you're using dbt (which, really, you should) then start with the basis of unique and not null tests. Keep adding as you're building. If you find a bug in your data, then write a test for it so you can catch it sooner. Start small and build the habit. Eventually you have a good number of tests and then you can invest even more in better coverage.

>  And, just because weâ€™re curious, whatâ€™s your current ratio of tests to models?

226 models, 909 tests",4
455,"how-to-get-started-with-data-testing","While our use of dbt is still experimental and limited in use at HubSpot, we have instrumented testing/validation for our fact table/derived table pipeline. Each table is conceptually very similar to a dbt model (or combination of models). The pipeline generates 400+ tables and predominantly represents the work of data analysts across the company. The bulk of reporting and analytics queries initiated from our BI tool (Looker) are run off these tables. 

We currently use airflow to orchestrate daily script runs that generate/append data to the tables and this includes running test scripts against the data in the tables. After the tests run, any tests that don't pass are configured to alert users in slack so that the errors can be investigated ASAP.


**What are some tests you currently run in dbt?**

1. **primary key checks:** (since snowflake does not enforce pk constraints). This can help identify fanout that was inadvertently introduced by changes in the up stream source system, an up stream script, or a code change to the current script.

2. **acceptable value checks:** typically for data stored as a string, can be useful in cases where you are applying logic to a set of values you are expecting in a given column. If a value suddenly dissapears or a new one appears you will want to be alerted and update your logic to appropriately handle the new value. (ie: a column called program_name has expected values of silver, gold, platinum, diamond and then one day titanium appears -- wtf?!)

3. **data volume checks:** this type of tests monitors the volume of total rows or new rows inserted into your table and compares it against a defined baseline threshold. If the volume is higher or lower than usual it's valuable to be able to investigate and identify possible upstream ingestion issues. Unmonitored, these types of issues can be problematic because most reports will still run but the numbers could be inaccurate.

4. **value comparison checks:** compare the values in 2 fields to ensure they conform to expected results and make sense. A common use case where we run this type of test is to check for overlapping or out of order date ranges in scenarios where rows should be mutually exclusive, as well as logical (ie: start_date is not AFTER end_date)

**Why are these tests important for you and your team?**

Above all else, these tests are a critical tool for helping establish and maintain trust with business users who rely on this data. 

**They help analysts:** 
* stay ahead of bad data
 * uncover problems *BEFORE* they impact reporting consumed across the business
 * monitor for upstream changes that are largely outside their control
 * produce consistent and accurate reports
 * measure the quality and reliability of their work
 * communicate and elicit empathy from software engineers who control data in the source systems
 * be intentional about what they are building and how it should work

**What advice do you have for an analyst who isnâ€™t currently testing their data but wants to get started?**

Don't hesitate to get started! Implementing testing will help create clarity and confidence in your data. 
* To avoid feeling overwhelmed, start with a focus on everything you build/update going forward, or your most heavily consumed data 
* If you know how to write a SQL query, you know how to write a validation test

 `SELECT 
     count(*) as row_count
     , count(distinct pkey) as pkey_count
     , sum(iff(field_name NOT IN ('VALUE 1', 'VALUE 2', 'VALUE 3','VALUE 4'),1,0)) as relationship_type_out_of_range
     , sum(iff(start_date > end_date,1,0)) start_end_date_check
             FROM schema.table_name
`
* At a very minimum you can start by checking your primary keys, and if you don't have those defined in the database that's okay, you should still know what 1 row of data represents for each table and can use a simple `row()` function to generate the key in the table

* If you can, I recommend storing the results of the tests in a table in the database so you can build some meta reporting on your data validity/pipeline health
 
* Adopt testing into your standard process and make it required to release new code

**And, just because weâ€™re curious, whatâ€™s your current ratio of tests to models?**
	
Here's the moment of truth -- right now we have < 10% coverage (40+ tests) for our 400+ tables. Test scripts are currently NOT a requirement at HubSpot and we definitely feel that pain.


Testing really should be considered analogous to brushing your teeth. Would you ever think twice about brushing your teeth? It's a basic form of hygeine that produces a STANK when skipped. Avoid the decay & test your data!",5
463,"faq-configuring-models-materializations-based-on-environments","We've had a couple of questions recently about configuring models based on your environment (or target) over on Slack, spefically around:
* Using views in `dev` and  tables in `prod`
* [Snowflake only] Using transient tables in `dev` and non-transient in `prod`.


----------
![55%20PM|690x105](upload://k09HpmsaEZuudzylv3S2FL64cKA.png)

----------

![43%20PM|690x92](upload://bsbDkue1F0SNg4EfiFnirX3wvF7.png) 

----------

The best approach is to configure this straight from your `dbt_project.yml` file:

```
models:
  materialized: ""{{ 'table' if target.name == 'prod' else 'view' }}""
```

If you need the output to be a boolean:
```
models:
  materialized: ""{{ 'table' if target.name == 'prod' else 'view' }}""
  my_project:
    for_export:
      enabled: ""{{ (target.name == 'prod') | as_bool }}""
```

Another approach is to set the materialization rule on a per-model level:
```
{{
    config(
        materialized=('table' if target.name == 'prod' else 'view')
    )
}}
```
However, if you are using Slim CI, this approach will lead to models running unnecessarily because the file's contents change between `prod` and `dev`. 

You could also conceivably create a [custom materialization](https://docs.getdbt.com/docs/creating-new-materializations) that packages up this logic, but I'd recommend this as a last resort!",1
471,"handling-merge-statements-in-incremental-models","I have a table that I need to update incrementally in snowflake.  Today I use the following query:

    MERGE INTO incremental_table t 
        using (
          with co as (
              select distinct * from (
                select
                   COL_A,
                   COL_B,
                   COL_C,
                   COL_D,
                   COL_E,
                   COL_F
                from
                    source_table 
                where
                    COL_A is not null
             )      
          )
          select
            *
          from 
            co
        ) s
        ON 
            s.COL_A = t.COL_A
            s.COL_B = t.COL_B
            s.COL_C = t.COL_C
        WHEN MATCHED AND s.COL_D < t.COL_D THEN
            UPDATE SET
                t.COL_D = s.COL_D
        WHEN MATCHED AND s.COL_E is not null AND t.COL_E is null THEN
            UPDATE SET
                t.COL_E = s.COL_E
        WHEN MATCHED AND s.COL_F is not null AND t.COL_F is null THEN
            UPDATE SET
                t.COL_F = s.COL_F
        WHEN NOT MATCHED THEN
            INSERT (
              col_a,
              col_b, 
              col_c,
              col_d, 
              col_e,
              col_f
            ) VALUES (
              s.col_a,
              s.col_b, 
              s.col_c,
              s.col_d, 
              s.col_e,
              s.col_f
            )  
    ;

I trying to figure out how to best model that in dbt?  As you can see different columns get updated depending on the various (non unique key fields).  Is there a way to add this statement to dbt and get it to increment the model, or do I need to break it up into a set of selects, one for each ""WHEN MATCHED""  and ""WHEN NOT MATCHED"" statement?

Any help or docs explaining that would be useful or how have others solved this?

Thanks in advance",1
471,"handling-merge-statements-in-incremental-models","Hey @axdahl - we tend to avoid `merge` statements like this for the same reason that we avoid writing insert/update statements on other databases: this type of DDL mixes data modeling logic with table-building logic. To be sure, I think it's totally reasonable to run a query like this, I just want to give some insight into why this exact merge statement isn't directly representable in dbt.

It's a little tricky for me to wrap my head around the particular logic you're outlining here. I think you'll need to use an `incremental` model, possibly joining back to the destination table. 

I'd try to frame it like this: write the select statement which generates the records you want to upsert into your destination table. If it turns out that the field you want to upsert on is non-unique, you can set dbt's [incremental_strategy](https://docs.getdbt.com/docs/snowflake-configs#section-merge-behavior-incremental-models-) to do a `delete+insert` instead of a `merge`.

Last thing: I'd recommend generating a surrogate key for the field that you're joining on (eg. `md5(col_a || col_b || col_c`) as it should make your join logic a whole lot cleaner here. More info on that [here](https://blog.fishtownanalytics.com/the-most-underutilized-function-in-sql-9279b536ed1a) and [here](https://github.com/fishtown-analytics/dbt-utils#surrogate_key-source).

Here's a quick example that should hopefully get you started! Let me know how it goes :)

```sql
with source_data as (

	select
		'abc' as id,
		1 as col_d,
		2 as col_e,
		3 as col_f

),

destination_table as (

	-- use {{ this }} to select from the destination table
	-- wrap it in {% if is_incremental() %} to make it work
	--   in full-refresh builds

	select
		'abc' as id,
		10 as col_d,
		null::int as col_e,
		12 as col_f

)

select
	greatest(s.col_d, t.col_d) as col_d,
	coalesce(t.col_e, s.col_e) as col_e,
	coalesce(t.col_f, s.col_f) as col_f

from source_data as s
join destination_table as t using (id)
```",2
471,"handling-merge-statements-in-incremental-models","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
486,"flattening-an-eav-model-in-sql-the-dry-way","I recently had the pleasure (:upside_down_face:) of flattening an EAV model in dbt.

# What _is_ an EAV model?
Let's just lift the description from Wikipedia:
> **Entityâ€“attributeâ€“value model**  ( **EAV** ) is a [data model](https://en.wikipedia.org/wiki/Data_model) to encode, in a space-efficient manner, entities where the number of attributes (properties, parameters) that can be used to describe them is potentially vast, but the number that will actually apply to a given entity is relatively modest.

Let's take an example of a `products` model. In a relational model, you might have a `products` table like so:
### dbt_swag_shop.products
| id | name             | price | brand            | size | color  | width_mm | volume_ml | is_bpa_free | 
|----|------------------|-------|------------------|------|--------|----------|-----------|-------------| 
| 1  | dbtshirt         | 2     | Asos     | S    | Grey   | NULL     | NULL      | NULL        | 
| 2  | Logo Sticker     | 3     | StickerMule | NULL | Orange | 50       | NULL      | NULL        | 
| 3  | dbt water bottle | 1     | Purist           | NULL | Orange | NULL     | 650       | True        | 

We can see here that the relational model doesn't extend very well â€“ each product has really different attributes, so this table would become really wide over time.

In an EAV model, this would instead be expressed through a series of tables like this:
### dbt_swag_shop.entities
| entity_id | entity_code | 
|-----------|-------------| 
| 1         | product     | 
| 2         | product     | 
| 3         | product     | 

## dbt_swag_shop.attributes
| attribute_id | attribute_code | data_type | 
|--------------|----------------|-----------| 
| 1            | name           | string    | 
| 2            | price          | integer   | 
| 3            | brand          | string    | 
| 4            | size           | string    | 
| 5            | color          | string    | 
| 6            | width_mm       | integer   | 
| 7            | volume_ml      | integer   | 
| 8            | is_bpa_free    | boolean   | 

### dbt_swag_shop.values_string
| value_id | attribute_id | entity_id | value            | 
|----------|--------------|-----------|------------------| 
| 1        | 1            | 1         | dbtshirt         | 
| 2        | 1            | 2         | Logo Sticker     | 
| 3        | 1            | 3         | dbt water bottle | 
| 4        | 3            | 1         | Asos             | 
| 5        | 3            | 2         | StickerMule      | 
| 6        | 3            | 3         | Purist           | 
| 7        | 4            | 1         | S                | 
| 8        | 5            | 1         | Grey             | 
| 9        | 5            | 2         | Orange           | 
| 10       | 5            | 3         | Orange           | 

### dbt_swag_shop.values_integer
| value_id | attribute_id | entity_id | value | 
|----------|--------------|-----------|-------| 
| 1        | 2            | 1         | 2     | 
| 2        | 2            | 2         | 3     | 
| 3        | 2            | 3         | 1     | 
| 4        | 6            | 2         | 50    | 
| 5        | 7            | 7         | 650   | 


### dbt_swag_shop.values_boolean
| value_id | attribute_id | entity_id | value | 
|----------|--------------|-----------|-------| 
| 1        | 8            | 3         | TRUE  | 

NB: The values are split into a table for each data type, since databases can't store a column that contains both strings and decimals.

The EAV approach offers flexibility, and makes it easy to get data in! A computer can easily read this data, but for us mere mortals, it's a little harder to instinctively understand, say, what the name of a product is.

These days, it's much more common to store these sparse attributes in a json dictionary (if you're using Snowflake or BigQuery, you'll have a pretty good time writing SQL against that data). The only benefits I can really see of the EAV model over a dictionary-based approach is that:
- if you want to change an attribute name (say `color` -> `colour`), you only have to update the name in one place
- data types are strongly forced in an EAV model

Given the choice, I'd prefer to work with attributes that are stored in a dictionary. However, as analytics engineers, often we can't choose the data structure that we have to work with. So if we have an EAV model, what's the best way to get it into a shape we can work with?

# SQL for flattening an EAV model
Let's say we want to build a table of products, with the attributes `name`, `price`, and `color` as columns. Here's the SQL:
```sql
with products as (
  select * from dbt_swag_shop.entities
  where entity_code = 'product'
),

name as (
  select * from dbt_swag_shop.values_string
  where attribute_id = 1
),

price as (
  select * from dbt_swag_shop.values_integer
  where attribute_id = 2
),

brand as (
  select * from dbt_swag_shop.values_string
  where attribute_id = 3
),

final as (
  select
    product.entity_id as product_id,
    name.value as name,
    price.value as price,
    brand.value as brand
  
  from products
  left join name using (entity_id)
  left join price using (entity_id)
  left join brand using (entity_id)
)

select * from final

```

Hold up! How did I know which `dbt_swag_shop.values_{{ data_type }}` table to query? And how did I know the `attribute_id`s to filter on? Well, I ran this query _before_ I wrote my SQL  :sweat_smile:
```sql
select * from dbt_swag_shop.values_string
where attribute_code in ('name', 'price', 'brand')
```

Which returns:
| attribute_id | attribute_code | data_type | 
|--------------|----------------|-----------| 
| 1            | name           | string    | 
| 2            | price          | integer   | 
| 3            | brand          | string    | 

Looking back at the main query, you can see that there's a lot of repetition a lot of repetition â€“ those subqueries all follow a patten, as do the column names. Where there's a pattern, there's often an opportunity to turn it into a loop! And what we actually want to do is iterate over the results of that second query!

# Putting this together in dbt
In dbt, the [run_query](https://docs.getdbt.com/docs/run_query) macro lets you run a query, and returns the results of that query to your jinja context! Perfect!

Here's the code I used to flatten out the EAV model given a list of columns I wanted to build:

```
{# These are the columns we will be creating #}
{% set attribute_codes=[
    'name',
    'price',
    'brand'
] %}

{#- This query gives us the information we need to be able to join the EAV model #}
{%- set product_attributes_query -%}
  select * from dbt_swag_shop.values_string
  where attribute_code in ('{{ attribute_codes | join (""', '"") }}')
{%- endset -%}

{#- Run this query, and load the results into the Jinja context -#}
{%- set product_attributes_results=run_query(product_attributes_query) -%}

{#- Light transformation to make the results easier to work with -#}
{%- if execute -%}
{%- set product_attributes=product_attributes_results.rows -%}
{%- else -%}
{%- set product_attributes=[] -%}
{%- endif -%}

{#- MODEL CODE -#}

with products as (
  select * from dbt_swag_shop.entities
  where entity_code = 'product'
),

{% for attribute in product_attributes %}

{{ attribute['attribute_code'] }} as (
    select * from dbt_swag_shop.value_{{ attribute['data_type']  }}
    where attribute_id = {{ attribute['attribute_id'] }}
),

{% endfor %}

final as (
    select
        product.entity_id as product_id,

    {% for attribute in product_attributes %}
        {{ attribute['attribute_code'] }}.value as {{ attribute['attribute_code' ] }} {{- "", "" if not loop.last }}
    {% endfor %}

    from products

    {% for attribute in product_attributes %}
    left join {{ attribute['attribute_code'] }} using (entity_id)
    {% endfor %}
)

select * from final
```
^ This code is pretty complex, but extremely flexible. If you want to add an extra column to your flattened model, you only need to add it to the `attribute_codes` list at the top of this code block.

But, I have traded readability for flexibility. If I were actually only creating three columns, I would likely use the first version of the query. Given I was actually using this to flatten a model with a dozen columns, which we want to add to later, it felt like it was worth it.

If you wanted to take it one step further, you could abstract this even more and package it up as a macro, so you can repeat this pattern to flatten all the entities (`products`, `stores`, `customers`, `brands`, etc.) in your EAV model by simply calling something like:
```
{{ flatten_eav_model(
  entity='product',
  columns=['name', 'price', 'brand']
) }}
```
I'll leave that for someone braver than me!",1
508,"limiting-dev-runs-with-a-dynamic-date-range","Hey folks,

So far I've been limiting the date range on base model view files when doing dev runs as follows:

```
{% if target.name == 'dev' %}
where date(date) >= date_add(current_date(), interval -3 day)
{% endif %}

 ```

This works fine, but with many models it's not so **DRY**  what with all this repeated code across the base model files. Thus I'd like to make some form of 'dev_limit_date' macro.

To note:
- I would like to set the date interval (i.e. '- 3 days') in the macro, and not in the separate model files, because I may change this ad-hoc depending on what is being tested in dev. I only want to state it once.

- 'date(date)' is subject to change across model files, depending on the underlying source date field to be filtered. So I may need to set the date field to be filtered in the model file, i.e. with a `{% set filter_date = 'date' %}`

I've previously tested this in a number of ways (e.g. here: https://getdbt.slack.com/archives/C0VLZPLAE/p1562678315423500?thread_ts=1562671748.420900&cid=C0VLZPLAE) but I cannot seem to get it running nicely. Anyone else tackled this problem before and have it up & running in their production environment?

Much appreciated,

Lewis",1
508,"limiting-dev-runs-with-a-dynamic-date-range","Hey there - check out this issue - sounds like there's some overlap with what you're looking to accomplish here. https://github.com/fishtown-analytics/dbt/issues/1592

Would love to further the discussion either in this thread, or on that issue!",2
508,"limiting-dev-runs-with-a-dynamic-date-range","Hey @drew,

Thanks for your suggestion -  thats for sure an interesting read and I've taken some points made in it into consideration. I'll continue the 'dev_limit' macro discussion here however.

What I'm now thinking is more of a macro that adds specified LIMIT where called in models, i.e. base for a limit on source tables. I've managed to successfully compose a macro that takes an argument that sets a specified LIMIT in terms of result row count (or takes a default limit) if target is dev - (I am checking in the target run file after running). But what I am struggling to do is add a disabling feature on this macro (that works) - i.e. via a second macro that can disable the first.

I'm setting the row count limit via the cli when running models in dev, i.e. as follows:

`dbt run --vars 'row_count: 100'` (sets a dev limit row count of 100, overwriting the default)
`dbt run`(runs with the default dev limit row count (i.e. here 10,000))
`dbt run --vars 'dev_disable: true'`(this would ideally disable the dev_limit_rows macro)

So, here are my files:

(model file) model.sql :
```
select *
from {{ source('data', 'source') }}
{{ dev_limit(row_count, dev_disable) }}
```
(macro) dev_limit_rows.sql : 
```
{% macro dev_limit_rows(row_count, dev_disable) -%}

{{ config(tags=[""dev_limit""]) }}

{# set row count limit on base tables for dev runs - '10000' is default value if not explicitly defined #}

{% if target.name == 'dev' %}

  limit {{ var(""row_count"", ""10000"") }}

{% endif %}

{%- endmacro %}
``` 
(macro) dev_limit.sql : 
```
{% macro dev_limit(row_count, dev_disable) -%}


  {%- if dev_disable -%}

    --no op

  {%- else -%}

    {{ dev_limit_rows(row_count, dev_disable) }}

  {%- endif -%}

{%- endmacro %}
```

I feel like I am close to cracking it, but have tried enough alterations now ðŸ˜… Something in the dev_limit.sql must be off. Any help/advice/direction is GREATLY appreciated!",3
508,"limiting-dev-runs-with-a-dynamic-date-range","Hey @lewis! This is cool :slight_smile:

The thing that jumps out at me here is your model SQL:
```
select *
from {{ source('data', 'source') }}
{{ dev_limit(row_count, dev_disable) }}
```

Should this instead be
```
select *
from {{ source('data', 'source') }}
{{ dev_limit(var('row_count'), var('dev_disable')) }}
```

Maybe one approach is to push the limit/disable logic into the `dev_limit` macro, so:

**dev_limit.sql**
```
{% macro dev_limit() -%}
  {%- if var('dev_disable', default=false) -%}
    --no op
  {%- else -%}
    {{ dev_limit_rows(var('row_count')) }}
  {%- endif -%}
{%- endmacro %}
```

**dev_limit_rows.sql**
```
{% macro dev_limit_rows() -%}
  {{ config(tags=[""dev_limit""]) }}
  {# set row count limit on base tables for dev runs - '10000' is default value if not explicitly defined #}

  {% if target.name == 'dev' %}
    limit {{ var(""row_count"", ""10000"") }}
  {% endif %}
{%- endmacro %}
```

**model.sql**
```
select *
from {{ source('data', 'source') }}
{{ dev_limit() }}
```

Would something like that work?",4
508,"limiting-dev-runs-with-a-dynamic-date-range","Hey @drew,

Thanks! Given your suggestions and a few tweaks I have got it running for the following use cases:

Running in dev with a default row limit (set in macro to 10,000):
`dbt run`
Running in dev with a specified limit via cli:
`dbt run --vars 'row_count: 100'`
Running in dev disabling the 'dev_limit' via cli:
`dbt run --vars 'dev_disable: true'`

I'm sharing macro files and an abbreviated model file below, hopefully this is helpful for others. Also maybe there is still yet a neater of doing this; someone might want to pitch-in, continuing this thread.

**model.sql**
```
select *
from {{ source('data', 'source') }}
{{ dev_limit() }}
```
**dev_limit.sql**
```
{% macro dev_limit(row_count, dev_disable) -%}


  {%- if var('dev_disable', default=false) -%}

    --no op

  {%- else -%}

    {{ dev_limit_rows(row_count) }}

  {%- endif -%}

{%- endmacro %}
```
**dev_limit_rows.sql**
```
{% macro dev_limit_rows(row_count) -%}

{{ config(tags=[""dev_limit""]) }}

{# set row count limit on base tables for dev runs - '10000' is default value if not explicitly defined #}

{% if target.name == 'dev' %}

  limit {{ var(""row_count"", ""10000"") }}

{% endif %}

{%- endmacro %}
```",5
508,"limiting-dev-runs-with-a-dynamic-date-range","That's really great! Thanks for sharing the finished product :)",6
510,"guide-notes-on-moving-from-postgres-to-snowflake","TL;DR - get ready for any SQL business logic to fail - in looker, DBT, etc. I highly recommend doing the migration over the weekend, give yourself a full day or two of work to fix up differences.

I spent this past weekend migrating from postgres to Snowflake. Tristan has already written a post on on redshift to snowflake, but I thought Iâ€™d briefly jot down my notes if you are considering a postgres->snowflake move
- schema search path: I set the default schema in my DBT pre-run-hook, no can do in snowflake, so I used regex to append schemas to all mentioned tables (I know, I know, use sources).
- typecasting pain: I did a lot of  `SUM(has_discounts) as n_discounts` , and am having difficulty casting booleans to a number
- datetime pain - I had variables like  `NOW() - MAX(table.created_at) AS time_since_update` , and this magically showed up in Looker as â€œ5 hours, 26 minutesâ€¦â€œ. Snowflake, I need to  `DATEDIFF`  and have to choose to look at minutes or hours or days.
- casing - snowflake has different casing preferences, that work very poorly with looker, which by default quotes all your column names. I removed all of the quotes in my LookML to fix this.


From Dan Hunt (DBT group)

Iâ€™ve also just done a Postgres -> Snowflake migration. The main points not mentioned that come to mind are related to Snowflake regular expressions:
-  **Regex matching:**  the Postgres shorthand  `column ~ 'regex'`  becomes  `rlike(column, 'regex')`
-  **Regex captures:**  extracting based on a regular expression capture changes from  `substring(column from 'regex')`  to  `regex_substring(column, 'regex', 1, 1, 'ie')`  to get the first match in a string (or similar, depending on case sensitivity, capture group, etc.)
-  **Regex escaping:**  in regex patterns, you need to double backslash to escape, so  `\.` becomes  `\\.` A few functions in Snowflake and  *not*  in Postgres have also been pretty helpful:
-  `ifnull(a, b)` : I prefer this function for readability to the Postgres equivalent,  `coalesce(a, b)`
-  `iif(condition, a, b)` : perhaps not everyone agrees here, but I prefer this to a  `case`  statement with only one condition for cleaner code.
- I havenâ€™t used  `try_cast`  yet, but this is also missing an equivalent from Postgres and potentially a big one for cleaning up conversions near the source layer


From gitlab - https://gitlab.com/gitlab-data/analytics/issues/645

* `now()` was replaced with `current_date`
* UDFs had to be re-written
* date math is totally different; we can now leverage the native `datediff` and `dateadd` to see the difference between two dates or a date and some interval ( `dateadd` can also subtract by use of negative intervals)
* `stringagg` to `listagg`
* snowflake errors out on `~` , `!~` , `<>` , etc
* `current` is a protected word",1
517,"building-models-on-top-of-snapshots","In our [recent guide](https://www.getdbt.com/blog/track-data-changes-with-dbt-snapshots/) to [snapshots](https://docs.getdbt.com/docs/snapshots) we wrote the following:
> **Snapshots should almost always be run against source tables**. Your models should then select from these snapshots, using the `ref` function. As much as possible, snapshot your source data in its raw form and use downstream models to clean up the data.

So if you've already got some snapshots in your project, here are some patterns we find useful when writing models that are built on top of a snapshot: 

### `ref` your snapshot

Just like models and seeds, you can use the ref [function](https://docs.getdbt.com/docs/ref) in place of a hardcoded reference to a table or view. It's a good idea to use `ref` in any models that are built on top of a snapshot so you can understand the dependencies in your DAG.

### Use `dbt_valid_to` to identify current versions
It might be useful for downstream models to only select the current version of a record -- use the `dbt_valid_to` column to identify these rows.
```
select
  ...,
  dbt_valid_to is null as is_current_version

from {{ ref('snapshot_orders') }}
```

### Add a version number for each record
Use a window function to make it easier for anyone querying this model to understand the versions for a given record. 
```
select
  ...,
  row_number() over (
    partition by order_id -- this is the unique_key of the source table
    order by dbt_valid_from
  ) as version

from {{ ref('snapshot_orders') }}
```


### Coalesce `dbt_valid_to` with a future date

Coalescing this field replaces NULLs with a date, making it easy to join to the snapshot in any downstream models (join conditions do not like NULLs). Danielle Dalton, from Rent the Runway, recently shared that RTR uses a [variable](https://docs.getdbt.com/docs/var) in their dbt project, `the_distant_future`, to make their future date consistent across models, like so:
```
select
  ...,
  dbt_valid_from as valid_from,
  coalesce(
      dbt_valid_to,
      {{ var('the_distant_future') }}
  ) as valid_to

from {{ ref('snapshot_orders') }}
```
:point_up: I like that a lot. 

## Union your snapshot with pre-snapshot ""best guess""

It's often the case that you start snapshotting a data source after it has had historical changes. In this case, it might be worth writing a query to construct your ""best guess"" of the historic values, and union it together with your snapshot for a complete history. The team at RTR add a column, `historical_accuracy`, to let their end users know whether the record is inferred or actual.

## Date-spine your snapshot

Sometimes it makes more sense to have a record per day, rather than a record per changed record. We use a pattern we refer to as ""date spining"" to achieve this â€“ in short we join snapshot to a table of all days to fan it out. We've written more about this pattern [in this article](https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265).

## Make your model resilient to the timing of the snapshot

Even if you plan to run a snapshot daily, the nature of ETL jobs is that they'll fail from time to time, resulting in missed days. Or, you may accidentally run a snapshot twice in a day. As a result, write the SQL in your model to be able to handle any missed days, or days with multiple snapshots.",1
517,"building-models-on-top-of-snapshots","",2
517,"building-models-on-top-of-snapshots","",3
517,"building-models-on-top-of-snapshots","Hey, Claire, thank you so much for this write-up. 
I'm interested in disambiguating operational concerns from modeling. IE I want to use snapshots as sources for models without having to physically instantiate them as part of a dev workflow(for ex. i may not have schema create permissions in an env and namespace collision makes working within a single schema difficult). I was thinking of implementing a custom materialization based off of 'incremental' or maybe a riff on your 'increment_by_period'  ('incremental_snapshot'?) that just adds a 'where eff_end_dt is null' to the end of a model's select statement. Then have snapshot specific envs configured for that materialization while other envs can point at raw tables/seed data(that may not have dbt scd columns). Also have thought that maybe implementing a custom strategy on incremental might work. Do you have any thoughts on these approaches?",4
517,"building-models-on-top-of-snapshots","I think I answered my own question by creating a sql_footer config",5
517,"building-models-on-top-of-snapshots","@krudflinger :  I would love to learn more about how you tackled this.  We are building up our models now, and we are struggling with some of the key realities that we think snapshots miss:

1. What happens when there is an ETL failure (@claire :  would like to see some examples of this from your 'resilient' section)

1. What happens when we learn that a field was incorrect, and we need to 'correct' history ?

1. How do you tackle this idea of pre-snapshot history? (@claire :  would love to see some ideas/examples on this -- i really like the idea of quality column)

1. What happens when you want (or need) to rebuild an environment?

We are exploring the idea of leveraging incrementals, and custom incrementals, to create 'archives' of the source data so we can play it back, but I'm not seeing a way to set a loop for operations (while *not_finished* do dbt run).  

We're just a month into this, so still eyes wide open, so any guidance from your experience would be great!

Thanks,
Ron",6
532,"pivoting-and-incremental-materializations","Pivoting data is a very common task for reporting models in dbt. So much so that one of the [oldest and most useful utility macros][dbt-utils-pivot] is for pivoiting.  In my experience though, pivoting doesn't play very nice with one of one of dbt's most powerful features - incremental materializations.

If you are a heavy user of incremental models, then the following sql will look familiar to you:

```sql
select
    event_at as event_at,
    count(distinct user_id) as daily_active_users
from raw_app_data.events

{% if is_incremental() %}
  where event_at > (select max(event_at) from {{ this }})
{% endif %}

group by 1
```

This might be so common in your projects that you might have [abstracted away][pageup-dbt-utils-timestamp-incremental] the incremental logic with the help of [handy macros][pageup-dbt-utils-timestamp-columns].  And this simple approach will work for 80% of all the models you write. Pivots however fall into that other 20%.

Take for example this pivot model which is to show a list of responses for an application:

```sql
SELECT new_rows.*
FROM (
    SELECT form_attr.application_id AS application_id,
           {{ pageup_dbt_utils.mapped_pivot(
               column='form_attr.attribute_key',
               mapping_table=ref('ref_application_form_fields'),
               mapping_key='map_key',
               mapping_value='mapped_value_name',
               agg='MAX',
               then_value='form_attr.attribute_val_text',
               else_value='NULL'
           ) }},
           MAX(form_attr.model_timestamp) AS aggregated_model_timestamp

    FROM {{ ref('work_application_form_attribute') }} AS form_attr
    -- other joins

    GROUP BY xx.application_id
) new_rows

{% if is_incremental() %}
  WHERE form_attr.model_timestamp > (select max(model_timestamp) from {{ this }})
{% endif %}

{{
  config({
    ""materialization"": ""incremental"",
    ""unique_key"": ""application_id""
    })
}}
```

First, lets look at [mapped_pivot][pageup-dbt-utils-mapped-pivot]. This is very similar to the dbt_utils pivot macro, but uses another table to get a list of columns. `ref_application_form_fields` looks like this:

| map_key | mapped_value_name |
|---------|-------------------|
| 1       | APPLICATION001    |
| 2       | APPLICATION002    |
| 3       | APPLICATION003    |
|         | etc...            |

The end result is a model with around 100 columns and ~6 million rows. Meaning that there are about 600 million rows of source data (with some blanks).

For a full refresh/initial load this model is about as optimal as it can be. But for and incremental run there are some noticable performance issues. In fact, for my data set this model takes almost as long to run for an incremental load as it does for a full load (about 800 seconds).  So there is definitely some room for improvement.

**A solution**

What we want to do to improve performance is to be smarter about filtering out rows before they are pivoted. This gets hard though as if 1 attribute row is updated, you actually need every row for that application included to complete the pivot.

Or do you?

What if we could pivot only the changed rows, then merge the updated values to get the complete model? This is what I will show below.

First, lets move our incremental filter to where it is most effective.

```sql
    ...

    FROM {{ ref('work_application_form_attribute') }} AS form_attr
    -- other joins

    {%- if is_incremental() %}
    WHERE form_attr.model_timestamp > (select max(model_timestamp) from {{ this }})
    {%- endif %}

    GROUP BY xx.application_id

    ...
```

Now incremental runs will filter to only changed values, resulting in partial pivot rows.  Next we need to make some changes to the pivot, as to make use of this partial set of data we will need some additional logic.

```sql
    ...

    SELECT form_attr.application_id,
           {{ pageup_dbt_utils.mapped_pivot(
               column='form_attr.attribute_key',
               mapping_table=ref('ref_application_form_fields'),
               mapping_key='map_key',
               mapping_value='mapped_value_name',
               agg='MAX',
               then_value='form_attr.attribute_val_text',
               else_value='NULL',
               add_update_flag_on_incremental=true,
               update_flag_suffix='__is_updated'
           ) }},

           MAX(form_attr.model_timestamp) AS aggregated_model_timestamp

    ...
```

There are now a few extra arguments passed into `mapped_pivot`. You can view the [documentation and implementation here][pageup-dbt-utils-mapped-pivot]. But what is going on is as follows:


* **add_update_flag_on_incremental** has made the pivot include a new column for every pivoted column. This column is a flag to indicate if this value has been updated.
* The column will have the suffix `__is_updated`. The list of columns created will now look like:
  * `APPLICATION001` 
  * `APPLICATION001__is_updated`
  * `APPLICATION002` 
  * `APPLICATION002__is_updated`
    ...

So the pivoted rows are now prepped, next step is to merge them. The easiest way to do this is to link back to the existing model.

```sql
    ...

    GROUP BY xx.application_id
) new_rows

{%- if is_incremental() %}
LEFT JOIN {{ this }} AS old_rows on old_rows.application_id = new_rows.application_id
{% endif -%}

    ...
```

We have everything we need to build a complete pivoted row now. And we can write a macro to make the merge easy.

```sql

SELECT new_rows.application_id,
       {{ pageup_dbt_utils.merge_updated_values(
            new_table='new_rows',
            old_table='old_rows',
            unique_key='application_id',
            merge_column_names=dbt_utils.get_column_values(
                table=ref('ref_application_form_answers'),
                column='mapped_value_name'),
            update_flag_suffix='__is_updated',
            else_value='NULL'
        ) }},
       new_rows.aggregated_model_timestamp

       ...
```

You can view the [documentation and implementation here][pageup-dbt-utils-merge-updated-values] for this macro. Basically, it is talking a list of column names, checking the `__is_updated` column and picking the column from `new_rows` if it is updated, otherwise taking it from `old_rows` if it is unchanged. If the pivoted row is completly new, there is a default value to use.

Another key part of what this macro does is that it _only does work during an incremental run_. Otherwise it takes only the columns from `new_rows`.

The final completed model looks as below:

```sql

SELECT new_rows.application_id,
       {{ pageup_dbt_utils.merge_updated_values(
            new_table='new_rows',
            old_table='old_rows',
            unique_key='application_id',
            merge_column_names=dbt_utils.get_column_values(
                table=ref('ref_application_form_answers'),
                column='mapped_value_name'),column='mapped_value_name'),
            update_flag_suffix='__is_updated',
            else_value='NULL'
        ) }},
       new_rows.aggregated_model_timestamp

FROM (
    SELECT form_attr.application_id,
           {{ pageup_dbt_utils.mapped_pivot(
               column='form_attr.attribute_key',
               mapping_table=ref('ref_application_form_fields'),
               mapping_key='map_key',
               mapping_value='mapped_value_name',
               agg='MAX',
               then_value='form_attr.attribute_val_text',
               else_value='NULL',
               add_update_flag_on_incremental=true,
               update_flag_suffix='__is_updated'
           ) }},
           MAX(form_attr.model_timestamp) AS aggregated_model_timestamp

    FROM {{ ref('work_application_form_attribute') }} AS form_attr
    -- other joins

    {%- if is_incremental() %}
    WHERE form_attr.model_timestamp > (select max(model_timestamp) from {{ this }})
    {%- endif %}

    GROUP BY xx.application_id
) new_rows

{%- if is_incremental() %}
LEFT JOIN {{ this }} AS old_rows on old_rows.application_id = new_rows.application_id
{% endif -%}

{{
  config({
    ""materialization"": ""incremental"",
    ""unique_key"": ""application_id""
    })
}}

```

And no post on performance would be complete without a pretty chart:

![Lovely|690x331](upload://jGyoqjGhQasljGiOpfCg99vvebT.png) 

~800seconds down to <50 seconds. Lovely :)

**Caveats**

This is not a perfect solution for all situations, it does make a lot of assumptions:

* This will not work if your source data contains more than 1 value for a given pivot key (eg if you are pivoting on count).
* Some database engines/models simply wont get any benefit from this. Either the system is already clever enough to optimize or there are other blockers to performance. So your mileage may vary.
* Does not work if you have a dynamic list of columns to pivot (but then incremental materialization hates this already).
* Efficiency aside, this code is nowhere near as easy to understand as the pivot we started with. This makes it harder to come back to fix, and for new users to understand what is going on.

Hopefully someone finds this useful.

[dbt-utils-pivot]: https://github.com/fishtown-analytics/dbt-utils/blob/master/macros/sql/pivot.sql
[pageup-dbt-utils-timestamp-incremental]: https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/materializations/timestamp_incremental.sql
[pageup-dbt-utils-timestamp-columns]: https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/sql/timestamp_columns.sql
[pageup-dbt-utils-mapped-pivot]: https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/sql/mapped_pivot.sql
[pageup-dbt-utils-merge-updated-values]:https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/sql/merge_updated_values.sql",1
535,"pass-column-to-macro-as-variable","I am trying to pass a column into a macro for a calculation - specifically to take a date and return the # of days in that month. My code looks like this:

select {{ days_in_month(a.event_date) }}
from [table_name] a

when I try to build the model using 'dbt run', I get this error message:  'a' is undefined

I can't find any examples in the docs where a column is passed into a macro, so maybe it's not possible?

Thanks in advance for your help!",1
535,"pass-column-to-macro-as-variable","Hey. You need to wrap the text you pass the macro in quotation marks. In Jinja, it will otherwise think you are passing it the variable `a.event_date`.

This _should_ compile (and run if macro is written correctly):
```sql
select {{ days_in_month('a.event_date') }}
from [table_name] a
```",2
535,"pass-column-to-macro-as-variable","Awesome, that did the trick. Thanks @dylanbaker!",3
535,"pass-column-to-macro-as-variable","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
557,"best-practices-for-frequent-batching","For those that batch hourly or more frequently, what problems have you run into and how did you overcome them? 

Curious to learn about best practices and how far people have pushed batching with dbt.",1
557,"best-practices-for-frequent-batching","Per Ben Edwards' excellent advice, you can make a custom dbt materialization for minimizing boilerplate. Let's leave out implementation details and instead talk about the concept, the inputs, and the output. 

if you can maintain a clear monotonically increasing primary key column with a consistent column name (let's call it  `primary_key`  for the moment), you can embed strong assumptions as code in a custom materialization - if your compiled sql should look like
```sql
[CREATE TABLE | INSERT INTO]{{incrementally_updated_materialized_view}} AS
SELECT *
FROM {{compiled sql select statement}}
WHERE {{primary_key}} > (SELECT MAX(primary_key) FROM {{incrementally_updated_materialized_view}})
```
then you can pretty easily template the whole thing as a custom materialization where all you need to do is specify which of the columns in <compiled sql select statement> is the primary key as an input to the materialization. If we call the primary key `incremental_key` and the incrementally updated model `this_is_fun` pulling from a table `fun.facts`, then your uncompiled sql might look something like 
```sql
#/path/to/models/this_is_fun.sql

{{materialized=simple_incremental, incremental_key='created_ts_utc'}}
SELECT created_ts_utc, foo, bar, baz
FROM ref('fun','facts');
```
which would become
```sql
INSERT INTO {{target}}.this_is_fun
SELECT created_ts_utc, foo, bar, baz
FROM fun.facts
WHERE fun.facts.created_ts_utc > (SELECT MAX(created_ts_utc) FROM {{target}}.this_is_fun)
```
and if the table didn't exist it would run a create statement.All the standard incremental macro stuff can be abstracted away if you're willing to accept constraints, and you can extend it as needed to handle incrementally updating tables with multiple highwater marks.",2
557,"best-practices-for-frequent-batching","Here is a simple one -- have the job time out if it executes for longer than you think it should. This will prevent cascading failures from queued jobs if a single query hangs. dbt Cloud lets you configure this out of the box",3
557,"best-practices-for-frequent-batching","Ok, finally found time to write this.

It is very possible to write super efficient dbt projects.  I would go as far to say that while dbt's first design goal is to make your engineering workflow easier, it's second is to make it faster to run. Recently my team scaled out our dbt-based data pipeline to work across 90 pilot tenants concurrently, executing every 15 minutes. I'll let you do the math on how many executions that is. Thanks to the tooling that dbt provides, we were able to get the average execution time down to under 2 minutes.

This is the result of months of dev, and there are quite a few things we learned.  Most importantly is to realise the tradeoff you are doing.  Beyond a certain point, **performance improvements come at the cost of increased complexity**. Complexity for your data analysts/engineers maintaining your models, complexity for your ops team maintaining your batching process. This complexity needs to be managed and mitigated to keep your code/pipeline maintainable.  So here are some tips for speeding up your code, and suggestions for keeping your code maintainable.

## Only do work when you need to

Less models to run is always going to be faster. If your EL tool has the ability to tell you what models have new data since the last time you started a batch, you can use that to build up a list of models to pass into dbt with `dbt run --models xxx+ yyy+ zzz+`. How effective this is depends on your sources and how frequently your sources update, it also depends on how you are combining these sources into models.

**Placing views in front of your source tables** is also very useful (and can help with basic data cleaning). Views for models are also good but be wary of potential performance impacts of views being used in downstream models.

Calling dbt with a filter on models sounds easy enough, and it has the benefit of not impacting your data analyst's workflow in any way. But it does introduce some larger concerns for your overall pipeline:

* Your EL process now needs to track state
* dbt needs to be told about EL's state
* dbt needs its own state to tell what it last processed
* What about when models change? Guess we need to check for that and `FULL_REFRESH` those models when they do.
* This is a lot of state to track, probably needs to be fault tolerant as well.

None of these concerns can be (or should be) solved by anything dbt offers. And some of these should be done regardless what frequency you run batches, as it is just good sense for automation.

## Faster models

### Incrementals

**`materialized=""incremental""` is your friend**. Incremental is the tool dbt offers to make fast materializations possible. Embrace its power, but you must also understand it. You should be very comfortable with how incrementals work, how your models are executed differently on full refreshes vs incremental runs, and know how you are handling data deletion if that is a thing in your system (in this age of GDPR, it's hard for deletion to not be required).  Having said that, try to replace all table materializations with incrementals.

Incrementals add complexity to your dbt project, as suddenly all your models require branching logic that changes the final sql. It won't take you long to realise that this code is very similar in almost all your models. It becomes boilerplate, code you can't live without, mostly copy/paste, but disastrous if it has a mistake in it.  At this point you should try to make the most of conventions in your code.

### Conventions / keeping things DRY

Incremental updates work by tracking some column in the referenced models and seeing if it is newer than the newest value in the existing model. **These versioning columns should be consistent throughout your project**. Same name, same datatype, same behaviour. Doing this means that your incremental logic will always look the same, and it allows you to macroify some [common patterns](https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/sql/timestamp_columns.sql) in your models.

If most models have the same pattern for incremental logic, then this should be your next target for conventions.  Try writing an **incremental model that generates the incremental conditions for you.** @drop_database_public has gone into how this can be done above, and you can [look here](https://github.com/pageuppeople-opensource/pageup-dbt-utils/blob/master/macros/materializations/timestamp_incremental.sql) for an example that I use.

These conventions won't work for all scenarios (remember the 80/20 rule?). And the regular incremental model is still there if you need it. But you should be manually implementing materialization logic by exception, not by convention.

### Ephemeral vs work table

Ephemeral models exist to allow you to split up complex models into easy to understand chunks. When going for fast batches though, you should consider replacing them with work tables that are materialized incrementally. This can have a big improvement on incremental performance of complicated models. Do as many work models as makes sense for your model. This all comes at the cost of space, hopefully that isn't a huge concern.

## Measuring performance

dbt has a wealth of data about it's runs that it doesn't expose via the command line. Even when it does, you want a way to measure across all your runs. Odds are, if you are reading this, **you have a data warehouse, so let's use it**. Use dbt's start and end hooks to record when a batch started and finished, and with what parameters. dbt has a very detailed [result model](https://docs.getdbt.com/docs/api-variable#section-result-objects), dump **all** of it to your data warehouse.

Next, you probably have a fancy BI tool plugged into your data warehouse, so lets use that. You should be able to answer the following questions easily:

* How fast are my jobs?
* What models consumed the most time over the last X runs?
* Did my code change improve or worsen performance?
* What are the most recent errors that caused models to fail?
* At any given time, what is the maximum age of the data in my data warehouse?

It should be easy to see how this data can be used by you, and your data analysts, and even by your consumers.

## Where is the limit?

Table transformations are the slowest possible model you can have. But they are a good baseline, their performance scales with the size of the dataset, not the size of the new data.  At the other end of the scale there is the ""perfect"" incremental model. This model has the perfect query plan, has every index it requires, and only works with data that has changed.  The performance of this model scales only with the volume of new data.  This is what you want to work towards, though it can be hard to tell how close you are to it. But there will get a point where you will start writing increasingly creative sql to try to squeeze out a few seconds more performance.  Don't going insane trying to make it faster :stuck_out_tongue: 

Tests are an interesting use case, as they will never positively affect your the speed of your runs. I feel they still have value as a way to sanity check that your models contain good quality data. Even then, don't test things that are already tested. For example, don't test a primary key is unique on a model if you already added a unique index as a post hook.

## Notes on scheduling

Not much to say here, whatever method you were using for a daily batch would probably work for hourly or less as well. One word of warning though, be very careful to ensure that you **never run multiple dbt jobs at the same time**. I've had it happen before, the results are not pretty.",4
557,"best-practices-for-frequent-batching","can you expand on 'never run multiple dbt jobs' at the same time? please?",5
561,"how-do-i-specify-a-different-schema-for-my-source-at-run-time","Is it possible for me to specify a different schema at the DBT command line to override the one that is present in my source .yml file? 

Here's an example of what we have today:
```
sources:
  - name: IC
    description: ""IC data from MySQL Prod""
    database: ic_reporting
    schema: icmysql
```

Currently all of my staging tables are referencing IC as a source.   I'd like to be able to change the schema to `icmysqlqa` when I'm running this code via my DBT Cloud job that runs for our QA environment.  For Production and Dev, I'd like for this value to be as is.

I also thought about creating another source called IC_qa but that would involve repeating the metadata I already have in the other source and that wouldn't be good for maintenance.

Thanks
W",1
561,"how-do-i-specify-a-different-schema-for-my-source-at-run-time","Hey @chanwd - this is a great question! I think you generally have two options here:

## 1. Use a `var()`

In this approach, you can supply a variable on the command line (or in your dbt Cloud step) which renders into the definition of the source. In practice, this might look like:

```yaml
sources:
  - name: IC
    description: â€œIC data from MySQL Prodâ€
    database: ic_reporting
    schema: ""{{ var('ic_schema', 'icmysql') }}""
```

This configuration sets up a dbt [Variable](https://docs.getdbt.com/docs/var) called `ic_schema` with a default value of `'icmysql'`. This value can be overridden on the command line using the `--vars` flag, eg:

```
$ dbt run --vars 'ic_schema: icmysqlqa'
```

When this variable is supplied, dbt will set the source schema for the `IC` dataset to be `icmysqlqa` instead of `icmysql`.

This is a pretty good approach if you have a single dataset where you want to toggle between dev and prod source datasets. It doesn't scale quite as well if you have more than one such dataset, or if you want to introduce additional environment-specific datasets.

## 2. Use YAML Anchors

The YAML spec supports [Anchors](https://confluence.atlassian.com/bitbucket/yaml-anchors-960154027.html), which should work in dbt source definitions. Anchors (and aliases) could be a good way to maintain separate definitions for these sources without duplicating any attached metadata/tests.

In practice, this might look like:
```
version: 2

sources:
    - name: ic_prod
      schema: icmysql
      tables: &tables
          - name: table1
            description: ""abc123""
          - name: table2
            description: ""def456""

    - name: ic_qa
      schema: icmysqlqa
      tables: *tables
```

This config would create two difference sources with an identical set of tables. With this syntax, you can also overrides tests/descriptions for specific environments if necessary - check out the ""Override values"" section in the link shared above!

The big benefit here is flexibility: you can have a dedicated source for each dataset + environment pair. If you go this route, you can switch between them in you staging models, eg:

```
-- models/ic/stg_model.sql

select  *

{% if target.name == 'qa' %}
from {{ source('ic_qa', 'table_1') }}
{% else %}
from {{ source('ic', 'table_1') }}
{% endif %}
```

Let us know which of these you end up going with!",2
561,"how-do-i-specify-a-different-schema-for-my-source-at-run-time","Thanks for taking the time to answer my question so thoroughly and promptly.

I opted for the 2nd option and created a 2nd source to represent my QA database information while also leveraging the aliases to avoid repeating information within the yml file.

I then created a macro that takes two parameters (schema and table) and called it `db_source`.

Something like this:
```
{% macro db_source(source_name, table_name) %}

{% if target.name == 'qa' %} 
{{ source(source_name + '_qa', table_name)}}
{% else %}
{{ source(source_name, table_name)}}
{% endif %}

{% endmacro %}
```

And then referencing it in all of my staging table SQL as:
```
from {{ db_source('ic', 'fct_table') }}
```

The one thing I do want to point out with the macro is that the expectation is that each of your sources are distinguished by a suffix, so QA should have a section with the same name as production but with `_qa` to distinguish the yml source name.

Something like this:
```
sources:
  - name: ic
  - name: ic_qa
```",3
561,"how-do-i-specify-a-different-schema-for-my-source-at-run-time","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
564,"testing-with-fixed-data-set","I would like to test our models with fixed test data, not the real data. These tests would verify the model output against a known input data. The goal is to separate the testing of the model logic and the actual data.

I'm thinking about seeding the model source tables with fixed data and then running the tests. We're using Snowflake and usually conduct development and testing in cloned test databases, hence truncating sources and replacing with test data is not a problem.

A problem I see with my approach is that these logic -tests would fail if run against CI or PROD databases, because there I can't seed the source tables. Is there a way to group tests and only run data tests for example?

Is anyone practising this type of logic testing?",1
564,"testing-with-fixed-data-set","Hey @jtke! Such a good question! I have done a version of this where I use fake data while developing models: usually my fake data consists of all my known edge cases so that I can check that my SQL works. Unfortunately, I usually only do that during development, and switch out my references to the fake data before merging the code.

However, I *have* thought about extending this to be more long-lived in my project! Here's what I would do:

## 1. Add seed files for your expected input and output to your dbt project

Name them sensibly, like `input_subscription_events` and `expected_subscription_periods`. Normally I like to design these seed files to have lots of tricky cases so I can be sure that my SQL is working as intended. Don't forget to run `dbt seed`.

If you want to run things with ""real"", but static, data, you could take the time to recreate the tables in a separate schema -- check out [this post](https://discourse.getdbt.com/t/how-do-i-specify-a-different-schema-for-my-source-at-run-time/561/3) on some approaches there!

### 2. In your models, `ref` the static data

But wrap it up in some conditional logic! (Yup, you can `ref` seeds if you decide to go down that route).

```sql
-- subscription_periods.sql

with subscription_events as (
  {% if target.name == 'ci' %}
  select * from {{ ref('input_subscription_events') }}
  {% else %}
  select * from {{ ref('stg_subscription_events') }}
  {% endif %}
)
...
```

### 3. Add an ""equality"" test to check that the expected input matches the expected output

We use [this test](https://github.com/fishtown-analytics/dbt-utils#equality-source) to check that two relations match exactly. You can see us use this testing pattern all throughout `dbt_utils` in its own integration tests (e.g. [here](https://github.com/fishtown-analytics/dbt-utils/blob/master/integration_tests/models/datetime/schema.yml))
```yml
# schema.yml
version: 2

models:
  - name: subscription_periods
    tests:
      - dbt_utils.equality:
          compare_model: ref('expected_subscription_periods')
```

### 4. Run your test with `--target ci` 
The magic incanation here is:
```
dbt test --target ci
```
It should all pass if your transformations work!

But then when you run it in any other environment, that test will fail, so....
### 5. Parameterize the `equality` test to accept an `environment` argument.

You'll have to add your own version of the `equality` test to your project, and add an extra argument. You want to get to something like this:
```
# schema.yml

version: 2

models:
  - name: subscription_periods
    tests:
      - jaffle_shop.equality: # replace with your project name
          compare_model: ref('expected_subscription_periods')
          env: ci
```

I haven't done this last step for this particular macro, but I wrote about the structure of making a test environment aware in [another Discourse article](https://discourse.getdbt.com/t/conditionally-running-dbt-tests-only-running-dbt-tests-in-production/322/2).

Now when you execute `dbt test --target ci`, you'll be comparing your static data. In any other environment, you'll be running the real-deal, and tests shouldn't fail in either.

Let me know how you go!",2
564,"testing-with-fixed-data-set","Hi

The approach I'm now pursuing is a bit similar, but I found it easier to use a separate run-unit-tests.sh -script to organize files to correct places and then call dbt inside the script. I have grouped data-tests by model under ""tests/"" and the setup script copies relevant tests under ""tests/run"" at runtime.

Test input data is in seed files named like ""test__<model_source_ref>.csv"" and I use a macro to substitute the real ref with the test-ref if target==""unit-test"". Preferrably I would like like to override ref() and make it do the substitution transparently. Don't know if that's possible. Anyway, I would like to avoid having all that conditional logic in the model file that you show in step #2.

I wish I could override project settings, like test-path, with command line parameters. Then I would not need to copy files around. Now I might end up modifying the project.yml in the setup script before running the unit tests, depending on how complex the seeding becomes.

This is still work in progress and not functional. Hopefully I get it working today.",3
564,"testing-with-fixed-data-set","I got the basics working and I can test a module with predefined data set. I made a bit ugly macro to replace the model ref with a test data source when target=unit-test. Any ideas how to make this better:

> {% macro ref2(table_name) %}
> 
> 	{% if target.name == 'unit-test' %}
> 	  {{ return(ref(table_name).database + '.' + ref(table_name).schema + '.test__' + ref(table_name).table_name) }}
> 	{% else %}
> 	  {{ return(ref(table_name)) }}
> 	{% endif %}
> 
> {% endmacro %}",4
564,"testing-with-fixed-data-set","You can use the `get_relation` method here to do this a bit more cleanly ([docs](https://docs.getdbt.com/docs/adapter#section-get_relation)) -- it's longer but IMO more readable.
```
{% macro ref_for_env(model_name) %}

{%- set normal_ref_relation = ref(model_name) -%}

{% if target.name == 'unit-test' %}

{%- set test_ref_relation = adapter.get_relation(
      database = normal_ref_relation.database,
      schema = normal_ref_relation.schema,
      identifier = 'test__' ~ normal_ref_relation.identifier
) -%}
      
{{ return(test_ref_relation) }}

{% else %}

{{ return(normal_ref_relation) }}

{% endif %}

{% endmacro %}
```
^ You might want to do some [whitespace control](https://jinja.palletsprojects.com/en/2.10.x/templates/#whitespace-control) on that.",5
564,"testing-with-fixed-data-set","I wanted to continue the conversation because I think having an easier method to do unit testing would definitely improve dbt.
In my opinion, using behave would be a great fit for dbt as it will make tests as readable by analysts as models are. I played with other methods but I think this is easier to read. Let's say we want to unit test for a model called ""fom"" that counts number of days in a month:

```
  Scenario: run a sample unit test
     Given calendar is loaded with this data
         | dt         | first_day_of_month |
         | 2020-01-01 | 2020-01-01         |
         | 2020-01-02 | 2020-01-01         |
         | 2020-02-02 | 2020-02-01         |
      When we run the load for fom
      Then the results of the model are
         | first_day_of_month | count_days |
         | 2020-01-01         | 2          |
         | 2020-02-01         | 1          |
```

There's a working POC in https://github.com/jmriego/dbt-bdd
Would something like this make sense? Any feedback would be great as we are prioritizing this again and would be happy to contribuite it to dbt if it's a good fit",6
564,"testing-with-fixed-data-set","@jmriego I've spent a few days working on something quite similar to this and I believe lack of self-contained unit-tests for DBT models remains on of the biggest painpoints for mature use of this project.  Most important to me is to create an easy-to-define unit-test format that does **not** require any changes to how existing DBT models are written so it can be used against any number of mature DBT projects with hundreds of models seamlessly. It should also require only having to define the fakes for a models' immediate dependencies, not its transitive dependencies.

I'm not keen on the recommended approach of a super-ref macro that may dynamically swap between real models and fakes, as now this feels like testing infecting the implementation. To test Model C which has depends on Source A and Model B, the general strategy for me is to create fakes for Source A (the relevant tables therein) and Model B, as you've done in your BDD `given` clause via dbt-seed. This sounds simple but a number of problems here:

1. The model being tested must be materialized ephemerally, or else the compiled sql will simply refer to the corresponding namespace in the DB giving us no surface to inject our fakes.
2. The model being tested must have its model dependencies materialized non-ephemerally or else the compiled sql will bring the model-dependencies sql in-line as a CTE which means we cannot inject a fake.
3. When creating seeds for the test's dependencies, there will be node namespace collisions between the seed fakes and the defined models (ie. the fake seed B vs. the Model B). These are sorta deliberate, as we want the fake seeds to be defined in the namespaces of the models they are faking, however DBT commands won't work on such a project structure as it must construct a DAG on all its invocations.

The main issue is that it is _quite_ wacky to get around some of the holes in DBT's internal APIs and in its current form it exists as a meta-program that makes external calls to the DBT binary rather than something easily pluggable within DBT. The program basically has to re-write the DBT project file (it must have different versions between creating the fakes and running the unit-tests to avoid namespace collisions when constructing DBT's acyclic graph) to inject a custom seed-configuration and make sure model configurations have the correct materialization strategy.

The program then has to create its own property-file to define the model-tests (which for now use dbtutil.equality) - the ugliness here is that DBT allow property-files to be defined in multiple files scattered throughout the defined project's source-path and there again will be namespace collisions. DBT does not provide a mechanism to ignore certain property-files or establish some order-of-precedence; it indiscriminately tries to load and parse all .yml extension files in the source-path, so now the program must *modify* any of these files, temporarily changing their extensions to force DBT to ignore them (yuck!)

I've gotten a working version of this, but not happy with the gross file-system side-effects and general hacky-smell of the process. The process fits terribly in DBT's existing abstractions and a more native-solution would be preferable. Until then, this is what I've settled on.",7
564,"testing-with-fixed-data-set","Definitely agree with you on that @clee , I both think unit testing is really important for mature use of the project and tests should not requrie modifying the existing DBT models.

The example I put there doesn't really use a super-ref macro but it still requires modifying the existing ref function. Using your example of testing Model C which has depends on Source A and Model B, the testing scripts will run dbt jobs but passing it variables looking like this: `{'A': 'testcase1', 'B': testcase1'}`

So the ref function doesn't guess where the test case should be but you can configure it at run time. As you I didn't find a perfect solution that would allow fakes but I think this is good enough for testing transformations",8
564,"testing-with-fixed-data-set","Hi, 

One thing that I've feel missing is the ability to control different aspects of a model during different tests. Ideally I would conditionally like to **change data source(s) based on which test is running** to add data sources that **can test both positive and negative cases**; as such global settings such as `env=ci` doesn't cut it (unless you want to sacrifice readability dumping all the test cases in 1 huge table and then filtering). 

I did a test with implementing a `cte` helper that returns the compiled code of a model. This allows one to switch out anything (not be most beautiful with string-replacements but works well enough, especially for high entropy strings from ref() and source() )

**Contrived example** 
`models/addder.sql`
```sql
    SELECT 
      *,
      a+b AS sum
    FROM {{ ref('some_other_model') }}
```

`/tests/adder_001.sql`
```
{% set model_to_test = cte('model.addder')
      | replace(ref('some_other_model'), ""mocked_src1"") 
%}


WITH
mocked_src1 AS (
  SELECT
    *
  FROM
  UNNEST(
   [
     struct(1 as a,  2 as b,  3 as expectedSum),
     struct(2 as a,  2 as b,  4 as expectedSum),
     struct(0 as a,  0 as b,  0 as expectedSum),
     struct(1 as a, -2 as b, -1 as expectedSum),
   ])
),

final AS (
  {{ model_to_test }}
)

-- Some SQL assertion based on the specific mocked data.
SELECT * FROM final WHERE sum != expectedSum
```

**The advantages being:** 
1) Pure dbt(Jinja/SQL); 
2) ""full control"" per test
3) No changes/branching/flags required in production models. 
4) Supports both virtual tables as per above and references to larger data sets / seeds. 
5) no yaml.
6) potential to be plugin free due to low code impact

**Downsides:** 
1) Using string replacements can be fragile. Works well for ref/source replacements since these are high entropy strings with low change of accidental replacements.
2) Requires a new macro (`cte`) in the DBT core but this was quite minimal.
3) The 10 line POC worked well in simple tests but might introduce unknown behavior that i'm unaware of.

I'm also considering if the `cte` helper should accept another argument which can override context-variables such as val(), maybe even refs but haven't been able to find a clean solution to do it yet. 

**Would love to hear if some community feedback on this approach.** I think think could solve a lot of unit testing cases with a simple approach but i might also been drinking too much cool aid :man_shrugging:",9
564,"testing-with-fixed-data-set","Did some more prototyping and it seems fully doable to implement both var/ref/source overrides in a cte-call. 

```sql
{% set model_to_test = cte(
    'model.adder',
    {
      'var': {
        'a_variable': '""replacement""'
      },
      'ref': {
        -- ref('model_name')
        'model_name': 'test_data'
      },
      'source': {
        -- source('source', 'model_name')
        'source.model_name': 'test_data'
      }
    }
  )
%}
```",10
564,"testing-with-fixed-data-set","Partially inspired by the comments in this thread, I recently put together a unit testing package that fits what I'm looking for (and hopefully what others are looking for too!):
1. Simple and integrated into dbt; no external scripts to run
2. Tests are defined in `schema.yml` and run via `dbt test` without extra vars/flags required
3. Mocks and expectations are mainly defined via seeds (but they can also be SQL/macros/models if desired)

https://github.com/mjirv/dbt-datamocktool

The project README has some more advanced usage, but the basic setup is really simple. Just create some seeds for your mock inputs and expected outputs, define your tests in `schema.yml`, and run `dbt test`:

```yaml
  models:
  - name: stg_customers
    tests:
      - dbt_datamocktool.unit_test:
          input_mapping:
            source('jaffle_shop', 'raw_customers'): ref('dmt__raw_customers_1') 
          expected_output: ref('dmt__expected_stg_customers_1')
    columns:
      ...
```
I'd be curious to get folks' thoughts on this and whether there's anything missing. Feel free to reply here or message me on Slack with any questions or ideas!.",11
564,"testing-with-fixed-data-set","Hi!
I have tested and like datamocktool! 

Do you have any plans / thoughts about how to test incremental models?


Regards, 
Petter",12
564,"testing-with-fixed-data-set","[quote=""mikael, post:9, topic:564""]
**Would love to hear if some community feedback on this approach.** I think think could solve a lot of unit testing cases with a simple approach but i might also been drinking too much cool aid :man_shrugging:
[/quote]

Hello Mikael, 

I really like your approach and it seems that solves many unit testing cases. Did you manage to make this thing work? Could you share with us some more information about how you developed and finally used the **cte** macro?

I would really appreciate any help from you because your approach to testing seems to be so useful.

Regards,
Aris",13
564,"testing-with-fixed-data-set","Did a POC PR but it never got passed the maintainers; might be rotten by now.

https://github.com/dbt-labs/dbt-core/pull/3476",14
564,"testing-with-fixed-data-set","Thank you so much for helping me. It seems that works properly up to now, apart from some cases in which models are materialized as view. In particular, sometimes when some of the models used in the tests are materialized as view (with a config block in the sql file, and not in the yml file), I see in the target/run/ directory that dbt tries to create a view like 

  **""create view mydb_dbt_test__audit.mymodel__dbt_tmp 
 as (...""**

and finally I take a message in the console like

**""dbt internally failed to execute test.dbt_tdd.mymodel: Returned 0 rows, but expected 1 row""**

Could you have any idea about why this happens?",15
564,"testing-with-fixed-data-set","Based on some more tests, I have ended up that the model_sql helper works properly when there isn't a config block at the start of the model .sql files. When there is one, the testing mechanism of dbt fails. Instead, if you add all the configuration of the models in the .yml files, the model_sql helper seems to work fine.",16
564,"testing-with-fixed-data-set","Hi,

we developed a framework for dbt uni testing, you can check it [here](https://github.com/EqualExperts/dbt-unit-testing)",17
566,"is-there-a-rule-of-thumb-for-what-to-put-in-a-macro-vs-a-user-defined-function","From a best-practices standpoint, what should go in a macro vs a UDF?",1
566,"is-there-a-rule-of-thumb-for-what-to-put-in-a-macro-vs-a-user-defined-function","First of all, it's worth asking: _can_ the function be written as a UDF? UDFs are scalar functions -- you can only use them where you'd normally write a `select` statement. For example you could write a UDF to get the UTM parameters out of a web address, but you can't write a UDF to run a query, hold the results in memory, and then loop through them (you'd _have_ to use a macro for that).

Assuming the thing you want to do  *can*  be written as a UDF, the next question is: can the function be written as a macro? There are some functions that need to be UDFs. A good example is a Redshift function that counts the items in a json list -- you actually need to write this function in python, and import the json package, and it's just not possible to do it in Jinja

OK so let's say you can write something as both a UDF and a Jinja function. Let's take a `f_date_to_string` function as an example. Here it is as a (Redshift) UDF: 
```sql
create function f_date_to_string (varchar, datetime)
returns varchar
 stable
as $$
  select
  case
    when lower($1) in ('year') then to_char($2, 'yyyy')
    when lower($1) in ('quarter', 'month') then to_char($2, 'yyyy-mm')
    when lower($1) in ('week', 'day') then to_char($2, 'yyyy-mm-dd')
    when lower($1) in ('hour', 'minute') then to_char($2, 'yyyy-mm-dd hh24:mi')
  end
$$ language sql
```

In your model, you'd call it like so:
```
select
  f_date_to_string('month', created_at)
from ...
```
(Pro tip that I haven't included: you should consider putting your UDF in a schema!)


And here it is as a macro:
```
{% macro date_to_string(date_part, timestamp) %}
  case
    when lower({{ date_part }}) in ('year') then to_char({{ timestamp }}, 'yyyy')
    when lower({{ date_part }}) in ('quarter', 'month') then to_char({{ timestamp }}, 'yyyy-mm')
    when lower({{ date_part }}) in ('week', 'day') then to_char({{ timestamp }}, 'yyyy-mm-dd')
    when lower({{ date_part }}) in ('hour', 'minute') then to_char({{ timestamp }}, 'yyyy-mm-dd hh24:mi')
  end
{% endmacro %}
```

And how you'd call it:
```
select
  {{ date_to_string(""'month'"", 'created_at') }}
```

Generally, the folks I work with are pro-macros: the macro syntax is a little easier to learn -- they look more like python and feel a bit more intuitive than the at-times weird syntax of UDF.

Personally, I lean a little more on the side of being pro-UDFs. Yes, the syntax is a little harder, but I think they are easier when it comes to actually writing your SQL -- with macros, you have to make sure you get all 'your' ""argument"" ""'quotes'"" right (which I rarely do first time).


Try both and see what you like! If you go down the UDF approach, [here]([https://discourse.getdbt.com/t/using-dbt-to-manage-user-defined-functions-redshift/18) is an article I wrote a while back about integrating them into your dbt project.",2
566,"is-there-a-rule-of-thumb-for-what-to-put-in-a-macro-vs-a-user-defined-function","In many databases, UDFs are only scalar. However, Snowflake does have the capability to do UDTFs (User-Defined Table Functions) which can return a table per se.

https://docs.snowflake.net/manuals/sql-reference/user-defined-functions.html

Also, from a database performance perspective UDFs and UDTFs may out-perform macros, depending on how they are written, what they are doing, and what the database query optimizer can do with them.

I prefer to make my functions, well, ""functional"", that is, idempotent with no side effects. By the time functions are actually indepdendently reading from and writing to database tables, they are pretty difficult to test and get pretty ugly.",4
573,"deployment-in-secure-environments","Original question:

> I'm stuck in a fairly locked down environment. Does anyone have experience running dbt in aws lambda or glue?

Honestly, I am surprised that more people aren't in this situation. Opening connections to places like dbt cloud (or even Snowflake :frowning:) is sometimes an unacceptable liability. Especially if you are only a [data processor](https://www.gdpreu.org/the-regulation/key-concepts/data-controllers-and-processors/). Luckily there are plenty of ways to do it in house.

## Assumptions

* The job running dbt does not have general access to the internet.
* The job running dbt does not have access to hub.getdbt.com
* The job running dbt cannot pull repositories from a third party location such as github.
* The job can make use of any internal tools provided by the ecosystem it is in.

I will post some options below, but everyone else feel free to share your knowledge.",1
573,"deployment-in-secure-environments","# Everything

## :negative_squared_cross_mark: Usage statistics

As you won't have access to the internet anyway, it is best to turn off anonymous usage statistics.  They actually take a few seconds to realise they don't have internet access, so save that time.

```yaml
config:
  send_anonymous_usage_stats: False
```

## :white_check_mark: Containers

For many solutions, they will be some variant of ""put a container in X"". Containers have all those great containery features, such as being able to deploy the same container anywhere etc. But for our locked down focus they also give us a way to pull dependencies. When creating your container image, gather all your dependencies at this point. This will mean you have nothing to do at runtime except call dbt.

Inside the running container you could also access environment variable or keystores/parameter stores to get the remaining configuration you need.

Here is part of an example dockerfile.

```dockerfile
# install dbt and only the adapter you need
RUN pip install dbt-core==0.14.2
RUN pip install dbt-postgres==0.14.2

# copy whatever scripts you need to run
COPY ./container_assets/*.sh /home/local/
RUN chmod +x /home/local/*.sh

# copy your dbt profile
RUN mkdir -p /home/local/.dbt
COPY ./container_assets/profiles.yml /home/local/.dbt

#Grant user permissions and switch to that user
RUN chown -R  local /home/local
USER local
WORKDIR /home/local

# copy your project directory
RUN mkdir -p /home/local/transform
COPY ./transform /home/local/transform

# run dbt deps to get dependencies
RUN cd /home/local/transform && \
    dbt deps

# on startup the pipeline runs
CMD ./dw_pipeline.sh
```

# AWS

# :negative_squared_cross_mark: Lambda

Pretty easy no for Lambda, if your code runs for longer than 15 minutes, it gets killed. No way to work around this.

# :negative_squared_cross_mark: AWS Glue

Seems like a very expensive way to run a long running script. @martin also mentions space issues, which means there may be a limitation on how big your scripts (which includes dbt) can be.

## :white_check_mark: ECR + AWS Batch

Store your job in a container on ECR, run it on ECS using AWS Batch.

Batch is a wrapper around ECS that is meant to allow you to more easily fire off jobs, and handle things like jobs with parallel tasks, job prioritization etc.  This is all very overkill given we are only looking to run 1 job at a time.  You will also need to put a lambda + scheduler in front of this to fire off the job however often you want. And you will need to query the api to make sure the last job finished before firing off a new one, which can be clunky to do with the batch api.

So why not just cut out the middle man?

## :white_check_mark: ECR + Lambda

Store your job in a container on ECR, run it on ECS using a scheduled lambda.

The ECS and AWS Batch apis are very similar, and you still end up doing the heavy lifting of scheduling yourself anyway, so might as well just call the ECS directly and have 1 less cog in the system that can go wrong.  This approach is very easy, can be automated, requires no internet access, and is cheap.

## ECS Side note - Fargate vs EC2 Cluster

General rule is, if you can fill up an entire EC2 Cluster with containers, it is probably cheaper than Fargate.  But that comes at the cost of having to manage the host vms yourself. And for dbt, you have 1 container that runs infrequently that barely needs any resources at all. 

Rough Fargate costs:  6 hour run of DBT with 0.25 vCPUs and 0.5 GB ram (yes dbt will happily run with that) and you are looking at $0.08 per run. Small enough to be a rounding error compared to your data warehouse infrastructure costs.

# Azure

I am not active on Azure at the moment, though I would love to see someone try and put dbt on Azure Functions using their premium plan, which removes the execution time limits.",2
575,"extracting-schema-and-model-names-from-the-filename","# Default Schema and Model Names in dbt
By default, dbt builds materialized models in the schema dictated by the user's profile `target`. Furthermore, dbt names the model accordingly using the filename of the model's SQL file. This is refered to as the model's ""alias"".

For example, let's suppose I'm doing some analysis on some Stripe payment data. If my target schema is `dbt_brandyn` and I have a model called `payments.sql`, the defaults described above would build my model in `dbt_brandyn.payments`. This is a great default for quick prototypes and small transformation pipelines. However, once you start implementing large dbt projects into production, you can imagine the need for more flexibility in where tables and views are materialized.

A while ago, dbt added the `schema` and `alias` parameters for models. Suppose we edit `payments.sql` to include the following configuration:
```sql
-- models/stripe_payments.sql
{{ config(schema='stripe', alias='payments', ...) }}
select ...
```
Then, in this case, the model is materialized as `dbt_brandyn_stripe.payments`. This might not be the schema that you'd prefer, but this points to the flexibility of dbt's architecture. Somewhere in dbt's code base, dynamic schema names and aliases are being generated. In fact, they are being generated by the `generate_schema_name` macro ([code here](https://github.com/fishtown-analytics/dbt/blob/fe48478993b299b66a337f84989d8d0f9ca3dcdc/core/dbt/include/global_project/macros/etc/get_custom_schema.sql#L17)) and the `generate_alias_name` macro ([code here](https://github.com/fishtown-analytics/dbt/blob/fe48478993b299b66a337f84989d8d0f9ca3dcdc/core/dbt/include/global_project/macros/etc/get_custom_alias.sql#L15)). It's worth reviewing those snippets and convincing yourself that they do, in fact, produce the results above.

# Our First Override
What's more, you can override these macros in your dbt project. Using an override, you can effectively define a function that specifies the schema and alias for each model based on parameters in your profile (e.g., `target`, if you want to distinguish between `dev` and `prod` runs) and many of the properties intrinsic to each node (e.g., it's name, config parameters like `schema` and `alias`, tags, materialization strategy, etc). It's extremely flexible and details for overriding these macros can be found in dbt's documentation.

We would probably prefer to materialize `payments.sql` as `stripe.payments` in our warehouse. In order to do so, we can override the `generate_schema_name` macro as follows:
```sql
-- macros/custom_naming_macros.sql
{% macro generate_schema_name(custom_schema_name, node) -%}
  {%- set default_schema = target.schema -%}
  {%- if custom_schema_name is none -%}
    {{ default_schema }}
  {%- else -%}
    {{ custom_schema_name | trim }}
  {%- endif -%}
{%- endmacro %}
```
Furthermore, let's say you have payments from two different sources: `stripe` and `braintree`. You might want to materialize one model as `stripe.payments` in your warehouse, and the other as `braintree.payments`. In dbt, you can't have two files with the same name, so instead of calling them `stripe/payments.sql` and `braintree/payments.sql`, you need to have distinct names (like `stripe_payments.sql` and `braintree_payments.sql`). Use the `schema` and `alias` parameters as above. For example:
```sql
-- models/stripe_payments.sql
{{ config(schema='stripe', alias='payments', ...) }}
select ...

-- models/braintree_payments.sql
{{ config(schema='braintree', alias='payments', ...) }}
select ...
```
# A More Advanced Example: Using the Filename
I'd like to provide an example of how we use these macros at my company. In particular, we have a number of contributors working in a Redshift warehouse. Our contributors are not super users and each has a staging schema like described earlier; mine is `dbt_brandyn`. When a user sets up their `dev` target in their profile, this will be the `schema` they use.

We settled on a naming convention for models that are materialized as tables and views. They are always named `schemaname__modelname.sql`, with two underscores separating `schemaname` and `modelname`. In production, we want these models to be materialized at `schemaname.modelname`. This naming convention also makes it obvious what table your referencing when you use `ref('schemaname__modelname')`.

However, when a user is developing, we need to build these models in the user's staging schema. We need to be careful not to materialize at `dbt_brandyn.modelname` because that would result in potential name collisions (e.g., in the payments example above, we can't materialize the Stripe payments and Braintree payments both at `dbt_brandyn.payments`). We settled on the following format: if I'm building a model in my staging schema using the `dev` target, it should be built to `dbt_brandyn.schemaname__modelname`.

Once we agreed on the above convention, we set out to make dbt do that. We override `generate_schema_name` and `generate_alias_name` as follows:
```sql
-- macros/custom_naming_macros.sql

{% macro generate_schema_name(custom_schema_name=none, node=none) -%}
  {%- if target.name == 'dev' -%}
    {{ target.schema }}
  {%- elif target.name == 'prod' and custom_schema_name is not none -%}
    {{ custom_schema_name | trim }}
  {%- elif target.name == 'prod' and custom_schema_name is none -%}
    {% set node_name = node.name %}
    {% set split_name = node_name.split('__') %}
    {{ split_name[0] | trim }}
  {%- else -%}
    {{ target.schema }}
  {%- endif -%}
{%- endmacro %}

{% macro generate_alias_name(custom_alias_name=none, node=none) -%}
  {%- if target.name == 'dev' and custom_alias_name is none -%}
    {{ node.name }}
  {%- elif target.name == 'dev' and custom_alias_name is not none -%}
    {{ custom_alias_name | trim }}
  {%- elif target.name == 'prod' and custom_alias_name is not none -%}
    {{ custom_alias_name | trim }}
  {%- elif target.name == 'prod' and custom_alias_name is none -%}
    {% set node_name = node.name %}
    {% set split_name = node_name.split('__') %}
    {{ split_name[1] | trim }}
  {%- else -%}
    {{ node.name }}
  {%- endif -%}
{%- endmacro %}
```
Note that via this implementation, contributors do not need to specify `schema` and `alias` parameters; instead, this macro extracts them from the filename of the SQL file, which is stored in `node.name`. However, if `schema` and `alias` parameters are specified, they are used accordingly. If you want to disable the `schema` and `alias` parameters completely, you can remove that logic the from macro outright. For example, `generate_schema_name` would become:
```sql
-- macros/custom_naming_macros.sql
{% macro generate_schema_name(custom_schema_name=none, node=none) -%}
  {%- if target.name == 'dev' -%}
    {{ target.schema }}
  {%- elif target.name == 'prod' -%}
    {% set node_name = node.name %}
    {% set split_name = node_name.split('__') %}
    {{ split_name[0] | trim }}
  {%- else -%}
    {{ target.schema }}
  {%- endif -%}
{%- endmacro %}
```
In this formulation, even if the contributor adds a `schema` parameter, it is ignored by the macro. This could lead to confusion among contributors who are familiar with how dbt ""should work"". Thusly, I like to add a compilation error if a `schema` parameter is defined for the model.
```sql
-- macros/custom_naming_macros.sql
{% macro generate_schema_name(custom_schema_name=none, node=none) -%}

  {%- if custom_schema_name is not none and node.resource_type in ['seed', 'model'] -%}
    {%- set error_message -%}
      {{ node.resource_type | capitalize }} '{{ node.unique_id }}' has a schema configured. This is not allowed.
    {%- endset -%}
    {{ exceptions.raise_compiler_error(error_message) }}
  {%- endif -%}

  {%- if target.name == 'dev' -%}
    {{ target.schema }}
  {%- elif target.name == 'prod' -%}
    {% set node_name = node.name %}
    {% set split_name = node_name.split('__') %}
    {{ split_name[0] | trim }}
  {%- else -%}
    {{ target.schema }}
  {%- endif -%}

{%- endmacro %}
```
Hopefully, this post illustrates the awesome flexibility of overriding these macros.",1
575,"extracting-schema-and-model-names-from-the-filename","Super cool writeup @brandyn! Thanks for sharing :D",2
575,"extracting-schema-and-model-names-from-the-filename","This is so so great! I've thought about doing this before, but have never done it myself -- really glad that this exists, thank you!",3
579,"can-i-create-an-auto-incrementing-id-in-dbt","Is it possible to create an auto number field while creating models?",1
579,"can-i-create-an-auto-incrementing-id-in-dbt","TL:DR: No, it's not possible to create an auto-incrementing ID in dbt.

In fact, auto-incrementing keys in general are a bad idea. The alternative is to generate a `surrogate_key` using a hash, and test the uniqueness of this key.

## Auto-incrementing IDs in the time of ELT
This is good context for anyone that doesn't come from this background!

A long time (a few years) ago, SQL statements that were used for ETL usually had two parts -- the first created the table using DDL (data definition language), and the second inserted/updated/deleted/copied data using DML (data manipulation language).

Here's an example that should works on Redshift:
```sql
create table if not exists analytics.customer (
  customer_key     integer identity(1,1),
  customer_id      integer not null,
  name             varchar(25) not null,
  address          varchar(25) not null,
  created_at       timestamp not null default current_timestamp
);

copy analytics.customer (
  customer_id,
  name,
  address
) from 's3://<your-bucket-name>/load/key_prefix'
credentials 'aws_access_key_id=<Your-Access-Key-ID>;aws_secret_access_key=<Your-Secret-Access-Key>'
;

```
This approach was often required because the same engineers writing the code to transform data, were also loading it into their warehouse from an S3 bucket. These days, it's a good idea to use Stitch or Fivetran to get data into your warehouse instead.

In this example there's an auto-incrementing key in this query (`customer_key`). Often, an auto-incrementing key was used to create a primary key for a table, especially when there can be duplicate records for the same ""natural key"" (in this case, if a customer changes address, there will be two records with the same `customer_id`, but different `customer_key`s.

## Does dbt support auto-incrementing keys?
In a word: no.

Behind the scenes, if you materialize a model as a table, dbt uses a `create table as` statement to create that table.

Let's say that you already have your customers table in your warehouse, now you can run a query like this to create the `analytics` version of customers:
```
create table analytics.customers as (
    select
        id,
        name,
        address,
        created_at
    from raw_jaffle_shop.customers
)
```
^ This is kind of a silly example because in all likelihood, you'll be doing some transformation along the way, but it's just an example :)

We can do this because in dbt we **assume that data is _already_ loaded into your warehouse**, this is why we refer to dbt as being the T in ELT (extract, load, transform) process

The `create table as` syntax **does not support auto-incrementing keys**, since you don't explicitly define the columns. Similarly, it doesn't support column default values, constraints, or encoding.

## Why use the `create table as` syntax then?

`create table as` statements are so powerful because they allow you express your transformations as `select` statements -- there's no need to run two separate DDL & DML statements. In a dbt project, **every model is a select** -- dbt handles wrapping the select statement in the appropriate DDL.

Defining every model as a `select` enables anyone that knows how to write these queries to own the transformation layer. We frequently see analysts with no prior knowledge of DDL and DML get up and running with dbt really quickly as a result!

## Why do you say that auto-incrementing keys are a bad idea?
Auto-incrementing keys make it impossible (or at least very difficult) to maintain separate development and production versions of your transformations. This is because an auto-incrementing key is not [idempotent](https://discourse.getdbt.com/t/understanding-idempotent-data-transformations/518).

If you were to create a development environment (say, a separate schema named `dev_claire`), and re-run all your transformations, the results of your auto-incrementing key would be different to your production environment. Maintaining a robust development environment is critical for a high-functioning data team, as it allows you to work quickly and test changes before deploying them.

## But I need auto-incrementing keys to track versions of a record!
One of the primary use-cases for auto-incrementing keys is to generate a unique, ""surrogate key"" in a table, where they may be multiple versions of the same ""natural key"".

For example:
* Your `customer` table has a `customer_key`, which is a unique, auto-incrementing key. This is required because your customer can change address over time.
* Your `order` table has a foreign key to customers (also `customer_key`). Since you're using `customer_key`, you can tell which version of the `customer` record was valid when the `order` was placed.

If you're using an auto-incrementing key on top of mutable source data, instead consider [snapshotting](https://blog.getdbt.com/track-data-changes-with-dbt-snapshots/) your source data and building models on top of your snapshots.

## OK, but I need a primary key for my model, what are my other options?
Yup, I agree with you there! We advocate for having a primary key (i.e. unique and not null) on every model. 

Every model should have some ""grain"" to it, i.e. what one record represents. For example:
* `customers` has one record per customer
* `subscription_periods` has one record per subscription, per valid period
* `ad_spend_by_campaign_by_day` has one record per campaign, per day

We use the `surrogate_key` macro from [dbt-utils](https://github.com/fishtown-analytics/dbt-utils#surrogate_key-source) to generate a primary key based on the grain of a model. For our `ad_spend_by_campaign_by_day` example:
```sql
select
  {{ dbt_utils.surrogate_key(
      'campaign_name',
      'date_day'
  ) }} as ad_spend_id,
  campaign_name,
  platform,
  date_day,
  spend,
  ...
  
```
Behind the scenes, this macro hashes the combination of columns, which means that your resulting `id` will always be the same for a particular record. Or, put another way, the generated `ad_spend_id` is now idempotent.

## So how do I force my surrogate key to be unique/not null?
In DDL statements, when you define a table first, you can add these column constraints to your table definition:
```sql
create table if not exists analytics.customer (
  customer_id      integer not null unique,
  ...
)
```
Not so with the `create table as` syntax! So how can you enforce this constraint?

(As an aside, did you know that Redshift doesn't actually enforce uniqueness constraints? Check out the code at the end!)

In dbt, rather than enforcing constraints on the table definition and hoping the transformation fails, we instead state our constraints as assumptions about the results of our `select` statements, and then [test those assumptions](https://docs.getdbt.com/docs/testing). Here's a test to make sure that our `ad_spend_id` is unique and not null:
```yml
version: 2

models:
  - name: ad_spend_by_campaign_by_day
    columns:
      - name: ad_spend_id
        tests:
          - unique
          - not_null
```
Behind the scenes, these tests get compiled to queries that return an error if the result > 0.
```sql
-- unique test
select count(*)
from (
    select
        ad_spend_id

    from analytics.ad_spend_by_campaign_by_day
    where ad_spend_id is not null
    group by ad_spend_id
    having count(*) > 1
) validation_errors

-- not_null test 
select count(*)
from analytics.ad_spend_by_campaign_by_day
where ad_spend_id is null
```

We can run these tests when writing code (since we can now maintain a separate development environment thanks to our idempotent primary keys), before merging it, and after our transformations have run in production. Basically -- we're going to find out whenever this assumption doesn't hold true anymore!

This pattern of testing is _much more powerful_ than column-level constraints, as you can define [custom tests]((https://docs.getdbt.com/docs/custom-schema-tests)) for any constraint that can be turned into a query. It's also much more readable IMO.

## Final thoughts
There are some teams using dbt who have tried to add auto-incrementing keys to their dbt models. If you go down this route, implementing dbt will likely feel like paddling upstream. To [quote myself](https://blog.getdbt.com/is-dbt-the-right-tool-for-my-data-transformations) (:roll_eyes:):

> dbt is definitely a _different_ approach to data transformation, and it could potentially feel a little unusual as you first get started. We encourage those who are just picking it up to find ways to work _with it_ instead of attempting to hack _around_ it: you may find that your code looks a little different than youâ€™re used to, but we think youâ€™ll be happier for it over the long run.



---
## Side notes
This shouldn't execute on Redshift, yet, it does. Good one Redshift! :clap:

```sql
$
create table jaffle_shop_dev.test_uniqueness (
  id integer unique
);

insert into jaffle_shop_dev.test_uniqueness (
    select 1
    union all
    select 1
);

select * from jaffle_shop_dev.test_uniqueness;
>>>
| id | 
|----| 
| 1  | 
| 1  | 

```",2
579,"can-i-create-an-auto-incrementing-id-in-dbt","Thank you for the explanation @claire.

What about the guideline that fact tables should be thin, long and avoid unique strings?

Do you have benchmarks on what effect 128 bit varchar keys have on performance?

Even with column store DBs there would be many unique values and high memory usage.",3
579,"can-i-create-an-auto-incrementing-id-in-dbt","@Kimcha As I understand it, the conventional wisdom against fact tables containing millions of unique `varchar` strings is based on storage considerations. (`varchar` is a byte-hungry data type, and global uniqueness means a given value can't be stored more efficiently as a numeric factor mapped to its original string value.)

One of the premises of modern databases, in addition to their columnar structure, is that storage is _cheap_â€”much cheaper than it was a few decades ago, when many principles of database design were canonized.

In fact, we've seen performance gains in Redshift by creating a common surrogate key across multiple tables that we can distribute and join on, rather than distributing evenly and joining on several columns at once. It's a `varchar` surrogate key, for instance, that powers our join between ad spend and session data when modeling UTM performance.

To take it a step further, we believe that storage is cheap and human intuition is expensive. To my analyst eyes, integer keys look like natural keys deriving from source databases proper; 128-bit hashes look like something produced as part of an analytics workflow. That ease of association is often worth a marginal increase in storage cost.

**That said**, there are functions available in most databases ([Redshift](https://docs.aws.amazon.com/redshift/latest/dg/r_STRTOL.html), [Snowflake](https://docs.snowflake.net/manuals/sql-reference/functions/hash.html), [BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/hash_functions)) that enable you to create/convert and store your surrogate keys as integers.

(I may be missing something major, in which case I'd be curious to hear more of your take!)",4
579,"can-i-create-an-auto-incrementing-id-in-dbt","Thank you for your response jerco. All of these are very interesting points.

To be honest I am not experienced enough with databases to comment on this. I only just started my DWH research. Most of what I have learned was based on Kimball's and similar star schema books.

Either way, after working with dbt for a bit there's no way I will go back to stored procedures and custom table creation-- even if there is a decent performance hit in my environment (MS SQL Server & Power BI).

DBT is just too elegant and speaks to my software-developer background very much :)",5
597,"installing-dbt-on-ec2-instance-aws","Looking to install DBT on an EC2 instance. The environment dictates it get installed on its own instance in our VPC using Terraform. Upon reviewing the install / environment docs, I wasn't able to find anything that pointed to pre-req's, sizing, etc. Any guidance is hugely appreciated.",1
597,"installing-dbt-on-ec2-instance-aws","Hi @cfranklin,
I'd take a look at the terraform docs:
https://www.terraform.io/docs/providers/aws/r/instance.html
- A t2.micro will be plenty enough.
- ubuntu or and AWS ami will work
- if you can't ssh into the ec2 instances for security reasons, look into setting up a bootstrap script that will inject your DB credentials into ~/.dbt/profiles from secrets manager and also setup a cronjob on it to run the work on the schedule you require.
Hope that helps.
Peter",2
597,"installing-dbt-on-ec2-instance-aws","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
612,"how-to-not-lose-your-mind-when-auditing-data-part-ii","This article is the second part of a series (I didn't know I was going to write a Part II, yet, here we are :man_shrugging:). Read the first part [here](https://discourse.getdbt.com/t/how-to-not-lose-your-mind-when-auditing-data/445).


**TL,DR:** The original macros in the [audit-helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/) package were super helpful in making sure you don't introduce regressions when refactoring SQL. Now, we have new macros that help identify the problematic column when a regression is introduced.

## Auditing data: no longer my least favorite task in analytics
In my previous article, I shared how our [audit-helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/) package can be used to help confirm that two queries  that _should_ give you the same results are _actually_ giving you the same results. Essentially, that you haven't introduced a regression.

I've been using it pretty heavily while migrating some legacy ETL code into dbt, and then refactoring it to make it follow our coding conventions, all while checking I'm doing a ""like for like"" replacement. And it's been going great! 

**If you find yourself refactoring SQL, you should definitely give this package a go.**

## Until it suddenly was my least favorite task again
Unfortunately, there have been a few cases lately where I _have_ introduced regressions to my SQL. Now I needed to dive into why my records weren't matching up.

### Problem 1: Mismatching column types

During a recent model migration, I went to run my validation query to compare my old relation to my new one, and I got this back:
```
[Amazon](500310) Invalid operation: UNION types character varying and bigint cannot be matched;
```
Cool cool cool.

![giphy%20(1)|500x180](upload://gliN75gZ1jYfggp4rvAEwyvSxHu.gif)

So one of my columns in my old relation is a `character varying`, whereas the new one is a `bigint`. But which column?!

The relation I was moving over to dbt was pretty wide (~40 columns), so I didn't want to inspect this by visually checking the results if I were to `select * from` my relation. Instead, I chose to query `information_schema.columns` to compare the column types, and check the the output of that query:

```sql
with a_cols as (
    select 
      * 
    from information_schema.columns
    where table_schema = 'old_etl_schema'
      and table_name = 'fct_orders'
),

b_cols as (
    select 
      * 
    from information_schema.columns
    where table_schema = 'dbt_claire'
      and table_name = 'fct_orders'
)

select
    column_name,
    a_cols.ordinal_position as a_ordinal_position,
    b_cols.ordinal_position as b_ordinal_position,
    a_cols.data_type as a_data_type,
    b_cols.data_type as b_data_type
from a_cols
full outer join b_cols using (column_name)
order by a_ordinal_position, b_ordinal_position

```
:point_up: :face_with_monocle: Oooh, the rare `full outer join` spotted in the wild! 

(If you aren't on Redshift skip this paragraph. If you are, my condolences....)

But it turns out that one of my relations is a [late binding view](https://blog.getdbt.com/using-redshift-s-late-binding-views-with-dbt/), and Redshift doesn't store late binding view columns in the the information schema. If I wanted to get the column types from this early binding view, I either had to:
* switch it to being an early-binding view, or
* instead query `pg_get_late_binding_view_cols()` ([docs](https://docs.aws.amazon.com/redshift/latest/dg/PG_GET_LATE_BINDING_VIEW_COLS.html))

![image|112x112,20%](upload://3tZ0Zv7iF7ffoC7wVUUPKj2wOii.png).

I went with the latter, because fortunately, @drew has already done all the hard work to [write this query in dbt](https://github.com/fishtown-analytics/dbt/blob/23484b18b71010f701b5312f920f04529ceaa6b2/plugins/redshift/dbt/include/redshift/macros/adapters.sql#L71) (let's face it, he's the real MVP).

(OK if you aren't on Redshift, you can come back now, we hope you enjoyed whatever party you got to go to during that part, we're all jealous!)

And then because I never want to solve this problem again, and I don't want anyone else to have to either, I packaged it up in a [macro](https://github.com/fishtown-analytics/dbt-audit-helper#compare_relation_columns-source) and added it to [audit-helper](https://github.com/fishtown-analytics/dbt-audit-helper).

```
{% set old_etl_relation=adapter.get_relation(
      database=target.database,
      schema=""old_etl_schema"",
      identifier=""fct_orders""
) -%}

{% set dbt_relation=ref('fct_orders') %}

{{ audit_helper.compare_relation_columns(
    a_relation=old_etl_relation,
    b_relation=dbt_relation
) }}
```
The compiled query helped me quickly find my offending columns:

| column_name | a_ordinal_position | b_ordinal_position | a_data_type       | b_data_type | 
|-------------|--------------------|--------------------|-------------------|-------------| 
| order_id    | 1                  | 1                  | integer           | integer     | 
| customer_id | 2                  | 2                  | **character varying** | **bigint**     | 
| ...         | ...                | ...                | ...               | ...         | 

Once I found that `customer_id` was the issue,  I was able to cast it to the correct data type, and run my `compare_relations` query.

### Problem 2: Lots of rows that don't match
So once I could run my comparison query, it turned out that **my records did not match up across my two relations**. In fact, almost 5k of my records didn't match:
| in_a  | in_b  | count | 
|-------|-------|-------:| 
| true  | true  | 35 657 | 
| true  | false |   4 805 | 
| false | true  | 4 774 | 

![image|112x112,20%](upload://3tZ0Zv7iF7ffoC7wVUUPKj2wOii.png) ![image|112x112,20%](upload://3tZ0Zv7iF7ffoC7wVUUPKj2wOii.png)

With almost 5k records, and 40 columns, this wasn't going to be something I could (read: wanted to) debug by hand.

The solution? A macro ofc! I wrote a macro where, for a given primary key (used to match records), and for a given column, it would tell me whether the values matched across two relations. Here's the SQL for comparing the `status` column:
```sql
with a_query as (
    select * from old_etl_schema.fct_orders
),

b_query as (
    select * from dbt_claire.fct_orders
),

joined as (
    select
        order_id
        a_query.status as a_query_value,
        b_query.status as b_query_value,
        case
          when a_query.status = b_query.status
            then 'âœ…: perfect match'
          ...
        end as match_status
    from a_query
    full outer join b_query using (order_id)
),

final as (
    select
        match_status,
        count(*) as count_records
    from joined
    group by 1
)

select * from final
```
:point_up: :face_with_monocle: _Another_ `full outer join`! So cool! And is that an _emoji_ in a query output? I've gone too far. Someone save me from myself, please.

Again, to save myself from ever writing this again, and so others don't have to either, I've added it to [audit-helper](https://github.com/fishtown-analytics/dbt-audit-helper/blob/3b479d1d10d81a93ce6fa9a7a73eb41d6df92a20/README.md#compare_column_values-source).
```
{% set old_etl_relation_query %}
    select * from old_etl_schema.fct_orders
{% endset %}

{% set new_etl_relation_query %}
    select * from {{ ref('fct_orders') }}
{% endset %}

{% set audit_query = audit_helper.compare_column_values(
    a_query=old_etl_relation_query,
    b_query=new_etl_relation_query,
    primary_key=""order_id"",
    column_to_compare=""status""
) %}
```
Then, to make my debugging really extra, I iterated through the columns of my relation, and printed the results to the command line. Here are the highlights:
![40%20PM|541x500](upload://kn2z9uj3jSQqYBRzlIvfimCvKfK.jpeg) 

From here, I could quickly tell that it was my `status` column was causing a lot of issues, and could debug my SQL accordingly. (The full instructions for this usage are [here](https://github.com/fishtown-analytics/dbt-audit-helper#advanced-usage))

## :tada:  Announcing audit-helper v0.0.3 :tada: 
These new macros, `compare_relation_columns` and `compare_column_values` are now in the [latest release of audit helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/). If you're new to dbt packages, check out the [docs](https://docs.getdbt.com/docs/package-management).",1
612,"how-to-not-lose-your-mind-when-auditing-data-part-ii","How did you trigger the comparisons on `dbt compile`?",2
612,"how-to-not-lose-your-mind-when-auditing-data-part-ii","Hi Vish ðŸ‘‹
I used the `print_table` method â€” check out the [README](https://github.com/fishtown-analytics/dbt-audit-helper#compare_column_values-source) for usage details.",3
618,"best-practices-for-cicd-deployment","Hey there! 

I am a relatively new dbt user who is nearing the stage of deploying to production, and my team is going to use CI/CD on gitlab. Are there any best practices or words of wisdom we should be aware of as we start setting this up? I'l be working with a dev who is much better at CI/CD but has almost no exposure to dbt architecture/set up. 

Thanks in advance!",1
618,"best-practices-for-cicd-deployment","dbt cloud is a great option depending on the size of your team and data engineering maturity. 

At GitLab, we run dbt in production via Airflow. Our DAGs are defined in [this part of our repo](https://gitlab.com/gitlab-data/analytics/tree/master/dags/transformation). We run  Airflow on Kubernetes in GCP. Our Docker images are stored in [this project](https://gitlab.com/gitlab-data/data-image).

For CI, we use GitLab CI. In merge requests, our jobs are set to run in a separate Snowflake database (a clone). Here's all the [job definitions](https://gitlab.com/gitlab-data/analytics/blob/master/transform/snowflake-dbt/snowflake-dbt-ci.yml) for dbt. The rest of the CI pipeline is defined [here](https://gitlab.com/gitlab-data/analytics/blob/master/.gitlab-ci.yml).

General principles, I think, are that you want to have your MRs run dbt using real data but writing to either a dev schema or a separate DB clone like we do. If you make dbt reference environment variables for where to write then you can control it quite nicely that way. (See our [profile here](https://gitlab.com/gitlab-data/analytics/blob/master/transform/snowflake-dbt/profile/profiles.yml) for details on that).

Hope this is useful!",2
618,"best-practices-for-cicd-deployment","@tmurphyThank you for sharing the knowledge. That would be super helpful. I was wondering how to pass airflow's macros, especially `{{ ds }}`  and `{{ execution_date }}`.  A possible solution I was thinking was using environment variables. So, I was encouraged, seeing the repository. Many thanks!

https://gitlab.com/gitlab-data/analytics/-/blob/master/dags/airflow_utils.py#L213",3
624,"new-package-dbt-date","I've been collecting simple date helper functions in a package for clients, and have now broken them out into their own package, `dbt-date`, which you can find here:
https://github.com/calogica/dbt-date

The current release is 0.1.3. The [README](https://github.com/calogica/dbt-date/blob/master/README.md) has, hopefully, enough information to get started, but please let me know if you have any questions! PRs with fixes and new date-related macros are always welcome!

(Note; If you're currently using any of the date macros from `dbt-extend`, they now, as of `dbt-extend 0.2.4`, point to their equivalent versions in `dbt-date`. I will deprecate them at some point in the future; for now they log deprecation warnings to the dbt logs, but not the terminal.)",1
624,"new-package-dbt-date","Great work on this @claus !

Were you planning on adding fiscal year dimensions? I'd be happy to contribute and add this to the package otherwise. :grinning:",2
624,"new-package-dbt-date","Thanks @MartinGuindon! That's a good point. I'll probably roll some of the code from this blog into a reusable macro.
https://calogica.com/sql/dbt/2018/11/15/retail-calendar-in-sql.html

Would love any PRs you want to add for date-specific features!",3
640,"group-by-1-is-a-maintenance-problem","I couldn't find any place to post comments directly on the [blog post](https://blog.getdbt.com/write-better-sql-a-defense-of-group-by-1/), but I wanted to call out a big problem with `group by 1` that the author didn't cover.

`group by 1` makes the order of columns in your `select` clause semantically meaningful! As in, changing the order of the columns in your `select` clause could change your output if you use `group by 1`.

That's such a dangerous line to cross that it totally kills any value I might see in using `group by 1`. Maintaining SQL that uses this pattern become a dangerous exercise. You reorder some columns just because it looks better to you, thinking it's a harmless change, but oops! The output is all different now.

The author writes:

> Plus, as someone that writes  *a lot*  of SQL, I very much appreciate the fact that  `1`  is exactly one character.

I think this reflects a typical divide in programming styles and patterns. Some are optimized for the person writing the code, and some are optimized for the person reading and maintaining the code. `group by 1` is very much a benefit to the writer, and a harm to the reader and maintainer.

The author opens her [post](https://blog.getdbt.com/write-better-sql-a-defense-of-group-by-1/) with a choice between two styles of writing a query. But she misses a third style:

```sql
with enhanced as (
    select
        date_trunc('month', order_created_at) as order_month,
        *
    from core.fct_orders
)
select
    order_month,
    count(order_id) as n_orders,
    sum(revenue) as total_revenue
from enhanced
group by order_month;
```

If you want not to repeat yourself in your `group by` clause, then common table expressions are the way to go. For the sake of maintainability, I'd steer clear of `group by 1`.",1
640,"group-by-1-is-a-maintenance-problem","My 2 cents:

I think @Claire proactively addresses this when she writes,

> If I want to know what grain my report is at, it's easy for me to parse the  `1` , jump up a couple of lines of SQL and realize it's at the grain of  `order_month` .

Your first column *should* indicate the grain of your model and *that* shouldn't change as you enhance the model over time. Changing the grain of a model creates a fundamentally new model.",2
640,"group-by-1-is-a-maintenance-problem","I think enforcing conventions like ""first column in `select` clause should be your `group by` column"", which perhaps is the author's ultimate goal, is best left to a linter or to code review.

But in the absence of such a linter, enforcing your convention by recommending a dangerous pattern like `group by 1` seems like a step back. It elevates the choice of `select` column order from a harmless choice of ""better style vs. worse style"" to a critical choice of ""correct results vs. incorrect results"".

And if your goal is better style, then just advocate the better style directly, rather than couple it to a dangerous pattern that raises the stakes of mistakenly deviating from the recommended style.",3
640,"group-by-1-is-a-maintenance-problem","Hey Nick! I'm a little confused at the moment.
> You reorder some columns just because it looks better to you, thinking itâ€™s a harmless change, but oops! The output is all different now.

Can you give an example of a query with ""group by 1"" where reorganized columns would give a different output?

In my experience, your SQL engine is going to return an error rather than a different result set. We test all our queries before deploying them, so fortunately this wouldn't make it into prod in our world.",4
640,"group-by-1-is-a-maintenance-problem","You have to group by everything that isn't aggregated, so changing from
```
select
  a
  ,b
  ,c
  ,sum(d)
from t
group by 1,2,3
```

to 
```
select
  b
  ,c
  ,a
  ,sum(d)
from t
group by 1,2,3
```

doesn't change the resultset in any meaningful way.

Changing it to 
```
select
  a
  ,b
  ,sum(d)
  ,c
from t
group by 1,2,3
```
certainly would, but as @claire points out, that would cause an error, since you'd no longer be grouping by all non-aggregated columns.   Maybe this is exactly what you're getting at, but this is why it's important to establish and enforce consistent code style.",5
640,"group-by-1-is-a-maintenance-problem","Oh, the situation is better than I thought. ðŸ˜…

[quote=""michael.dunn, post:5, topic:640""]
that would cause an error, since youâ€™d no longer be grouping by all non-aggregated columns. Maybe this is exactly what youâ€™re getting at
[/quote]

I am not concerned about errors, since those are easy to see and understand. I was concerned about a seemingly-innocent change in the query silently producing incorrect results, but indeed that doesn't seem to be the case due to how `group by` works.

I didn't think it through with a specific example for `group by` because my main concern is with ordinal references, and ordinal references generally _are_ a bad idea for the reasons I outlined.

An easy example is with `order by`:

```sql
select
    col1,
    col2,
    col3,
    ...
from le_table
order by 1, 2;
```

Changing the order of columns in, or adding new columns to, your `select` clause will change your overall result ordering. If you somehow build on this ordering with window functions, then it may also change the meaning of your results entirely.

Another example of how ordinal references can unexpectedly cut you is with functions that take positional parameters (as opposed to keyword parameters), where reordering the input parameters changes the behavior of the function:

```python
# Did I use the function correctly?
compute_result(input, lookup)

# Here, it's obvious.
compute_result(lookup_table=lookup, input_table=input)
```

So indeed, what I said here about `group by 1`:

> You reorder some columns just because it looks better to you, thinking itâ€™s a harmless change, but oops! The output is all different now.

is incorrect.

However, though it's not as bad as I initially claimed, `group by 1` still introduces unnecessary maintenance risk due to its use of ordinal references.

For example, changing this:

```sql
select
    name,
    count(*)
from person
group by 1;
```

to this:

```sql
select
    city,
    count(*)
from person
group by 1;
```

will quietly produce different results. With `group by name`, however, the change will produce an error.

It seems like an overly contrived example, but in a large, messy codebase with multiple contributors, scenarios like this are possible. Data testing will certainly catch a problem like this, but this is a more difficult bug to catch as compared to a query that simply fails. Patterns that make bugs harder to create and easier to spot should be preferred to those that do the opposite.

So, though my initial claim was incorrect, I think the minor added maintenance risk described above, combined with the cognitive load of having to dereference ordinals to meaningful names, still make `group by 1` a bad pattern for code readers and maintainers. (And that goes for ordinal references in general.)",6
640,"group-by-1-is-a-maintenance-problem","Great write-up.

Not sure about these though:
> changing [query grouping by name] to [query grouping by city] will quietly produce different results

Wouldn't you *expect* it to produce different results, given you're changing the columns you're selecting?

>the cognitive load of having to dereference ordinals to meaningful names

You *know* that you're grouping by everything that isn't an aggregate, and if you've followed the advice to put all non-aggregates before all aggregates, I don't see why you'd ever need to mentally map the ordinals of a `group by` to the column names.  5 non-aggregated columns â†’ `group by 1,2,3,4,5`.  Don't really care which column is 2 and which is 5, because it *literally* doesn't matter.

Your point about ordinal references is well-taken in the context of `order by`, but for a `group by` statement, I just don't see the impact.",7
640,"group-by-1-is-a-maintenance-problem","[quote=""michael.dunn, post:7, topic:640""]
Wouldnâ€™t you *expect* it to produce different results, given youâ€™re changing the columns youâ€™re selecting?
[/quote]

Yes, sorry, I should have been clearer. Of course, any change to the columns in your `select` clause will change your output. What I meant in the case of `group by 1`, though, was that the semantics of the query will have changed.

Maybe this concern simply reflects a bias I have. When I look at a SQL query as a maintainer, I treat the different parts of the query with different levels of sensitivity. To me, the `select` clause is the ""safest"" to touch, because modifying it simply adds, removes, or reorders attributes while returning the same logical result. And if I mess that up my query will quickly and loudly fail.

With ordinal references, however, that's no longer true. To return to my example, if I think I no longer need `name` in this query:

```sql
select
    name,
    count(*)
from person
group by 1;
```

and replace it with another column, not only have I changed what attributes the query is returning, but I've changed how the aggregation is being done. That's a more significant level of change -- a change to the meaning of the result -- and that's what I was pointing out.

In a world without ordinal references, I don't have to think that extra bit about whether changing the order or content of the `select` clause is affecting the semantics of my query. It simply changes the attributes being returned.

[quote=""michael.dunn, post:7, topic:640""]
> the cognitive load of having to dereference ordinals to meaningful names

You *know* that youâ€™re grouping by everything that isnâ€™t an aggregate, and if youâ€™ve followed the advice to put all non-aggregates before all aggregates, I donâ€™t see why youâ€™d ever need to mentally map the ordinals of a `group by` to the column names. 5 non-aggregated columns â†’ `group by 1,2,3,4,5` . Donâ€™t really care which column is 2 and which is 5, because it *literally* doesnâ€™t matter.
[/quote]

Hmm, I'm not following. My point is that `group by 1` is not meaningful to a human. The `1` has to be mentally translated into a meaningful name. In her post, @claire wrote:

> In my opinion, the second query is cleaner and easier to read. If I want to know what grain my report is at, it's easy for me to parse the  `1` , jump up a couple of lines of SQL and realize it's at the grain of  `order_month` .

This process of dereferencing the ordinal is a cognitive load. It's not an issue if you have short queries that diligently adhere to a style guide, but when you have large, messy codebase, this load can be noticeable. The load is borne by the person reading or maintaining the code, and less so by the person who initially wrote it. It can be avoided entirely by using CTEs and meaningful aliases, as I demonstrated at the end of my [initial post](https://discourse.getdbt.com/t/group-by-1-is-a-maintenance-problem/640?u=nchammas).

I don't feel as strongly about `group by 1` as I did when I wrote that post. Thank you for helping me clarify my thinking there. But overall, I'm still against the idea. I think I've made the case as best as can be made, but having explained my thought process out loud, I'll take one final go at summarizing my argument.

Avoid ordinal references in SQL (including `group by 1`) because:
* They create a meaningless layer of indirection.
* They create an unnecessary coupling between the order and selection of columns in your `select` clause and the overall semantics of your query.

Prefer proper names and aliases because:
* They directly confer meaning to the reader.
* They decouple the details of how your `select` clause is structured from the overall semantics of your query.",8
640,"group-by-1-is-a-maintenance-problem","Honestly, I'm just excited that there are people who care as much about their SQL style as I do -- I really appreciate your input to the discussion!

I actually don't have anything _against_ `group by column_name`, and will happily adopt it if a client I'm working with wants to use it. I just think `group by 1` can get a bad rap, which I believe is underserved! (Sure the article had a lot of hyperbole in it, but it's a blog post, not technical documentation, so we're allowed to push the boundaries a little :wink:) 

As I said in the original article: 
> At the end of the day, be opinionated in your SQL style ... and *stick with it consistently*.",9
647,"is-it-possible-to-have-multiple-files-with-the-same-name-in-dbt","Hi, as discussed on Slack I move this discussion to this page to make it available for future reference :slight_smile: 
This question was probably asked several times before but I havenâ€™t found anything with regards to it. 
Is it possible to have multiple files with the same name in dbt? 
Letâ€™s say a staging table which already filters data from another table is called  `stg_events` . I am using this with different definitions in different models but in the same project as I am filtering this huge event table based on my requirements for a certain model. 
As I am using custom schemas the database does not have any problems with it. But the dbt ref function will have problems with it, correct? 
And if that's true, do you have any recommendations for a table naming convention?

Thanks a lot.
Best,
Andre",1
647,"is-it-possible-to-have-multiple-files-with-the-same-name-in-dbt","Hey @drege :wave:
Thanks so much for moving this to Discourse!

You're completely right, filenames for models in dbt must be unique, even if they live in different directories, or are materialized in separate [custom schemas](https://docs.getdbt.com/docs/using-custom-schemas).

This is because the `ref` function only takes one argument -- the filename. I you try to have two files with the same name, dbt will give you back an error like this:
```
$ dbt run
Running with dbt=0.14.2
Encountered an error:
Compilation Error
  dbt found two resources with the name ""stg_payments"". Since these resources have the same name,
  dbt will be unable to find the correct resource when ref(""stg_payments"") is used. To fix this,
  change the name of one of these resources:
  - model.jaffle_shop.stg_payments (models/staging/jaffle_shop/stg_payments.sql)
  - model.jaffle_shop.stg_payments (models/staging/stripe/stg_payments.sql)
```

To get around this limitation, we often following a naming pattern like `<source>__<object>.sql`. For this example we'd have:
* `stg_stripe__payments.sql`, and
* `stg_jaffle_shop__payments.sql`

We've written lots more on the naming conventions we use over in this article:
https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355

Pro tip: if you consistently use the same separator in your filenames, you can actually tell dbt to use the first part of your filename as the custom schema, and the second part as the view/table name. @brandyn recently wrote a great post on this over here:
https://discourse.getdbt.com/t/extracting-schema-and-model-names-from-the-filename/575",2
647,"is-it-possible-to-have-multiple-files-with-the-same-name-in-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
666,"should-i-have-an-organisation-wide-project-a-monorepo-or-should-each-work-flow-have-their-own","I would like to know if it is best to have one DBT project for the whole organisation or separate ones per team/workflow. What are the advantages/disadvantages of both?

Is there a way to have a utilities repo where common functions/macros can be shared and stored.

How do you organise projects for multiple teams to work on?",1
666,"should-i-have-an-organisation-wide-project-a-monorepo-or-should-each-work-flow-have-their-own","Thanks for the great question, Liam! I've been thinking a lot about this topic recently. Just to be clear, these are definitely my opinions, not yet established Fishtown Facts<sup>TM</sup>.

## Is it possible?

Can an organization's dbt codebase be split across multiple projects/packages/repositories? **Absolutely.** dbt ships with a first-class understanding of [packages](https://docs.getdbt.com/docs/package-management#section-what-is-a-package-), which are really other people's projects (or dbt's own) that sit beneath your ""main project""â€”your primary codebase and working directory.

There are many public dbt packages, such as the ones at [hub.getdbt.com](https://hub.getdbt.com/), but it is equally possible to install _private_ packages, as long as you have access to that package's codebase. For example, a private package may be hosted in a private repository in your organization's GitHub account. As long as your personal GitHub account has access to that repository, and your computer has an SSH key configured in your GitHub account, you will be able to pull down a clone of that repo upon running `dbt deps`. So long as you've properly configured a deploy key or the dbt Cloud GitHub app, your production runs will be just fine, too.

## Is it desirable?

We already create, maintain, and install **public packages** to do a bunch of things:

* Versatile macros, especially for â€œHow to Write this in SQLâ€ problems ([dbt_utils](https://hub.getdbt.com/fishtown-analytics/dbt_utils/latest/))
* Common staging models for one data source ([facebook](https://hub.getdbt.com/fishtown-analytics/facebook_ads/latest/))
* Ad hoc queries you find yourself writing over and over ([audit_helper](https://hub.getdbt.com/fishtown-analytics/audit_helper/latest/))
* Follow industry standards for boilerplate-yet-complex modeling ([snowplow](https://hub.getdbt.com/fishtown-analytics/snowplow/latest/))

The chief reason for creating these packages is _reusability_. In all cases, though, we put in a _lot_ of extra work to make these packages as cross-compatible as possible. Where possible, there's handling for multiple data warehouses, SQL syntaxes, and data loading vendors. Packages like `snowplow` also include several configurable variables that change based on an organization's desired applications.

By contrast, you're the only ones who are going to be using your own data transformations. You know who's loading your data and which warehouse you're running on. The primary motivations to create **private packages** are driven by _people_ and _workflow_.

When I talk about using separate packages, I am talking about storing separate dbt projects in separate git repositories, where one is / some are ultimately installed as a package dependency of another (""main"" project). I'm _not_ talking about the following cases:

**Unrelated projects:** Two dbt projects that have no formal relationship. One doesn't `ref()` models in another or share any of its `sources`. This might make sense if you're using dbt to sub in for ETL-like operations, such as ""loading"" tables via postgres foreign data wrapper. You want to keep your true analytics work separate from code that is less dbt-onic.

**""Local"" packages:** Multiple dbt projects contained in the same repo. For whatever reason, it makes sense to have a separate project that is an extension or variation of its ""parent"" project. A good example of this is our integration testing setup in packages with good coverage, such as `dbt-utils` and `snowplow`.

## So, why multiple repos?

### git reasons

* You want different analysts to have different permissions on different repos.
* Too many cooks! Your dbt repository has dozens of concurrently open issues/PRs,
and analysts' work is overlapping or mutually blocking.

### dbt reasons

* Each analyst has to navigate hundreds of model files, when their work only touches a few dozen. (Not to mention the delayed development process of needing to run all those models in their sandbox schema.)
* Different teams have different code style, review process, and chief maintainers. Or, you might want to physically separate legacy cruft from net-new models guided by best practices.

## Is it feasible?

In any event, there are some real logistical and organizational challenges to
working across multiple repositories. Separating code does not _solve_ problems
of team cohesion; if anything it will exacerbate them.

| Motivation | Caveats | Possible approach | How could dbt better support this? |
| -------- | ------- | -------- | --------------------------------- |
| Diff permissions on repos | Analysts canâ€™t run dbt deps if they donâ€™t have access | All analysts at least read-only on all repos. Data is permissioned, but transformation logic is still visible to all. | Env-aware deps, env-aware model enabling in dbt_project.yml |
| Too noisy, competing changes | Breaking changes are still there, but now quieter | Use GitHub releases. Full CI build of all packages (slow) | Version resolution for non-hub packages, or private hub. Smarter CI??
| Too many models | dbt run will still run `+my_package` | Analysts always run dbt run -m my_package only. Have a â€œsuperpackageâ€ that installs all deps for prod runs. | A dbt run flag to only compile + run models defined in the current package? |
| Kick out some models because code style or legacy cruft | Someoneâ€™s package needs to be highest-up. Canâ€™t ref both ways. | Pick the best-maintained package as â€œmainâ€ project. | idk! |

The big theme here is that teams need have mutual agreements (""contracts"") about what their sources, models, macros, and tests ought to do, and protocol around when and how they are allowed to introduce breaking changes. With that in mind, here are a couple scenarios where I think multiple projects make a lot of sense:

* The SLA between data engineers and data analysts is rooted in dbt `sources`. It is the data engineers' responsibility to ensure successful loading of data into the warehouse (or staging of files in external storage), registry as dbt `sources`, and guaranteed `freshness` thresholds. The package of unified `sources` could then flow into one or more dbt projects, maintained by team(s) of analysts.
* The forecasting & risk team wants to build on top of core business logic maintained by analysts in finance & operations, but they also wish to keep private their potentially sensitive modeling that projects organizational growth. They ""extend"" the main dbt project by installing it as a package dependency, pinning to a release tag. Crucially, they also keep an open line of communication with the analysts ""upstream"" of them, to prevent database errors that can't be caught by that team's CI.

I'm curious to hear what other folks have to say on the topic! Over the past few months, we've been witness to dbt rolling out at larger and larger organizations. The question of how to smartly separate analytics work across larger and more heterogenous teams is only going to become more important in the months and years to come.

*p.s.* These thoughts grew out of a presentation and conversation among Fishtown's consulting team. Major thanks to Erica for her notes from that meeting :)",2
666,"should-i-have-an-organisation-wide-project-a-monorepo-or-should-each-work-flow-have-their-own","[I would like to follow up to this thread with a link to this post I made](https://discourse.getdbt.com/t/how-to-configure-your-dbt-repository-one-or-many/2121). It takes a lot of what I learned working on larger projects at Fishtown and gives guidance on what options are available to you.",3
682,"is-it-possible-to-have-incremental-logic-in-ephemeral-tables","I was wondering if it was possible to have an ephemeral model but to be able to use incremental logic in there?

The reason is that recently we were trying to move some of our CTE into ephemeral tables but we don't want to pull all the records every time our ETL runs.

A workaround to this is to create a CTE and include the incremental logic there (assuming the model is materialized as incremental).   The long term issue with that is if the CTE is useful for other new models, it would be nice to not have to repeat the CTE in that new models.  If we go the route of materialized it as its own model, then we start to clutter our schema with a lot of intermediate tables.

As a test, I created a child model that is materialized as ""ephemeral"" but contained incremental logic.

![33%20AM|690x269](upload://e5Bc2j20vyJi3VexbimUeRjKXaC.png) 

I then created a parent model that is materialized as ""incremental"".   I ref the child model.

![41%20AM|386x272](upload://4ZAE5Cz2oFHBEQGO93g6k4EvJ87.png) 

To test it, I ran the parent model once to populate the table.   I ran it a 2nd time in the hopes that it would interpolate the child SQL as a CTE within the parent.   It did not and pulled all the records, essentially ignoring the incremental logic in the child.  

Anyhow, I thought I'd share this particular experience and hoping that we'll have the ability to create incremental ephemeral tables.",1
682,"is-it-possible-to-have-incremental-logic-in-ephemeral-tables","Hi @chanwd,  sql code in your incremental block will never be evaluated, returning all rows every time.

Have a look at the is_incremental() macro definition : 

https://github.com/fishtown-analytics/dbt/blob/dev/louisa-may-alcott/core/dbt/include/global_project/macros/etc/is_incremental.sql

It will always return false because your model is ephemeral.

Hoping it helps,
Best
@fabrice.etanchaud",2
682,"is-it-possible-to-have-incremental-logic-in-ephemeral-tables","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
712,"what-does-self-serve-in-analytics-mean-to-you","Nearly every client we work with at Fishtown Analytics talks about their desire to create a self-service culture around data. But this means a lot of different things to different people. Iâ€™m doing some initial research for an article that will eventually live on the dbt blog. If you have opinions on this topic, I would love to hear from you!

Some questions Iâ€™m thinking aboutâ€¦

**What does â€œself-serveâ€ in analytics mean to you?**

* Really good reporting in a BI tool that most of the company has access to and views regularly?
* Power users that are not on the data team, but are comfortable doing some slicing & dicing on top of existing charts?
* End users who are comfortable creating charts & dashboards from scratch and even writing their own SQL?
* Something else?

**How self-service is analytics currently at your company?** Do users have the tools but arenâ€™t adopting it? (or still just exporting data to Excel to do their own analysis?) Are they pressuring for more self-serve capabilities/training?

**How self-service do you want analytics at your company to be?**

* Which roles in your organization should be able to self-serve their analytics?
* What should stakeholders be empowered to do, and what should the data team be responsible for?

**Whatâ€™s â€œnormalâ€?** How self-service do you think a company should be? Are there companies doing an outstanding job on this? Do you think your company is normal-ish? Above average? Is it something youâ€™re actively trying to improve?

**What are the barriers youâ€™ve encountered to creating an empowered, self-service culture?** Are end users resistant to using a new tool? Is data too messy, complicated? Are you struggling to get buy-in from leadership?

**How important is data documentation in creating and maintaining a self-service culture?** Do your stakeholders currently read data documentation? Or they still come to analysts for quick answers?

**Is there anything youâ€™ve found to be particularly useful in creating a self-service culture?** Training? Finding internal â€œdata champions?â€ Whatâ€™s working for you?",1
712,"what-does-self-serve-in-analytics-mean-to-you","To me the important thing to remember is that Self Serve analytics is a spectrum of different activities, but ultimately its anything that takes work, particularly repeating work off of the plate of the data team.

The simplest. but sometimes most effective way to enable self serve analytics is to build great dashboards and make sure your team knows about them. If there is a question you are getting asked frequently and you find yourself pulling the same data for multiple users, then it almost always is a good use of your tine to set that up as a dashboard (and schedule it out if appropriate)

The next level is _simple user engagement_ with your BI tool. The most common use case for this is being able to manage existing filters on a report (ie change the region from Americas to APAC). Basically everyone you work with can get to this level of fluency and it can help make your life a lot easier to train people to use filters. Another aspect that's along the same level is changing date aggregates (being able to switch a report that is quarterly to a monthly one without having to ask us). A half step up from this would be switching out a single dimension in a looker report. This level is really great because it cuts down on a large amount of requests and is also really difficult for end users to mess up.

The next level is the first level of what I'd call ""power users"" and its when users feel comfortable building their own reports in your BI tool. THIS IS A DANGEROUS LEVEL. While its great to have end users building out their own reports, you'll often find nuances in the data that they have missed purely by not being as close to it as you are. Our process for this is to allow users to build their own looks and dashboards, but we ask that they are all run by us before being put into use. It's best to focus on domain expertise here - so to get your marketing team comfortable using a few different dimensions and measures that particularly affect their team and have them come to you with anything else. In the long run you gain some modest time savings in the actual report building, but a lot of the true value at this stage is in increasing the data fluency of your end users. If you're using looker, it's helpful to have some stripped down explores for users on this level.

The final stage are your true ""data champions"". These are the people who know the data model almost as well as you. Not only do they build reports right - they surprise you in finding new insights in the data and new avenues for exploration. Treasure these people.

As for what level of self serve to expect and how to get it going, I've found that it all comes back to trust and relationship building. People won't really read data dictionaries. It's much more important to have good example dashboards and reports for them to build off of. Training people in the basics of your BI tool can go a long way towards them being able to organically pull their own metrics.

As far as what's normal - I don't think there is really any ""normal"". Whatever level works best considering your team, the technical literacy of your business users and the complexity of your data will inform what's right for your organization. Ultimately, the best thing you can do for self serve data is the same as the best thing you can do for non-self-serve analytics, which is focus on having a comprehensible, well maintained data model (if only there was a tool that was good for that...)",2
712,"what-does-self-serve-in-analytics-mean-to-you","**What does â€œself-serveâ€ in analytics mean to you?**
In my view, it's imperative for domain owners to ""own"" their own data. As in, they need to know how to access the basic performance metrics that relate to their domain and be able to perform simple analysis to understand how to make decisions that they encounter on a day-to-day basis. Of course, you can't expect a marketer to run a regression analysis, but they should be able to find basic data about the performance of their campaigns (how many transactions did my email drive?) and be able to dig at least one layer deeper (how much was driven by different pre-defined customer groups?).

Ideally, business users would be able to construct their own basic reports and dashboards, but in reality, there will likely always be a few ""power users"" across functional teams who can assist their teammates - with final review from the data team. And, of course, there will be times where an analyst is needed to set up new reporting, which the end user can then access whenever they need it.

All of this depends on having an analytics team to make sure the data that's surfaced to end users is as clear and easy to use as possible, is accurate and reliable, and is maintained as the business changes. The priority should be automation and iterative improvement, so self-service for the basics can be maintained into the future.

**How self-service is analytics currently at your company?**
Not very self-service at all. We have an unfortunate combination of data access issues combined with not a lot of data literacy. 

We've been stuck in the era of Google docs that aggregate data from Looker reports and compare to targets that are housed in Excel. We have Looker built out, but it's overwhelming for end users, so only a handful of people actually use it. This is partially from poor set up and maintenance within Looker and partially because of the poor schema design of our analytical data warehouse in combination with compounding pressure from a changing business. We've recently launched Metabase, but our operational databases are too complex for a non-coder or non-analyst to navigate, so we're stuck waiting for our new data models to be built before we can really open it up.

Another side-effect of being stuck in the era of excel is that there have been only a few people who have built and maintained reporting over the years, as there are only a few who know how to work in Excel. This has meant that the company as a whole has developed a reliance on these few people to get all of their data and reporting from. Lots of ""where did this data come from?"" ""why don't these numbers match?"" ""why are we up/down compared to target/LY"" that those few people have been answering because they're the only ones who knew how.

**How self-service do you want analytics at your company to be?**
Very - for the simple stuff. Retail is not rocket science. Every person at the company should be able to build a basic report if they need to. Key people/teams (product, marketing, social, planning, merchandising, operations, tech) should be able to do surface-level analysis to inform future testing/improvement.

An example - marketing should be able to analyze creative performance reporting and assess the success of their campaigns (potentially in partnership with planning). A marketing analyst/data scientist can help with things like building an attribution model and identifying new triggers for marketing automation (things a marketer can't do on their own).

The value analytics can bring is in creating data models that are easy to understand and use, maintaining view-level platform(s) for end business user use, doing analysis that requires a bit more than slicing the data via an existing attribute, and helping to connect the dots across the entire org (facilitating collaboration and communication through the use of data). An analytics team can't do that if they're bogged down doing simple analysis that a marketer (for example) could be doing (and I would argue should be doing).

**What are the barriers youâ€™ve encountered to creating an empowered, self-service culture?**
Once someone has developed a dependency on someone else to do something for them, it's really hard to break that habit, especially if you are pushing them to learn a new skill set. You just have to keep advocating for data, sending them the pre-made reports that you've created, and showing them how to do things instead of sending them the end result (if it's simple enough). Then they'll build confidence, you'll have built a collaborative relationship with them, and the trust will be there to build on going forward. Delegation is also important - if you're doing something that isn't driving value (copying numbers into a google doc), then try to find the relevant domain owner and give them the link to the Looker report or find a way to automate it away so no one is spending their time doing that.

Another challenge has been understaffing. Due to the difficulties with our existing ETL (among other reasons), our data team has slowly dwindled to two (soon to be one) analysts and a lone data engineer. There's only so much you can do with a data team that represents 1% of your global company and supports three markets, while you're trying to simultaneously prop up your dying ETL and build a new one. We've been hard at work building out the team roadmap, laying the foundations of understanding the value of data and good infrastructure, and advocating for additional resources.

Beyond that, for us, it's really just been about the difficult data models. But that's in the process of being fixed thanks to dbt + Fishtown and increasing support from leadership to shift focus toward the project.

**How important is data documentation in creating and maintaining a self-service culture?**
It's very important at Birchbox, as our operational databases have *nuances*. We're trying to find a balance though, as not everyone likes to use documentation. Keep the documentation within the data models rich for more in-depth users and future team members, but keep it light and simple for end business users.

**Is there anything youâ€™ve found to be particularly useful in creating a self-service culture?**
Persistence. Advocacy. Friendliness. Compassion. Trust. Integrity. Transparency.

Tell people about the work that your team is doing and how it will benefit them. Listen to people when they're telling you about the questions they have or the problems they're trying to solve. Be empathetic toward them. Admit when mistakes are made and be clear about how you're correcting them. The people relationships are as important as the data itself in getting people to buy in. Constant reminders that you exist and are delivering value alone is powerful.",3
712,"what-does-self-serve-in-analytics-mean-to-you","[quote=""evaughan, post:1, topic:712""]
What are the barriers youâ€™ve encountered to creating an empowered, self-service culture?
[/quote]

I've been working on ""self-serve"" at Dialogue a lot over the past few months and I think designing for it really comes down to this question. The other questions are really interesting, but for brevity and poignancy, this one covers a lot of ground. Ultimately, this question is what has helped us separate the problem into two parts:

**1. Technical expertise with SQL and data modelling:** These users know what data they're looking to analyze, they have all the definitions and business knowledge to extract insights, but they don't know SQL. We're solving this with tools like Metabase and more and more denormalization of data. Medical team members, for example, have lots of expertise but no experience with joining data. What we do to serve these members is take applications events and enrich them with dimension and facts about the care the patient had previously received.

**2. Domain expertise, an inability to discover data:** the more complex problem is when a user, such as a product manager, has a business question that they want to translate into a data question. For this case, we're still testing solutions but what we know is this has to do with surfacing the right data and making it usable for the end user in a seamless way. Right now we're thinking about Lyft's Amundsen and DBT Docs and a few others, but this problem is much harder to solve with a tool. In the mean time, data office-hours and trainings are helping fill this hole.",4
712,"what-does-self-serve-in-analytics-mean-to-you","**What does â€œself-serveâ€ in analytics mean to you?**

As mentioned it means different things to different people and I recognise its ambiguity and definitional issues. 

Currently, I think of it as infrastructure that allows business users to easily access, engage and query appropriate data from the warehouse (semantic or reporting layer) and customise or augment data from warehouse with their own data to create a new curated set. 

I also see it as a spectrum as a opposed to a binary.

The infrastructure I spoke of consists: 

* Analytics tools (e.g. Tableau, Amplitude) 

* Context/documentation (e.g. data dictionary, analysis wiki pages) 

* Resources for learning (e.g. courses, workshops, office hours)

Thinking about data culture and where it fits into self-service is an interesting question. Perhaps we can say a strong data culture increases the take up rate of self service analytics. 

**How self-service is analytics currently at your company?**

Given the companies maturity and stage, confident itâ€™s above average. 

Currently we are using Tableau and Amplitude as our two key self serve tools. 

In terms of other infrastrucutre we run Tableau workshops, provide data office hours, and hold a community of practice every 2 weeks for all business users interested in data. 

We have light metadata on our data (on Tableau Online and YAML files) and interestingly no data dictionary as yet. Although it is something we are starting to consider. 

**How self-service do you want analytics at your company to be?**

Make progress in getting business users comfortable so that a portion of ad-hoc requests can be done by themselves.

Also getting more predictive with our work. That is a number of functions in the business are in a good position to predict what will happen based on their data.

**What are the barriers youâ€™ve encountered to creating an empowered, self-service culture?**

* Lack of data literacy and skills amongst business users

* Unaware of data that is available

* Keeping with the status quo

**How important is data documentation in creating and maintaining a self-service culture?**

As mentioned before have done little documentation, so perhaps a little unaware of the importance. Will have a better 

Sense when we do increase documentation and notice (if any) difference. 

But we have paid close attention to naming of events, dimensions, measures, tables and columns very very carefully. 

**Is there anything youâ€™ve found to be particularly useful in creating a self-service culture?**

* Data Community of Practice

* Weekly Business Reviews with leadership

* Onboarding workshops for BI tools 

* Office hours

Currently working on a Data 101 course that covers data warehousing, data sensemaking (asking the right questions, systematically analysing the data from a number of angles, presenting results etc.), experiments, BI tools and more. Thinking it could be a mixture of in person delivery and also online course content.",5
712,"what-does-self-serve-in-analytics-mean-to-you","TLDR: self serve analytics is important but is insufficient on its own. It is not a promised land of data informed decision making.
â€”

Agree with the thinking that has been shared here so far.

For me the shift in my thinking that has taken place over the past few months as our self serve infrastructure has come together, has been that it doesnâ€™t remove the need for data analysts and the skill of data sensemaking. 

With my data warehousing background I have always valued well structured data sets and quality dashboards, and mistakenly felt that if we could achieve that, a deluge of data analysis and insight would flow from that.

I still believe they are important and necessary however what Iâ€™d acknowledge is that most â€œbusiness usersâ€ like product managers have a lot on their plate already, and beyond understanding / reporting on their domain, itâ€™s unlikely most of them will be able to dedicate the time necessary to deeply explore their data or make sense of it. That is where dedicated data analysts with specialised skills in the space can play a role, to produce valuable insights that wonâ€™t pop out of your standard dashboards or simple cross tabs and line charts.

My thinking has been informed by Stephen Fewâ€™s thoughts on the matter, but Iâ€™m only really grokking it now.

See https://www.perceptualedge.com/blog/?p=2467",6
720,"does-dbt-cloud-store-any-source-data","Does DBT Cloud store any of my warehouse source data ? I expect the answer is NO since DBT just runs queries against the source - but I just want to check. What I am really asking is there any data governance risk in using DBT Cloud in dev and prod vs. running DBT inside, say, a customer's AWS VPC.",1
720,"does-dbt-cloud-store-any-source-data","Hey @johnoscott! Great question. I'm going to answer based on dbt Cloud's current design and implementation, although it's conceivable that some parts of this answer could change in the future as the product evolves. If anyone is reading this in the _distant future_, feel free to ping me to see if anything has changed and I'll keep this thread updated.

At the moment, dbt Cloud stores the following data persistently:
1. your dbt Cloud account information. things like job definitions, database connection information, users, etc.
2. logs associated with jobs and interactive queries you've run. 
3. your dbt ""assets"": things like `run_results.json` and `manifest.json`. 

In #1, we can be sure that this does not include any raw data from your warehouse because we know exactly what type of information is stored here. For #2 and #3, that gets just slightly more complicated, because _you control these assets_. 

Here's one example: it's totally possible to write dbt code fetches all customer data from your customers table and then writes it out to the logs. This is almost definitely a _bad idea_, but it is possible to do. If one were to write this code, the logs would contain all customer data and therefore dbt Cloud would store it.

So, the more complicated answer is ""no, dbt Cloud doesn't store you data from your warehouse unless you specifically write some particular piece of code that will cause that data to be written to the logs, or to a compiled dbt asset."" :) 

Finally, dbt Cloud _does_ have data from your warehouse pass through its infrastructure when writing interactive queries in the IDE. If you write `select * from customers limit 100`, the data from your customers table will pass through the dbt Cloud infrastructure on the way to your browser. At the moment, there are no steps in that process that persist the data: dbt Cloud doesn't perform any caching or other behavior whereby that data lives on our servers outside of your browser session.

As a result of all of the above, dbt Cloud's data security / governance responsibilities are somewhat more straightforward than they would be if it were, for instance, a data warehouse or a data integration tool (both of which persist your data). Even so, dbt Cloud typically has to have a very high level of access to your data warehouse in order to do its job, and as such we take security extremely seriously. Take a look at our [security page](https://www.getdbt.com/security/) to learn more, and feel free to ping us at support@getdbt.com if you have specific questions about your account.",2
720,"does-dbt-cloud-store-any-source-data","Hey @tristan ! Thanks for this detailed answer. Just checking to see if there are any updates to be aware of in June 2022. 

Huge dbt fan by the way :slight_smile:",3
720,"does-dbt-cloud-store-any-source-data","Hey @lucash - still the same! I was going to quote the data storage section of our [security page](https://www.getdbt.com/security/#data-storage), and then I realised that it was basically a paraphrasing of Tristan's original reply.

Since this was written, we have also achieved a bunch of security-related examinations, including SOC2 Type II, ISO27001:2013 and ISO27701:2019. For the long versions, check out the [full compliance section](https://www.getdbt.com/security/#compliance).

Resetting the timer on this:

[quote=""tristan, post:2, topic:720""]
itâ€™s conceivable that some parts of this answer could change in the future as the product evolves. If anyone is reading this in the *distant future*, feel free to ping me to see if anything has changed
[/quote]

Looking forward to writing the next update to this from  my flying car :red_car::dash:",4
720,"does-dbt-cloud-store-any-source-data","@joellabes Awesome! Thank you for the response. I figured it wouldn't be too different, if at all, just wanted to confirm :slight_smile:",5
724,"saas-multi-schema-best-practices","I was hoping to get some thoughts on what you would consider best practices for a typical SaaS situation:

* We are a B2B SaaS tool
* We use PostgreSQL as our main operational database
* Each one of our customers has their own schema in our database (all have the same data structure - we just have separate schemas to prevent leaks in case of bugs)
* There are many of these schemas (as in thousands)
* We also have a Redshift with lots of other data from Salesforce, Stripe, etc and currently use DBT + DBT Cloud to model that and prepare for our BI tools

For the sake of the argument, let's say we have a table ""sale"" in each of these PostgreSQL operational schemas. For analytics purposes, we'd like to be able to answer questions like how many sales there were in a given day, across _all_ of our customers, i.e. all of the schemas.

**How would you setup the data pipeline and model this?**

My initial thoughts are:

*Option A* - we could replicate all of the schemas from Postgres to Redshift using something like FlyData, then use DBT to create a view across all the ""sale"" tables across all of these schemas in Redshift. Not sure this would even work.

*Option B* - we use some kind of ETL tool to somehow listen to changes on all the ""sale"" tables in the all schemas in Posgres, then write a ""clean"" version into a unified ""sale"" table in Redshift. Not sure what tools would be good for this either.

Any thoughts and guidance would be appreciated! Thanks.",1
724,"saas-multi-schema-best-practices","I've got a similar (although not exactly the same) situation in both MSSQL and MongoDB (2 different SaaS products).  In MSSQL, we've got 1 database per ""State"" and in Mongo, we've got 1 db per customer.

Right now, for MSSQL, I'm using Stitch to replicate each table from the source db into its own schema (BigQuery dataset in this case).  I then use `dbt` to loop through a list of these schemas to create a monster view across all the source tables.  So if we've got 12 databases (again 1 for each state), then I end up with one `PurchaseOrders` view that essentially unions all the individual `db_{{statecode}}.PurchaseOrders`  tables together.  Then I can easily query across all of them as one - or if performance is an issue, I materialize it as a table.  With 1000+ of these, this seems like it might hit a wall, but perhaps not?

For Mongo, where my numbers are headed in your direction (we've got 100+ dbs today); I'm planning on rolling my own singer tap configs and just add the tenant/customer id as a new column for every collection.

Would definitely be interested in others' solutions as well.",2
724,"saas-multi-schema-best-practices","Hey @daniellang -- welcome :wave:

A little late to the party here, but hopefully you'll let me join.

My preferred approach looks like @warpraptor's! We use stitch to replicate each database into a separate schema, then, in dbt, union them all together.

Here are some tips:
**1. We define our raw data as [sources](https://docs.getdbt.com/docs/using-sources).**
A neat trick here is to use [yaml anchors](https://medium.com/@kinghuang/docker-compose-anchors-aliases-extensions-a1e4105d70bd) to reduce duplication of code:
```
version: 2

sources:
  - name: shopify_us
    tables: &shopify_tables
      - name: products
        columns:
          - name: id
            tests:
              - unique
              - not_null
      - name: orders
        columns:
          - name: id
            tests:
              - unique
              - not_null

  - name: shopify_uk
    tables: *shopify_tables

  - name: shopify_au
    tables: *shopify_tables
```

**2. Use jinja in dbt to union the tables together**
For our first version, we'll often just union together the tables manually (using a little Jinja to help us out)
```
{%- set countries = ['us', 'uk', 'au'] -%}
with products as (
{% for country in countries %}
  select * from {{ source('shopify_', ~ country, 'products') }}
  {% if not loop.last %} union all {% endif %}
{% endfor %)
),
...
```
Then, inevitably, there will be some table that has the columns in a different order. So we'll use the  [union_relations](https://github.com/fishtown-analytics/dbt-utils#union_relations-source) macro from dbt utils here to help us out:
```
{%- set countries = ['us', 'uk', 'au'] -%}

{%- set shopify_product_relations=[] -%}

{% for country in countries %}
    {% do shopify_product_relations.append(source('shopify_' ~ country, 'products')) %}
{% endfor %}

with products as (
    {{ dbt_utils.union_relations(shopify_product_relations) }}
),
...
```
As the number of tables we need to union grows, we end up repeating the first 6 lines of this across multiple models. So we might then abstract it into a [macro](https://docs.getdbt.com/docs/macros). Here's an example:
```
{% macro union_shopify_sources(table_name) %}

    {%- set countries=['us', 'uk', 'au'] -%}

    {%- set shopify_relations=[] -%}

    {% for country in countries %}
        {% do shopify_relations.append(source('shopify_' ~ country, table_name)) %}
    {% endfor %}


    {{ dbt_utils.union_relations(shopify_relations) }}

{% endmacro %}
```
Then our downstream model looks like:
```
with products as (
    {{ union_shopify_sources('products') }}
),
...
```
Even though we've reduced the repetition in our code, there's a tradeoff: we've obfuscated this code a little -- it's harder to understand exactly what's happening without going to another code location.

**3. Prefix unique IDs with country codes**
My last tip for this kind of work is that I generally create a new unique ID for the unioned data by prefixing the old ID with a country code, e.g. `US-187`, `AU-87546`. I find that this is the most user-friendly version of the unique ID, and it allows me to write clean `join`s downstream (since I also do this for foreign keys). In one project, I even went so far as to macro-fy this logic in my `union_shopify_sources`, so that any field named `id` or ending with `_id` automatically had the country code prefixed.",3
724,"saas-multi-schema-best-practices","Thank you @claire and @warpraptor - this is very helpful and great to hear what you're doing.

I had thought of something similar but ran into a couple limitations due to the fact that we have thousands of schemas and new ones created all the time (basically whenever a new trial gets started):

* A view would be slow at query execution time across that many schemas
* A materialized table would be pretty slow and expensive at dbt job execution time as we would recreate the entire table every single time dbt runs
* Stitch cannot automatically sync all newly added schemas from Postgres to Redshift as far as I know

Because of that, we ended up creating a slightly different approach and it's working great so far:

1. We added a changed timestamp column to all the tables in our production database that we care about. We use a Postgres trigger to update it on every write
2. We have a AWS Lambda function run every 30 minutes, iterate over all the schemas, query the tables with a timestamp cursor that we save for each schema+table in DynamoDB
3. Any changed rows since the last execution get batch sent to Amazon Kinesis Firehose, which writes them to a table in Redshift. These are the updates, not the final table. So for example we have a `sale_cdc` table that has all the changed rows appended every time
4. We have a dbt job that uses a window function on the CDC table to only get the most recent record for each unique schema + id combination and then materializes it into a final `sale` table that now contains all the sale rows from all the schemas in a format that is easy to use for further modeling and querying

Hope this might help someone in the future!",4
724,"saas-multi-schema-best-practices","@daniellang thank you for your post!
Could you elaborate on items 3-4 and/or share your code?",5
724,"saas-multi-schema-best-practices","About yaml anchors for mere mortals https://www.youtube.com/watch?v=2cxBftrVc2Y&t=247",6
733,"testing-data-transformations","# Introducing dtspec

Hello dbt community!  I'm using this forum to introduce a new framework for testing data transformations: [dtspec](https://github.com/inside-track/dtspec). One topic that has come up in several dbt slack conversations and other [discourse posts](https://discourse.getdbt.com/t/testing-with-fixed-data-set/564/4) is how difficult it can be to write tests that show how the output of a data transformation may behave given some variation with its inputs.  dtspec is designed to simplify the process of building test data and asserting how it should be transformed.

dtspec is a very new project.  We've implemented it at my company, InsideTrack, to test many of our dbt models.  I'm reaching out to the dbt community to gather feedback on whether this could be a useful
tool for other data shops.

## jaffle_shop

I've set up a basic proof-of-concept [dbt jaffle_shop project](https://github.com/gnilrets/jaffle_shop-dtspec/tree/dtspec) that demonstrates how dtspec can be used in dbt projects.  The interested reader is encouraged to review the [spec file](https://github.com/gnilrets/jaffle_shop-dtspec/blob/dtspec/dtspec/spec.yml) for that project, and even take it for a test drive locally.  Try tweaking some of the models or expectations to see what happens when the expectations are not actually met by the transformations.

One of the more amazing things that happened while I was writing the spec for the `jaffle_shop` project was that _I was wrong (gasp!)_ about how I thought a certain model behaved.  When I wrote the test case, I wrote it the way I thought the transformation worked, and dtspec alerted me that the expectation was not being met.  I was then able to dig into the model SQL more to figure out where my assumptions were wrong.  I then fixed the test case and moved on.  **This is the real power of dtspec!** Data transformations can get very complex.  Different developers, both new and seasoned, are going to have different understandings about how the models function.  If, based on our necessarily limited understanding, we modify the SQL in a way that breaks some known edge case, dtspec will alert us.

## Basic concepts

With dtspec, a user writes a **spec.yaml** file describing the expected behavior of the data transformations they wish to test.  dtspec uses this spec file to generate data that is then loaded into an empty test data warehouse.  dbt (or really any other data transformation system) is then run in that test warehouse.  The outputs of the transformations (e.g., dbt models) are then extracted out of the data warehouse.  dtspec then compares the actual results with the expected results defined in the spec file.  Any discrepancies are reported to the user.

The spec file contains a few key components, which are detailed below and include some examples from the [jaffle_shop POC](https://github.com/gnilrets/jaffle_shop-dtspec/blob/dtspec/dtspec/test.py).

### identifiers

A single dbt run in a modern data warehouse can be time consuming, even if the amount of data being transformed is small. dtspec is designed to minimize the number times that dbt needs to be run in order to be tested (in most situations, dbt would only need to be run once).  dtspec accomplishes this by collecting all of the data described in the cases and stacks them on top of each other.  Identifiers are used to declare which columns uniquely identify records as belonging to a particular test case.  See the [dtspec README](https://github.com/inside-track/dtspec#hello-world-with-multiple-test-cases) for more details on the topic.  In a lot of situations, we can get away with a generic identifier for the `id` columns:

```yaml
identifiers:
  - identifier: generic
    attributes:
      - field: id
        generator: unique_integer
```

### sources

The sources in a spec file describe the data transformation inputs.  These map directly to dbt sources.  The user can specify which columns can be used to uniquely identify records within a case by defining an `identifier_map`:

```yaml
sources:
  - source: raw_customers
    identifier_map:
      - column: id
        identifier:
          name: generic
          attribute: id

  - source: raw_orders
    identifier_map:
      - column: id
        identifier:
          name: generic
          attribute: id
      - column: user_id
        identifier:
          name: generic
          attribute: id
```

### targets

These are the outputs of the data transformations.  In dbt, these are known as models:

```yaml
targets:
  - target: dim_customers
    identifier_map:
      - column: customer_id
        identifier:
          name: generic
          attribute: id
```

### factories

Factories describe how to generate data for the sources to be used as inputs for data transformations.  A single factory can be, and usually is, composed of multiple sources that are in some way related to each other (e.g., foreign key relationships). Factories can be combined with other factories and tweaked for specific test cases, thereby providing a flexible mechanism for describing different data scenarios.

```yaml
factories:
  - factory: CustomerWithOrderAndPayment
    data:
      - source: raw_customers
        table: |
          | id    | first_name |
          | -     | -          |
          | cust1 | Kelly      |

      - source: raw_orders
        table: |
          | id     | user_id |
          | -      | -       |
          | order1 | cust1   |

      - source: raw_payments
        table: |
          | id       | order_id |
          | -        | -        |
          | payment1 | order1   |
```

### scenarios and cases

Scenarios and cases are the heart of dtspec.  Scenarios are collections of cases that share a common base factory and typically describe a single data target/model.  Cases describe the expected output of a transformation, given some inputs (inherited from the scenario, and often tweaked a bit).

```yaml
scenarios:
  - scenario: Building dim_customers
    factory:
      parents:
        # This factory is used by default in all of the cases that belong to this scenario.
        - CustomerWithOrderAndPayment

    cases:
      - case: populating number of orders
        factory:
          data:
            - source: raw_orders
              table: |
                | id     | user_id |
                | -      | -       |
                | order1 | cust1   |
                | order2 | cust1   |
                | order3 | cust1   |

        expected:
          data:
            - target: dim_customers
              table: |
                | customer_id | number_of_orders |
                | -           | -                |
                | cust1       | 3                |
```

### Testing output

The output of a run of dtspec (with a test case failure) looks something like this

```bash
Asserting Building dim_customers: target has data PASSED
Asserting Building dim_customers: populating number of orders FAILED
DataFrame.iloc[:, 1] are different

DataFrame.iloc[:, 1] values are different (100.0 %)
[left]:  [2]
[right]: [3]
Actual:
  customer_id number_of_orders
0       cust1                2
Expected:
  customer_id number_of_orders
0       cust1                3

Asserting Building dim_customers: populating most recent order date PASSED
Asserting Building dim_customers: populating customer lifetime value PASSED
Asserting Building dim_customers: unknown payment methods are not ignored when populating customer lifetime value PASSED
Asserting Building fct_orders: target has data PASSED
Asserting Building fct_orders: populating the right payment column PASSED
Asserting Building fct_orders: unknown payment methods still show up in totals, but nowhere else PASSED
Asserting Building fct_orders: multiple payments for the same order PASSED
```


## Comparison with other kinds of dbt tests

dtspec is **not** meant to replace the schema or data tests that are currently baked in to dbt.  dtspec is meant to supplement these tests and enhance overall test coverage.  The dbt schema and data tests are tremendously valuable for ensuring the quality of production data.  However, they can be difficult to use to test hypothetical data situations or complex, record specific, transformation logic.  dtspec is mean to fill that gap.

## How we're using dbt and dtspec at InsideTrack

At InsideTrack, we're using dtspec to run over 150 tests against 50 models in a Redshift data warehouse.

For organizational purposes, we've split up our `spec.yml` file into multiple files and then process them with Jinja to knit them back together into a single spec file (and thus we get all the other great things that come along with Jinja).  Running all of the tests takes about 5 minutes, and we've built our spec parser a way to run just specific models and tests to speed up develoment iterations.

We've also got scripts to copy our production source data schemas into a small Redshift test cluster.  That test cluster is used by our CI system to run dtspec tests after every commit and prior to every deploy.

We're also toying with the idea of migrating our warehouse from Redshift to Snowflake.  In principle, we should be able to perform that migration without modifying any of the dtspec files.  We'll certainly have to adjust some of the model SQL to accomplish this migration, but once we can get all of our dtspec tests passing, we'll have excellent confidence that the migration will be successful.

## Where to learn more

Please check out both the [main dtspec github repo](https://github.com/inside-track/dtspec) and the [jaffle_shop POC](https://github.com/gnilrets/jaffle_shop-dtspec/blob/dtspec/dtspec/test.py).

Comment on this post, or reach out to me on dbt slack if you have any questions!",1
733,"testing-data-transformations","Update: last year, we migrated our data warehouse from Redshift to Snowflake.  dtspec was instrumental in giving us the confidence to migrate.  It caught several errors in several cases where the Snowflake SQL was syntactically valid, but gave different results than in Redshift.  Without dtspec, we wouldn't have caught many business logic errors until users noticed in production.",2
733,"testing-data-transformations","Second Update: I released some new work that makes integrating dtspec with dbt much easier: https://discourse.getdbt.com/t/testing-dbt-data-transformations/2454",3
735,"date-dimensions","Hello All --

We have built a date dimension. Just wanted to share our code and results here and see if others have built their date dimensions differently, anything that you would add/remove, etc.?

> {{
>   config(
> pre_hook=""ALTER SESSION SET WEEK_OF_YEAR_POLICY=1, WEEK_START=7""
>   )
> }}
> 
> WITH DATE_SPINE AS 
> (
> 	{{ dbt_utils.date_spine(
> 			datepart=""day"",
> 			start_date=""to_date('01/01/2018', 'mm/dd/yyyy')"",
> 			end_date=""TO_DATE(CONCAT(YEAR(CURRENT_DATE) + 6,'-12-31'))""
> 		 )
> 	}}
> )
> SELECT
>  CAST(DATE_DAY AS DATE) AS DATE_KEY
> ,CAST(DAYOFYEAR(DATE_KEY) AS INT) AS DAY_OF_YEAR
> ,CAST(DECODE (RIGHT(EXTRACT(DAYOFMONTH FROM DATE_KEY), 1),
>             1 , 'st',
>             2 , 'nd',
>             3 , 'rd',
>             'th' -- Default option, all other days ending aside from 1st, 2nd, and 3rd end in 'th'
>          ) AS VARCHAR(2)) AS DAY_SUFFIX
> ,CAST(YEAR(DATE_KEY) || RIGHT('0' || WEEK(DATE_KEY), 2) AS INT) AS WEEK_KEY
> ,CAST(WEEKOFYEAR(DATE_KEY) AS INT) AS WEEK_OF_YEAR
> ,CAST(DAYOFWEEK(DATE_KEY) AS INT) AS DAY_OF_WEEK
> ,CAST(DAYNAME(DATE_KEY) AS VARCHAR(5)) AS WEEK_DAY_SHORT_DESC
> ,CAST(DECODE (EXTRACT(DAYOFWEEK FROM DATE_KEY),
>             1 , 'Sunday',
>             2 , 'Monday',
>             3 , 'Tuesday',
>             4 , 'Wednesday',
>             5 , 'Thursday',
>             6 , 'Friday',
>             7 , 'Saturday'
>          ) AS VARCHAR(9)) AS WEEK_DAY_DESC
> ,CAST(TRUNC(DATE_KEY, 'Week') AS DATE) AS FIRST_DAY_OF_WEEK
> ,CAST(LAST_DAY(DATE_KEY, 'Week') AS DATE) AS LAST_DAY_OF_WEEK
> ,CAST(YEAR(DATE_KEY) || RIGHT('0' || MONTH(DATE_KEY), 2) AS INT) AS MONTH_KEY
> ,CAST(MONTH(DATE_KEY) AS INT) AS MONTH_OF_YEAR
> ,CAST(DAYOFMONTH(DATE_KEY) AS INT) AS DAY_OF_MONTH
> ,CAST(MONTHNAME(DATE_KEY) AS VARCHAR(5)) AS MONTH_SHORT_DESC
> ,CAST(TO_CHAR(DATE_KEY, 'MMMM') AS VARCHAR(50)) AS MONTH_DESC
> ,CAST(TRUNC(DATE_KEY, 'Month') AS DATE) AS FIRST_DAY_OF_MONTH
> ,CAST(LAST_DAY(DATE_KEY, 'Month') AS DATE) AS LAST_DAY_OF_MONTH
> ,CAST(YEAR(DATE_KEY) || QUARTER(DATE_KEY) AS INT) AS QUARTER_KEY
> ,CAST(QUARTER(DATE_KEY) AS INT) AS QUARTER_OF_YEAR
> ,CAST(DATE_KEY - TRUNC(DATE_KEY, 'Quarter') + 1 AS INT) AS DAY_OF_QUARTER
> ,CAST('Q' || QUARTER_OF_YEAR AS VARCHAR(5)) AS QUARTER_SHORT_DESC
> ,CAST('Quarter ' || QUARTER_OF_YEAR AS VARCHAR(50)) AS QUARTER_DESC
> ,CAST(TRUNC(DATE_KEY, 'Quarter') AS DATE) AS FIRST_DAY_OF_QUARTER
> ,CAST(LAST_DAY(DATE_KEY, 'Quarter') AS DATE) AS LAST_DAY_OF_QUARTER
> ,CAST(YEAR(DATE_KEY) AS INT) AS YEAR_KEY
> ,CAST(TRUNC(DATE_KEY, 'Year') AS DATE) AS FIRST_DAY_OF_YEAR
> ,CAST(LAST_DAY(DATE_KEY, 'Year') AS DATE) AS LAST_DAY_OF_YEAR
> ,CAST(ROW_NUMBER()  OVER (PARTITION BY YEAR(DATE_KEY), MONTH(DATE_KEY), DAYOFWEEK(DATE_KEY) ORDER BY DATE_KEY) AS INT) AS ORDINAL_WEEKDAY_OF_MONTH
> ,CAST(CASE 
>       WHEN MONTH(DATE_KEY) = 1 AND DAYOFMONTH(DATE_KEY) = 1
>             THEN 'New Year''s Day'
>       WHEN MONTH(DATE_KEY) = 1 AND DAYOFMONTH(DATE_KEY) = 20 AND ((YEAR(DATE_KEY) - 1) % 4) = 0
>             THEN 'Inauguration Day'
>       WHEN MONTH(DATE_KEY) = 1 AND DAYOFWEEK(DATE_KEY) = 2 AND ORDINAL_WEEKDAY_OF_MONTH  = 3
>             THEN 'Martin Luther King Jr Day'
>       WHEN MONTH(DATE_KEY) = 2 AND DAYOFMONTH(DATE_KEY) = 14
>             THEN 'Valentine''s Day'
>       WHEN MONTH(DATE_KEY) = 2 AND DAYOFWEEK(DATE_KEY) = 2 AND ORDINAL_WEEKDAY_OF_MONTH  = 3
>             THEN 'President''s Day'
>       WHEN MONTH(DATE_KEY) = 3 AND DAYOFMONTH(DATE_KEY) = 17
>             THEN 'Saint Patrick''s Day'
>       WHEN MONTH(DATE_KEY) = 5 AND DAYOFWEEK(DATE_KEY) = 1 AND ORDINAL_WEEKDAY_OF_MONTH  = 2
>             THEN 'Mother''s Day'
>       WHEN MONTH(DATE_KEY) = 5 AND DAYOFWEEK(DATE_KEY) = 2 
>             AND LAST_VALUE(DAYOFMONTH(DATE_KEY))  OVER (PARTITION BY MONTH_KEY
>                       ORDER BY TO_DATE(DATE_KEY)) - 7 <= DAYOFMONTH(DATE_KEY)
>             THEN 'Memorial Day'
>       WHEN MONTH(DATE_KEY) = 6 AND DAYOFWEEK(DATE_KEY) = 1 AND ORDINAL_WEEKDAY_OF_MONTH  = 3
>             THEN 'Father''s Day'
>       WHEN MONTH(DATE_KEY) = 7 AND DAYOFMONTH(DATE_KEY) = 4
>             THEN 'Independence Day'
>       WHEN MONTH(DATE_KEY) = 9 AND DAYOFWEEK(DATE_KEY) = 2 AND ORDINAL_WEEKDAY_OF_MONTH  = 1
>             THEN 'Labor Day'
>       WHEN MONTH(DATE_KEY) = 10 AND DAYOFWEEK(DATE_KEY) = 2 AND ORDINAL_WEEKDAY_OF_MONTH = 2
>             THEN 'Columbus Day'
>       WHEN MONTH(DATE_KEY) = 10 AND DAYOFWEEK(DATE_KEY) = 31
>             THEN 'Halloween'
>       WHEN MONTH(DATE_KEY) = 11 AND DAYOFWEEK(DATE_KEY) = 5 AND ORDINAL_WEEKDAY_OF_MONTH  = 4
>             THEN 'Thanksgiving Day'
>       WHEN MONTH(DATE_KEY) = 12 AND DAYOFMONTH(DATE_KEY) = 25
>             THEN 'Christmas Day'
>       WHEN MONTH(DATE_KEY) = 12 AND DAYOFMONTH(DATE_KEY) = 26
>             THEN 'Boxing Day'
>       ELSE NULL
>  END AS VARCHAR(50)) AS HOLIDAY_DESC
> ,CAST(CASE WHEN HOLIDAY_DESC IS NULL THEN 0 ELSE 1 END AS BOOLEAN) AS IS_HOLIDAY     
> FROM DATE_SPINE
> 
> {{
>   config(
> post_hook='{{ add_not_null_constraints([""DATE_KEY""
>     ,""DAY_OF_YEAR""
>     ,""DAY_SUFFIX""
>     ,""WEEK_KEY""
>     ,""WEEK_OF_YEAR""
>     ,""DAY_OF_WEEK""
>     ,""WEEK_DAY_SHORT_DESC""
>     ,""WEEK_DAY_DESC""
>     ,""FIRST_DAY_OF_WEEK""
>     ,""LAST_DAY_OF_WEEK""
>     ,""MONTH_KEY""
>     ,""MONTH_OF_YEAR""
>     ,""DAY_OF_MONTH""
>     ,""MONTH_SHORT_DESC""
>     ,""MONTH_DESC""
>     ,""FIRST_DAY_OF_MONTH""
>     ,""LAST_DAY_OF_MONTH""
>     ,""QUARTER_KEY""
>     ,""QUARTER_OF_YEAR""
>     ,""DAY_OF_QUARTER""
>     ,""QUARTER_SHORT_DESC""
>     ,""QUARTER_DESC""
>     ,""FIRST_DAY_OF_QUARTER""
>     ,""LAST_DAY_OF_QUARTER""
>     ,""YEAR_KEY""
>     ,""FIRST_DAY_OF_YEAR""
>     ,""LAST_DAY_OF_YEAR""
>     ,""IS_HOLIDAY""]) }}'
>   )
> }}

![19 AM|690x48](upload://wd2GXk4Oj31BPZSjODKdPjgR9YU.png) 

![58 AM|690x65](upload://5UuVqn9DHj2J4y7vdqe6zgNHRWA.png)",1
735,"date-dimensions","Hey @josh!

I wrote about the approach we took at GitLab around this a bit ago. Take a look and let me know what you think: https://discourse.getdbt.com/t/building-a-calendar-table-using-dbt/325",2
735,"date-dimensions","Thanks @emilie I took a look at the date dimension code that you referenced. Looks like we have many similar columns. You also have some fiscal year columns that we don't. On the other hand, we have some additional columns like  DAY_SUFFIX (e.g. ""1st"", ""2nd""), WEEK_DAY_DESC (not just ""Wed"" but ""Wednesday"", ""Thursday), some keys like MONTH_KEY, and, most importantly, a holiday indicator and actual names for specific holidays.

Also I think some of the calculations in your date dimension could be simplified as we have done, e.g. instead of:

`FIRST_VALUE(date_day) OVER (PARTITION BY year_actual, month_actual ORDER BY date_day) AS first_day_of_month,`

you could do: 

`TRUNC(date_day, 'Month')`

for this:

    decode(extract('month',date_day),

               1, 'January',

               2, 'February',

               3, 'March',

               4, 'April',

               5, 'May',

               6, 'June',

               7, 'July',

               8, 'August',

               9, 'September',

               10, 'October',

               11, 'November',

               12, 'December') AS month_name,

you could do:

`TO_CHAR(date_day, 'MMMM')`

and for this:

    (year_actual || '-' || DECODE(extract(quarter,date_day),

               1, 'Q1',

               2, 'Q2',

               3, 'Q3',

               4, 'Q4')) AS quarter_name,

you could do:

`'Q' || EXTRACT(QUARTER FROM date_day)`

Just simplifying and making the code cleaner. You could also simplify many of the other window functions in the same way using the TRUNC() and LAST_DAY() functions.

Do you really not have a need  for holiday calculations? We often need to do workday calculations or similar things and need to know the holidays for that.",3
735,"date-dimensions","Very cool! Just wanted to mention this macro I added to the [dbt-date](https://github.com/calogica/dbt-date/) package a few weeks ago that does some of that as well.
https://github.com/calogica/dbt-date/blob/master/macros/get_date_dimension.sql",4
735,"date-dimensions","@josh Thanks for the feedback. I created https://gitlab.com/gitlab-data/analytics/merge_requests/2121 to make those improvements and add some global holidays. We haven't had a need for them yet, but that's no reason not to add it!",5
743,"a-dynamic-schema-test-for-nested-repeated-bigquery-fields","Nested and repeated records (`STRUCT` and `ARRAY` of `STRUCT` types) in BigQuery are really powerful for performance and organizing, but can be frustrating to test in dbt because they require some extra SQL (namely, the use of `UNNEST`) to expose correctly.

I spent some time a few months ago working on a macro to dynamically unnest a column and run a not null test based on a `.` delimited specifier. I learned a lot about Jinja, especially its recursive capabilities. Here's how I did it.

## Example case

Imagine we have a nested order/order items table called `orders` that looks like this:

| order_id   | items.item_id | items.quantity | promo_codes          | address.city | address.state |
|------------|---------------|----------------|----------------------|--------------|---------------|
| 6301418492 | 13939         | 1              | []                   | New York     | NY            |
|            | 32482         | 4              |                      |              |               |
|            | 59221         | 1              |                      |              |               |
| 4929138818 | 13193         | 3              | [HOLIDAY2019, EMP40] | Baltimore    | MD            |

In this table, `items` is an `ARRAY` of `STRUCT` (nested and repeated), `promo_codes` is an ARRAY of `STRING` (repeated but not nested), and `address` is a `STRUCT` (nested but not repeated).

Our goal is to write a generalizable schema test called `not_null_nr` that finds null records in any kind of nested/repeated data at any depth of nesting.

We should be able to write our `schema.yml` file like this:

```yaml
columns:
  - name: items.item_id
    tests:
      - not_null_nr
  - name: promo_codes
    tests:
      - not_null_nr
  - name: address.city
    tests:
      - not_null_nr
```

## Writing the tests manually

Let's start by imaging how we would write these tests manually in SQL. If we want to test that `order_id` is not null, we can write a simple test like this:

```sql
select order_id
from orders
where order_id is null
```

If we try to write the same test on a nested, repeated field like `items.item_id`...

```sql
select items.item_id
from orders
where items.item_id is null
```

...we get an error from BigQuery:

```text
Cannot access field item_id on a value with type ARRAY
```

Turns out, if we want to test `item_id`, we have to `UNNEST` and join our `items` field to our `orders` table. Now our test might look like this:

```sql
select order_id, item.item_id
from orders
left join unnest(items) item
where item.item_id is null
```

What about our `address.city` field, which is nested but not repeated? In that case the test is simpler because we don't need to `UNNEST`.

```sql
select order_id, address.city
from orders
where address.city is null
```

But wait, there's more! It's also possible that we  have a field that is repeated but not made up of `STRUCT` types, like an `ARRAY` of `INT64`.

Take a look at our `promo_codes` column, which falls into this category. We want to make sure none of those promo codes are null. We write a test in that situation like this:

```
select order_id, code
from orders
left join unnest(orders.promo_codes) code
where code is not null
```
In this test we still need to `UNNEST`, but we don't need a `.` selector because the items in the array are not `STRUCT` type.

## Generalizing the schema test

So to generalize this schema test properly, it needs to cover 3 cases*:
1. The field is **nested** but **not repeated**, e.g. `{'item_id': 100, 'qty': 10}`
1. The field is **nested and repeated**, e.g. `[{'item_id': 100, 'qty': 1}, {'item_id': 200, 'qty': 2}]`
1. The field is **not nested** but is **repeated**, e.g. `[1, 2, 3, 4]`

**There is one more case, neither nested nor repeated, but I'm leaving it up to the user to run regular schema tests on those columns instead.*

The macro should accept the name of the model and the name of the field to be tested. We should be able to specify any depth of nesting in the field name, e.g. `level_1.level_2.level_3.level_4` and the macro should perform the appropriate `UNNEST`. This suggests some kind of recursion, which we'll get into in a minute.

## Constructing the macro

I start off by defining the macro itself and the parameters it should accept. It has to start with `test_` so dbt recognizes it as a custom schema test.

```jinja
{% macro test_not_null_nr(model, column_name) -%}
```

We're going to be getting into some nested loops, and Jinja doesn't have the ability to track variables from outer loops within the inner loop (see the section [here](https://jinja.palletsprojects.com/en/2.10.x/templates/#assignments) on Scoping Behavior).

As a workaround, I create a `namespace` called `vars` with two variables, `level` and `column_list`, which will allow us to track some variables globally.

```jinja
{%- set vars = namespace(level=1, column_list=[]) -%}
```

Next, I split the `column_name` on `.` so we can see the requested levels of nesting and assign it to a new variable called `hierarchy`.

```jinja
{%- set hierarchy = column_name.split('.') -%}
```

I pull all the database fields from the model and get the one that corresponds to our top level `hierarchy` field so we have a rich representation of the column and its children. For example, dbt has a flag to tell us if columns are repeated or not based on the information it gets from the BigQuery.

The `selectattr` filter here gets the item in the list `columns` that has attribute `name` equal to the first element in our `hierarchy` list. 

```jinja
{%- set columns = adapter.get_columns_in_table(model.schema, model.table) -%}
{%- set root = columns | selectattr('name', 'equalto', hierarchy[0]) | list -%}
```
Now that we've set up our variables, I inject the basic SQL to construct the test, counting the number of rows with a `FROM` that corresponds to our first level (the model itself).

```sql
select count(*)
from {{ model }} level_{{ vars.level }}
```

Next, I'm going to recurse through the children of our `root` object until we arrive at the final level of `hierarchy`. Jinja2 has a [recursive loop definition](https://jinja.palletsprojects.com/en/2.10.x/templates/#list-of-control-structures).

```jinja
{% for child in root recursive %}
```
The loop is responsible for generating the correct `UNNEST` statements to unroll the table to a state where we can access the final field in `hierarchy`. Within the loop, I do the following:
1. Add the `name` of the child to the `column_list`, which we'll use for creating strings like `level_1.level_2.level_3`
1. Whenever we reach a child that is a repeated field (as indicated by the `mode` attribute on the child), we increase `vars.level`, we need to `UNNEST` and join the field so we can access it.
1. If the child has a `fields` attribute, we need to go deeper, so I call the `loop` variable to recurse on the field that matches the name in the corresponding level of `hierarchy`.

```jinja
{% for child in root recursive %}
  {%- set vars.column_list = vars.column_list + [child.name] -%}
  {%- if child.mode == 'REPEATED' -%}
    {%- set vars.level = vars.level + 1 %}
    left join unnest(level_{{ vars.level - 1 }}.{{ vars.column_list | join('.') | string }}) as level_{{ vars.level }}
    {%- set vars.column_list = [] -%}
  {%- endif -%}

  {%- if child.fields -%}
    {{ loop(child.fields | selectattr('name', 'equalto', hierarchy[loop.depth]) | list) }}
  {%- endif -%}

{% endfor %}
```

Finally, I add a `WHERE` clause to perform the actual test. At this point, `vars.level` corresponds to the deepest level (the loop exited at) and we can build the field selector from our `column_list`.

```jinja
where level_{{ vars.level }}
{%- if vars.column_list -%}
.{{ vars.column_list | join('.') | string }}
{%- endif %}
is null
```

## The completed macro

As far as I can tell (I've tested this on a bunch of sample combinations), this works. Of course, adapting this for a unique test or other type of custom schema test will require more effort, but the bones should be there.

If you end up identifying any issues or improving on this in any way, let me know!

```jinja
{% macro test_not_null_nested(model, column_name) -%}

{# Workaround to avoid Jinja loop scope issues #}
{%- set vars = namespace(level=1, column_list=[]) -%}

{%- set hierarchy = column_name.split('.') -%}
{%- set columns = adapter.get_columns_in_table(model.schema, model.table) -%}
{%- set root = columns | selectattr('name', 'equalto', hierarchy[0]) | list -%}
select count(*)
from {{ model }} level_{{ vars.level }}

{% for child in root recursive %}
{%- set vars.column_list = vars.column_list + [child.name] -%}
{%- if child.mode == 'REPEATED' -%}
{%- set vars.level = vars.level + 1 %}
left join unnest(level_{{ vars.level - 1 }}.{{ vars.column_list | join('.') | string }}) as level_{{ vars.level }}
{%- set vars.column_list = [] -%}
{%- endif -%}

{%- if child.fields -%}
{{ loop(child.fields | selectattr('name', 'equalto', hierarchy[loop.depth]) | list) }}
{%- endif -%}

{% endfor %}

where level_{{ vars.level }}
{%- if vars.column_list -%}
.{{ vars.column_list | join('.') | string }}
{%- endif %}
is null

{%- endmacro %}
```",1
743,"a-dynamic-schema-test-for-nested-repeated-bigquery-fields","Thanks a lot for this!

I got some deprecation notices, so had to change this line:

    {%- set columns = adapter.get_columns_in_table(model.schema, model.table) -%}

To these instead:

    {%- set relation = api.Relation.create(schema=model.schema, identifier=model.table) -%}
    {%- set columns = adapter.get_columns_in_relation(relation) -%}",2
763,"creating-a-custom-jinja-filter","is there any way to use (and register) a [jinja custom filter](https://jinja.palletsprojects.com/en/2.10.x/api/#custom-filters) inside dbt?

I would like to slugify some col names (we generate dynamic col from row's values.",1
763,"creating-a-custom-jinja-filter","Looking for exactly the same thing.... want a regex_replace for this. Looks like there was a GH issue that's been closed for now.

https://github.com/fishtown-analytics/dbt/issues/480",3
763,"creating-a-custom-jinja-filter","Although this still isn't possible (and comes up from time to time), we have made good progress on knocking off the sorts of things that make them particularly necessary. 

dbt now includes native support for:
- the [`re` regex module](https://docs.getdbt.com/reference/dbt-jinja-functions/modules#re) (in v0.19.0), which enabled [slugify](https://github.com/dbt-labs/dbt-utils#slugify-source) in dbt-utils
- [zip](https://docs.getdbt.com/reference/dbt-jinja-functions/zip), [set](https://docs.getdbt.com/reference/dbt-jinja-functions/set) and [itertools](https://docs.getdbt.com/reference/dbt-jinja-functions/modules#itertools) in v1.2.0",4
764,"did-you-know-dbt-ships-with-its-own-project","Have you ever executed `dbt run` in a pretty small project and been confused at the number of macros in the output?

    $ dbt run
    Running with dbt=0.15.0
    Found 1 model, 0 tests, 0 snapshots, 0 analyses, 138 macros, 0 operations, 0 seed files, 0 sources

138 macros? Huh?

Well, this is a result of a pretty cool design pattern, which is that **dbt ships with its own global project**. Then, dbt overlays your own project on top of this project, resulting in the 138 macros! They're not in _your_ project, they're in the _global_ project!

If you dive into the [source code](https://github.com/fishtown-analytics/dbt) for dbt, you'll find a directory called `global project` ([here](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/core/dbt/include/global_project)). In it, you'll find a file called `dbt_project.yml`, as well as a folder for `docs` and `macros`. Looks familiar right?

![Screen Shot 2019-12-20 at 3.54.14 PM|690x259](upload://ql6Z0SWUqPfB008IJJJJQ4SPrno.png) 

This ""global"" project includes lots of the snippets of Jinja that get used in every project, including:

- Materializations + the macros contain the relevant SQL to power them ([here](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/core/dbt/include/global_project/macros/materializations))
- Schema tests (which we just define as a macro that is prefixed with `test_`), e.g. `test_unique` ([here](https://github.com/fishtown-analytics/dbt/blob/dev/0.15.1/core/dbt/include/global_project/macros/schema_tests/unique.sql#L2)).
- The macro that determines the name of the schema that a model should be built in, `generate_schema_name` ([here](https://github.com/fishtown-analytics/dbt/blob/dev/0.15.1/core/dbt/include/global_project/macros/etc/get_custom_schema.sql#L17)), and the table/view name, `generate_alias_name` ([here](https://github.com/fishtown-analytics/dbt/blob/dev/0.15.1/core/dbt/include/global_project/macros/etc/get_custom_alias.sql#L15)).
- The default text for your overview when you run `dbt docs generate`, also known as the `overview` ([here](https://github.com/fishtown-analytics/dbt/blob/dev/0.15.1/core/dbt/include/global_project/docs/overview.md)).

Take a look around, and you'll start to see some familiar things! 

If you dive deeper, you'll discover there's also some magic around `default` macros and `adapter` macros, as well as separate projects for each  `adapter` (e.g. plugins for [Postgres](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/plugins/postgres/dbt/include/postgres), [Redshift](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/plugins/redshift/dbt/include/redshift), [BigQuery](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/plugins/bigquery/dbt/include/bigquery) and [Snowflake](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.1/plugins/snowflake/dbt/include/snowflake)) , but we'll leave that for another article.

**Cool, cool, cool... so why am I telling you this?**

Well there's a pretty nifty design pattern in dbt â€” if *your* dbt project has a macro with the same name as one of the global macros, dbt will favor the macro defined in your project over the global implementation. You see this crop up in a few places in the docs:

- To define your own logic for schema names, add a `generate_schema_name` macro to your project ([docs](https://docs.getdbt.com/docs/using-custom-schemas#section-changing-the-way-dbt-generates-a-schema-name))
- To define your own logic for relation names, add a `generate_alias_name` macro to your project ([docs](https://docs.getdbt.com/docs/using-custom-aliases#section-generate-alias-name))
- To write your own overview for your docs, add an `overview` docs block to your project ([docs](https://docs.getdbt.com/docs/documentation-website#section-setting-a-custom-overview))
- To override the default implementation for the `unique` -ness test (for example, to only test in defined environments), add a `test_unique` macro to your project ([Discourse example](https://discourse.getdbt.com/t/conditionally-running-dbt-tests-only-running-dbt-tests-in-production/322/3)). Pro-tip: start by copying the code from the default implementation, and edit from there.

Broadly, I think it's pretty cool to just understand a little more of how dbt works under the hood, so I wanted to share this explanation.

Another great example of leveraging this design pattern is when you want to make changes to an existing materialization â€” for example Warby Parker wanted to adapt the `create_table_as` macro on Postgres to be able to use the `UNLOGGED` parameter ([Postgres docs](https://www.postgresql.org/docs/9.1/sql-createtable.html). They were first able to test out their changes by adding a `postgres__create_table_as` macro to their own project to override the default implementation. Once they got it working as expected, they were able to [contribute it back to dbt](https://github.com/fishtown-analytics/dbt/pull/1650/files)!",1
769,"can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project","Hi friends, 

I'm a newbie. And I have a question about using multiple BigQuery datasets (or projects).
dbt means by ""warehouse"" as ""BigQuery: a single BigQuery account, with access to one or more projects"". So in the documents, it just shows how to config and work with one dataset only.

My scenario is as shown below.
In the same project of bigquery, I have multiple datasets of data sources e.g. recurly,  saleforces, ...
and a dataset for transformed data e.g. edw.

Is it possible in dbt project when using multiple BigQuery datasets (or multiple BigQuery projects)? If so, please let me know how to configure in profiles.yml and give me a sample model using these configs.

I'm looking forward to hearing from you.

Thanks,
Hang Tran",1
769,"can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project","I'm dealing with the exact same issue.
My query consists out of data from different bigquery projects and different bigquery datasets. But if I read the documentation it seems like only querying within the same Bigquery project is allowed.
Any updates if this is possible? 

Kind regards,
Ruben",2
769,"can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project","Yes, this is possible to do using custom [database](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-database) and [schema](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas). 

In the simplest form you would use the `database` and `schema` parameters. You can use those parameters when defining `sources`, to specify where your source datasets are. And you can use them to define where dbt should build your `models`, either in your `dbt_project.yml` or `model` config. 

For even more control you can also consider changing the `generate_schema_name` or `generate_database_name` macros.

Also note that the `project_id` you define in your `profiles.yml` will be the project used for BQ billing.

Hope that helps,",3
769,"can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project","Thanks for your comprehensive answer! :smiley: You've even already answered the question I wanted to ask next :smiley:",4
769,"can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
786,"dynamically-load-relations-only-if-they-exist","I have a model that has multiple sources as dependencies, but not all fonts are available at the time dbt runs because they are dynamic.

Is there a way to executing a select (or part of the code) only if a table (or schema) exists?

maybe with the [get_relations_by_prefix](https://github.com/fishtown-analytics/dbt-utils#get_relations_by_prefix) helper? Thanks!",1
786,"dynamically-load-relations-only-if-they-exist","I came up with this macro, based on the  **get_relations_by_prefix** to return a relation if exists

    {#
        Return a relation if exists in a given schema and (optionally) database

        Arguments:
        schema (required): The schema to inspect for relations.
        prefix (required): The prefix of the table/view (case insensitive)
        exclude (optional): Exclude any relations that match this pattern.
        database (optional, default = target.database): The database to inspect for relations.

    #}
    {% macro has_relation(schema, prefix, exclude='', database=target.database) %}

        {% set relations = dbt_utils.get_relations_by_prefix(schema, prefix, exclude, database) %}

        {% if not relations %}
            {{ return(False) }}
        {% else %}
            {{ relations }}
        {% endif %}

    {% endmacro %}

In my code i have some if's like

    {% set fb_region_country_relation = has_relation('facebook_ads', 'insights_campaign_region_country') %}

    {% if fb_region_country_relation %}
    fb_region_country as (
        select
            {{ dbt_utils.surrogate_key('region', 'country') }}::CHAR(36) as id,
            {{ padronize_string_field('region', 'region', 'varchar(150)') }},
            {{ padronize_string_field('country', 'country', 'varchar(150)') }}

        from {{ ref('_fb_ads_insights_campaign_region_country') }}

        {% if is_incremental() %}
            where id not in (select id from {{ this }})
        {% endif %}
    ),
    {% endif %}",2
786,"dynamically-load-relations-only-if-they-exist","When you use the word model do you mean one table, or a dbt project with multiple tables?  For example, we have a file structure with a schema per folder and multiple .sql files inside of the folder to build all of the tables in that schema.  We have the ability to either schedule all tables in that folder to run at one time, or just one table, based on which dbt run statement we execute, for example- ""dbt run -m schema_folder"" vs ""dbt run -m schema_folder.table_file"".

It sounds like maybe, instead you're saying that you have dynamic sources in the ETL used to build one table?  For example, the sql to build one table might look like the following -
select
a.*
,b.*
,c.*
from campaign_country_a
join campaign_country_b on a.id = b.id
join campaign_country_c on a.id = c.id

It sounds like, when a campaign is turned off, then the table with data for that campaign is no longer available?  If that's the case, then why is that upstream table being dropped?",3
786,"dynamically-load-relations-only-if-they-exist","@ksapelak  Yeah, we have an ELT script that extracts data from different marketing sources based on client configuration, then we build our dw using these tables, we have a multi database redshift with one database per client.

Both dimensions and facts are builded using different sources, ex Facebook ads and LinkedIn ads, and both share commom dim and facts, but If one of our clients enable only fb ads we need to build only using fb ads as source",4
786,"dynamically-load-relations-only-if-they-exist","Hi @edbizarro !
Why don't you create an empty view for the tables that a custmer does not yet have?

    create view some_name as
    select null as c1, null as c2, null as c3 
    where 1 = 2

Something like above allows you to create a view with the name you need and as many columns as you need but without any content, so you can keep selecting from it without knowing if the customer has that source or not.
You can even just start always with views like that and them point the right view to the real table containing the data when some of the data starts to come in.

Ciao, Roberto",5
826,"can-you-define-column-data-types-in-dbt","Hello DBT, can you kindly advise if there is a way to define ""column type"" (eg: varchar, date) in dbt?",1
826,"can-you-define-column-data-types-in-dbt","Because dbt uses ""Create Table As Select"" statements (CTAS) to create tables in relevant database platforms, instead of doing separate ""CREATE TABLE"" and then ""INSERT"" statements, it doesn't natively have a way to specify datatypes.

However, what we do in our dbt models is to force a CAST on every line of the projection part of of the SQL Statement (e.g. the column list normally immediately after SELECT) to ensure that the datatypes coming out are what we think they are.

So, like this:

    WITH SUBQUERY AS
    (SELECT
    MD5_BINARY(TO_VARCHAR(DIM_AIRLINE_KEY) || '|' || FLIGHT_NUM || '|' || FLIGHT_LEG_NUM) AS DIM_FLIGHT_LEG_SCHEDULE_KEY,
    MD5_BINARY(TO_VARCHAR(DIM_AIRLINE_KEY) || '|' || FLIGHT_NUM) AS DIM_FLIGHT_SCHEDULE_KEY,
    -- All of these 5 fields excluded from the star are already in the dimension.
    {{ dbt_utils.star(from=ref('stg_flight_leg'), except=[""START_DIM_AIRPORT_KEY"", ""END_DIM_AIRPORT_KEY"", ""DIM_AIRLINE_KEY"", ""FLIGHT_NUM""]) }}
    FROM {{ ref('stg_flight_leg') }})
    SELECT 
        -- Not casting BINARY fields, doesn't make sense
        FACT_FLIGHT_LEG_KEY,
        DIM_FLIGHT_LEG_SCHEDULE_KEY,
        DIM_FLIGHT_SCHEDULE_KEY,
        CAST(FLIGHT_POSITION_DATE AS DATE) AS FLIGHT_POSITION_DATE,
        DIM_AIRPLANE_KEY,
        CAST(FLIGHT_DURATION AS INT) AS FLIGHT_DURATION,
        CAST(FLIGHT_START_DATE AS DATE) AS FLIGHT_START_DATE,
        CAST(FLIGHT_START_TIME AS TIME) AS FLIGHT_START_TIME,
        CAST(FLIGHT_START_TIMESTAMP AS TIMESTAMP) AS FLIGHT_START_TIMESTAMP,
        CAST(FLIGHT_END_DATE AS DATE) AS FLIGHT_END_DATE,
        CAST(FLIGHT_END_TIME AS TIME) AS FLIGHT_END_TIME,
        CAST(FLIGHT_END_TIMESTAMP AS TIMESTAMP) AS FLIGHT_END_TIMESTAMP,
        CAST(FLIGHT_START_LAT AS FLOAT) AS FLIGHT_START_LAT,
        CAST(FLIGHT_START_LONG AS FLOAT) AS FLIGHT_START_LONG,
        CAST(FLIGHT_END_LAT AS FLOAT) AS FLIGHT_END_LAT,
        CAST(FLIGHT_END_LONG AS FLOAT) AS FLIGHT_END_LONG,
        CAST(CNT_LEGS AS NUMBER(18,0)) AS CNT_LEGS,
        CAST(FLIGHT_LEG_NUM AS NUMBER) AS FLIGHT_LEG_NUM,
        CAST(START_AIRPORT_DISTANCE_KM AS FLOAT) AS START_AIRPORT_DISTANCE_KM,
        CAST(END_AIRPORT_DISTANCE_KM AS FLOAT) AS END_AIRPORT_DISTANCE_KM
    FROM
    SUBQUERY WHERE DIM_FLIGHT_LEG_SCHEDULE_KEY IN
    (SELECT DIM_FLIGHT_LEG_SCHEDULE_KEY FROM {{ ref('dim_flight_leg_schedule') }} )

Except for the _KEY fields, which we know are binary so we don't bother with the cast.",2
826,"can-you-define-column-data-types-in-dbt","[quote=""josh, post:2, topic:826""]
-- All of these 5 fields excluded from the star are already in the dimension.
[/quote]

I'm wondering what is the best practice of _where_ to do this casting?

For example, my first step is always to create a `base_` table that renames columns. Seems like that would be the right place to do the casting so that the types automatically propagate to downstream queries.

Another consideration is that when you CAST something you always have to specify the column name, which can lead to a lot of repeating as in Josh's example above.

That could probably be solved through a macro. I checked dbt_utils and don't see anything like that yet.

So then the structure could be something like:
``` sql
with source as (
    
    select * from stg.ga_page_stats
    
),

casted as (
    select 
        mcast(date, date),
        mcast(pagePath, varchar(3000)),
        mcast(sourceMedium, varchar(512)),
        mcast(campaign, varchar(512)),
        mcast(keyword, varchar(512)),
        mcast(adContent, varchar(512)),
        mcast(fullReferrer, varchar(512)),
        mcast(pageviews, int),
        mcast(uniquePageviews, int)
    from source 
),

renamed as (
    select 
        date as date,
        pagePath as page_path,
        sourceMedium as traffic_source_medium,
        campaign as traffic_campaign,
        keyword as traffic_keyword,
        adContent as traffic_content,
        fullReferrer as full_referrer,
        pageviews as pageviews,
        uniquePageviews as unqiue_pageviews
    from casted 
),
```

Or am I overcomplicating things here?",3
826,"can-you-define-column-data-types-in-dbt","We do this casting on every single model. The issue of ensuring that datatypes are what we expect they are is not just an issue for models that are immediately adjacent to sources but for every single model along the way. From an engineering perspective we never want to assume - we always want to verify/confirm/dictate/be prescriptive.

Your idea for a macro to make the CASTing syntax a little more elegant is cool, I like that.",4
826,"can-you-define-column-data-types-in-dbt","A post was split to a new topic: [Casting column datatypes based on a seed](/t/casting-column-datatypes-based-on-a-seed/4877)",5
826,"can-you-define-column-data-types-in-dbt","Weâ€™ve also had a need to cast data types. We unpacked a JSON string. 
This is what we did:
https://discourse.getdbt.com/t/auto-pivot-json-string-using-schema-yml-in-bigquery/1592",6
826,"can-you-define-column-data-types-in-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
856,"handling-unstructured-data-from-mongodb","Hi everyone,

Our MongoDB has unstructured data that we are bringing in as a string - such as:

    {""_eventid"": ""1234"", ""Keywords"":""""}
    {""_eventid"": ""4567"", ""Keywords"":{""signup"":True} }

When it comes to DBT, what is best practice?:

1. Extract attributes as columns at the staging layer (i.e. event.data->>'_eventid' AS eventid)
2. Load the data as is into the staging layer, and then extract the unstructured data

Are there any alternative/better ways of handling unstructured data? We are pulling this data into BigQuery

Thanks!",1
856,"handling-unstructured-data-from-mongodb","If you are following an ELT pattern, then loading into your source table as-is and then extracting the semi-structured data is what I do (I use snowflake so optimizing for cost may be different). I usually make a parsed view over the top of the data, then an incremental table off of that view.

A single json object may be normalized (broken out) into multiple models, for example and order might have a total, some customer data, and have an array of items that have prices associated with them, I would build an order object, and an order detail (which contained the times in the order) off of it.",2
921,"unioning-identically-structured-data-sources","If you're working with an app that has one database per client, or a company that has one Shopify store for each ecommerce store they manage, it's often a good idea to union together each of these tables into one master table.

Here's how I would do it for a client that has a separate Shopify store for each country of operation.

## 1. Use sources
```
version: 2

sources:
  - name: shopify_au
    tables: 
      - name: orders
      - name: customers

  - name: shopify_us
    tables: 
      - name: orders
      - name: customers
```
Then, if you want to get fancy, use [yaml anchors](https://confluence.atlassian.com/bitbucket/yaml-anchors-960154027.html) to reduce duplicated code:
```
version: 2

sources:
  - name: shopify_au
    tables:  &shopify_tables
      - name: orders
      - name: customers

  - name: shopify_us
    tables: *shopify_tables

```
This extends pretty well when working with lots of countries! Unfortunately at present there's no way to iterate over a list in your source definition.

## 2. Union together your sources in one model
For each table, I'd create a model whose **only** function is to union together the different sources (not to do any renaming/recasting).

**Why _only_ union in this model?** I am a big advocate for doing one logical thing in each model (when that logical thing is relatively easy to define), as it makes it easier to explain to someone what the model does. I like to think of things in terms of ""contracts""/a guarantee to end users, which says ""this model unions data only"", as I find these sorts of models are easier to maintain

In keeping with the [way we structure out projects](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355), I'd name this something like `models/staging/shopify/union/union_shopify__orders.sql`.

In an ideal world, where the tables have the exact same columns and the same column order, you can simply do something like this:
```
{% set countries =  ['au', 'us'] %}

{% for country in countries %}
  select 
      *,
      '{{ country }}' as country
  from {{ source(country, 'orders') }}
{% if not loop.last -%} union all {%- endif %}
{% endfor %}

```
:point_up: This has the advantage of being reasonably readable for someone who is still getting used to Jinja.
 
If your columns are in a strange order/some are missing from a table, you can use the [union_relations](https://github.com/fishtown-analytics/dbt-utils#union_relations-source) macro to magic over that:
```
{% set countries =  ['au', 'us'] %}
{% set shopify_order_sources = [] %}
{% for country in countries %}
{% do shopify_order_sources.append(source('shopify_' ~ country, 'orders') %}
{% endfor %}

{{ dbt_utils.union_relations(shopify_order_sources) }}
```
OK, a little more complex, but you can still figure out what's happening!

From here, you can choose how much macro-magic you want to apply. Some options:
1. Use a `var` in your `dbt_project.yml` file to set the `countries` list
2. Create a `union_shopify_sources` macros that has the logic for building the `shopify_order_sources` list, so you can call it like so (obviously name it appropriately for your use-case)
```
{{ union_shopify_sources(countries=['au', 'us'], table_name = 'orders') }}
```
3. Parse the `graph` jinja variable (currently undocumented since it's liable to change) to dynamically build the list of countries without having to manually define it. Here's a hint:
```
{% macro get_sources(source_prefix, table_name) %}
    {% set sources = [] %}
    {% if execute %}
    {% for node_name, node in graph.nodes.items() %}
        {% if node.resource_type == 'source' and node.name == table_name and node.source_name.startswith(source_prefix) %}
            {% set new_source = source(node.source_name, node.name) %}
            {% do sources.append(new_source) %}
        {% endif %}
    {% endfor %}
    {% do return(sources) %}
    {% endif %}
{% endmacro %}
```
(This has the disadvantage of not rendering correctly in the docs)

**A word of caution:** All of these add complexity to your project â€” it's much harder for someone who didn't write this logic to understand exactly what is going on and make changes when required. Consider what's right for your use case â€” if you are unioning 70 countries and 45 tables, it is totally appropriate to use macros, but if you have 2 countries and 3 tabes, go with the simple version first!

# 3. Construct new unique keys
If you're using multiple shopify stores, and your source data uses incrementing keys, you're going to end up with `orders.id = 1` for multiple orders!

My preference is to construct new unique keys like `AU-1`, `US-1` instead: these keys are really easy to read and make debugging much easier than constructing a hashed key.

There's a few different approaches we've taken over time:
- The layer after our `union_shopify__orders` model is `stg_shopify__orders`, and we'll write the SQL explictly there (again, way more readable)
```
select
  upper(country) || '-' || id as order_id,
  upper(country) || '-' || customer_id as customer_id,
  ...

```
- We might get a little fancy in our `union_shopify__orders` model with some Jinja directly in there:
```
{% set countries =  ['au', 'us'] %}
{# assume all tables have the same columns, which is fine, because we are selecting * later based on this assumption #}
{% set columns = get_columns_in_relation(source('shopify_au', 'orders') %}

{% for country in countries %}
  select 
      {% for col in columns %}
      {% if col.name == 'id' or  col.name[-3:] == '_id' %}
      upper('{{ country }}') || '-' || {{ col.name }} as {{ col.name }},
      {% else %}
      {{ col.name }},
      {% endif %}
      {% endfor %}
      '{{ country }}' as country
  from {{ source(country, 'orders') }}
{% if not loop.last -%} union all {%- endif %}
{% endfor %}


```
- If we chose to create our own macro to union together our sources, we might embed that logic directly in the macro.

# But what is we have hundreds of tables in each schema?
The above approach means you would need to create hundreds of models to create the union-ed version of the data (and each model would contain a call to a single macro).

At some point, this starts to break down. An alternative approach might be to use an [operation](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations#operations) to:
1. List all the objects in a schema
2. Iterate through that list and create a ""super"" view in a new schema, which unions all the countries together â€” the code will likely look similar to the above macro
3. Create sources in your dbt project that point to the super view
4. Run this operation at the start of any dbt job

With this approach, you lose the ability to view the lineage of the super view in your project's docs (since lineage depends on using sources and refs), however it does declutter your project. FWIW, I think this approach is warranted when doing this at scale â€” if transformations are ""mechanical"" (i.e. easy to define expected input and output), operations can come in handy here :slight_smile:",1
921,"unioning-identically-structured-data-sources","Excellent write-up! 

Our team is taking a similar approach for making a master metric table that unions a bunch of staged metric tables together. We are using the `dbt_utils.get_relations_by_prefix` and `dbt_utils.union_relations` combination and found it very powerful and readable.

**Is it possible to force this ""master"" model to run after the staged models?** Our current approach is to run `dbt` twice, once with `--models tag:staging` and then `--models tag:master`. This doesn't feel like the best approach, but it does allow us to generate the ""master"" model from successful staged models, regardless of failed staged models - which is desired.",2
921,"unioning-identically-structured-data-sources","Hi Joe,

So the `get_relations_by_prefix` macro returns a list of Relations (a view or table) in the warehouse, rather than a list of models. As a result, dbt can't infer the dependencies between the upstream model and your master model. 

There's no real way to fix this, your workaround seems reasonable! The other option is to use subdirectories to group together your models and run them like `--models path.to.subdirectory.*`

Your idea is a good one though, check out this related issue:
https://github.com/fishtown-analytics/dbt/issues/1212",3
921,"unioning-identically-structured-data-sources","Hello,

These approaches in this article have been very helpful for our team, as we deal with a growing mutli source system. One question: we have some unique sources where [YAML Overrides](https://support.atlassian.com/bitbucket-cloud/docs/yaml-anchors/#Override-values) would help keep the config clean and readable. Does anyone know if that feature supported in dbt? We've tried unsuccessfully to implement it...",4
921,"unioning-identically-structured-data-sources","YAML overrides are indeed supported in dbt! Feel free to [join us in Slack](community.getdbt.com) and share your question there â€” I think we'd need some more info to help debug here (code snippets etc.)",5
921,"unioning-identically-structured-data-sources","[quote=""claire, post:1, topic:921""]
`{% if not loop.last -%} union all {%- endif %}`
[/quote]

Hi Claire,

This is very inspiring! 
Can you shed me some lights on how to do a `left join ... using ... ` instead of `union all` in similar use case?

Thanks",6
921,"unioning-identically-structured-data-sources","Turned out that I figured out how to use `left join` (in case of different tables have different columns that need be consolidated together into one)

```
{% set selected_list = var('selected_lists') %}

select *
{% for model in selected_list %}
  {% if loop.first %}
  from {{ ref(model) }}
  {% else %}
  left outer join {{ ref(model) }}
    using(provider_key, contract_class)
  {% endif %}
{% endfor %}",7
921,"unioning-identically-structured-data-sources","[quote=""s.chaser, post:7, topic:921""]
in case of different tables have different columns that need be consolidated together into one
[/quote]

@s.chaser you could also check out [union_relations](https://github.com/dbt-labs/dbt-utils#union_relations-source) from dbt_utils",8
921,"unioning-identically-structured-data-sources","question, if i have three variable that need to loop, is it possible to loop three section based on three different variables?  for example, i have below three variable, and i want to have three {{for in }} in the select section, how can i do that, i tried directly put all three variable in the top and add three {{for in }} in the select section, only first variable loop works, the rest two doesn;t work, any ideal?
{% set ledger_type =['AA','E1','AJ','AU','BA','1F','2F','3F','4F','1B','2B','3B','4B','JA','PA','HA','JY' ,'EB','F1','F2','F3','F4','B1','B2','B3','B4']%}

 {% set acct_cat_code =['1', '2', '3',  '7', '4', '5', '6','8', '9','10', '11', '12', '13', '14', '15', '16', '17', '18', '19','20', '21', '22', '23', '24']%}

 {% set bu_cat_code =['01', '02', '03', '04', '05', '07', '09', '10', '11', '12', '13', '14', '15', '18', '19', '20', '21', '22', '23', '06', '08', '16', '17', '25','29','30','31', '24','26', '27', '28']%}",9
921,"unioning-identically-structured-data-sources","Hi Joel,  is there one idea or step using unions_relations to avoid the duplicates using ""Primary_key"" Fields that I have on both... tables",10
921,"unioning-identically-structured-data-sources"," A bit of an update:
The Fivetran free dbt package has a nice wrapup for get_relation and union_relations for the task.


[fivetran_utils.union_data](https://github.com/fivetran/dbt_fivetran_utils#union_data-source)

Example:

Model:
```sql
 {{config(materialized='table')}}
 
 
 {{
     fivetran_utils.union_data(
         table_identifier='T1',
         default_schema = 'public'
     )
 }}
```
Execution variant:
`C:\Users\serge\venvs\srtd\Scripts\dbt.exe run --vars ""union_databases: ['raw','analytics']""`

Generated code:
```
(
             select
 
                 cast('""RAW"".""PUBLIC"".""T1""' as 
     varchar
 ) as _dbt_source_relation,
                 
                     cast(""VL"" as character varying(3)) as ""VL"" 
 
             from ""RAW"".""PUBLIC"".""T1""
 
             
         )
 
         union all
         
 
         (
             select
 
                 cast('""ANALYTICS"".""PUBLIC"".""T1""' as 
     varchar
 ) as _dbt_source_relation,
                 
                     cast(""VL"" as character varying(3)) as ""VL"" 
 
             from ""ANALYTICS"".""PUBLIC"".""T1""
 
             
         )
``` 
 Snowflake result:
![image-1|690x117](upload://1is2uiOL9rv38K052ZiPOnWBTBR.png)

Slack discussion:
https://getdbt.slack.com/archives/CBSQTAPLG/p1657208903142319?thread_ts=1656702086.994559&cid=CBSQTAPLG",11
921,"unioning-identically-structured-data-sources","Hi Claire! What if my final union statement is too large for my redshift to proccess it? Is there a solution for that? Something like incremental inserting with looping by schema name?",12
967,"custom-materialization-to-load-from-external-table-into-snowflake","Hi All,

Did any one test or create a Custom Materialization which uses COPY INTO command to load data from External table to snowflake ?

If you have created a Custom Materialization can you please share the details.",1
967,"custom-materialization-to-load-from-external-table-into-snowflake","Hi @sudhirnune! You might find this post useful:

https://medium.com/hashmapinc/dont-do-analytics-engineering-in-snowflake-until-you-read-this-hint-dbt-bdd527fa1795",2
967,"custom-materialization-to-load-from-external-table-into-snowflake","Unfortunately that article is not relevant to this question, which is how to use Snowflake's COPY INTO command in dbt. The article doesn't mention COPY INTO at all.
It would be really useful to have a way to do this with dbt though.",3
967,"custom-materialization-to-load-from-external-table-into-snowflake","There's an example of how to use a custom materialization to load data using COPY INTO here (see section ""third attempt""): http://mamykin.com/posts/fast-data-load-snowflake-dbt/

In this example, the custom materialization creates a temporary external stage (to an S3 bucket) and uses the COPY INTO .. FROM SELECT... syntax.",4
967,"custom-materialization-to-load-from-external-table-into-snowflake","Hey @NathanG! My intent wasn't to state that that article was specifically the answer to a `copy into` materialization, but it _is_ an example of a user's custom materialization (which the author seemed to be looking for) :D 

Thanks for sharying the mamykin article--good stuff.",5
967,"custom-materialization-to-load-from-external-table-into-snowflake","Another neat way of using DBT to load data into Snowflake is by using External Tables.  DBT has supported Snowflake's external tables since v0.14.1.  If you define your datasets in your buckets as external tables and you have a sensible folder structure that allows you to define a partition key then you have all the ingredients to use the simple `insert` statements to load from external tables to Snowflake tables.

We do this at Canva and we made a custom materialisation called `insert` because we did not want the overhead of two writes that the `incremental` materialisation performs (for a good reason).

So at the end our compiled dbt SQL looks like

Pre Hook -> refresh external tables

insert into target select * from external table where { filter criteria }",6
967,"custom-materialization-to-load-from-external-table-into-snowflake","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
1008,"how-to-prevent-accidental-full-refreshes-for-a-model","A question that pops up once in a while on Slack is how to prevent full refreshes of incremental models, often because rebuilding the table from scratch would take many hours, or even multiple days.

Here's two ways to approach this:
1. Completely disable the ability to fully refresh a model
2. Add a protection layer to prevent accidental refreshes

**Approach 1**
Let's say you absolutely, positively want to prevent ANY full refresh for a model. You could add the following code at the top of your model:

```
{% if execute %}
  {% if flags.FULL_REFRESH %}
      {{ exceptions.raise_compiler_error(""Full refresh is not allowed for this model. Exclude it from the run via the argument \""--exclude model_name\""."") }}
  {% endif %}
{% endif %}
```

This can be read as: When executing dbt, if the --full-refresh argument is used, raise a compiler error and prevent the model from being executed.

Doing a `dbt run --full-refresh` will fail. If you have multiple incremental models and you are ok with the other ones being refresh, you would need to use model selection syntax to exclude the models that are disallowed to be fully refreshed.

**Approach 2**
A more flexible approach is to allow full refreshes but only if a specific variable with a specific value is present. This hopefully ensures that the action is explicit, and not the result of an accidental/careless `dbt run --full-refresh`.

We can slightly modify the code presented earlier to add an extra condition:

```
{% if execute %}
  {% if flags.FULL_REFRESH and var('allow_full_refresh', False) != True %}
      {{ exceptions.raise_compiler_error(""Full refresh is not allowed for this model unless the argument \""--vars 'allow_full_refresh: True'\"" is included in the dbt run command."") }}
  {% endif %}
{% endif %}
```

Almost the same code, but we're adding an `and` condition to check the variable `allow_full_refresh`, which has a default value of False if the variable doesn't exist when dbt is executed.

To fully refresh this model, one would simply run the command `dbt run --full-refresh --vars 'allow_full_refresh: True'`.

---

This is only an example. Instead of checking a variable, you could test on which environment the full refresh is executed through `target.name` to allow on prod and disable on dev, for example, or the role used on Snowflake via `target.role`.

Relevant documentation:
https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-variables/
https://docs.getdbt.com/docs/writing-code-in-dbt/getting-started-with-jinja/
https://docs.getdbt.com/docs/writing-code-in-dbt/jinja-context/flags
https://docs.getdbt.com/docs/writing-code-in-dbt/jinja-context/execute/
https://docs.getdbt.com/docs/writing-code-in-dbt/jinja-context/target

Don't hesite to ask on Slack in #modeling if you want to take a new approach for this and you're unsure whether its possible and how to do it. We'll be happy to help!",1
1008,"how-to-prevent-accidental-full-refreshes-for-a-model","Update: DBT 0.18 has now a mechanism to block full refreshes within the model config, making this workaround uncessary.

Check out the docs:
https://docs.getdbt.com/reference/resource-configs/full_refresh/",2
1008,"how-to-prevent-accidental-full-refreshes-for-a-model","In a Github deployed project the   `full_refresh=false`  flag can add drag to developing new features. The toggle must be removed in one merge, then added back in the next. 

Here's an alternative to  `full_refresh=false`  that can be overridden with a CI variable.
```
{{
  config(
    materialized = 'incremental', 
    unique_key = 'unique_key',
    full_refresh=true if flags.FULL_REFRESH and var('full_refresh_force', false) else false
  )
}}
```

`dbt run -s my_model`
-> incremental
`dbt run -s my_model --full-refresh`
-> incremental
`dbt run -s my_model --full-refresh --vars '{""full_refresh_force"": true}'`
-> full refresh",3
1019,"migrating-tableau-workbooks-from-redshift-to-snowflake","For various reasons, my organization has decided to migrate our warehouse platform from Redshift to Snowflake.  I am so glad we are currently using dbt because we'll be able to build cross-platform SQL that we can run simultaneously on both platforms.  This will allow us to ensure that the new data pipelines are sound before we fully transition to Snowflake.

The reporting situation is a little tricker though.  We use Tableau for most of our BI reporting.  Some initial trials of manually migrating Tableau workbooks indicated that this would be an exceptionally slow, manual, and error prone process.  If we were to migrate all of our reports using only Tableau Desktop, I estimate we'd need to spend hundreds of hours of developer time to accomplish it, which is just not feasible.  Fortunately, Tableau workbooks are just XML documents.  With the help of some code provided by another dbt community member (thanks Claus Herther!) I was able to put together this Python script to help with the process.  Hopefully other members of the community can make use of this too.

[Public gist](https://gist.github.com/gnilrets/bc1e85aee26105013b08e038b19a42f7)

```python
'''
Tableau Workbook Redshift->Snowflake Migrator
=============================================
This is heavily based on previous work here https://github.com/calogica/tableau-redshift-snowflake-converter/blob/master/Tableau%20Redshift%20to%20Snowflake%20Migration%20Script.ipynb
This script converts Tableau packaged workbooks from Redshift to Snowflake.  It works by
parsing the workbook XML.  It will replace datasource connections so they point to your
Snowflake instance instead of Redshift.  Additionally, the script will UPCASE schemas,
tables, and any columns that don't have characters that need to be quoted.
Known limitations:
  - This will not work on converting published data sources.
  - Reports that use custom SQL may require additional manual configuration.
Usage:
  Ensure the following environment variables are set:
    SNOWFLAKE_ACCOUNT_NAME - Name of the snowflake account (not including the snowflakecomputing.com bit)
    SNOWFLAKE_USER_NAME - Snowflake user account name
    SNOWLFAKE_ROLE_NAME = Role to use for user (uppercase)
    SNOWFLAKE_DB_NAME - Name of Snowflake database (uppercase)
    SNOWFLAKE_WAREHOUSE - Snowflake warehouse (uppercase)
    SNOWFLAKE_SCHEMA - Snowflake schema (uppercase)
  Run the script and specify the path to the Tableau workbook, relative to this script:
    >>> python tableau_snowflake_migrator.py MyWorkbook.tbwx
'''

import sys
import io
import os
import zipfile
import shutil
import re
import logging
import xml.etree.ElementTree

ACCOUNT_NAME = os.getenv('SNOWFLAKE_ACCOUNT_NAME')
USER_NAME = os.getenv('SNOWFLAKE_USER_NAME')
DB_NAME = os.getenv('SNOWFLAKE_DB_NAME')
WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE')
SCHEMA = os.getenv('SNOWFLAKE_SCHEMA')
ROLE_NAME = os.getenv('SNOWFLAKE_ROLE_NAME')

logging.basicConfig()
LOG = logging.getLogger('migrator')
LOG.setLevel(logging.INFO)


def migrate_to_snowflake(packaged_workbook_path):
    unpacked_workbook = _unpack(packaged_workbook_path)
    workbook_xml = _migrate_xml(unpacked_workbook['workbook_path'])
    _save_migrated_workbook(workbook_xml, unpacked_workbook['workbook_path'])
    _repack(unpacked_workbook['extract_dir'], file_ext=unpacked_workbook['file_ext'])


def _unpack(packaged_workbook_path):
    file_ext = f'.{packaged_workbook_path.split(""."")[-1]}'
    packaged_workbook_path = os.path.abspath(packaged_workbook_path)
    extract_dir = packaged_workbook_path.replace(file_ext, '')

    if not os.path.exists(packaged_workbook_path):
        raise Exception(f'File {packaged_workbook_path} does not exist')

    with zipfile.ZipFile(packaged_workbook_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    LOG.info('Unzipped %s to %s', packaged_workbook_path, extract_dir)

    file_ext_wb = file_ext.strip('x')

    file_path_wb = packaged_workbook_path.replace(file_ext, file_ext_wb)
    file_path_wb = os.path.join(extract_dir, os.path.basename(file_path_wb))

    return {'extract_dir': extract_dir, 'workbook_path': file_path_wb, 'file_ext': file_ext}


def _repack(extract_dir, file_ext='.twbx'):
    zip_filename = f'{extract_dir}-Snowflake{file_ext}'
    LOG.info('Repackaging as %s', zip_filename)

    zipf = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)
    for root, dirs, files in os.walk(extract_dir):
        for file in files:
            full_path = os.path.join(root, file)
            rel_path = os.path.relpath(full_path, extract_dir)
            zipf.write(full_path, arcname=rel_path)


def _migrate_xml(workbook_path):
    tree, replace_vars = _xml_replacements(workbook_path)
    doc = _global_replacements(tree, replace_vars)
    return doc


def _xml_replacements(workbook_path):
    tree = xml.etree.ElementTree.parse(workbook_path)
    root = tree.getroot()
    replace_vars = {}

    for datasource in root.findall('.//datasource'):
        if not (datasource.get('hasconnection') or datasource.get('hasconnection') is None):
            continue

        for named_connection in datasource.findall('.//named-connection'):
            LOG.debug(
                'Replacing %s %s %s',
                named_connection.tag,
                named_connection.get('name'),
                named_connection.get('caption'),
            )

            named_connection.set('caption', f'{ACCOUNT_NAME}.snowflakecomputing.com')
            named_connection.set(
                'name', named_connection.get('name').replace('redshift', 'snowflake')
            )

            LOG.debug(
                '>> with %s %s %s',
                named_connection.tag,
                named_connection.get('name'),
                named_connection.get('caption'),
            )

            for connection in named_connection.getchildren():
                connection.set('class', 'snowflake')
                connection.set('schema', f'{SCHEMA}')
                connection.set('dbname', f'{DB_NAME}')
                connection.set('server', f'{ACCOUNT_NAME}.snowflakecomputing.com')
                connection.set('service', f'{ROLE_NAME}')
                connection.set('username', f'{USER_NAME}')
                connection.set('warehouse', f'{WAREHOUSE}')
                connection.set('port', '')

        for relation in datasource.iter('relation'):
            if relation.get('connection') is None:
                continue

            relation.set('connection', relation.get('connection').replace('redshift', 'snowflake'))
            relation.set('table', relation.get('table').upper())

            redshift_name = relation.get('name')
            snowflake_name = redshift_name.upper()
            replace_vars[redshift_name] = snowflake_name
            relation.set('name', snowflake_name)

        for metadata_record in datasource.findall('.//metadata-record'):
            if metadata_record.get('class') != 'column':
                continue
            for metadata in metadata_record.getchildren():
                if metadata.tag in ['remote-name', 'remote-alias']:
                    snowflake_value = metadata.text
                    has_quotables = re.search(r'[^a-z0-9_]', metadata.text) is not None
                    starts_with_num = re.search(r'^[0-9]', metadata.text) is not None
                    if not (has_quotables or starts_with_num):
                        snowflake_value = metadata.text.upper()
                        replace_vars[metadata.text] = snowflake_value

                    LOG.debug(
                        'Replacing %s %s with %s', metadata.tag, metadata.text, snowflake_value
                    )
                    metadata.text = snowflake_value

                if metadata.tag == 'parent-name':
                    metadata.text = metadata.text.upper()

    return (tree, replace_vars)


def _global_replacements(tree, replace_vars):
    with io.BytesIO() as bs:
        tree.write(bs)
        doc = bs.getvalue().decode()

    for redshift_value, snowflake_value in replace_vars.items():
        doc = doc.replace(f'[{redshift_value}]', f'[{snowflake_value}]')
    return doc


def _save_migrated_workbook(doc, file_path_wb):
    with open(file_path_wb, 'w') as xmlfile:
        xmlfile.write(doc)


if __name__ == '__main__':
    migrate_to_snowflake(sys.argv[1])
```",1
1024,"what-is-the-dbt-logo","I can't tell what the orange logo is supposed to be. I gotta know!",1
1024,"what-is-the-dbt-logo","Good question @trevor -- we don't know either! When we commissioned the logo, we asked our designer to come up with something abstractly mechanical looking. It's not really _supposed to_ look like anything, but we've heard some folks say it looks like:
 - A gear
 - A wrench
 - A starfish
 - A [bike crank spider](https://www.google.com/search?q=bike+crank+spider&rlz=1C5CHFA_enUS740US741&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjbt5aKgtToAhWDiOAKHV-3ClcQ_AUoAnoECAsQBA&biw=1403&bih=993&dpr=2)

What do you think it looks like? :)",3
1024,"what-is-the-dbt-logo","You forgot the most common: a fidget spinner",4
1024,"what-is-the-dbt-logo","Ah! Good to know. I've spent entirely too much time trying to figure it out. These were my best guesses:

Top view of a bobsled with wings
Jack (i.e. the game jacks with the rubber balls)
Top of an airplane
The top-view of a stick shift with a circular ball",5
1024,"what-is-the-dbt-logo","It's clearly a VESA compliant monitor mounting bracket. :slight_smile:",6
1024,"what-is-the-dbt-logo","> Itâ€™s clearly a VESA compliant monitor mounting bracket. :slight_smile:

When I first encountered dbt and saw the logo, that's exactly what came to mind! :sweat_smile:",7
1024,"what-is-the-dbt-logo","Fun thread! My first impression was some form of rotary engine, which Wikipedia characterizes as:

> A very efficient solution to the problems of power output, weight, and reliability

That mental image meshes well with my experience using dbt so far to handle our data. ðŸ˜",8
1024,"what-is-the-dbt-logo","First look definitely gave me fidget spinner vibes and I invented a backstory that someone at fishtown might have been obsessed with them. Also, I use my spinner when I wait for a run to complete.",9
1024,"what-is-the-dbt-logo","it looks like a Shuriken",10
1028,"how-would-you-validate-macro-input","example (prevent users from calling a macro with FALSE instead of False ): 
```
{% macro temp(bool1=False) %}
{% if bool1 not in [True, False] %} 
BLA BLA BREAK THE SQL
{% endif %}

[actual code.]
{% endmacro %}
```",1
1028,"how-would-you-validate-macro-input","@vish I think you should be able to use jinja exceptions here: https://docs.getdbt.com/docs/writing-code-in-dbt/jinja-context/exceptions/",2
1028,"how-would-you-validate-macro-input","thanks, 
is there any way to code a schema test to expect an error ? 
i.e. I want to pass a wrong value, and the tests should expect a failure ?",3
1028,"how-would-you-validate-macro-input","@vish Keep in mind the schema tests run against the database after your data is built, but you're hoping to catch a coding exception at compile-time when the macro is used to build the SQL.  I think @joellabes has the right approach: have the macro test its input and throw an exception if there's an out-of-bounds value.",4
1028,"how-would-you-validate-macro-input","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
1067,"dbt-bitemporality-and-snapshots","Bitemporality is the tracking, in a database, of two different aspects of temporal information associated with each tuple:

1. Valid Time: The time period during which a fact is true in the real world
2. Transaction Time: The time period during which a fact stored in the database was known.

I think we all understand why we need to track the notion of Valid Time; Valid Time is usually what is being tracked in a Type 2 SCD dimension. However, it can be helpful to also track Transaction Time to understand the difference or the lag between when a fact was or will be true in the real world, and when we became aware of it in the database. One application of this that I was recently exploring was trying to understand data latency from data being a) created/edited in upstream source systems to b) landing in our raw tables to c) being processed via snapshots and stored in a snapshot table.

I found that I couldn't really accurately differentiate between these steps because the details of how dbt processes timestamps and applies them seems to vary depending upon the snapshot strategy used. There doesn't seem to be a way with the ""timestamp/lastchanged"" strategy to actually answer the question ""When was this row written into the snapshot?"" because the `dbt_updated_at` column actually is set to the value provided in the lastchanged timestamp from the upstream source system, not the timestamp of being written into the snapshot.

So the challenge is that dbt snapshots, in their current incarnation, don't really differentiate between the these aspects of temporality. Or sometimes they are mixed depending on which snapshot approach is used. When using the `timestamp` strategy, which depends on a lastchanged timestamp coming from the source database, dbt uses the source timestamp in question to set the various dbt timestamps in the snapshot table (e.g. dbt_updated_at, dbt_valid_from, and dbt_valid_to). However, when using the check strategy, dbt instead uses the database time at which the snapshot transaction was applied or changed.

In the former case, whether the lastchanged timestamp is reflective of the Valid Time or the Transaction Time depends on the notion of what the source system is doing. It's more likely to be the Transaction Time but may not be. In the latter case of the check strategy, it's likely going to be the Transaction Time. This gets even more confusing when loading historical data all at once into a snapshot. It's not really clearly enunciated, however, and recently when I switched some snapshots from the ""check"" strategy to the ""lastchanged"" strategy (as a thankful result of us finding how to get lastchanged timestamps from an upstream SaaS service which is providing us data) the pattern of how the timestamps were written changed and I could see dbt switching from using the (BigQuery in this case) database `CURRENT_TIMESTAMP()` to using the values from the source system timestamp.

There is probably yet another time dimension to this in that the ""Transaction Time"" as per the source system we are getting the data from is probably different from the ""Transaction Time"" as per the actual database platform that we are loading the raw data into and doing processing with (like Snowflake or BigQuery). So perhaps part of my issue is also further differentiating between these two Transaction Times.

Many databases such as SQL Server, Oracle, Db2, Teradata, and others support bitemporal tables. Snowflake, BigQuery, and Redshift don't appear to support it but I'm not making an argument here for actually using database-engine supported temporality with dbt, but rather giving examples to show that the concept is well established and available in commercial software. References here:

1. https://en.wikipedia.org/wiki/Temporal_database
2. https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables
3. https://www.ibm.com/support/knowledgecenter/en/SSEPGG_10.1.0/com.ibm.db2.luw.admin.dbobj.doc/doc/c0058481.html
4. https://dbanotes.com/oracle-12c-temporal-database-f1fbdebffdf
5. https://docs.teradata.com/reader/_LRlKl9_m2VqMOOEPhMinA/lebc4XM7guEZ~dGnbOIVBg

Also, aspects of bitemporality have been integrated into various ANSI SQL standards. I would be interested in having the notion of bitemporality more clearly explained and managed in dbt snapshots to help better with managing these aspects of time. Would others be interested in the same thing?",1
1067,"dbt-bitemporality-and-snapshots","I know this doesn't address your larger questions around bitemporal modeling, but I've recently started adding my own `CURRENT_TIMESTAMP()::TIMESTAMP AS dbt_snapshot_at` column to all of my snapshots to track when the dbt snapshot was taken.",2
1067,"dbt-bitemporality-and-snapshots","Really good post Josh. It is absolutely true that the whole SQL ecosystem does a really bad job at properly managing the temporal aspects of data.

The reasons for this are many. The concept of transactions really messes up natural concepts of time - so that does not help. Just look at e.g. `statement_timestamp()`,  `transaction_timestamp()`, `clock_timestamp()` from Postgres,  and while doing that, note that they don't offer a `commit_timestamp()` because that is ""tricky"", although Spanner manages it with [`pending_commit_timestampp()`](https://cloud.google.com/spanner/docs/reference/standard-sql/functions-and-operators#pending_commit_timestamp) , albeit not as a value readable within a transaction of course. So, that means we have 4 DBMS timestamps before we start even looking at application generated timestamps. Typically these will be set close to, but rarely  identical to, one of the DBMS timestamps, and so can also be considered as a strand of ""transaction/recorded time"".

Other reasons include temporal being a bit of a backwater in database research. Darwen had to badger Date for many years to get him to take the topic seriously, and even then they focused on the consequences of interval (aka range) values/types to the relational model more than the arrow of time itself. Not does their ambivalence to transactions help (see page 447 of the 2nd edition of  [Time and Relational Theory](https://www.oreilly.com/library/view/time-and-relational/9780128006313/), where they try and fail to answer their own question on ""how transaction timestamps might be implemented in a real system)""). Other, more mainstream(?) academic work such as [Snodgrass](https://en.wikipedia.org/wiki/Temporal_database#History) notwithstanding, this whole area is under researched IMHO.

And I would say that most the work on Temporal Tables in SQL DBMSes has been focused on OLTP use cases, rather than analytical ones such as ETL populated ""type-2"" dimensions. Maybe this is not such the case with Teradata's implementation - but that not something I have worked with myself.

Now, Josh, you said at the start that ""Valid Time is usually what is being tracked in a Type 2 SCD dimension"". I would disagree. Almost always those tables hold one kind of transaction/recorded time. I.e. what did the customer record (that we extracted from the source) look like yesterday, a week ago etc.   Very rarely are business time columns used. E.g. if a customer says ""I moved city last month"" or ""I'm moving city next month"", the SCD won't be back or forward dated to reflect the ""true in the real world"" timeline.  We don't want our historic reports chaining just because a customer ""forgot"" to tell us something in due time.

Actually, the ""bi-temporal"" (end [""tri-temporal""](https://en.wikipedia.org/wiki/Temporal_database#Tri-temporal)!) stuff is all an unimportant distraction. Anything other than the recorded/transaction timeline is just normal columns. Recording when a customer says something happened in the real world, or when a fraud investigator says something actually happened are all just normal business data items. Trying to make them special under the ""bi-temporal"" (or should that be n-temporal) banner obscures the more important matter of the serializable nature of data recording.

So what to do? I'm sure dbt could pay more attention to this. Even simple things such as recording `dbt_snapshot_at` as @gnilrets suggests would help. Leveraging Time Travel facilitates (where available/performant) to ensure dbt knows what the snapshot point is in the local warehouse timeline, would also help. Understanding what any timestamps coming from the source systems actually represent would be better still (e.g. CDC'ed sources can sometimes include a true(ish) source commit timestamp). Still I'm not sure dbt really works at a high enough level of abstraction to incorporate such matters.",3
1070,"using-dynamic-schemas-for-snapshots","Unlike  models, snapshots build into the same `target_schema`, no matter who is running them. This helps ensure that whenever you `ref` a snapshot, you're using the production version of the snapshot, so that your dev transformations reflect reailty.

However, this can cause problems if you want to test the `snapshot` command in `dev` or even during a `CI` run, since the database user you run this command with in dev/CI may not have the correct privileges to mutate the snapshot table.

If this is causing headaches for you, there is a workaround! Here's how I recently did it:

### 1. Use the `generate_schema_name` macro to make the `target_schema` environment-aware
The `generate_schema_name` macro is how dbt constructs an environment-aware schema name for a model ([docs](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas)), and we can leverage it here to do the same thing.

Normally, I'd configure this directly from the `dbt_project.yml` file for all my snapshots to make them behave consistently. But for reasons that I'd prefer to not get into (read: I don't fully understand) we have to do this directly from the snapshot itself

`snapshots/orders_snapshot.sql`:
```jinja2
{% snapshot orders_snapshot %}

{{
    config(
      target_database='analytics',
      target_schema=generate_schema_name('snapshots'),
      unique_key='id',

      strategy='timestamp',
      updated_at='order_date',
    )
}}

select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

```


If you're using [custom databases](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-database/),  it might also  be a good idea to do the same for your `target_database` configurations (i.e. `target_database=generate_database_name('analytics')`)

Now when you `$ dbt run`, the snapshot will be built in a different schema depending on who is running it.
For my setup:
```sql
-- in prod:
analytics.snapshots.orders_snapshot

-- in dev:
analytics.dbt_claire_snapshots.orders_snapshot
```

### 2. Don't `ref` your snapshots
The docs say you should select from snapshots using `ref`:

```sql
select * from {{ ref('orders_snapshot') }}
```

But, if you've followed step one, you really shouldn't use `ref`. This is because we want to make sure we are always selecting from the production version of the snapshot, not your `dev` or `ci` version. If we use `ref`, that won't be the case.

So, what's an analytics engineer to do?

#### Option 1: Use sources
Some folks have worked around this by choosing to define their snapshots as [sources](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources/), and instead using the `{{ source(...) }}` function to select from their snapshot. That's a reasonable approach, but the one downside is that you'll end up with two nodes representing the same thing in the docs â€”one for the `snapshot`, and one for the `source` that represents the `snapshot`. Not too bad of a trade-off in the schema of things!

![dbt-dag (1)|690x240](upload://uLrelqF4K5Gh8SdVWb8bwDMgWmw.png) 

#### Option 2: Create a macro to `ref` for you

What if, instead of `{{ ref('orders_snapshot') }}`, we could do something like:
```
select * from {{ snapshot('orders_snapshot') }}
```
And have it always compile to the production version of the snapshot, while maintaining the edge in the DAG between the snapshot and the model?

We can! (Quelle surprise!)

Here's a macro that I recently used:

`macros/snapshot.sql`
```jinja2
{% macro snapshot(snapshot_name) %}

{% set snapshot_relation=ref(snapshot_name) %}

{# Make sure this is the actual name of your target in prod #}
{% if target.name != 'prod' %}

{# Change this part based on how you name things in prod #}
{% set prod_snapshot_relation = adapter.get_relation(
      database=snapshot_relation.database,
      schema='snapshots',
      identifier=snapshot_relation.identifier
) %}

{% endif %}

{#
Use an or operator to handle:
1. the case where the prod version has not yet been created
2. the case where we are in prod
#}

{{ return(prod_snapshot_relation or snapshot_relation) }}

{% endmacro %}

```
Et voilÃ ! In both prod and dev, my model will select from:
```
select * from analytics.analytics.orders_snapshot
```
And my documentation looks right! 
![dbt-dag (2)|690x240](upload://7sffKgP9FRbhsrV8nmaHTVqKat7.png) 

Here's a Loom video where I run through this (it's a little long, but I go into much greater depth):

https://www.loom.com/share/98940b99e1b446ab92594ebd88195c78",1
1070,"using-dynamic-schemas-for-snapshots","Hi Claire, thanks for the workaround on this case.
I'm wondering if I can use `tag` in the snapshot config and then exclude the tag (`--exclude tag:snapshot`) while running the `dbt snapshot` on production. Can I do that as workaround?",2
1070,"using-dynamic-schemas-for-snapshots","Hi @teguhn â€” the best way to find the answer to these kinds of questions is to try it out yourself! Let me know if it works ;)",3
1070,"using-dynamic-schemas-for-snapshots","Hi @claire ! Thanks for sharing this and going into so much detail :raised_hands: 

I found this while looking for some ideas to improve how we are operating (in Snowflake) - which is trying to clone the prod snapshot into test schemas each run to keep the data fresh without potentially impacting any of the prod snapshots. Do you have any insight into if that is a valid tactic? Or is there no real risk in having the production snapshot used across test and ci schemas instead of a clone?

Thank you for any insight!",4
1070,"using-dynamic-schemas-for-snapshots","I just wanted to put this note here for anyone that utilizes this technique:

If you are attempting to run SlimCI, as of writing, the generate_database_name and generate_schema_name **within** the snapshot configuration will mark these files as state: modified.

Environment awareness logic for snapshots (and perhaps other models, though I've not tested that) must live within dbt_project.yml in order to not trigger files as modified when comparing across targets (i.e. target == prod and target == ci)

While I did utilize the snapshots macro listed here to have dev point to prod snapshots, this did not solve the issue I was hoping to solve (slimCI running all snapshots in their own temporary schema, which then runs ever child model if using state:modified+)

The magic sauce is explained here: https://docs.getdbt.com/reference/node-selection/state-comparison-caveats#false-positives",5
1070,"using-dynamic-schemas-for-snapshots","Proposing another workaround (when using Snowflake):

I wanted to avoid snapshots being deployed into default_schema (`target.schema`), which in my case would be `MRT_schema` in deployment and `USER_schema` in development. Instead, I wanted it to be deployed into `INT_schema`, but still be built in `USER_schema` in development. 

And without Jinja being nessecary (!) this was done as following: 

**SETUP**
1. First make two different folders inside snapshot, to group snapshots whether they were to be publicly available in the organization (hence `deployed`), or still only in development (hence `dev_only`). 
![image|315x93](upload://rYz7JEVp5ehpQGLO1Cp2EgAxr9m.png)

2. Go into `dbt_project.yml` and edit the code to the following: 
```
snapshots:
  <project_name>:
    deployed: 
      +target_schema: INT_schema
    dev_only:
      +target_schema: target.schema  # user schema 
```

**USE**:
- When developing the snapshot and wanting it to be for developing uses only, leave the `snapshot.sql` in `dev_only`.
- When wanting to publish the dataset, so others in the organization may use it - but still in a custom schema, move the file into `deployed`.

This gives both the flexibility and control during both dev and production.",6
1077,"testing-case-sensitive-column-names","Our users love case sensitive column names like CustomerID.
When I try to put a test (unique or not null) on this column, the db does not recognize the column.
I used ""CustomerID"" and 'CustomerID' but when the statement is sent over to the db the "" and ' are removed and are thus not existing columns.
Is there a workaround for this other than not using case sensitive columns?",1
1077,"testing-case-sensitive-column-names","Hey @marc.clijsters - check out the docs for [column-level quoting](https://docs.getdbt.com/docs/building-a-dbt-project/testing-and-documentation/schemayml-files#quote) in schema.yml files. This feature is new in dbt v0.16.0 :)",2
1077,"testing-case-sensitive-column-names","Thanks @drew !

I am having the same issues but it appears the document seems to have moved?  I get redirected to a more generic doc at:
 https://docs.getdbt.com/reference/declaring-properties/#quote

instead of the original link that went to:
/docs/building-a-dbt-project/testing-and-documentation/schemayml-files#quote

Is there an updated URL for that document?

Thanks!

-Stirling",3
1077,"testing-case-sensitive-column-names","Found it:
https://docs.getdbt.com/reference/resource-properties/quote/#add-tests-to-a-quoted-column-in-a-source-table",4
1077,"testing-case-sensitive-column-names","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
1096,"what-are-the-best-practices-to-partition-big-tables-in-redshift","In Redshift we often need to split a huge event table into different smaller table. The objectives are to increase performance and facilitate end users analytics queries. Let's also assume each event row belongs to an event type and that we have time series data.

What are the best practices regarding such an endeavor and the common pitfall to avoid?",1
1096,"what-are-the-best-practices-to-partition-big-tables-in-redshift","Hi Guillaume!

Let's call this idea ""soft-partitioning"" - this would be when instead of keeping all rows for an entity in one table, you split the table into ""partitions"" and create a view to union them together so that they can be queried as a single entity. In some databases, partitioning is supported natively, but in Redshift that is not the case. 

In the example you've provided, let's start by imagining that all event types have the same set of columns, and any extra data has been normalized into other tables, and you're okay with keeping those per-event fields outside of the soft-partitioned table.

In that case, we would have a set of tables (let's call them `a,b,...,z`) containing `m` columns, which can be accessed like:

```sql
SELECT shared_col_1, shared_col_2, ... shared_col_m
FROM a
```

The main question we need to answer in this case is ""how do we partition"", aka what do a, b, c, etc represent?

Well, this really depends on your data-generating-process, and your usage pattern. Unfortunately, making an informed table optimization decision **also** requires going down some pretty deep rabbit holes about Redshift, so let's dive right in.


### Interlude: Redshift Architecture
At the file-system level, Redshift logically subdivides tables into columns, and columns are logically divided into sorted and unsorted regions, and these column-regions are divided along a dist-key and distributed among the [slices](https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html) of the cluster and split up into 1MB `blocks` of compressed data. When redshift performs a read, it checks the min/max values _per block_, and determines which block(s) can be skipped.  

Not only that, these blocks are immutable. if a single value in a block changes, the whole block is invalidated. so, if a `last_name` column is updated for a person who changed their name, the 1MB block containing that `last_name` value is marked as invalid (""tombstoned"" in Redshift parlance) and a new block is created, and this block-replacement happens, implicitly, within a transaction. 

Not only that, if the data was sorted before, it's possible that the update caused the block to become unsorted (such as when updating a sortkey column). Now your table needs to be vacuumed and analyzed again. If it's a big table, then you may be facing an unfortunate trade off between read performance for that table on one hand and table-write-availability and cluster performance on the other. Vacuums are expensive, and they prevent writes on the target table while the vacuum is writing. 

Similar considerations should be taken for inserts - you choose the ""order"" of your data, and if possible, you want to insert ""in order"" as often as possible to avoid data being written to the blocks in the unsorted region - the less sorted your data is, the higher the percentage of data that needs to be scanned to complete your query. Again, on a big table, we care about that!

As a final topic in this Redshift architecture deep dive, let's consider how redshift deals with views of unioned tables. 

it is the case that when you have a view defined like

```sql
CREATE VIEW breakfast_events AS

SELECT 'make coffee' AS event_type, coffee_events.* FROM coffee_events

UNION ALL

SELECT 'make eggs' AS event_type, eggs_events.* FROM eggs_events

UNION ALL 

... etc
```

Then Redshift's **query planner** ""knows"" that if you select from that view `WHERE event_type = 'make eggs'` that it does not actually need to execute the view as written -- it doesn't even need to look at block statistics to know, with total certainty, that `'make eggs'` events come from the UNION'ed subqueries for which the event_type was hardcoded to `'make eggs'`. If we did _not_ hardcode this value (such as if event_type were pulled from the underlying table), then redshift would need to depend on its statistics or possibly scan all data to make that determination. 

So, when we hardcode the ""partition"" value into the subqueries for this view, we give redshift's planner perfect information to act on. 


## Putting it together
So, with that information, we can combine our knowledge of Redshift with our understanding of the problem domain.

### Date-based partitioning: A Classic
Traditionally, data is partitioned into sets of ""hot"" and ""cold"" tables. In most circumstances, this means monthly partitions, because most arriving events will be from the last month or two, and anything else is a freak occurrence or is a planned backfill. It's a solid strategy for keeping your data in sort order. In particular, even if you have `event_type` in your data, one could argue that it's rare to look at just one event over a very long period of time - usually you look at a `session` and all events associated with it. For most systems, `session`s last span less than 24 hours more than 99% of the time. Therefore, partitioning your data by month makes a decent amount of sense!

But if the data is really large, you might not actually have enough granularity for ""highly selective"" scans -- depending on the sort order of the blocks within a partition, you might not skip any blocks at all, and the scan could still be slow, because for any growing product, this month and last month will have many more events than most prior months, especially if growth is exponential.  So let's say in order to ensure selectivity, we make the first sortkey column of the table `event_time`. 

But wait, what if we want to filter on a specific event_type? We're going to have to do a full scan of all partitions within the specified date range. if you are _looking_ for a certain event, you may not know the exact time, and could be stuck doing full scans of multiple partitions. 

You could use event_type as the first sortkey column, but the you may lose selectivity on event-agnostic queries. Does that matter? _It depends._ if you have enormous data, such that each event contains many blocks, then making event_time a secondary sortkey could still provide decent selectivity. But that would likely have to be on the order of 10s of millions of events per event type to see appreciable benefit. If you've got that, then try it!


## categorical partitioning - worth considering
If instead, you partitioned by `event_type`, you can then sort your data by event_time without any guilt. on any time-selective query (`event_time between x and y`), you'll get good block selectivity as long as the event_type has a lot of data (and if an event type doesn't have a lot of data, frankly, you don't really have a scale challenge for that event type, so it is moot - if that event type grows, then the date range per block will tighten, and selectivity will improve). 

Furthermore, if an event arrives late, it goes into a dedicated unsorted region _for that event_ -- you're less likely to receive a similar benefit in date based partitioning, especially if the sortkey is event type --- in that case, basically all data will arrive out of sort order, and so every partition will need to be frequently vacuumed. You'd have to choose between offering good event_type selectivity on one hand, and frequently vacuuming on the other. No such trade-off exists for categorical partitioning with a time based sortkey.

In other words, more needs to go right for classic date based partitioning to provide more benefits.


## Extending the problem: varying data structure across categories

So far, we assumed that all events have the exact same fields. But that often is not true. What if, as before, each table has `m` shared columns, but also some varying number of event specific extra columns? let's express those using a notation like `extra_col_1, ... extra_col_n_<table>` such that table `a` has `n_a` extra columns, and table z has `n_z` extra columns. 

Another challenge is that those extra columns don't represent the same things. In snowflake or bigquery the best approach is to pack those into a variant (semistructured) column for the columns that don't match and call it a day. 

In redshift you might get away with json strings if the number of extra attributes is small and sparse, but it just isn't the redshift way. 

Nor would it be great to just add every extra column to each table -- you'd have `m + n_a + n_b + ... + n_z` columns, which could get large quickly -- remember, each column is allocated at least one block per slice, and on a cluster with hundreds of slices, you could easily allocate tens or hundreds of gigabytes to completely empty columns. 

*But storage is cheap on RA3 nodes* - that's true, but there is a performance cost -- those blocks still need to be allocated and written to disk, even if they're empty. adding tens or hundreds of sparse columns can significantly impact write performance. For that reason, it's worth considering another option.

### ""virtual"" columns
Remember how the word ""virtual"" was cool in the 90's? I have news for you, virtual is _still_ cool. Since we're going to wrap the union-of-tables in a view, the tables don't **actually** need to each have all the same columns -- you can hardcode them as null in the view definition for the tables which don't have the particular columns in question.

#### Example 
Let's continue with the notation from earlier: for events `a,b,c,...,z`, there would be corresponding tables `a,b,c,...z`, and each table can be said to have `m` shared columns and `n_<table>` extra columns (eg `n_a, n_b,...`). 

So, they'd be `UNION`'ed like:
```sql
SELECT 
â€‹
	a.shared_col_1, a.shared_col_2,...a.shared_col_m,
â€‹
	a.a_extra_col_1, a_extra_col_2, ... a_extra_col_n_a, 
â€‹
	NULL AS b_extra_col_1, NULL AS b_extra_col_2,..., z_extra_col_n_z
FROM a
â€‹
UNION ALL 
â€‹
SELECT 
	b.shared_col_1, b.shared_col_2, ..., b.shared_col_m,
â€‹
	NULL AS a_extra_col_1, NULL AS a_extra_col_2, ... a_extra_col_a_z,
â€‹
	b.extra_col_1, b.extra_col_2, ..., b.extra_col_n_b,
â€‹
	NULL AS c_extra_col_1, NULL AS c_extra_col_2, ..., NULL AS z_extra_col_n_z
FROM b
â€‹
UNION ALL
â€‹
... /* union tables c through x */
â€‹
UNION ALL
â€‹
SELECT z.shared_col_1, z.shared_col_2, ..., z.shared_col_m,
â€‹
NULL AS a_extra_col_1, a_extra_col_2, ... x_extra_col_n_x,
â€‹
z.z_extra_col_1, z.z_extra_col_2, ..., z.z_extra_col_n_z
FROM z
```

^That's how I'd do it. There are still some open questions, which I'll briefly touch on.

## That seems like a pain to implement..
yes. You'll definitely want a macro for this. I've said it before that in DBT, union macros are easy, but I'd find myself swallowing my words in implementing this. It's not extremely complicated, but it's not trivial.

What will you need above and beyond the standard loop over the table list? 

the `adapter` object has methods to get all the columns of a table. if you do this for all tables, you can:  
* convert the lists into sets, and the intersect all of them to get the set of shared columns. 
* take the set of shared columns and perform a set difference from each of the column sets to get the sets of extra columns per event type
* use `adapter` to get the data types of the extra columns -- this is (or at least once was) necessary because redshift NULL values are varchar by default -- the hardcoded nulls (eg `NULL AS a_extra_col_1`) you need to cast them to the correct type, so you'll need to pull all the data types and match them to the generated lists of extra columns
* do some complicated looping to insert the non-null extra columns in the right place (for tables b to x, the extra columns will sit somewhere in the middle of the column list) 


## What about distkeys?
well, that's kind of a different question! I'll always answer *""what should my distkey be""* with *""what are you joining to?""*.  Usually, events are joined to users. At this point, I'd recommend checking the skew of the proposed distkey. That's a tutorial in and of itself so I'll leave it as an exercise to the reader. But if you aren't going to be consistently joining to one table, or if the skew of the join column is highly skewed, use DISTSTYLE EVEN and call it a day!


...

Whew okay that's my answer.  Happy Trails!


P.S.

Dear Reader:
If you're a redshift engineer reading this, and you would like to issue a correction, please by all means do so!",2
1096,"what-are-the-best-practices-to-partition-big-tables-in-redshift","(Also note: this reply is a rephrase of a conversation we had in DBT slack, sorry for the length!)",3
1110,"using-dbt-snowflake-shares","# How Drizly is improving collaboration with external partners using dbt & Snowflake shares

Drizly is the leader in alcohol e-commerce. We partner with local retailers so you can shop the large selection of beer, wine and liquor across stores, right from your phone, and get it delivered in under 60 minutes. Along with providing this service, we strategically partner with alcohol brands and suppliers. These include companies like ABI, who own brands such as Bud Light, Budweiser, and Michelob, or Diageo, who owns many liquor brands such as Casamigos, Bulleit, and Smirnoff. These partners are interested in Drizly's unique data set for many reasons. It can be used to segment marketing campaigns, fix inventory gaps, and just create a better overall customer experience.

We used to share this data via adhoc SFTP jobs. This wasn't scalable (each new partner required a lot of work), and was tedious to get right, often resulting in chain emails with 50+ messages to get the details right.

Since migrating to Snowflake, we started leveraging their [Data Sharing](https://docs.snowflake.com/en/user-guide/data-sharing-intro.html) features to share data with our suppliers securely, and in a scalable way.

I recently spoke about this on an Office Hours ([slides](https://docs.google.com/presentation/d/1IrtPg9eBHw68gn-O3-wqvbYygGgy8ZjBfdeEOGT7Yn0/edit#slide=id.g745995065c_0_66)):

[https://youtu.be/SJ-nuPkri-A](https://youtu.be/SJ-nuPkri-A)

This has removed an enormous headache for us while creating a ton of value for our partners. Our approach will be useful for just about any business that works with external partners:

- SaaS companies who want to provide data to their customers (e.g. an HR software company that wants to enable customers to query their own data)
- Marketplaces / platforms that want to share data with their suppliers, who often can't get this data themselves
- Organizations that have strategic partnerships and want to share data securely

# Creating a share manually

The below SQL gives you an idea of how to create a share manually.

```sql
create or replace share sales_s;

alter share sales_s add accounts=xy12345;

grant usage on database analytics to share sales_s;

create schema if not exists sharing;

create secure view analytics.sharing.sales as (
  select ...
);

grant usage on schema analytics.sharing to share sales_s;

grant select on view analytics.sharing.sales to share sales_s;
```

There's a few things to note about shares:

- Only the `accountadmin` role can run a `create share` command
- When sharing a view, it must be a `secure` view

And for our use case, we had a few extra considerations:

- We only want to share prod data, not dev ones
- Suppliers must *only* be able to read their own data

But manually creating shares was not enough. Instead, we chose to use dbt for this process because:

- By version controlling our code, we implement extra security around what data gets shared and with which account
- By using dbt hooks to run this code, this process becomes scalable â€” the effort to add one (or a dozen!) new suppliers is minimal.

So, on to dbt!

# Using dbt to automate this process

## Step 1: Create reader accounts

Create a [reader account](https://docs.snowflake.com/en/user-guide/data-sharing-intro.html) for a supplier so they can login to Snowflake. We use terraform to automate and version control this process.

## Step 2: Map internal supplier IDs to Snowflake accounts IDs

Use a [seed](https://docs.getdbt.com/docs/building-a-dbt-project/seeds/) for this. This means the list of accounts are version controlled, reducing the risk of any mistakes happening.

`seeds/supplier_share_access_rules.csv`

```csv
company_id,account_id
4,FKA85312
5,UVZ23642
6,RPX31467
```


## Step 3: Create the share

We use a [pre-hook](https://docs.getdbt.com/reference/resource-configs/pre-hook) to do this. To make things tidier (and so that we could re-use it across multiple  models),  we bundled the code into a macro.

```sql
models:
  sharing:
    pre-hook:
      - ""{{ create_share('drizlys_share', ['FKA85312', 'UVZ23642', 'RPX31467'] }}""
```

Our macro also includes logic to make sure this only runs in prod (leveraging the [target variable](https://docs.getdbt.com/docs/writing-code-in-dbt/jinja-context/target/)). And the SQL uses the `if not exists` syntax to avoid errors.

```sql
-- macros/create_share.sql
{% macro create_share(share_name, accounts) %}
  {% if target.name == ""prod"" %}

    {% set sql %}
      CREATE SHARE IF NOT EXISTS {{ share_name }};
      GRANT USAGE ON DATABASE {{ target.database }} TO SHARE {{ share_name }};

      {% for account in accounts %}
        ALTER SHARE {{ share_name }} ADD ACCOUNTS = {{ account }};
      {% endfor %}

    {% endset %}

    {% set table = run_query(sql) %}

  {% endif %}

{% endmacro %}

```

## Step 4: Create the view to be shared

We only want suppliers to be able to see their own data. Rather than create a model for each supplier that we share separately, we use Snowflake's [current_account()](https://docs.snowflake.com/en/sql-reference/functions/current_account.html) function to create a filter, joining back to our seed file to make sure we are filtering correctly.

```sql
SELECT *
FROM {{ ref('store_order_items_supplier_vw') }}
WHERE company_id IN (SELECT company_id
                      FROM {{ ref('supplier_share_access_rules') }}
                      WHERE UPPER(account_id) = CURRENT_ACCOUNT())
```

## Step 5: Share the view

Use a [post-hook](https://docs.getdbt.com/reference/resource-configs/post-hook/) to grant the share. Again, we used a macro here.

```sql

{{ config(
  materialized='view'
  secure=true,
  post_hook=""{{ share_view ('SHARING', 'SHARED_STORE_ORDER_ITEMS_VW', 'drizly_share' }}""
) }}
```

Again, this only runs in prod!

```sql
{% macro share_view(view_schema, view_name, share_name) %}
  {% if target.name == ""prod"" %}

    {% set sql %}
      GRANT USAGE ON SCHEMA {{ target.database }}.{{ view_schema }} TO SHARE {{ share_name }};
      GRANT SELECT ON TABLE {{ target.database }}.{{ view_schema}}.{{ view_name }} TO SHARE {{ share_name }}
    {% endset %}

    {% set table = run_query(sql) %}

  {% endif %}

{% endmacro %}
```

## Step 6: Relax ðŸ˜Œ

And that's it! Next time you `dbt run` in prod, the shares will be successfully created!

Once this is setup, adding a new supplier only takes a few minutes â€” we just add them to our dbt project and terraform config, and everything gets created as expected.

## Future improvements

We're pretty happy with this process so far. From here, the only thing we are going to tweak is setting up [resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors.html) within our Terraform config to help control cost, since reader account credits are charged to our account. If you'd like to follow the issue thats open with terraform regarding resource monitors, you can find that [here](https://github.com/chanzuckerberg/terraform-provider-snowflake/issues/145).",1
1112,"zero-copy-clone-a-snowflake-schema-when-the-target-schema-uses-a-custom-schema","Hello all. I'm just a couple weeks into DBT (or is it dbt?) and I'm really very impressed.

I have a question for the group. 

At my company we run our CI/CD through on-prem Azure DevOps and I'm setting up the pipeline. We use Snowflake and I've noticed some people on this forum mentioning their use of zero-copy clones in their Dev or QA environments.

I'm also using custom schemas so that each user can create their own schemas in development and QA.

I'm not sure the best way to do a zero-copy clone of my production BI schema. And I'm not sure where to put the logic. I'd like to do something like this below, but can't quite get the syntax down and I'm not sure on-run-start is the appropriate place for it, and I'm not sure I can run generate_schema_name in the project.yml.

    on-run-start:
    - ""if target.name != 'prod' then create or replace schema {{ generate_schema_name() }} clone bi_db.production_bi_schema;""

Any help would be appreciated. I assume once I get the logic down I can put it into a macro, but I'm just looking for any help. Thanks so much! Great community.",1
1112,"zero-copy-clone-a-snowflake-schema-when-the-target-schema-uses-a-custom-schema","I figured out a way of doing it:

    on-run-start:
        - '{{ clone_schema(""bi_db.production_bi_schema"") if var(""reclone"",false) == true }}'

When I run dbt, from the command line, I provide reclone as a var, indicating whether I want to re-clone this schema.

The `clone_schema` macro looks like this:


    {% macro clone_schema(schema_to_clone) -%}

        {% set all_tables_query %}
            show tables in schema {{ schema_to_clone }}
        {% endset %}

        {% set results = run_query(all_tables_query) %}

        {{ ""create or replace schema "" ~ generate_schema_name(var(""custom_schema"")) ~ "";"" }}

        {% if execute %}
            {% for result_row in results %}
                {{ log(""create table "" ~ generate_schema_name(var(""custom_schema"")) ~ ""."" ~ result_row[1] ~ "" clone "" ~ schema_to_clone ~ ""."" ~ result_row[1] ~ "";"") }}
                {{ ""create table "" ~ generate_schema_name(var(""custom_schema"")) ~ ""."" ~ result_row[1] ~ "" clone "" ~ schema_to_clone ~ ""."" ~ result_row[1] ~ "";"" }}
            {% endfor %}
        {% endif %}
    {%- endmacro %}

You could also clone views, but I don't use views. Because we decide whether to clone at the command line through vars I can set it in my build pipeline appropriately.

I actually started out just cloning the entire schema in one statement. The problem is that the role doing the cloning becomes the owner of the schema, but not of any of the tables, which retain their original privileges. But if you clone each item one-by-one, then the role you're running under is the owner of all the objects.",2
1112,"zero-copy-clone-a-snowflake-schema-when-the-target-schema-uses-a-custom-schema","It sounds like you're unblocked @trevor but I wanted to offer a couple of additional resources for how we do this at GitLab. 

This is how we clone the database: https://gitlab.com/gitlab-data/analytics/-/blob/master/orchestration/manage_snowflake.py 

Then in our CI Process, we have relevant clone jobs: https://gitlab.com/gitlab-data/analytics/-/blob/master/.gitlab-ci.yml#L65

By default, the `clone_analytics` job runs on every Merge Request. This allows MRs to run on raw data on the analytics clone. 

The name of the clone comes from the name of the branch. If the branch is named `apple_banana`, the database name is `APPLE_BANANA`. Then instead of outputting to `analytics_database.analytics_schema.magical_table`, the MR outputs to `apple_banana.analytics_schema.magical_table`. 

Hope that is useful!",3
1125,"error-function-not-implemented-when-running-dbt-in-docker-with-python-flask","Hi Everyone,

Am trying to run my dbt code in a Docker container environment by creating a ""python flask"" application to pass the dbt execution commands as POST API parameters. When I try to do that. Image is getting built fine. And for a sample test when I pass dbt --version it is completing successfully. But if i execute any other dbt command like debug , test , run. Am getting Function not implemented error.

Below is the output sample:

{
    ""Command_Result"": ""Success"",
    Running with dbt=0.16.1
    dbt version: 0.16.1
    python version: 3.8.2
    python path: /usr/local/bin/python
    os info: Linux-4.4.0-x86_64-with-glibc2.2.5
    Using profiles.yml file at /dbt/profiles.yml
    Using dbt_project.yml file at /dbt/dbt_project.yml

  Configuration:  
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]

Required dependencies: - git [OK found]
Connection:
method: service-account
database: <database_name>
schema: <schema_name>
location: US
priority: interactive
timeout_seconds: 300
Encountered an error:
[Errno 38] Function not implemented

}

Please help!!

Thanks in advance!

Thanks,
Gowtham",1
1125,"error-function-not-implemented-when-running-dbt-in-docker-with-python-flask","There's a few things that can cause this, but that message came from docker - something dbt is doing is not available on your docker service. You can examine the log file dbt writes to `logs/dbt.log` and look for a stack trace to see what exactly it's upset about, but that looks like it's happening when dbt tries to connect to your data warehouse. Given the location of that error, maybe something about threading.Lock objects isn't supported?

Unfortunately, various hosted docker solutions (and various docker-providing Linux distros!) have their own silly and obscure bugs around returning ENOSYS (the underlying message here) in various places, so it's hard to say exactly what is going on or how you could fix it. A sampling from a quick search:

* If you're on Windows, it's possibly this bug: https://github.com/docker/for-win/issues/5955
* If you're on GCE, it could be this: https://stackoverflow.com/q/59938948/11262881
* If you're on Azure, this could be it: https://stackoverflow.com/q/51616058/11262881

And I know Ubuntu in the 14.04 era used to return ENOSYS all the time for filesystem reasons. I'm sure that's improved, but older Linux distributions/docker versions are more likely to ENOSYS.",2
1125,"error-function-not-implemented-when-running-dbt-in-docker-with-python-flask","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
1128,"data-modeling-approaches-in-modern-data-times","Hey folks! :wave: 

As discussed in the dbt Slack I am moving this to the discourse:
I have a very general question for you and it is something iâ€™ve been wondering a lot recently. 
According to classical business intelligence theory a Data Warehouse should include some kind of transformation/data modelling like normalization, star-/snowflake-/galaxy- and/or data vault schema. Often you hear a mixture of some of those system being applied, most commonly normalization or data vault and then e.g. a star schema for end user access to the data via the BI Toolâ€¦

However, I feel like this is something mostly huge corporations are doing as the need for such a transformation is much higher because of much bigger data / way more different data sources / a much bigger data analysis department and other reasons.

To me it seems as if a lot of â€œyoungerâ€ companies that donâ€™t have that much data / different sources / not as many data people are skipping these classical transformations or use it only partly (e.g. when giving access to colleagues transform the data from raw to star schema right away).
I am wondering nowâ€¦ is this something that is actually happening? Or are most of you actually following that â€œclassicalâ€ way? :thinking: 

How are you approaching that? Are you using these modelling techniques in your company? And if so how does your data stack look like (like how many data people, how many sources, small or big data - of course only if you are happy to share it :slightly_smiling_face: )

I would be very interested in getting an idea about how other companies are approaching this and how commonly people are following the standard way or deviate from it - looking forward to your experiences!

PS: I know that there is already a great discussion about Kimball modeling here: https://discourse.getdbt.com/t/is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse/225/6 . However I am more interested in how people are actually setting up their warehouses in their companies and what factors are influencing which decisions. Also I am not necessarily talking about Kimball only.

Thanks in advance :slight_smile:

Best,
Andre",1
1128,"data-modeling-approaches-in-modern-data-times","Re-posting my reply from the Slack:

Really good question. My thinking is that these schemas can be useful, but should only be used  *if they are useful to you*, not just because itâ€™s what people usually do.

For example, at Zearn, we use a snowflake-ish schema, but itâ€™s by necessity. In our prod db, we have several large event tables, and several of our user-level dimension tables have different ways of joining to other tables depending on what kind of user you are looking at. In Redshift, doing all those joins would be really slow.

Creating a snowflake schema with dbt allows us to combine all our event tables into one and take advantage of Redshiftâ€™s dist/sort keys to make joins to dimensions work well. So it works well for us. But if you donâ€™t get some kind of defined benefit from it (e.g. speed, ease of understanding the tables, etc.), it doesnâ€™t seem like a good idea.

For context, we have one data person (me) and only one data source we do this for (our production database). Our events tables are in the low billions of rows range, with some dimension tables in the millions of rows.",2
1128,"data-modeling-approaches-in-modern-data-times","At thriftbooks we kind of have our data in three layers:
1. Raw data. Atomic events and assorted data at various levels of tidiness (i.e. not super tidy for human consumption)
2. Normalized analytics tables from that random disorganized data - this allows the engineers to keep things organized, sanitized, named correctly, column orders right. Without duplicating data.
3. De-normalized tables created from the normalized tables to allow for easy human consumption without having to join. With the de-normalization comes some duplication. There is a small number of these tables - 10 or so - that have the ability to answer 99% of people's questions.

I don't know what the different methodologies are for data warehousing. I've heard of Kimball and others, but never really looked into them. We've found our process to be easy for our engineers to work with, easy for our customers to query, and efficient for our data engine (Snowflake at this time) to process.",3
1128,"data-modeling-approaches-in-modern-data-times","@trevor that's a neat approach. Do you have an example of what might be in each of the three layers? I am trying to wrap my head around whether/when something like that would be useful for us.",4
1128,"data-modeling-approaches-in-modern-data-times","This is an excellent question.  I work for a large non-profit and we have been grappling with this question as we engage in a re-design of our data warehouse as we move to the cloud.  We have settled on:
1. An Extract process to take source data from sources and dump it raw into cloud storage.  We call this our data lake.
2. A secondary Extract process to load from cloud storage into our cloud data warehouse.
3. Kimball Star Schema Modeling.  We do this primarily to reduce complexity so we can have a layer where the data is easy to understand.  This reduces long term technical debt.
4. Some of our analytical tools can use the star schema directly, and we are done.  Other analytical tools need a wide table that exposes hundreds of columns.  We call this a ""context"", and we define it as a fact table with all possible dimension tables joined together into one wide table.

We are in the process of creating this next generation data warehouse.  We have implented all of the technical pieces, and we are now in our implementation stage where we are modeling our tables and building it step by step.  Time will tell how well it works, but we already see value.",5
1128,"data-modeling-approaches-in-modern-data-times","Thanks for sharing that @Craig and @trevor 
I am wondering: Is the Kimball Star Schema the layer only non-analytics people and the tools are using or are most of the data analysts also directly accessing that star schema?
One thing which is puzzling me is that for data analysts (or bi managers or analytics engineers, however you would call them) isn't there a lot of data exploration involved? Especially investigating into topics or creating reports that are not yet accessible via the star schema?
I have the same question for creating a normalized table layer. Do you try to involve all the data you get from the start and normalize it although there is no clear goal to use it yet or do you add new tables/columns only once they are needed?",6
1128,"data-modeling-approaches-in-modern-data-times","Hi @drege! Good questions. 

**Do you try to involve all the data you get from the start and normalize it although there is no clear goal to use it yet or do you add new tables/columns only once they are needed?**

I've been at ThriftBooks for over 8 years - as a developer, dev manager, director, and VP. I've seen the types of questions people are asking for a long time, so I have a good sense for what data to include and what data to not include. But generally, if the data fits along the grain and might answer a question, I include it. Adding an extra column to a columnar database is not a big deal. If you don't add it, people won't be able to see it. If you do add it, maybe nobody will want it and it'll just clutter things up. I tend to err on the explorability side of things. I'd rather give them the opportunity to see that a field is available and have them decide to not use it than to not show it to them.

**Isnâ€™t there a lot of data exploration involved? Especially investigating into topics or creating reports that are not yet accessible via the star schema?**

We tell our analysts if there's anything they need, let us know and we'll add it as quickly as possible. If there's a question they want answered, and the data isn't in the modeled tables, and we have it in our raw data, we add it usually by the next day. Our models are pretty comprehensive because of this. At the beginning, we were adding a lot of tables. Now though, we add or modify something a few times a week.",7
1128,"data-modeling-approaches-in-modern-data-times","Also interesting that @Craig uses both star schema and wide, de-normalized tables. I think if you can build one, it's easy to build the other. If you have a star schema, you just join up all the facts and dimensions and you've got a wide de-normalized table. If you have a wide de-normalized table, you just separate out the facts from the dimensions and you're done. The reason we go w/ the wide de-normalized tables is b/c they are faster to query in Snowflake. A single massive table with billions of rows can be queried pretty easily w/ an extra-small, but when we started joining tables together, unless they had clustering keys the performance was not as good. Clustering keys cost money to maintain, and larger warehouses cost money to run. So the big table works for us.",8
1128,"data-modeling-approaches-in-modern-data-times","@Craig curious how this journey has gone a year in. We are in the process of building out our analytics stack at my company and I've gone back and forth with this. I haven't found much material on how this is being implemented, use cases, etc.",9
1128,"data-modeling-approaches-in-modern-data-times","Reading this as part of dbt onboarding and wanted to share a ""how we did it"" from a 2-3 person Analytics Engineering team responsible for delivering data to both in-house stakeholders and client-facing reporting. I'd say maybe mid-sized data on reverse supply chain and recommerce (~100M rows of units through the system)

Notably, we also had a number of (Snowflake) SQL power users in our data stakeholder group, including people on the Ops team on warehouse floors, so intuitive navigation in the db was a high priority in our modeling. 

We settled on a schema per business entity (e.g. unit) and then four types of tables:

* `<entity>_details`: all the immutable information about the entity (e.g. `unit_details` would have things like client_id and created_timestamp)
* `<entity>_current`: everything in the `_details` model, plus the current value of any mutable fields (e.g. `status` or `warehouse_location`)
* `<entity>_aggregates`: everything in the `_current` model, plus any aggregates from other entities (e.g. `storage_location_aggregates` had a `unit_count` based on `unit_current.storage_location`)
* Any number of `<entity>_<event>` tables, comparable to `fact` tables, where we preserved the event stream of anything that could impact each entity (e.g. `unit_received`, `unit_sorted`, `unit_fulfilled`).

While there's obviously some cruftiness to building table, table-plus, and table-plus-plus, we found this was an acceptable amount of duplication to give users the ability to join to a single table depending on the type of field they needed (i.e. if they just needed `status`, they could stop at `_current` rather than `aggregates`).  That said, dbt's style guide has suggested it might have been better to JUST surface events and `_aggregates` in the `marts` pattern.

Still, even this approach was a huge improvement from just-in-time query builds.",10
1128,"data-modeling-approaches-in-modern-data-times","Hi Drege,

I think it depends on our actual requirements: the big corporations tend to use more sophisticated designs such as Data Vault because they have sophisticated requirements (security, privacy, as in GDPR, data governance, agile extension, multi-cloud, etc.). 

However, there is a misconception about Data Vault: we sometimes hear that DV is too complicated for small use cases, but that's actually not true: if you have low requirements, DV keeps simple as well. But it also has a lot of solutions for more advanced scenarios, but every solution will add something to the resulting model. So the model looks (and becomes) more complicated, but the underlying reason is actually that your requirements are complicated and that is reflected by the resulting solution. 

One of our clients gave me excellent feedback on this: he was involved in a non-Data Vault project and told me that they were actually super fast in delivering something. However, it only worked for a while until the requirements increased and the solution more or less broke (because it wasn't extensible). 

Everyone knows that the data volume , velocity increases over time. So I guess that everybody agrees that the solutions must scale in this regard. However, its also true that the requirements how we process data increase over time (think about the introduction of GDPR in Europe, which was a steep step for many). So a solution should also scale in regards to the requirements IMHO. 

Hope that helps a bit,

Mike",11
1129,"dw-design-which-books-should-i-read-in-2020","On a related note [to this discourse post](https://discourse.getdbt.com/t/data-modeling-approaches-in-modern-data-times/1128), Iâ€™m interested to hear what resources people recommend for developing a strong theoretical grounding in DW design.

Iâ€™m after a book or a few long and in-depth blog posts to explain the pros and cons of different models so that I can then make informed decisions given a particular use case. I've previously enjoyed Martin Fowler's lengthy posts on software architecture, but he hasn't done much on data architecture (and [what is there](https://martinfowler.com/articles/data-monolith-to-mesh.html) is laden with buzzwords and light on technical detail for my liking).

I know The DW Toolkit by Kimball would be a standard reference, but following on from the discussion on [this discourse post](https://discourse.getdbt.com/t/is-kimball-dimensional-modeling-still-relevant-in-a-modern-data-warehouse/225/4) it seems like there is some debate about its relevance in the MPP database age (although it still seems like a good starting point to me).

Can anyone recommend any  **favourite DW design books**? Or books / long blog posts in general that are must-reads for a technical data lead?

In return, I recommend my favourite technical data architecture book, [Designing Data-Intensive Applications by Martin Kleppman](https://www.amazon.co.uk/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321#customerReviews), which is AMAZING as a foundation for understanding and combining all kinds of different data systems.",1
1129,"dw-design-which-books-should-i-read-in-2020","Hi @dan,
Here are two great books (in my opinion) : 
- Building a scalable dwh with datavault 2.0 (by Linstedt & Olschimke)
- The elephant in the fridge (by Giles)

It's all about scaling, automation and capturing business rules correctly.

(Edited) I forgot to mention Roelant Vos' famous blog !! Icing on the cake, I noticed, while looking for the blog's url, that Roelant Vos himself is testing the integration of his 'Virtual DataWarehouse' automation tool with DBT... It's a small world !

[http://roelantvos.com/blog/generating-dbt-etl-for-snowflake/](http://roelantvos.com/blog/generating-dbt-etl-for-snowflake/)

Best,",2
1129,"dw-design-which-books-should-i-read-in-2020","Thanks the suggestions @fabrice.etanchaud! I've seen Datavault come up a few times so this is a good prompt to read a bit further into it, and I've saved the blog post for weekend reading!

I also received a few more suggestions on the dbt Slack:
* Sean C: I've found [Agile Data Warehouse Design](https://www.amazon.com/Agile-Data-Warehouse-Design-Collaborative/dp/0956817203) is a good book for this. Half of it is about the process of working with stakeholders and designing models. And then the other half is about design patterns for common types of dimensions. (a few people agreed on this one)
* Jeff A: I  **LOVE**  [Mastering Data Modeling: A User Driven Approach](https://www.oreilly.com/library/view/mastering-data-modeling/9780133122633/) for getting a firm understanding how (and how well) a data model maps data to reality and vice versa.   It was the book that made data modeling ""click"" for me.  After that the Kimball stuff made much more sense.",3
1129,"dw-design-which-books-should-i-read-in-2020","There was some more discussion on the original Slack thread, Dave says:

>Kimball is still relevant, it's also worth learning about Inmon and about Data Vault and plain old transactional database modelling. The core parts of Kimball are
> 1) semantic layer - converting technical naming into business-aligned naming.
> 2) De-normalizing as appropriate
> 3) laser focus on business perspective
>
> What I don't think is so useful is when people read all the technical parts and develop a fixation on the perfect one-size-fits-all-queries star/snowflake schema. There's some context around when a lot of that was written, mainly to do with the limitations of database technology. Much of it is still relevant, the laws of physics haven't changed, but some things can be relaxed now.
> For example it's not such a big deal to store multiple redundant copies of denormalized tables that are aggregated in different ways or at different granularities. Especially with cloud based systems like Redshift Spectrum, Athena, Snowflake etc storing the data on blob storage is virtually unlimited and relatively cheap.
> I also think it is valuable to realise that if you do create a one-size-fits-all data warehouse then it will not be optimised for any particular use-case. It will be a bit lowest-common-denominator. You can spend months and months trying to build something ""perfect"" like that and then discover the 5 -10 really important use cases don't run quickly at all.
> One place where dim / fact tables can clash with MPP architecture is that you really want to avoid shuffling between nodes. It's always going to be faster to have zero joins in a query because a single table can be partitioned across nodes and queries parallelised easily... assuming queries use predicates that are in sympathy with the partitioning strategy. That said, small dimension tables can be stored redundantly in full, on each node, with larger fact tables partitioned across multiple nodes. Co-located (merge) joins are faster, but not as fast as no joins. And as soon as you get complex subqueries then the query engine no longer has sorting guarantees (and hence merge joins) and colocation can't be relied on either.
> What remains is the laser focus on the business requirements. An architecture that is a bit messy and organic, but has some denormalized datamart-style tables is going to be more useful more quickly than a full blown data warehouse.
> If you do have extensive data lineage and auditing requirements, then certainly using Data Vault as an intermediary storage will be well worth the additional development time required.
> Ultimately the best thing you can do is equip yourself with knowledge of all the different approaches and use the ones that are relevant, disregarding the dogma.

To which Jose followed up:

> I absolutely agree with everything in your comment Dave. To be honest, the technology has advanced a lot since these data warehouse designs methodologies were created. So many people tend to either ignore them completely (so there's a risk of not having a proper design for a company wide data warehouse that covers the requirements from all departments) or they say the new design paradigms focus only on the technical side and don't go in enough detail over the business requirements so they think they have to follow every single Kimball/Inmon detail. It's really important to understand the methodologies and the why of them to make proper decisions",4
1129,"dw-design-which-books-should-i-read-in-2020","Hi Fabrice,

thanks for the praise :slight_smile: 

There is some background for the book - when we wrote it, we wanted to address two target audiences: the professional DWH developer and the beginners, probably sitting in college and learning data warehousing for the first time (with Data Vault 2.0). So I believe / hope its a good starting point for many youngsters.

From our experience, dbt works great with Data Vault 2.0, especially if you follow the ""data-driven"" approach (due to the flexibility of Data Vault, there are many approaches to the modelling style). We have made great experience with DV & dbt at clients of varying scale. 

Again, thanks for the praise.
Mike",5
1134,"accessing-a-dynamically-set-variable-in-multiple-models","I know I can execute a query in a model and assign the result to a variable in this way:

{% if execute %}
  {% set last_audit_id = run_query(""SELECT LAST_AUDIT_ID FROM CFG.PAR_SCDM_AUDIT WHERE FLOW = 'ORDER_MOVEMENT' "").columns[0][0] %}
{% endif %}

but this variable is local to this model.
Is there a way to do the same thing in dbt_project.yml and thus feed a project-wide variable?
Thanks
Daniele",1
1134,"accessing-a-dynamically-set-variable-in-multiple-models","You can't dynamically set a global/project-wide variable, but if you need to use the same value in multiple locations you could extract it into a macro: 

```sql
{% macro get_last_audit_id() %}
  {% if execute %}
    {% set last_audit_id = run_query(""SELECT LAST_AUDIT_ID FROM CFG.PAR_SCDM_AUDIT WHERE FLOW = â€˜ORDER_MOVEMENTâ€™ "").columns[0][0] %}
  {% else %}  
    {% set last_audit_id = -1 %}
  {% endif %}

  {% do return(last_audit_id) %}
{% endmacro %}
```

And then you can call it from multiple locations 

```sql
-- model_1.sql
{% set last_audit_id = get_last_audit_id() %}
select * from {{ ref('some_table') }}
where id > {{ last_audit_id }}
```

```sql
-- model_2.sql
{% set last_audit_id = get_last_audit_id() %}
select * from {{ ref('another_table') }}
where id > {{ last_audit_id }}
```

:warning: Note that this won't be a static value across all invocations in a run. If you need that to happen, you might have to change your macro to write to some sort of temporary table, and then have the macro read from _that_.",2
1134,"accessing-a-dynamically-set-variable-in-multiple-models","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
1151,"creating-a-dev-environment-quickly-on-snowflake","If your organization has a large dbt project, and you need to run dbt with a new target schema (for example, a new employee joined and wants to start contributing. Or, you borked your own target schema and want to start from scratch), here's a handy macro to help you out.

**Note:** it only works on Snowflake, and assumes that you don't use mutliple schemata.

```

{% macro clone_prod_to_target(from) %}

    {% set sql -%}
        create schema if not exists {{ target.database }}.{{ target.schema }} clone {{ from }};
    {%- endset %}

    {{ dbt_utils.log_info(""Cloning schema "" ~ from ~ "" into target schema."") }}

    {% do run_query(sql) %}

    {{ dbt_utils.log_info(""Cloned schema "" ~ from ~ "" into target schema."") }}

{% endmacro %}


{% macro destroy_current_env() %}

    {% set sql -%}
        drop schema if exists {{ target.database }}.{{ target.schema }} cascade;
    {%- endset %}

    {{ dbt_utils.log_info(""Dropping target schema."") }}

    {% do run_query(sql) %}

    {{ dbt_utils.log_info(""Dropped target schema."") }}

{% endmacro %}

{% macro reset_dev_env(from) %}
{#-
This macro destroys your current development environment, and recreates it by cloning from prod.

To run it:
    $ dbt run-operation reset_dev_env --args '{from: analytics}'

-#}
    {% if target.name == 'dev' %}

    {{ destroy_current_env() }}

    {{ clone_prod_to_target(from) }}

    {% else %}

    {{ dbt_utils.log_info(""No-op: your current target is "" ~ target.name ~ "". This macro only works for a dev target."", info=True) }}

    {% endif %}

{% endmacro %}

```

Run it with:
```
$ dbt run-operation reset_dev_env --args '{from: analytics}'
```

Then, you can go about working on the models you want to develop without having to rebuild from scratch.

We also use the `clone_prod_to_target` macro as an operation in our CI runs to make them faster â€“ by cloning our prod schema, our incremental models only process new data.",1
1151,"creating-a-dev-environment-quickly-on-snowflake","Oh yeah! Thanks for sharing, @claire, this is awesome.",2
1151,"creating-a-dev-environment-quickly-on-snowflake","[quote=""claire, post:1, topic:1151""]
```
{% macro destroy_current_env() %}

    {% set sql -%}
        drop schema if exists {{ target.database }}.{{ target.schema }} cascade;
    {%- endset %}

    {{ dbt_utils.log_info(""Dropping target schema."") }}

    {% do run_query(sql) %}

    {{ dbt_utils.log_info(""Dropped target schema."") }}

{% endmacro %}
```
[/quote]

@claire What would be the best way to drop the schema at the beginning of the run, but then clone the schema at end of the run (from within the macro)? Is that even possible or practical?",3
1151,"creating-a-dev-environment-quickly-on-snowflake","I'm not quite sure what you're trying to achieve here â€” can you let me know why you're looking to implement this pattern (it might help me give better advice!). On the surface, this is certainly possible through the use of [hooks and/or operations](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations)",4
1151,"creating-a-dev-environment-quickly-on-snowflake","@claire Sorry for the confusion. I wanted to implement the same process you were, but with a different pattern where the destruction of the dev env and the zero-copy-clone would happen separately. This was for a staging environment we're creating and I wanted to make it as modular and dynamic as possible to ensure portability and reusability. I decided to just call the separate macros on start and end and removed the need for the third macro that runs the other two when run from the dbt CLI. I think that's probably the best way for us at the moment, but it seemed like one of the only ways.",5
1151,"creating-a-dev-environment-quickly-on-snowflake","@claire any particular reason you opted into `CREATE IF NOT EXISTS` rather than `CREATE OR REPLACE` ?",6
1151,"creating-a-dev-environment-quickly-on-snowflake","This is great idea.  I created a macro based on @claire code.  It would not run since I don't have dbt_utils package installed (I have not been able to get it installed).  So I did remove the code that reference dbt_utils references.  I was able to execute the dbt run-operation however; it did not create the clone.  There are no errors yet the clone does not get created in Snowflake.  Am I missing something?

```
* This macro is to create a clone from the DEV_FIVETRAN database inside Snowflake



   WAREHOUSE defaults to  DBT_TRANSFORM_COMPUTE_XS 

   USER_ROLE defaults to  DATA_ENGINEER_ROLE

   Clone name defaults to DEV_FIVETRAN_ developer initials

   Any previous clones with this name will be replaced.



  dbt run-operation developer_clone_creation  --args '{INITIALS: WMM}'

*/

{% macro developer_clone_creation (INITIALS) %}



  USE WAREHOUSE DBT_TRANSFORM_COMPUTE_XS;

  USE ROLE DATA_ENGINEER_ROLE; -- or whatever roles need to use the clone



  CREATE OR REPLACE DATABASE DEV_FIVETRAN_{{[INITIALS]}} CLONE DEV_FIVETRAN;



  --Special grants for clone access

  GRANT USAGE ON DATABASE DEV_FIVETRAN_{{[INITIALS]}} TO ROLE DATA_ENGINEER_ROLE ;



{%endmacro %}

```",7
1156,"yaml-anchors-dont-work-with-lists","Hi all,
I tried to set up anchoring while declaring tests as recommended here: https://docs.getdbt.com/docs/building-a-dbt-project/testing-and-documentation/testing/

But i can't figure out how it should work. I have this example:
>      columns: &columns
>        - '""date""'
>      models:
>        - name: ga_data
>          tests: 
>            - not_null:
>                column_name: *columns
But what i get is an error message:
>      001003 (42000): SQL compilation error:
>      syntax error line 3 at position 6 unexpected '['.

This is the SQL that is put into target/schema_test:
>     select count(*)
>     from XXXXXX.GOOGLE_ANALYTICS.ga_data
>     where ['""date""'] is null

Can you help me out?
Ultimately, I want to have an easy way of testing ALL columns in a model for not_null without have to retype everything for each column.",1
1156,"yaml-anchors-dont-work-with-lists","Hi @germanguy ðŸ‘‹

Anchors can only be used for dictionaries, whereas here you're using it for a list â€” check out this [related issue](https://github.com/yaml/yaml/issues/35) on the Jinja library.",2
1156,"yaml-anchors-dont-work-with-lists","Hi claire, thanks for your help.
Did you post the correct link? This leads to an issues regarding ""merge arrays"" in the yaml github repo, which actually shows lists instead of dictionaries.
I found a link that explains it, https://docs.ansible.com/ansible/latest/user_guide/playbooks_advanced_syntax.html",3
1156,"yaml-anchors-dont-work-with-lists","Yep! The linked issue is a request for the functionality you're trying to use, and the response basically being like ""nah this won't happen""",4
1156,"yaml-anchors-dont-work-with-lists","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
1187,"how-to-import-variable-i-e-set-foo-bar-from-another-file-no-loader-for-this-environment-specified","I have a variable that I need to re-use in different files, and I have trouble importing it. E.g. several hundred lines of:

{% set auto_curated_columns = { ""oldname"":""newname"", ""old_col_2"":""new_col_2""} %}

I would like to do
{% from 'file_with_definition' import auto_curated_columns %}
but that errors out with: ""no loader for this environment specified"".

Vars defined in dbt_project.yaml is not a good fit extensive implementation details should really not go there.

It wouldn't even fit into an environment variable.

A template would not cut it - I am using the variable in loops, conditionals, to derive code, to derive column names such as import_status_of_{{auto_curated_columns.loop}}, etc. (Sticking the file with the definitions into the macros directory doesn't work either.)

For now, I'm stuck pasting the variable definition into tests and yaml-files. At least for models, I can define vars:auto_curated_columns in dbt_project.yml and then {{ set auto_curated_columns = fromjson(var('auto_curated_columns'))}}.

How do I import a variable directly, instead?
How do I use Jinja in schema.yml?",1
1187,"how-to-import-variable-i-e-set-foo-bar-from-another-file-no-loader-for-this-environment-specified","[quote=""TJB, post:1, topic:1187""]
For now, Iâ€™m stuck pasting the variable definition into tests and yaml-files. At least for models, I can define vars:auto_curated_columns in dbt_project.yml and then {{ set auto_curated_columns = fromjson(var(â€˜auto_curated_columnsâ€™))}}.
[/quote]

Were you able to find a solution to this? I also need to do exactly the same.",2
1187,"how-to-import-variable-i-e-set-foo-bar-from-another-file-no-loader-for-this-environment-specified","I ended up defining lots of variables in dbt_project.yml because that is the only spot that is imported into multiple files.",3
1187,"how-to-import-variable-i-e-set-foo-bar-from-another-file-no-loader-for-this-environment-specified","You can create a macro which returns a dictionary generated from yaml set as a variable within the macro. e.g.
```sql
{% macro load_curated_columns() %}
{% set yml_str %}

auto_curated_columns:
  old_col_1: new_col_1
  old_col_2: new_col_2

{% endset %}
{% set conf_yml = fromyaml(yml_str) %}
{{ return(conf_yml) }}
{% endmacro %}
```

Then, when you need to load these into your model or test, you can
```sql
{% set auto_curated_columns = load_curated_columns()['auto_curated_columns'] %}
select
    {{ auto_curated_columns.old_col_1 }} as {{ auto_curated_columns.new_col_1 }}
    {{ auto_curated_columns.old_col_2 }} as {{ auto_curated_columns.new_col_2 }}
from old_table
```
or whatever.",4
1187,"how-to-import-variable-i-e-set-foo-bar-from-another-file-no-loader-for-this-environment-specified","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
1240,"no-active-warehouse-selected-error","Hi Team,
I am new to dbt. I am trying to execute the first example model provided by dbt but getting the following error message ""000606 (57P03): No active warehouse selected in the current session. Select an active warehouse with the 'use warehouse' command.""  My warehouse is up and running in the snowflake environment.

Could you pls help me to resolve this issue.

Regards
Ganesh",1
1240,"no-active-warehouse-selected-error","Hey Ganesh, 

I am also new but I think I got my dbt-snowflake running properly.

You could [check out my blog post draft](https://www.robin-beer.de/drafts/how-to-setup-dbt-dataops-with-gitlab-cicd-for-a-snowflake-cloud-data-warehouse.html) and let me know whether it helps!

It's actually still in draft status, but as you asked this question, I wanted to already share it with you and maybe improve it together. :) 

Bests
Robin",2
1240,"no-active-warehouse-selected-error","Hi Robin,

Thanks for your help. It got fixed by selecting the correct snowflake user role. 

Once again appreciate your help.

Regards
Ganesh",3
1240,"no-active-warehouse-selected-error","Happy to hear that the problem got fixed! :slight_smile:

Did the article help or did you already fix it before?

Either way, would be happy for some critical feedback on the article!

Bests
Robin",4
1240,"no-active-warehouse-selected-error","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
1247,"passing-a-json-value-to-a-dbt-macro","Hi all,

I'm starting out with Macros and created one which looks up a value in a table and returns the value. I'm passing in a Foreign Key ID however this key is stored in JSON. I am utilizing Snowflake as the Data Warehouse and this JSON sits in a Variant data type column. 

I'm able to use Snowflakes JSON notation in my models (alias.column_name:exampleA:exampleB ) and this works fine. However, when using this notation to pass the foreign key to the macro, I get the following error:

expected token ',', got ':'
    line 10
      {{ get_device_type(rd.deal_json:device:product:product_type_id) }}

Can anybody help me out, is this a syntax error or is my understanding of macros wrong?",1
1247,"passing-a-json-value-to-a-dbt-macro","I suspect you want to pass a string to the macro, because jinja is just a big text templating system. Try wrapping the argument in quotes. Either single or double will do.

 e.g.
 `{{ get_device_type('rd.deal_json:device:product:product_type_id') }}`",2
1247,"passing-a-json-value-to-a-dbt-macro","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
1274,"how-do-you-deal-with-missing-dimensions-for-foreign-keys","Hi,

What's the best practice in DBT for dealing with missing dimensions for foreign keys?

For example, I have a fact_orders that contains a foreign key to a dim_funnel. The rows in dim_funnel have to be maintained manually, so it can frequently happen that orders happen on a funnel, but the dimension row for the funnel hasn't been added yet.

In a traditional DWH model we would include a special row like ""Missing Funnel"" with a -1 key for example. And then in the fact table query we would check if the correct foreign key exists in the dimension table and if not set it it to -1.

A similar situation can happen when dimension records are deleted, but the facts referencing it are kept.

I guess when generating the fact table, I could do a join on the dimension to check if the key is there, but this prevents one of primary benefits of using hash keys-- being able to load dimensions and facts in parallel.

So I am just wondering how you guys are handling such a situation.

Thanks, Kim",1
1274,"how-do-you-deal-with-missing-dimensions-for-foreign-keys","We still create a ""-1"" ""Unknown"" row in our dimensions. Although it's not actually ""-1"" literally, it's the hash of ""-1"". And then we have a variable in our dbt_project.yml file like this:

`  unknown_key: ""CAST(FARM_FINGERPRINT('-1') AS INT64)""`

Then when we build our fact tables and need to populate the keys to join to the dimension tables, we do something like this: 

	COALESCE(pers.person_key, {{ var('unknown_key') }} ) AS person_key,

I never really thought of one of the primary benefits of hash keys as being able to load dimensions and facts in parallel per se.",2
1274,"how-do-you-deal-with-missing-dimensions-for-foreign-keys","Fantastic, that makes a lot of sense. Thank you for sharing.

And in this example `pers` is the joined dimension table, right?

And how do you ""append"" your -1 dimension row in the dimension?

Do you just add a `union` at the beginning of the dim table?",3
1274,"how-do-you-deal-with-missing-dimensions-for-foreign-keys","Yes, in this case `pers` is the joined dimension table. And yes we just do a `UNION` with the query that is producing the dimension to append this row.",4
1274,"how-do-you-deal-with-missing-dimensions-for-foreign-keys","Hi @Kimcha, there is an interesting article from Roleant Vos on the subject (in the data vault realm) : [unknown-keys-zero-keys-or-ghost-keys-in-hubs-for-dv2-0](http://roelantvos.com/blog/unknown-keys-zero-keys-or-ghost-keys-in-hubs-for-dv2-0/), with a taxonomy on missing values. In my case, we use two distinct missing key values : one when the key is missing but mandatory (problem in the loading or in source data), and another when the key is missing but optional (loading and data ok but data is missing for 'business' reason).

Best,
Fabrice",5
1278,"trigger-a-dbt-cloud-job-on-merging-to-master-branch","## Background

dbt Cloud is an excellent service and great value, providing an easy way to manage scheduled dbt jobs. As well as scheduling, it allows for Webhooks to be used so that a job can be triggered to run on a pull request (PR) to a branch specified in your dbt Cloud deployment environment. Jobs triggered via the webhook will be run against a new schema in your datawarehouse that will be cleaned up at the end of the run. Great for testing, but what about running a real production job?

In addition to schedules and webhooks, dbt Cloud also provides an API for triggering runs, which you can use to run a job that will use the regular schema(s) you use for jobs. 

## Why?
The main reason you might want to do this is to minimize how often a job runs that consists entirely of views, or to generate the dbt docs. On a target warehouse like Snowflake, this could mean a virtual warehouse spends less time on and more time in suspended mode, where you don't have to pay for it. The alternative, recreating view definitions on a schedule, is pointless busywork given dbt Cloud jobs usually run only from the master branch and the only time the views could possibly change is when new code is merged to the master branch.

I will illustrate two alternative ways to trigger a dbt Cloud job to run *on merge to master branch* using CI tools, firstly with **Github Actions** and then with **CircleCI**. 

The approach taken here could be adapted for use with other CI tools, and should work with Git repository hosts other than Github such as Gitlab or Bitbucket, although I haven't tested any of those.

## dbt Cloud
1. Create or open an existing dbt Cloud job

Here you would normally set up a Schedule or Webhook. There is a section for ""API"", but nothing needs to be done here, it just provides a link to the docs and a basic POST example. Take note of your account ID and job ID, at this point, those IDs should be obvious in your webbrowser's URI.


2. Get your dbt API key

Open your ""My Account"" settings and go to ""API Access"", copy the API Key, you will need it later for either solution.


## Github Actions
_Use these instructions with Github Actions, for CircleCI skip to the next section_ 

1. Log in to GitHub and open your dbt repository

2. Click on ""Settings"" and then ""Secrets""

3. Add a github secret called `DBTCLOUDAPIKEY` paste in your dbt cloud API key that you copied earlier

4. Add a new github actions workflow

Contents of github/workflows/main.yml file:
  * You will need to replace the `<GOES HERE>` parts with your dbt account_id and job_id
```
name: Trigger dbt cloud job to prep views and build docs
on:
  push:
    branches: [ master ]

jobs:
  prep:
    runs-on: ubuntu-latest
    steps:
    - name: Run dbt cloud job
      run: |
        RESPONSE=$(curl -v -H ""Authorization:Token ${{ secrets.DBTCLOUDAPIKEY }}"" -H ""Content-Type:application/json"" -d '{""cause"":""Triggered by Github Actions"",""git_branch"":""master""}' https://cloud.getdbt.com/api/v2/accounts/<ACCOUNT_ID_GOES_HERE>/jobs/<JOB_ID_GOES_HERE>/run/)
        STATUSCODE=$(echo ""$RESPONSE"" | jq '.status.code')
        echo ""$RESPONSE"" | jq '.status.user_message'
        if [[ $STATUSCODE != 200 ]]; then echo ""$RESPONSE"" && bash -c ""exit 1""; fi

```

5. Ensure this is merged in to the master branch, the next time something is merged this workflow should run.

## CircleCI version
_These instructions assume you already have a CircleCI account and have it connected with your GitHub (or other) git host_ 

1. Log in to your CircleCI account
2. Open ""Projects""
3. If it is not already enabled you may need to ""Set Up Project"" for your dbt git repo
4. Add a _project_ environment variable called `DBTCLOUDAPIKEY` containing your dbt API key copied earlier

Contents of the `.circleci/config.yml` file:
  * You will need to replace the `<GOES HERE>` parts with your dbt account_id and job_id
```
version: 2.1

workflows:
  version: 2
  dbtcloud:
    jobs:
      - trigger_dbt_cloud_job:
          filters:
            branches:
              only:
                - master

jobs:
  trigger_dbt_cloud_job:
    docker:
      - image: circleci/python:3.8.1
    steps:
      - run:
          name: curl
          command: |
            RESPONSE=$(curl -v -H ""Authorization:Token ${DBTCLOUDAPIKEY}"" -H ""Content-Type:application/json"" -d '{""cause"":""Triggered by CircleCI"", ""git_branch"":""master""}' https://cloud.getdbt.com/api/v2/accounts/<ACCOUNT_ID_GOES_HERE>/jobs/<JOB_ID_GOES_HERE>/run/)
            STATUSCODE=$(echo ""$RESPONSE"" | jq '.status.code')
            echo ""$RESPONSE"" | jq '.status.user_message'
            if [[ $STATUSCODE != 200 ]]; then echo ""$RESPONSE"" && bash -c ""exit 1""; fi
```
5. Ensure this is merged in to the master branch, the next time something is merged this pipeline should run


## What are those bash commands?
The script sends a request to the dbt Cloud API to run a job , and stores the reponse. The `status` attribute is extracted from that response, which will log `success!!` if everything went well, or something else if not. It then checks to see if the response code is 200 (success) and if not will log the entire response and also force a non-zero exit code so that the entire CI workflow/pipeline will fail.

## Caveats
The dbt API, Github Actions, CircleCI and other URIs mentioned here could change in the future, so refer to the relevant docs for latest up-to-date information.

The API key you use will be tied to your account, so it will be best currently for the dbt administrator to do this. It would be nice if dbt had a way to generate API Key for some _service account_ user but I am not aware of any such functionality.

## References
[dbt Cloud docs - importing a project by git URL](https://docs.getdbt.com/docs/dbt-cloud/cloud-configuring-dbt-cloud/cloud-import-a-project-by-git-url)
[dbt API docs](https://docs.getdbt.com/dbt-cloud/api/)",1
1278,"trigger-a-dbt-cloud-job-on-merging-to-master-branch","I'm wondering about the data params POSTed to the `jobs` end point. I don't see those in the  API docs anywhere. It's good to know that the git branch can be set. Can I also override the *database* established in the environment?

If both those params are available, it would be possible to quickly spin up a test db with a random name in Snowflake, load the seeds, run the tests, then shut down and drop the db. Otherwise I'm not sure how we could achieve that.",2
1278,"trigger-a-dbt-cloud-job-on-merging-to-master-branch","In dbt Cloud, you have a project with an environment that has a connection. The connection defines a specific database so I don't think you can do what you suggested by orchestrating dbt Cloud with CI. 

You can certainly do it with a new schema instead of a database. That's exactly how the _Webhooks_ mode of a dbt job works. It will create a new schema and drop it for you when done. 

If you use _Webhooks_ - as opposed to a schedule or the API - it will be triggered when a PR is created. That earlier point in time is much more appropriate for running full end-to-end test. The purpose of what is outlined here is not for running tests, but really for minimizing the amount of time Snowflake is running; maximizing the amount of time spent suspended, saving time and money.",3
1278,"trigger-a-dbt-cloud-job-on-merging-to-master-branch","[quote=""DAVOS, post:1, topic:1278""]
It would be nice if dbt had a way to generate API Key for some *service account*
[/quote]

:+1: That should definitely go on to the road map, pretty sure we're not the only ones not to want to have personal credentials in an automated workflow.

And thanks for writing that up, hooking Circleci in could make my life loads better.",4
1294,"how-to-specify-target-schema-for-udfs","Hi all,

I started implementing the approach illustrated in this [post](https://discourse.getdbt.com/t/using-dbt-to-manage-user-defined-functions-redshift/18) for BigQuery. 

```sql
-- macros/udfs/file_name.sql
{% set schema = 'schema_name' %}

{% macro macro_name() %}
CREATE OR REPLACE FUNCTION {{target.schema}}.function_name(input STRING) 
RETURNS 
...
```
I was hoping the schema variable defined at the top could be passed into `{{ target.schema}}` block, but it seem didn't work as expected (no function created in my target schema).

Two questions here:
- How to set target schema for udf macro like how we did for model config?
- Does compiled macro go into somewhere like `target/`?",1
1294,"how-to-specify-target-schema-for-udfs","As @claire replied in this [thread](https://discourse.getdbt.com/t/using-dbt-to-manage-user-defined-functions/18/13?u=alex_unsw)

After surrounding macro name with `run_query` block like this
```jinja
{% macro create_udfs() %}
...
{% do run_query(fn_metadata_header()) %}
...
{% endmacro %}
```
The macro is successfully executed.",2
1294,"how-to-specify-target-schema-for-udfs","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","> *This post is heavily inspired by work done by Alan from tails.com â€” thanks for sharing your approach on a [dbt office hours](https://www.youtube.com/watch?v=-K_O6x533vE)!*

### What's a blue/green deployment?
Blue-green deployment is a technique that reduces downtime and risk by running two identical production environments. Let's take a website as an example â€” when you need to deploy a new version of your website, the new version is created in a separate environment. **After** the build is successful, traffic is routed to the new build. If anything goes wrong, you just don't switch over the traffic! :boom:

In analytics, we can take a similar approach:
- Maintain two separate databases (or schemas!) â€” for our example we're going to call them `stage` and `prod`.
- Whenever you perform your production run of dbt project, run it in the `stage` database
- Once all your tests have passed, switch out the `stage` database for the `prod` database.

Hey presto! No bad data creeping into `prod` ever :D 

### Doing this on Snowflake
1. Set up two new databases, and grant ownership of these databases to the role that runs your dbt project
```sql 
create database stage;
grant ownership on database stage to role transformer;

create database prod;
grant ownership on database prod to role transformer;

```
2. Update your dbt project to write to the `stage` database (this will mean updating either your `profiles.yml` or your connection in dbt Cloud).
3. Add an [operation](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations#operations) to your project to swap the databases  (more on this below).
```
# macros/swap_database.sql
{% macro swap_database() %}

    {% set sql='alter database prod swap with stage' %}
    {% do run_query(sql) %}
    {{ log(""database swapped"", info=True) }}

{% endmacro %}
```
4. Add a macro to your project to override the `ref` function (more on this below)
```sql
-- Macro to override ref and to render identifiers without a database.

{% macro ref(model_name) %}

  {% do return(builtins.ref(model_name).include(database=false)) %}

{% endmacro %}

```
4. Add the step to swap the databases to your deployment process â€” you might end up with steps like this:
```
$ dbt run
$ dbt test
$ dbt run-operation swap_database
```
That's it! Now, if your run fails at any step, bad data won't be promoted to `prod`!

### What's this `swap` syntax?
Snowflake has a handy tool to help us out here â€” the `swap` syntax. It essentially performs two renames at once â€” you can find more info out [here](https://docs.snowflake.com/en/sql-reference/sql/alter-database.html) (`cmd + f` for `swap with`).

So by executing:
```sql
alter database prod swap with stage;
```
We're essentially executing these two things at the exact same time, with zero downtime:
```sql
alter database prod rename to stage;
alter database stage rename to prod;
```

### Why do I need to override `ref`?
By default, dbt resolves any `ref` function to a fully qualified object reference. This means that this:
```sql
select * from {{ ref('dim_customers') }}
```
Gets resolved to:
```sql
select * from stage.core.dim_customers
```

In Snowflake (and other databases), views are saved as queries. If the query has a reference to `stage.core.dim_customers`, we could end up with errors like this after we've done the swap:
```txt
SQL compilation error: Schema 'STAGE.CORE' does not exist or not authorized.
```
Or worse, we could be selecting from an out of date view (i.e. the one that got ""demoted"" to the `stage` database).

To get around this, we need to tell dbt: ""well ackshullllly, don't put the database name in there"". So this:
```
select * from {{ ref('dim_customers') }}
```

Should get resolved to this:
```
select * from core.dim_customers
```
Since this is a relative reference, as long as `core.dim_customers` exists in this `prod` database (which it should!), the view will work as expected.

To change the behavior of dbt, we can add our own `ref` macro. Whenever you add your own macro with the same name as a dbt macro, dbt will honor _your_ version (related: [dbt ships with a project](https://discourse.getdbt.com/t/did-you-know-dbt-ships-with-its-own-project/764)).

There's a bit of magic to do with [builtins](https://docs.getdbt.com/reference/dbt-jinja-functions/builtins/) here, but the important thing is this â€” use the code above to get dbt to _not_ use the database name when resolving `ref` functions.

### Loom

Here's a video of me walking through this code in a sample dbt project :point_down:

https://www.loom.com/share/f7c115b6073a4c7da5a7e5c192703825

Happy `swap`-ing!",1
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","This is amazing.  Too bad BigQuery won't let you rename a dataset :(",2
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","100% agree. Fortunately @claus found a pretty reasonably workaround â€” check it out [here](https://calogica.com/sql/bigquery/dbt/2020/05/24/dbt-bigquery-blue-green-wap.html)!",3
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Claire! We've implemented the blue/green method for our dbt runs but I'm noticing a peculiar situation... 

When we run dbt into staging and then swap the schemas, the view definitions are still referencing the staging schema even after the swap macro. Is this to be expected? 

I figure if I really need to, I can run soemthing along :
`$ dbt run -m config.materialized:table --target stage`
`$ dbt run-operation swap_schema`
`$ dbt run -m config.materialized:view --target prod`",4
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Ok, so it sounds like you're swapping between schemas, right? Assuming the command looks something like:
```
{% macro swap_schema() %}

    {% set sql='alter schema prod swap with stage' %}
    {% do run_query(sql) %}
    {{ log(""schema swapped"", info=True) }}

{% endmacro %}
```

If that's the case, you might need to alter the `ref` function even further!

```
{% macro ref(model_name) %}
  {% do return(builtins.ref(model_name).include(database=false).include(schema=false) %}
{% endmacro %}
```

Then a model like this `customers` model:
```sql
{{ config(materialized='view') }}
select * from {{ ref('stg_customers') }}
```

Ends up compiling to this:
```sql
create view my_database.stage.customers as (
  select * from stg_customers
)
```
(Note the lack of schema prefix in the view definition)

But then you get into all kind of strange things:
- How do you get Snowflake to use the right schema when there's no schema reference in the view definition? Does Snowflake automatically search for `stg_customers` the same schema the `customers` view was created in? (I'm not sure!) Or do you have to tell anything (or anyone) that selects from these views to `use schema prod` before selecting from a view... (in which case, that's not great)
- This would probably prevent you from [using custom schemas](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas/) in the future

You _could_ implement an approach like the one you suggested, but I think it will look more like the below, since the initial command should run everything (otherwise, tables might be built on top of views that haven't been updated).
```
$ dbt run -m --target stage
$ dbt run-operation swap_schema
$ dbt run -m config.materialized:view --target prod
```

Overall, I guess I'd push back on swapping schemas here, and encourage you to think about swapping databases instead. It just feels much tidier!",5
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Hi Claire
Firstly thanks to you and the team for the fantastic dbt Fundamentals course, which I have just completed and have found excellent.
My question: do you have any best practice advice for performing a blue/green deploy of our dbt project in Redshift? Thanks.",6
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","I do complete builds of our small data warehouse, and rename the new database to make it production. While it's not a blue/green deployment, it's similar and these tips were helpful!

I also had to override the source macro in manner similar for ref, otherwise views that referenced sources would have the database name in their definitions and they would fail to execute.

    {% macro source(source_name, table_name) %}
      {% do return(builtins.source(source_name, table_name).include(database=false)) %}
    {% endmacro %}

Hope this is helpful to someone!",7
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Sorry about the thread necro but wanted to add to this body of knowledge for folks that are wanting to do blue/green deploys but with schema swaps instead in a single dbt Cloud job (should work with dbt-core too) that has the following steps:

1. `dbt run` on the `blue_` schema.
2. `dbt test` on the `blue_` schema.
3. `dbt run-operation swap_schema` to swap `blue_` schema with the `green_` schema.
4. `dbt docs generate` on the `green_` schema.

*Note: the `swap_schema` macro is left as an exercise to the reader.*

With a custom `generate_schema_name` macro like so:

```
{% macro generate_schema_name(custom_schema_name, node) -%}

    {%- set default_schema = target.schema -%}

    {%- if var('is_pre_swap') is not none -%}
        blue_{{ default_schema }}
    {%- else -%}
        green_{{ default_schema }}
    {% endif -%}
 
{%- endmacro %}
```

And a sequence of run steps like so:

```
$ dbt run --vars 'is_pre_swap: 1'
$ dbt test --vars 'is_pre_swap: 1'
$ dbt run-operation swap_schema
$ dbt docs generate
```

This allows a single dbt Cloud job to do what we want since at the final step (`docs generate`), the `is_pre_swap` variable will be evaluated to `none` so dbt will look to generate docs on the `green_` schemas correctly.",8
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","I assume the `reporter` role would need read permission on both the `staging` and `production` databases? Is there some programatic way to only allow `reporter` to see `production` with the name swapping?

Just slightly wrestling with the idea of `reporter` seeing both the `staging` and `production` databases.",9
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","@claire and @jameshd , any advice here on how to implement blue-green deployment using dbt-core on redshift?",10
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","This sounds really interesting. My only doubt is: what about incremental models? Since the former PROD DB tables demoted to STAGE don't have the latest incremental batch, you'll be missing that info during the next RUN, which will be based on these demoted tables.",11
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Hi All, in order to make it running in Redshift I did the following adjustments:
1) All dbt transformation is done in a schema called schema_name_stage. Thus I created a macro to create this schema if it does not exist at the beginning.
# macros/create_stage_schema.sql
{% macro create_stage_schema() %}
    {% set sql='CREATE SCHEMA IF NOT EXISTS  schema_name_stagev AUTHORIZATION user' %}
    {% do run_query(sql) %}
    {{ log(""schema schema_name_stage created"", info=True) }}
{% endmacro %}
   This macro is called on on-run-start.

2) instead of swaping database I swape schemas. in redshift using the macro below: 
# macros/swap_schema.sql
{% macro swap_schema() %}
    {% set sql0='DROP SCHEMA IF EXISTS schema_name_previous CASCADE' %}
    {% do run_query(sql0) %}
    {% set sql1='ALTER SCHEMA schema_name_prod RENAME TO schema_name_previous' %}
    {% do run_query(sql1) %}
    {% set sql2='ALTER SCHEMA schema_name_stage RENAME TO schema_name_prod' %}
    {% do run_query(sql2) %}
    {{ log(""schemas swapped from schema_name_stage to schema_name_prod"", info=True) }}
{% endmacro %}

3) Thus the final pipeline looks like:
3.1. dbt run
3.2. dbt test
3.3 dbt run-operation swap_schema
I only run the step 3.3 if both run and test worked using github actions.

I hope this helps others.",12
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Yes - we effectively run two versions of our modelled data ""unaudited"" and ""audited"".
Each time the job runs it does everything in the unaudited schema including all tests.
Only if it passes does it move onto audited schema.
This has saved our production data many times and works with many incremental models.

I used this video [https://www.youtube.com/watch?v=jGwUonA3mDg](https://www.youtube.com/watch?v=jGwUonA3mDg) to set it up (it talks about BigQuery but the principles work for Redshift).

Claus was incredibly helpful in supporting me and gave me the key tip:
Take a look at the [slides](https://calogica.com/assets/wap_dbt_bigquery.pdf) (slide 26 and on) for how I proposed doing this on dbt Cloud. 
tl;dr is that you could use `var` for this instead of `target` and get the same result. So each job on dbt Cloud would pass in the relevant `var` to control the build

Therefore we pass in a --vars varaible in our dbt Cloud set-up, so our job looks like this:
1. dbt seed --vars 'audit: true'
2. dbt run --vars 'audit: true'
3. dbt test --vars 'audit: true'
4. dbt source snapshot-freshness --vars 'audit: true'
5. dbt seed
6. dbt run
7. dbt test
8. dbt source snapshot-freshness

You could then unload part/all of the unaudited schema if you want, but we find running two copies is an acceptable cost for the benefit.",13
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Hi!

Running in the same problem here and we were wondering if the `result`method could help here to achieve the same kind of checks?",14
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Basically the idea would be to:
- `dbt run` on the audit target
- `dbt test`on this audit target which will write a manifest
- `dbt run --exclude result:error` on prod target
- `dbt test` on prod target

Would this work as intended?",15
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Hello, I am setting up this system currently, curious what you found to be the permission situation with this deployment strategy? Did you give the reporter role permissions to both staging and production? I'm a bit confused if production permissions remain with the production database with the swap or not. The way I understand it, it seems like it is necessary to give reporter permissions to both databases.

Would appreciate any insight you have, thanks!",16
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","what about creating each time via dbt cloud a new schema as yyyymmddhhss_some_name and then update all views all  used by users to query the downstream models (1 TRAN)  to point to the latest using some smart query on information schema ...
No need to swap anything - just run dbt operation to make some cleanup  from time to time",18
1349,"performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake","Hiya

Snowflake's doc on this ([here](https://docs.snowflake.com/en/sql-reference/sql/alter-database.html#parameters)) says the following on the `SWAP WITH` syntax:

> `SWAP WITH target_db_name`
> Swaps all objects (schemas, tables, views, etc.) and metadata, including identifiers, between the two specified databases. **Also swaps all access control privileges granted on the databases and objects they contain**. `SWAP WITH` essentially performs a rename of both databases as a single operation.

My understanding of this is that you would only need to grant the `reporter` role on the production database. Although.. I haven't tested this out yet :))",19
1369,"a-partial-workaround-for-dbt-utils-star-not-working-with-ctes","Since `dbt_utils.star` doesn't work on CTEs, I've made a wee macro which solves my most common need: pulling all but a couple of columns out of a CTE in the same model file. 

To use it, you need to set the columns in your `select` statement to a variable. Then you can pass it into the CTE as normal, but also pass it into the `star_cte` macro, which will split on commas (except in brackets) and then take the last element after splitting on spaces to get the column's name/alias.

A super-contrived example:
```
with organisations as (select * from {{ ref('organisations') }}),

transformed as (
  select 
  {%- set select_columns = ""
    org_id,
    'a' b,
    dateadd(month, 1, getdate()) as some_date,
    region_name
  ""-%}

  {{select_columns}}
  from organisations
),

final as (
  select {{ star_cte(select_columns, 'transformed', except=['org_id', 'b']) }}, count(*)
  from transformed
  group by 1, 2
)

select * 
from final
```

renders to:
```
with organisations as (select * from ""reporting"".""dev_jlabes"".""organisations""),

transformed as (
  select
    org_id,
    'a' b,
    dateadd(month, 1, getdate()) as some_date,
    region_name
  
  from organisations
),

final as (
  select   
        transformed.some_date, 
        transformed.region_name, count(*)
  from transformed
  group by 1, 2
)

select * 
from final
```

The macro itself is pretty brief: 
```
{%- macro star_cte(select_columns, cte_name = '', except=[]) -%}
    
    {%- set include_cols = [] -%}
    {%- set nsp = namespace(is_in_brackets = False) %} {# ""Move the variable's scope to be global"" #}

    {%- for col in select_columns.split(',') -%}
        {%- if ')' in col %} {# ""Argments to functions shouldn't be treated as column names"" #}
            {%- set nsp.is_in_brackets = False %}
        {%- elif '(' in col %}
            {%- set nsp.is_in_brackets = True %}
        {%- endif %}

        {%- set col_tidied = col.strip().split(' ') | last -%}

        {%- if col_tidied not in except and not nsp.is_in_brackets -%}
            {%- set _ = include_cols.append(col_tidied) -%}

        {%- endif -%}
    {%- endfor -%}

    {% for col in include_cols %}
        {% if cte_name | length > 0 %}
            {{- cte_name -}} .
        {%- endif -%}
        {{- col -}}
        {{- "", "" if not loop.last }}
    {%- endfor -%}

{%- endmacro -%}
```

It doesn't help if you're using ephemeral models upstream, but that use case doesn't apply to me. This just helps DRY up an otherwise-messy piece of our models.",1
1397,"how-do-you-manage-snowflake-privileges","Hello everyone! My company has been using Snowflake for about a year. The first few months we didn't pay a lot of attention to privileges. As more people started using Snowflake and as we started having more databases, schemas, and objects, it became difficult to manage b/c we didn't understand really how privileges in Snowflake are meant to work.

So after a little trial and error, we've come up with some hard-and-fast rules that help us keep Snowflake privileges simple and organized:
* Don't grant more than one role the ability to create objects in a single database.
* Every custom role has to eventually roll up to sysadmin.
* Never grant the database_owner roles to any other role except sysadmin.

There are lots of other things we do, but those are the most important. Some more details of how we've set things up can be found [here](https://trevorscode.com/toward-a-standard-model-for-snowflake-roles-and-privileges/). 

Our system is simple, but it works very well for us. I imagine there may come a time when it doesn't work. I'd welcome perspectives of when that might be. Perhaps others have use cases that require more sophisticated setups. What do those look like? I'm eager to learn and collect for the community some good strategies for making and keeping Snowflake privileges manageable.",1
1397,"how-do-you-manage-snowflake-privileges","Thank you for starting this convo. We had a similar experience where we learned a lot about Snowflake along the way (that's just being nice to ourselves :D )

Given this is a big topic and I'm no expert, I will just comment on your first bullet point. We think about this a bit differently. Albeit, most other things you say definitely rings true for us too.

We think about the creation/ownership of objects by schema across databsaes as well. So we have a role for general purpose loading into the RAW schemas and another role for transformation which is what dbt uses to create the objects in the STG and MART schemas.

We did this because we wanted dbt to have as little privileges as possible given we wanted as many people from the business to use dbt as possible.

This is actually similar to the design you linked. But I guess the main point is, we found it useful to have a higher privileged role ""Loader"" to be able to perform operations across databases. Then have a lower privilege role, ""Transformer"" that could own and create objects within that database.

This allows the Loader role to be much more powerful which gives a lot of flexibility upstream in the pipeline given all the various options of how data can be loaded into Snowflake.",2
1397,"how-do-you-manage-snowflake-privileges","Hi @robli, thanks for the response! If I understand correctly, you have a role `LOADER` that has the `CREATE TABLE` privilege in your raw schemas across many databases. Then your transformer probably has `SELECT ON FUTURE TABLES` in that schema. Do you have another role in your raw schemas that can create objects as well? If you do, then your transformer would have access to them and you wouldn't care who the owner was. Am I following?

I guess the only issue there would be if a human wanted to manually manage those tables - it would have to be a human that either switches between two roles, or has a composite role that encompasses the two. Which, if you follow rule 2, could just be sysadmin. I think that makes sense, especially since in the raw schemas or databases, you don't really care who created it and you don't change the schema - schema change is left up to whatever tool created it - i.e. fivetran, snowpipe loader, etc.

That makes sense. Thanks for the feedback. I'll have to think a bit about that and let it percolate a bit. Good comment.",3
1397,"how-do-you-manage-snowflake-privileges","Yeah you got it! :smiley: 

To summarise, that's right the TRANSFORMER role has read-only to future resources. Having a separate READONLY role that is granted to TRANSFORMER might be a good way to achieve it because a human can have the READONLY role as well to troubleshoot. So it's just another hop for some more flexibility.

You are right, we have a composite role. We have multiple layers of ADMIN because we don't want the lower tier ADMIN to have the powerful ""manage grants"" privilege which would allow it to transfer ownership or resources because we want a human to have this role but we don't want them transferring things around in the whole Account. Instead the lower tier ADMINs just have grants from the LOADER and TRANSFORMER which own all the tables/views/schemas they create + the READONLY. This should be plenty for a human admin (think developer).

A good outcome of this is that the LOADER role can't see the TRANSFORMER assets (i.e. STG and MARTs) which is nice because it could be a 3rd party tool/SaaS thing that has it like you mention. But if a human wants to do things, they can have the ADMIN role (and simply have the same access as owning the LOADER and TRANSFORMER's assets all in one).

Then you can have even higher tier ADMIN roles for tools like Terraform to automate all this for you.",4
1397,"how-do-you-manage-snowflake-privileges","I'll add a reply that focuses on management tooling, rather than policy. Here is a tool I do not yet have experience with but I'm planning to try very soon!

Permifrost is a python tool for managing permissions on a Snowflake data warehouse in a declarative fashion.

> Given the parameters to connect to a Snowflake account and a YAML file (a ""spec"") representing the desired database configuration, this command makes sure that the configuration of that database matches the spec.

https://gitlab.com/gitlab-data/permifrost/ 
https://pypi.org/project/permifrost/ 

Sample spec: https://gitlab.com/gitlab-data/permifrost/blob/master/tests/permifrost/core/permissions/specs/snowflake_spec.yml",5
1397,"how-do-you-manage-snowflake-privileges","Thanks for dropping the link to Permifrost! We're actively developing it (after a brief pause because I was on parental leave) and would love feedback, input, and contributions from the community. 

It's fairly opinionated about how to manage permissions, and it doesn't currently manage object creation/deletion, but it's working really well for our core use case now: clear understanding of permissions on existing objects.

Happy to answer any questions :slight_smile:",6
1397,"how-do-you-manage-snowflake-privileges","Good conversation.  A management pattern I use for Snowflake was inspired by @randy.  It introduces the concept of ""Object Access"" (OA) roles and ""Business Function"" roles.  OA roles are used for different types of access to objects.  Given a database called ""App"" you may have OA roles like this:

* OA_APP_CREATE_SCHEMA
* OA_APP_READ
* OA_APP_WRITE

These OA roles are only granted to the  ""Business Function"" (BF) roles.  Note: no users are granted to OA Roles.  Rather, a BF role represents a user group defined by a function in the business.  For example, you may have BF roles like:

* BF_APP_DATA_ENGINEER (member of the three OA roles above)
* BF_APP_DATA_ANALYST (only a member of the OA_APP_READ role granting read, but not write or create).

Users are only granted BF roles and BF roles are only granted OA roles.  Enforcing these rules is essential for a cohesive and predictable environment.  Adopting a consistent naming convention really helps to keep it all straight as well.

These concepts logically separate object access security from user access security by isolating the privileges into an OA or BF role.  The outcome is easier management of roles and privileges for the admin and fewer (or no) holes in the security architecture.",7
1401,"dbt-is-a-dwa-data-warehouse-automation-market-player","Hello everybody. 

As for now I have been working with DBT for several months on MS SQL Server. 

First of all, have to say it is absolutely amazing, just to name a few areas: 

* SQL templates on Jinja - beautiful and promising. Saw a lot of implementations of templates for SQL: SQL + customs scripts, SQL + SQL, SQL + C# etc. - always had a feeling that it does not fit well to each other. Jinja just looks like it always has been here 
* Graph tasks (DAG) management - is what we always needed
* Environments management/CI/CD/DevOps - the ideas of DevOps in analytics started to crystallize finally into something mature
* Integrated Data Catalog - when Gartner marked data catalog as an obsolete technology https://www.gartner.com/en/newsroom/press-releases/2018-09-11-gartner-hype-cycle-for-data-management-positions-three-technologies-in-the-innovation-trigger-phase-in-2018 it provided a lot of confusion about the future of data catalogs at all. Now it is just exciting to see working elements of Data Catalog maturity level 4 by Deloitte `https://www2.deloitte.com/content/dam/Deloitte/de/Documents/deloitte-analytics/Deloitte_Analytics_CatalogingData-Whitepaper.pdf` 
* Open Source (and the answer to that confusing question â€œhow to get it deployed?â€ - â€œpip installâ€) is a huge adoption catalyst
* And all that beauty is integrated of course together with logging, snapshots etc

The question is **why DBT is standing somehow apart from DWA (Data Warehouse automation) market although it solves a lot of tasks in the area**? 

For example, I could not find any mention of â€œDWAâ€ in the forum. And digging through the DWA market for years it was quite hard to figure out that DBT is actually doing a big part of the same job. 

A few links from the DWA market: 

`https://en.m.wikipedia.org/wiki/Data_warehouse_automation` 

DWA comparison tool 

https://www.datawarehouseautomation.guide/dwa-tools/ 

Most famous DWA market player 

`https://www.wherescape.com/` 

DWA solution Integrated with Microsoft Visual Studio: 

`https://www.dimodelo.com/` (it was one of my first attempts to use DWA tool in end-to end solution `http://sergeyavdovin.com/contents/resume/AdventureWorksDWHandReportingSolutionHROverview.pdf` ) 

Microsoft about DWA market 

`https://azure.microsoft.com/en-us/blog/accelerate-your-cloud-data-warehouse-with-automation-tools/` 

And sure, I do agree that the community is a big part of the DBT adoption. Worth to mention that my own adoption of DBT started when I found PSA (persistent staging) template discussion on the forum `https://discourse.getdbt.com/t/implementing-a-persistent-staging-area-with-a-dynamic-incremental-model/252`. 
So, I hope this post will help to grow the community a bit by extending it to the bigger DWA area. 

Cheers 
Sergey",1
1401,"dbt-is-a-dwa-data-warehouse-automation-market-player","Hey Sergey, Excellent post (sorry I am 2 years late :grinning:) 

As a former WS employee, couldn't agree more. Infact, we are getting plenty of inquiries from our NZ based clients as they get ready to adopt cloud and SaaS model to data warehouse and data transformation. dbt definately is the evolutionary next step. 

A bit of self promotion: we have built some materilizations that deliver the same functionality as Stage Tables, Datastores and Type 2 Dimensions and Facts/ other 3NF tables that can be found in WS.

happy to talk more to the community, if there's an interest

Peeyush",2
1423,"using-get-column-values-with-ctes","**Using: dbt 0.17.0 with BigQuery**

I'm new to dbt and looking to how to better incorporate macros from `dbt_utils` into my workflow. In particular, I'm interested in using `pivot` and `get_column_values` but I have been unsuccessful so far in incorporating it in my model code with CTEs defined in the same model. Here's what I have done so far. 

I have taken the [example](https://github.com/fishtown-analytics/dbt-utils/blob/dev/0.18.x/macros/sql/pivot.sql) from github and created a new model like so: 

-- example_model.sql

```sql
with data as (
    SELECT 'S' as size, 'red' as color
    UNION ALL SELECT 'S' ,'blue'
    UNION ALL SELECT 'S', 'red'
    UNION ALL SELECT 'M', 'red'
),
SELECT * FROM data
```
Then in another file I have the following: 
```sql
SELECT
    size,
    {{ dbt_utils.get.pivot('color', dbt_utils.get_column_values(ref('example_model'), 'color')) }}
FROM ref('example_model')
GROUP BY size
```
So far so good and this works as expected. 

However, my problem is that I don't actually want to create a new model for the first table -- it's much too small and not used for anything else (as are a lot of other temporary/CTE tables that I create). Thus, what I really want to be able to do is something like this (all in the same model file): 

```sql 
with data as (
    SELECT 'S' as size, 'red' as color
    UNION ALL SELECT 'S' ,'blue'
    UNION ALL SELECT 'S', 'red'
    UNION ALL SELECT 'M', 'red'
),
SELECT
    size,
    {{ dbt_utils.get.pivot('color', dbt_utils.get_column_values(data, 'color')) }}
FROM data
GROUP BY size
```

However, I can't seem to get this work. If I use `data` (no quotes), I get a `'data' is undefined error`; if I use `'data'` I get an `'str object' has no attribute 'database'` error. 

Thus my question is: what is the right way to refer to `data` such that I can use it in `dbt_utils.get_column_values` and without creating a new model in the process?

Thanks for your help",1
1423,"using-get-column-values-with-ctes","Hi there! Unfortunately `get_column_values` is not compatible with CTEs, as it relies on the object you pass it being a table or view in the database!



Here's some workarounds, in order of preference
1. If you're using Snowflake, use the built-in [pivot function](https://docs.snowflake.com/en/sql-reference/constructs/pivot.html) instead
2. Your first option works! You can keep this as a separate model! By the way, have you considered converting this to a [seed](https://docs.getdbt.com/docs/building-a-dbt-project/seeds) instead? That way you can store these values in a CSV. 
3. You can write your own macro, `get_column_values_from_query`, which does the same thing as `get_column_values` but doesn't depend on being passed a table or view.

I've included some sample macro code, but this is a huge tradeoff â€” is it really worth introducing all this code when you could just have a separate model (or seed!) for the CTE? Anyway....

```
{% set data_query %}
SELECT 'S' as size, 'red' as color
UNION ALL SELECT 'S' ,'blue'
UNION ALL SELECT 'S', 'red'
UNION ALL SELECT 'M', 'red'
{% endset %}

with data as (
    {{ data_query }}
)

SELECT
    size,
    {{ dbt_utils.pivot('color', get_column_values_from_query(data_query, 'color')) }}
FROM data
GROUP BY size

```
Note that I had to `set` a value for `data_query` â€” earlier you were using `data` as an argument, but there wasn't a variable named `data` in the Jinja context, so it would have been evaluating to `None`.

From here, you'll need to write your own macro â€“ here's some starter code. It works, but it isn't pretty

```
-- macros/get_column_values_from_query.sql
{% macro get_column_values_from_query(query, column) -%}

{#-- Prevent querying of db in parsing mode. This works because this macro does not create any new refs. #}
    {%- if not execute -%}
        {{ return('') }}
    {% endif %}

    {% set column_values_sql %}
    with cte as (
        {{ query }}
    )
    select
        {{ column }} as value

    from cte
    group by 1
    order by count(*) desc

    {% endset %}

    {%- set results = run_query(column_values_sql) %}
    {{ log(results, info=True) }}
    {% set results_list = results.columns[0].values() %}

    {{ log(results_list, info=True) }}
    {{ return(results_list) }}

{%- endmacro %}

```",2
1423,"using-get-column-values-with-ctes","Hi Claire, 

Thats what I thought, but thank you for the confirmation! 

You raise a good point about the tradeoffs -- I'll definitely go back and take a look and see if some of them should just be a separate model and seed. Having said that, I do appreciate you taking the time to write some starter code! In another attempt I also tried to use `run_query`, but I can now see how I used it incorrectly. 

One question about the code. Could you explain why you included this block?

```python
{%- if not execute -%}
        {{ return('') }}
{% endif %}
```
Is this related to the comment you included above about ""Prevent querying of db in parsing mode""? If so, what exactly does that mean?

Thanks again!",3
1436,"creating-an-erd-for-your-dbt-project","While the [auto-generated documentation](https://docs.getdbt.com/docs/building-a-dbt-project/documentation/) for your dbt project ships with a DAG view, sometimes, you really want to see an Entity Relation Diagram (ERD) of your project instead.

*DAG of project:*
The DAG shows the lineage of data.

![Screen Shot 2020-07-20 at 1.05.40 PM|690x178, 75%](upload://j6t1m1TPLjWRwd4s5ul4ZGbraKb.png) 

*ERD of project*
An ERD shows how the tables relate to each other
![Screen Shot 2020-07-20 at 1.08.03 PM|690x396, 75%](upload://1zGjAJDhOTaVT46ah1EyWqtADLU.png) 


At present, there's no built-in way to do this in dbt, but we have an [open issue](https://github.com/fishtown-analytics/dbt-docs/issues/84) here for the feature request.

So, how can you create an ERD outside of dbt?

**Code libraries:**
- [SchemaSpy](https://github.com/schemaspy/schemaspy): ""Tangentially related for transactional DBs â€” highly recommend this project for Postgres, MySQL, and other relational DBs to auto generate navigable ERDs to a static website. I run this once a quarter on our Postgres DB and publish to an S3 bucket."" â€” Sean McIntyre
- [ERAlchemy](https://pypi.org/project/ERAlchemy/) (thanks @abelsonlive)


**SaaS tools:**
- [dbdiagram](https://dbdiagram.io/d/5c79327af7c5bb70c72f2a11): Write relationships in code, that then get turned into a diagram.
- [SqlDBM](https://sqldbm.com/Home/): ""SqlDBM is excellent and has reverse engineering from numerous databases, including Snowflake"" â€” @gordonwong)

 **UI SaaS tools** (i.e. manually drawing these diagrams)
- LucidChart (thanks @Andrea and @tmurphy)  
- Visio (thanks Jeff A)

A few words of warning though:
1. ERD generators may not work with your analytics database. As @josh explains:

>I think a lot of these tools work fairly well if the PK/FK relationships are stated in the database. And in a well-engineered OLTP system this is normally done. It makes a lot less sense to put the relationships in a Snowflake or a Redshift, because for the most part they are not enforced and it is not actually possible to enforce them. So itâ€™s not nearly as common to do so. But the downside of that is that the tools like ERD drawers and BI tools have a lot harder time than deducing the table relationships.

2. Separating your documentation from your codebase means one will always lag behind the other. As @Andrea writes:
> I did make ERDs with Lucidchart for the data mart - didnâ€™t add it to dbt but to the self-service how-to documentation I created in the BI tool we were using. I only did it for 5 rock solid essential time tested star schema tables that I knew I wouldnâ€™t have to worry about editing a lot.  Everything people are saying about maintaining documentation divorced from code absolutely applies here - check out [this post](https://locallyoptimistic.com/post/data_dictionaries/) on Locally Optimistic about â€œWhy you donâ€™t need a data dictionaryâ€ - it delves into some of the problems of documenting data. 

If you have a tool you use for ERDs, feel free to add it in the comments! I also suspect there's a way to parse the `manifest.json` file and use a language like dbdiagram's [database markup language](https://www.dbml.org/home/) to turn the relationships in your project into entity-relationships!",1
1436,"creating-an-erd-for-your-dbt-project","I've been working with more folks who use Collibra and Alation. How they are creating ERDs is exactly what Claire referred to, by using the manifest.json file. The field you want is `parent_map` which provides a list of the nodes (the models) and the nodes that they depend on.",2
1436,"creating-an-erd-for-your-dbt-project","Snowflake users should be defining PKs on tables where possible, even if these aren't enforced it's a good practice as helps other users (and, as mentioned, various data tools) interpret your data model.",3
1436,"creating-an-erd-for-your-dbt-project","We use Oracel SQL Data Modeler with our snowflake star-schema designs. I actually do go through the painful process of documenting all columns, tables and even relationships (constraints) even though they are ""documentation"" as far as Snowflake is concerned. My main motivation is to retain our metadata. Modeling tools (and even tools like dbt) come and go but metadata is forever (sort of). I would say that a good 50% of the comments on our columns/tables today was originally part of our early LAMP installation from MySQL. Lots of reverse-engineering/forward-engineering has kept our documentation intact even as we migrated databases, modeling tools and integration platforms. The Information Schema is at least a (sort of) standard that can preserver your work.",4
1436,"creating-an-erd-for-your-dbt-project","ðŸ‘‹ I (quickly) put together a ruby script to generate a diagram on dbdiagram.io. It pulls data from:
* `schema.yml` to get models and associations (via `tests.relationships.to`)
* `catalog.json` to get columns.

It's available as a gist: https://gist.github.com/pcreux/b2e4a288b272fb17a36d319734fbb8ee.",5
1436,"creating-an-erd-for-your-dbt-project","I haven't tested this out, but conceptually this is very cool!",6
1436,"creating-an-erd-for-your-dbt-project","I've similarly created a PowerShell script that pulls the data from `manifest.json` and outputs a PlantUML diagram: https://gist.github.com/panasenco/31d4c12f0cadce91f576a818018970a0",7
1436,"creating-an-erd-for-your-dbt-project","How would you do the ""pull data from schema.yml to get models and associations"" part in the case where there are multiple levels of model property .yml files?  Looks like your script assumes it's stored in a single schema.yml file

Below quote from: https://docs.getdbt.com/reference/model-properties

> You can name these files `whatever_you_want.yml` , and nest them arbitrarily deeply in subfolders within the `models/` directory.",8
1436,"creating-an-erd-for-your-dbt-project","Hey folks, I just published CLI for generating ERD-as-code DBML from manifest json:
https://github.com/datnguye/dbterd

Check it out!
Feel free to let me know for any questions or requests.
Cheers",9
1445,"an-approach-to-building-dbt-packages","In late May, Kristin Bagnall, Fivetranâ€™s [dbt](https://www.getdbt.com/) Package Lead, gave an office hours session focused on how to write and distribute dbt packages for the dbt platform. Weâ€™ll look at Kristinâ€™s session and also focus on the numerous Fivetran dbt packages that have been published to date.

https://www.youtube.com/watch?v=lnOYHGaOKwg&list=PL0QYlrC86xQnxYB7LfyMEd5S63bYF8mZN

## What is a dbt Package?

Packages are dbt projects that you can install in your own dbt project.

There are two main use cases for packages.

First, weâ€™ll start with common staging models for a single data source. An example of this is Fivetranâ€™s [dbt package for Netsuite](https://hub.getdbt.com/fivetran/netsuite/latest/). Models are great for one particular source data or for sources that have industry-standard metrics or any code that analysts are re-creating

Secondly, we have packages that serve as macros, such as [dbt-utils](https://hub.getdbt.com/fishtown-analytics/dbt_utils/latest/), which will help you with any complex SQL querying. So for example, in [dbt-utils](https://github.com/fishtown-analytics/dbt-utils), thereâ€™s a macro that you can pivot some data from rows to columns, which is quite helpful.

To see all available dbt packages - be sure to check out the [dbt Hub](https://hub.getdbt.com/)
![hub screenshot|690x368](upload://uj3M3lqMNRJyQ9gNpTDHrILciir.png)
## How We Write Our dbt Packages

Letâ€™s walk through the steps we follow at Fivetran for building a new dbt package from scratch.

### Research Customer Needs

The first step is to interview the stakeholders of the source data and find out:

* What theyâ€™re doing with their data
* How theyâ€™re modeling it
* Any pain points theyâ€™ve encountered when modeling data.

In our case, at Fivetran, the whole goal of creating these packages is to make customers' and analysts' lives easier so that they can better understand their data and get answers quicker.

### Understand source application and schema

Once weâ€™ve gained enough feedback, we try to imagine what can be created with a data source that involves little customization with the intent to generate the query in a way that it will work accurately for everyone, so that's pretty important.

So generally when we talk to customers, if they have some customization and they're building this really specific model, it's not a good candidate for a package model. It's generally something that they'll have to write on their own.

### Draft Queries and Test

The most time-intensive part of the process is drafting queries and testing those queries. Itâ€™s important to QA the queries as much as possible and ensure that resulting data matches whatâ€™s in your reporting tool, if itâ€™s relevant to your package and process.

### Build The Package

Now, itâ€™s time to convert SQL queries to models. In our case, we use dbt-utils on all of the packages so far for the cross-database macros. All of the packages are compatible across Redshift, BigQuery, and Snowflake. To make this possible, we utilize some of the macros that Claire and the Fishtown team have built.

### Add Documentation

Lastly, but we add documentation to all of the source tables that are coming in through Fivetran and also the transformed models. In our case, we add descriptions to tables, columns and so on.

### Test, Launch and Solicit Feedback

We also add testing, so for example, dbt has a non-null and a unique test you can run on any column of data and we do this for all primary keys. We do that for all the primary keys, and they also have a couple of other tests that are pretty helpful.

Once weâ€™re done, we ping the dbt Slack channel and gain beta testers. Through dialogue and testing, we make revisions and ultimately post it on the dbt Hub.

## Looking At Fivetranâ€™s dbt Packages

Fivetran has published numerous dbt packages and will continue to bring additional resources for our customers. Our packages will take care of basic table standardization, add data tests and enable you to start building visualizations immediately.

So far, Fivetran has built and published the following packages:

* Fivetran [dbt package for NetSuite](https://fivetran.com/blog/fivetran-dbt-NetSuite)
* Fivetran [dbt package for Mailchimp](https://fivetran.com/blog/fivetran-dbt-Mailchimp)
* Fivetran [dbt package for Salesforce](https://fivetran.com/blog/fivetran-dbt-Salesforce)
* Fivetran [dbt package for GitHub](https://fivetran.com/blog/fivetran-dbt-github)
* Fivetran [dbt package for Marketo](https://fivetran.com/blog/fivetran-dbt-marketo)
* Fivetran [dbt package for Asana](https://fivetran.com/blog/fivetran-dbt-package-for-asana)

## Get Started Today

To learn more about package building and Fivetranâ€™s approach, watch Kristinâ€™s full dbt Office Hours session. 

https://www.youtube.com/watch?v=lnOYHGaOKwg

We welcome you to get going and try these packages for yourself!",1
1445,"an-approach-to-building-dbt-packages","Just want to piggy back here and highlight a couple of wonderful community packages: 

* At GitLab, we put together a package to help provide visibility into our Snowflake costs. The package is here: https://gitlab.com/gitlab-data/snowflake_spend You can see it live in our dbt project: https://gitlab.com/gitlab-data/analytics/-/tree/master/transform/snowflake-dbt @tmurphy and I welcome any MRs to make it better! 
* @claus has put together a number of awesome dbt packages including:
   * dbt-extend: https://github.com/calogica/dbt-extend
   * dbt-date: https://github.com/calogica/dbt-date
   * dbt-test: https://github.com/calogica/dbt-test
* I don't know the best person to tag, but I've heard about people using the dbt_ml package to work with BigQuery with great success: https://github.com/kristeligt-dagblad/dbt_ml (This one is on dbt hub, but extra shoutout since it is community contributed.)

I think one of the best things about an open source project is that the community can contribute to it in multiple ways, including these, which is less about contributing to dbt _directly_ and about contributing to how you use it. 

Are there any other dbt packages floating out there that aren't on dbt hub?",2
1445,"an-approach-to-building-dbt-packages","@emilie thanks for reminding me that I need to get these packages cleaned up and better documented. Too many work projects interfering with the side projects! :slight_smile:",3
1478,"build-snapshot-based-tests-to-detect-regressions-in-historic-data","Every morning your CEO opens up their favorite dashboard. But one particular Monday, she finds that all of the historic revenue figures have changed! Suddenly, your Slack is filling up with messages asking what went wrong! And honestly, you're not sure at first â€” all of your `dbt test` steps passed earlier today, how could things have changed? Turns out, that one of the upstream teams has changed possible classifications which your model and tests didn't account for properly, and they applied those changes retroactively for the entire year.

You find the root-cause and fix it, however, you spent a couple of hours debugging what has happened, and your CEO is not particularly happy with you and seems to be growing suspicious of whether you really know what you're doing. No one wants to be there, right?

While dbt's schema tests are extremely useful, sometimes you need an extra layer of assurance that nothing is broken to get ahead of these issues. In this write-up, we're going to share how Zoox uses dbt's snapshots to detect when historic data changes as the result of execution of your pipeline, so that our data-team is the first one to know about those issues and can be proactive instead of reactive.

**Note:** We're assuming you are familiar with snapshots â€” if you're not, go read [this article](https://blog.getdbt.com/track-data-changes-with-dbt-snapshots/) about them first.

## The basic idea

We use dbt snapshots to detect changes in historic values for our rollup metrics and leverage the way snapshots invalidate the records that have changed for our test. A minimum working project that implements the whole workflow can be found [here](https://github.com/fishtown-analytics/dbt-labs-experimental-features/tree/master/snapshot-testing) (huge shoutout to [Claire](https://discourse.getdbt.com/u/claire/summary) for putting it together!). Feel free cloning and trying it out!

Now let's go through the logic in more detail.

## Test implementation

Lets say we have a `fct_orders` table:


| order_id | customer_id | order_date | amount |
|----------|-------------|------------|--------|
| 1        | 1           | 2020-07-01 | 10     |
| 2        | 3           | 2020-07-01 | 20     |
| 3        | 94          | 2020-07-02 | 1      |
| 4        | 50          | 2020-07-03 | 25     |
| 5        | 64          | 2020-07-03 | 17     |
| 6        | 54          | 2020-07-04 | 6      |


In our project, we build a snapshots on top of our most important models (like `fct_orders`) that roll the table up to one record per day. For this rollup query, assume that the order can be refunded/altered within the next calendar day, and that will alter the order amount. Hence, the revenue for orders that is older than 2 days should never change, right?

```sql
{% snapshot historic_revenue_snapshot %}

    {{
        config(
          target_schema='dbt_snapshots',
          strategy='check',
          unique_key='date_day',
          check_cols=['total_revenue']
        )
    }}

    select
        order_date as date_day,
        sum(amount) as total_revenue
    from {{ ref('fct_orders') }}
    where order_date < current_date - interval '1 day'

    group by 1

{% endsnapshot %}
```

After our first `dbt snapshot`, it looks something like this:


| date_day   | total_revenue | dbt_valid_from | dbt_valid_to |
|------------|---------------|----------------|--------------|
| 2020-07-01 | 30            | 2020-07-15     | null         |
| 2020-07-02 | 1             | 2020-07-15     | null         |
| 2020-07-03 | 42            | 2020-07-15     | null         |
| 2020-07-04 | 6             | 2020-07-15     | null         |


Each day, we run this command, and since our historic daily revenue never changes, the `dbt_valid_to` field always should always be `null`. Right? Nothing ever goes wrong when it comes to data.

Except one day, something does go wrong. Somehow,  `order_id = 4` got filtered out, so our revenue for `2020-07-03` drops from `42` to `17` (it's an illustrative example so let's not get hang up on the feasibility of a record disappearing).

When we run our `dbt snapshot` command we can see that this historic value has changed â€” dbt has invalidated the old record, and created a new record for the affected day.


| date_day       | total_revenue | dbt_valid_from | dbt_valid_to   |
|----------------|---------------|----------------|----------------|
| 2020-07-01     | 30            | 2020-07-15     | null           |
| 2020-07-02     | 1             | 2020-07-15     | null           |
| **2020-07-03** | **42**        | **2020-07-15** | **2020-07-16** |
| **2020-07-03** | **17**        | **2020-07-16** | **null**       |
| 2020-07-04     | 6             | 2020-07-15     | null           |


So, the concept here is **we can add an `is_null` test to the `dbt_valid_to` column to detect when a metric has changed.**

We can do this with a [data test](https://docs.getdbt.com/docs/building-a-dbt-project/tests/#data-tests), or by creating a [custom schema test](https://docs.getdbt.com/docs/guides/writing-custom-schema-tests) and applying it to the column.

In case if you go down the path of schema test, that's how your `schema.yml` in `snapshots` folder would look like:

```yaml
version: 2

snapshots:
  - name: historic_revenue_snapshot
    columns:
      - name: dbt_valid_to
        tests:
          - is_null # you need to create this test in your project
```

Then when you run `dbt test` you will see the following output:

```sql
$ dbt test
Running with dbt=0.17.1
Found 0 models, 1 test, 1 snapshot, 0 analyses, 137 macros, 0 operations, 1 seed file, 0 sources

19:06:32 | Concurrency: 8 threads (target='redshift')
19:06:32 |
19:06:32 | 1 of 1 START test is_null_historic_revenue_snapshot_dbt_valid_to..... [RUN]
19:06:34 | 1 of 1 FAIL 1 is_null_historic_revenue_snapshot_dbt_valid_to......... [FAIL 1 in 1.96s]
19:06:34 |
19:06:34 | Finished running 1 test in 4.50s.

Completed with 1 error and 0 warnings:

Failure in test is_null_historic_revenue_snapshot_dbt_valid_to (snapshots/schema.yml)
  Got 1 result, expected 0.

  compiled SQL at target/compiled/acme/snapshots/schema.yml/schema_test/is_null_historic_revenue_snapshot_dbt_valid_to.sql

Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
```

*(The code for the custom schema test can be found [here](https://github.com/fishtown-analytics/dbt-labs-experimental-features/blob/master/snapshot-testing/macros/test_is_null.sql))*

Now, when any historic records change, we'll be proactively notified about it!

## Healing the test

Now that we have a rogue record, our `is_null` test is going to fail every single time we run `dbt test`, even though we might have already fixed the issue.

To make our test ""self-healing"", we have an extra [operation](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations/#operations) in our project to move invalidated records into an audit table.

You can find the full code [here](https://github.com/fishtown-analytics/dbt-labs-experimental-features/blob/master/snapshot-testing/macros/historic_revenue_snapshot_cleanup.sql), but the idea is that you want to insert the invalidated version of the record into a different table, and delete it from the original table:

```sql
INSERT INTO dbt_snapshots.historic_revenue_snapshot_invalidated (
            SELECT
                *,
                CURRENT_TIMESTAMP as _inserted_at
            FROM {{ ref('historic_revenue_snapshot') }}
            WHERE dbt_valid_to IS NOT null
        );

DELETE FROM {{ ref('historic_revenue_snapshot') }} WHERE dbt_valid_to IS NOT null
```

The `historic_revenue_snapshot_invalidated` contains all records that have caused an error, while the `historic_revenue_snapshot` table no longer has records that will fail the `is_null` test:


| date_day       | total_revenue | dbt_valid_from | dbt_valid_to   |
|----------------|---------------|----------------|----------------|
| 2020-07-01     | 30            | 2020-07-15     | null           |
| 2020-07-02     | 1             | 2020-07-15     | null           |
| **2020-07-03** | **17**        | **2020-07-15** | **null**       |
| 2020-07-04     | 6             | 2020-07-15     | null           |


So failures will only cause an error once, and it's extremely easy to debug any past failures thanks to the `_invalidated` table (it's a good practice to additionally have snapshots of your core tables to make debugging even easier, i.e. so that you will be able to zero in on exact record that caused you trouble, even if you're not running any tests on them).

## Putting it all together

Running this in production does get a little complex â€” we chose to create one job per important model that covers all the steps.

```
$ dbt run-operation historic_revenue_snapshot_cleanup
$ dbt run -m +fct_orders
$ dbt snapshot --select historic_revenue_snapshot
$ dbt test -m historic_revenue_snapshot
```

By adding this to your production run of dbt, you can get ahead of any mistakes, building trust with your stakeholders in the process!",1
1478,"build-snapshot-based-tests-to-detect-regressions-in-historic-data","Hey @evgeny.r thanks so much for this inspiring article!

We actually created a regression testing setup based on it and just published a [Medium article](https://medium.com/agrando-engineering/build-trust-in-your-data-with-regression-tests-2cb45154c4bf) of our approach. 
Also referring back to this post since you got us introduced to the solution. Big thanks for that!


Best,
Thomas",2
1478,"build-snapshot-based-tests-to-detect-regressions-in-historic-data","@evgeny.r  I have one query regarding using snapshot for fact tables.

Snapshot uses SCD-2, which makes me think this is for tracking dimensions and not facts.

Do you think this will scale well, or have you seen it scale well for large data. I am talking about hourly pipelines for 20 or 50+ api sources that are bringing in order transactions data on lets say a black friday.

Snapshots on fact table makes me think we are trying to apply a dimension technique (that too a slowly changing one) for a change data capture on facts that are incoming in bulk and changing way faster than dimensions would.

@somtom I checked your article too and realized you were snapshotting summary facts (sessions) which are surely not going to be huge compared to actual sessions data.

Do you think snapshotting is a viable option for stagged data (stg_sessions) and not modelled data (fct_sessions). Or will it become a performance bottleneck. 

Any help would be appreciated. :slight_smile:",3
1478,"build-snapshot-based-tests-to-detect-regressions-in-historic-data","@urwa when snapshotting staged data there might be different things to consider when it comes to performance concerns or the questions whether you need them at all:

* Can you snapshot it properly at all? Do you for instance have a unique key or do you have an EL job that appends newly or changed records? If the latter, a snapshot might not make too much sense since your EL job is kinda doing snapshots from the source.
* Would you expect the historical staging data to change at all? If not you might not need snapshots. If historical data does not change, then your snapshot table should not be much bigger than the original staging table itself.
* ..

Those are just some quick thoughts that came to my mind.

In our case, as you correctly wrote, we are snapshotting historical summary stats - mostly on modeled fact tables. Thus, we can check that they don't change. The main reasons for changes are that certain dimensions are adjusted in the source or that we do code changes that have side effects. We did not yet use snapshotting for staging tables, though.",4
1487,"writing-packages-when-a-source-table-may-or-may-not-exist","Note â€” this article is intended for:
- anyone that writes dbt modeling packages (likely a consultant or vendor)
- OR anyone who likes seeing fancy things done with dbt ;) 

In other words: you probably don't need this.

## Scenario
Let's say you're a consultant that's working with a number of merchants that use the same ecommerce software â€” **Shopiary**â„¢. Your clients replicate the data into their warehouses using their favorite EL tool, and most of the time, the tables (and columns within the tables) are the same for each client â€” with one exception. Some stores implement a rewards program, and have a corresponding `rewards_status` table, whereas other stores do not.

You decide to write a dbt package to re-use across your clients (and open source it so the community can benefit too ;) ), **but** you need to figure out a way to handle the fact that sometimes the `rewards_status` table exists, and sometimes it doesn't.

## Part 1: Figuring out if a table exists in your warehouse
As someone who keeps up with the latest dbt developments, you're using [sources](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources/) in your dbt package. So you add the `rewards_status` table to your `shopiary` source:

```yaml
version: 2

sources:
- name: shopiary
  tables:
    - name: customers
    - name: orders
    - name: payments
    - name: rewards_status # we're not sure if this exists for all users of this package
    
```

An important note, it's probably a good idea to remove any `freshness` checks on this table (otherwise you'll get an error):
```yaml
    ...
    - name: rewards_status 
      freshness: null

```

In your downstream model (say, `stg_rewards_status`) you know the first thing you need to do is figure out a way to check if this table _actually_ exists. If we naÃ¯vely write logic like this:
```
{% set table_exists=source('shopiary', 'rewards_status') is not none %}

{% if table_exists %}

{{ log(""Table exists"", info=True) }}

{% else %}

{{ log(""Table does not exist"", info=True) }}

{% endif %}

```
dbt will return `table_exists=true`, since we're actually checking for the existence of the Source in your dbt project, rather than the existence of the table in your database (this is very nuanced).
```
$ dbt compile
Running with dbt=0.17.1
Table exists
Found 6 models, 25 tests, 0 snapshots, 0 analyses, 268 macros, 0 operations, 0 seed files, 4 sources

15:03:57 | Concurrency: 1 threads (target='learn')
15:03:57 |
Table exists
15:03:58 | Done.

```

So we need to do something slightly different. We can use the [`get_relation` macro](https://docs.getdbt.com/reference/dbt-jinja-functions/adapter#get_relation) to instead check your database instead of your dbt project:

```
{%- set source_relation = adapter.get_relation(
      database=source('shopiary', 'rewards_status').database,
      schema=source('shopiary', 'rewards_status').schema,
      identifier=source('shopiary', 'rewards_status').name) -%}

{% set table_exists=source_relation is not none %}

{% if table_exists %}

{{ log(""Table exists"", info=True) }}

{% else %}

{{ log(""Table does not exist"", info=True) }}

{% endif %}

```

```
$ dbt compile
Running with dbt=0.17.1
Table does not exist
Found 6 models, 25 tests, 0 snapshots, 0 analyses, 268 macros, 0 operations, 0 seed files, 4 sources

15:07:00 | Concurrency: 1 threads (target='learn')
15:07:00 |
Table does not exist
15:07:01 | Done.
```
Cool! We can set a variable that tells us ""this table doesn't exist"".


Let's quickly check that it works for the positive case too (i.e. this isn't a false negative) â€” we _know_ `customers` exists for everyone:

```
{%- set source_relation = adapter.get_relation(
      database=source('shopiary', 'customers').database,
      schema=source('shopiary', 'customers').schema,
      identifier=source('shopiary', 'customers').name) -%}

{% set table_exists=source_relation is not none  %}

{% if table_exists %}

{{ log(""Table exists"", info=True) }}

{% else %}

{{ log(""Table does not exist"", info=True) }}

{% endif %}

```

```
$ dbt compile
Running with dbt=0.17.1
Table does not exist
Found 6 models, 25 tests, 0 snapshots, 0 analyses, 268 macros, 0 operations, 0 seed files, 4 sources

15:08:09 | Concurrency: 1 threads (target='learn')
15:08:09 |
Table exists
15:08:10 | Done.
```

Huh, weird, it said it didn't exist, and then it did exist.... what gives?

## Part 2: Understanding the difference between the `parse` time and `run time`

When you execute `dbt compile` or `dbt run`, dbt parses your project twice:
* **First parse:** dbt reads all of the files in your project and generates a ""manifest"" comprised of models, tests, and other graph nodes present in your project. During this phase, dbt uses the  `ref`  statements it finds to generate the DAG for your project.  **No SQL is run during this phase**.
* **Second parse:** dbt compiles (and runs) each node (eg. building models, or running tests).  **SQL is run during this phase**

If you've ever used the [`execute` macro](https://docs.getdbt.com/reference/dbt-jinja-functions/execute/), you might have come across this behaviour

So, the reason customers ""does not exist"" and then suddenly ""exists"" is because:
* **First parse:** no SQL run, so the `source_relation` gets returned as `none`, resulting in a false negative.
* **Second parse:** This time dbt actually runs SQL against the `information_schema` of your database to check if the table exists. The `source_relation` gets returned when it exists.

OK, so maybe we can just use the results of the second parse to enable or model?

## Part 3: ""Enabling"" and ""disabling"" models based on whether the table exists (ðŸ™…â€â™€ï¸)

It may seem like a reasonable way to handle this optional table is to use the [`enabled` config](https://docs.getdbt.com/reference/resource-configs/enabled/) to turn models on and off based on the existence of the table.

This might look something like:
```
{%- set source_relation = adapter.get_relation(
      database=source('shopiary', 'rewards_status').database,
      schema=source('shopiary', 'rewards_status').schema,
      identifier=source('shopiary', 'rewards_status').name) -%}

{% set table_exists=source_relation is not none  %}

{{ config(enabled=table_exists) }}

select ...
```
Unfortunately, this is not possible:
- dbt needs to know if a model is enabled at parse time, and since we're relying on querying the database to check if a model is exists (i.e run time), things are gonna get hairy!
- Even if there _were_ a workaround (maybe there is â€” I gave up on this approach when I got to here Â¯\_(ãƒ„)_/Â¯), this introduces complexity in any downstream models that `ref` the `stg_rewards_status` model â€” you'd need to selectively enable/disable that part of your code :grimacing: 

## Part 4: Fake it 'til you make it :sunglasses: 

Fortunately, there is a cool workaround here!
- Check if the table exists, using some of the code we just wrote
- If it exists, write the transformation the way you intend to
- If it doesn't, create a dummy, empty table instead with the exact same column names. Make sure you cast your `null`s to the right data-types in case they need to match datatype in downstream joins.

```sql
{# Not all stores have a concept of a rewards status, so it won't exist in every deployment #}

{%- set source_relation = adapter.get_relation(
      database=source('shopiary', 'rewards_status').database,
      schema=source('shopiary', 'rewards_status').schema,
      identifier=source('shopiary', 'rewards_status').name) -%}

{% set table_exists=source_relation is not none   %}

{% if table_exists %}

select
    id as reward_id,
    customer_id,
    tier

from {{ source('shopiary', 'rewards_status') }}

{% else %}

select
    null::integer as reward_id,
    null::integer as customer_id,
    null::integer as tier

-- this means there will be zero rows
where false

{% endif %}

```

Now, you can `ref` the model, and join to the `stg_rewards_status` table, just like any other model â€” `null` values will just flow though instead of real values. If your client later starts syncing the `rewards_status` table, the information will flow through as expected!

```sql
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        rewards_status.tier as rewards_tier -- this will just be null if the table doesn't exist

    from {{ ref('stg_customers')}} as customers
    left join {{ ref('stg_rewards_status') }} as rewards_status using (customer_id)
```

:sunglasses: :sunglasses: :sunglasses:

## Part 5: Other options!

There's always more than one way to peel a potato. You could also investigate:
- Abstracting the `table_exists` logic into a macro â€” a great idea if there are multiple optional tables!
- Creating the source table in the warehouse (this probably looks something like running a manual `create table` statement, using the role used by your EL tool). More complexity before getting to dbt, but keeps the package tidy.
- Having a user defined [variable](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-variables/) in your `dbt_project.yml` for whether the models related to `rewards_status` are enabled or not, and leaving it up to your user to set it to `true` or `false`. More configuration, but less code in your dbt project.",1
1487,"writing-packages-when-a-source-table-may-or-may-not-exist","I had trouble applying this to external tables in redshift. Turns out, as of this posting, the get_relation call does not include external tables in redshift.  There is an open issue for this [https://github.com/fishtown-analytics/dbt/issues/2289](https://github.com/fishtown-analytics/dbt/issues/2289) which includes a potential work around.",4
1528,"testing-incremental-models","I love how dbt allows me to test schema and data :heart:, but there's one thing I haven't figured out - it's testing of incremental models.

My typical CI test run looks like this

    dbt deps --profiles-dir $DBT_PROFILE_DIR
    dbt seed --profiles-dir $DBT_PROFILE_DIR
    dbt run  --profiles-dir $DBT_PROFILE_DIR
    dbt test --profiles-dir $DBT_PROFILE_DIR

That works well for the `--full-refresh` case, which is basically what CI does, since it always run in a clean environment and DB. But the `--full-refresh` case is rare in production and what runs every day is processing incremental changes, which is left untested.

Not sure if it's relevant, but I use static data in my seeds to cover all edge cases. So the increment isn't always ""today"", but rather some static date in the past.

What would be a good approach to correctly test full + incremental transformations?

Cheers!",1
1528,"testing-incremental-models","
One thing that I've found makes this testing easier is to tag each row to identify whether it was built by your full-refresh or by a subsequent incremental.   You can do that with the is_incremental() function.

To make use of that, you can just add a column to each incremental table:   ""build_source"", like so:

{% if is_incremental() %}
   'incremental'     
{% else %}
   'full'
{% endif %}                       as ""build_source"",

When your model is built with --full-refresh (full build).  and then without it (incrementally), the rows will either have 'incremental' or 'full' as the value of column ""build_source"".

Once the column is populated, you can make use of that in your asserts and other test queries to see what the incremental did, like:

	select * from inventory
	where build_source = 'incremental'
	order by inventory.item_id, updated_at

... and of course you can write more sophisticated queries to test different conditions explicitly.",2
1528,"testing-incremental-models","@terry_joyce that is a very useful suggestion, thanks! Can you advise how you implemented testing in CI? We want to run dbt twice - once for the build_source run and a second time for the incremental run, and the source data will need to be updated between runs",3
1528,"testing-incremental-models","Hi Rob,

Since the table now contains the `build_source` column, it's a simple matter to identify which records were touched by the incremental.   You'd want to check that you are updating the records you intend to. 

If your incremental is efficient, it will have little overlap: records that get re-written on multiple passes of the incremental.   But it's not always desirable to have zero overlap.  Likely your incremental logic depends on one or more high water marks.  These are often max values of timestamp columns.  But depending on the nature of the source data, sometimes the timestamps you're relying on are created a bit out of order, and if you had no overlap you'd miss some rows.    

So that's one of the things you'd test for:  are you missing any records from the source data?   That probably requires a comparative query of the transformed table and the source data, looking at a specific datetime range after your last full build. 

You definitely want to run your schema tests on the updated models. Look for unexpected null values and non-unique keys where you expect them.  That might tell you if you're missing data, or if you're duplicating data.   Generally if your unique key is specified correctly, dbt will make sure you don't duplicate data.

I also like to do some column-by-column compares of rows created by incremental vs. before the incremental ran.   

Another aspect is efficiency. How long do these updates run per model? That you can just measure with the runtime information you'll get from dbt cloud or your logs.   It might take some experimentation to find the most efficient way to structure the dbt model to operate efficiently during incremental updates.


I'm sure there's a lot more written on this subject of testing incrementals.  It's a big topic, and one to which a more specific answer will include ""Depends on ...""",4
1547,"clean-your-warehouse-of-old-and-deprecated-models","dbt models persist in your production database even after they're deleted from your project. This adds clutter to the warehouse and can slow down operations like database or schema cloning, which are used frequently in CI/CD for snowflake dbt projects.

I wrote a macro that executes DROP statements for all the tables and views in your target schema that don't exist in your project. How does it work? First, it fetches all the models, seeds, and snapshots in your project. Then, it queries the information schema for all the models in your target schema excluding the ones fetched above. Finally, it formats (and then executes) DROP statements for each table/view that isn't in your project.

To invoke from the command line, use: `dbt run-operation drop_old_relations`

**Warning: This will delete everything in your target schema that isn't in your dbt project.**

**Warning: This does not cover [aliases](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-aliases/#why-alias-model-names)**

**I strongly recommend running `dbt run-operation drop_old_relations --args '{""dryrun"": True}'` first to see the DROP commands**

Thanks a lot to Randy of Hashmap for providing much of the code! You can find his original version [here](https://dev.azure.com/hashmap-randy-pitcher/_git/hashmap-randy-pitcher-dbt-workspace?path=%2Fdbt%2Fmacros%2Fcleanup%2Fdrop_old_relations.sql), which drops models based on the last_altered date in the info schema using a pre-set cutoff

```
{% macro drop_old_relations(dryrun=False) %}

{% if execute %}
  {% set current_models=[] %}

  {% for node in graph.nodes.values()
     | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""])%}
    {% do current_models.append(node.name) %}

  {% endfor %}
{% endif %}

{% set cleanup_query %}

      WITH MODELS_TO_DROP AS (
        SELECT
          CASE 
            WHEN TABLE_TYPE = 'BASE TABLE' THEN 'TABLE'
            WHEN TABLE_TYPE = 'VIEW' THEN 'VIEW'
          END AS RELATION_TYPE,
          CONCAT_WS('.', TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME) AS RELATION_NAME
        FROM 
          {{ target.database }}.INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = '{{ target.schema }}'
          AND TABLE_NAME NOT IN
            ({%- for model in current_models -%}
                '{{ model.upper() }}'
                {%- if not loop.last -%}
                    ,
                {% endif %}
            {%- endfor -%}))
      SELECT 
        'DROP ' || RELATION_TYPE || ' ' || RELATION_NAME || ';' as DROP_COMMANDS
      FROM 
        MODELS_TO_DROP

  {% endset %}

{% do log(cleanup_query, info=True) %}
{% set drop_commands = run_query(cleanup_query).columns[0].values() %}

{% if drop_commands %}
  {% if dryrun | as_bool == False %}
    {% do log('Executing DROP commands...', True) %}
  {% else %}
    {% do log('Printing DROP commands...', True) %}
  {% endif %}
  {% for drop_command in drop_commands %}
    {% do log(drop_command, True) %}
    {% if dryrun | as_bool == False %}
      {% do run_query(drop_command) %}
    {% endif %}
  {% endfor %}
{% else %}
  {% do log('No relations to clean.', True) %}
{% endif %}

{%- endmacro -%}
````
EDIT: Added dryrun feature",1
1547,"clean-your-warehouse-of-old-and-deprecated-models","I love this! I modified the script slightly. The modified version below:
1. Provides the option of a dry run where the commands are printed, but not executed
2. Will match any schema prefixed with the target schema, rather than only the exact target schema (since we use multiple schemas of the form `<target_schema>_<schema_name>`). The exact target schema will still be matched. This introduces some risk if target schemas could overlap with *prefixes* of other schema names, so use with caution. It only matches strictly prefixes though, not a full wildcard, so if you're consistent with prefixing schemas this shouldn't be an issue.
3. Is clear about excluding non-standard table types (e.g. for us, we don't want this to delete our external tables, which are managed slightly outside of these parameters through dbt).

Modified version:
```py
{% macro drop_old_relations(dry_run='false') %}
{% if execute %}
  {% set current_models=[] %}
  {% for node in graph.nodes.values()
     | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""])%}
    {% do current_models.append(node.name) %}
  {% endfor %}
{% endif %}
{% set cleanup_query %}
      with models_to_drop as (
        select
          case 
            when table_type = 'BASE TABLE' then 'TABLE'
            when table_type = 'VIEW' then 'VIEW'
          end as relation_type,
          concat_ws('.', table_catalog, table_schema, table_name) as relation_name
        from 
          {{ target.database }}.information_schema.tables
        where table_schema ilike '{{ target.schema }}%'
          and table_name not in
            ({%- for model in current_models -%}
                '{{ model.upper() }}'
                {%- if not loop.last -%}
                    ,
                {% endif %}
            {%- endfor -%}))
      select 
        'drop ' || relation_type || ' ' || relation_name || ';' as drop_commands
      from 
        models_to_drop
      
      -- intentionally exclude unhandled table_types, including 'external table`
      where drop_commands is not null
  {% endset %}
{% do log(cleanup_query, info=True) %}
{% set drop_commands = run_query(cleanup_query).columns[0].values() %}
{% if drop_commands %}
  {% for drop_command in drop_commands %}
    {% do log(drop_command, True) %}
    {% if dry_run == 'false' %}
      {% do run_query(drop_command) %}
    {% endif %}
  {% endfor %}
{% else %}
  {% do log('No relations to clean.', True) %}
{% endif %}
{%- endmacro -%}
```",2
1547,"clean-your-warehouse-of-old-and-deprecated-models","Thanks for sharing! Have you considered how to extend this to account for aliases?",3
1547,"clean-your-warehouse-of-old-and-deprecated-models","What do you mean by aliases?",4
1547,"clean-your-warehouse-of-old-and-deprecated-models","Where the table created in the db is different than the model name.

### Why alias model names?[#](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-aliases/#why-alias-model-names)

The names of schemas and tables are effectively the ""user interface"" of your data warehouse. Well-named schemas and tables can help provide clarity and direction for consumers of this data. In combination with [custom schemas](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas), model aliasing is a powerful mechanism for designing your warehouse.",5
1547,"clean-your-warehouse-of-old-and-deprecated-models","interesting, i didn't know about that feature. you could probably extend this by grabbing the alias name in the initial loop of the dbt graph. something like this maybe?

```
  {% for node in graph.nodes.values()
     | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""])%}
    {% if node.alias %}
    {% do current_models.append(node.alias) %}
    {% else %}
    {% do current_models.append(node.name) %}
    {% endif %}
```",6
1547,"clean-your-warehouse-of-old-and-deprecated-models","Here's my take with support for multiple databases and aliases:

```
{% macro drop_old_tables(dry_run='false') %}
    {% if execute %}
        {% set current_model_locations={} %}
        {% for node in graph.nodes.values() | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""])%}
            {% if not node.database in current_model_locations %}
                {% do current_model_locations.update({node.database: {}}) %}
            {% endif %}
            {% if not node.schema.upper() in current_model_locations[node.database] %}
                {% do current_model_locations[node.database].update({node.schema.upper(): []}) %}
            {% endif %}
            {% set table_name = node.alias if node.alias else node.name %}
            {% do current_model_locations[node.database][node.schema.upper()].append(table_name.upper()) %}
        {% endfor %}
    {% endif %}
    {% set cleanup_query %}

        with models_to_drop as (
            {% for database in current_model_locations.keys() %}
            {% if loop.index > 1 %}
            union all
            {% endif %}
            select
                table_type
                , table_catalog
                , table_schema
                , table_name
                , case
                    when table_type = 'BASE TABLE' then 'TABLE'
                    when table_type = 'VIEW' then 'VIEW'
                end as relation_type
                , concat_ws('.', table_catalog, table_schema, table_name) as relation_name
            from {{ database }}.information_schema.tables
            where table_schema in ('{{ ""', '"".join(current_model_locations[database].keys()) }}')
            and not (
                {% for schema in current_model_locations[database].keys() %}
                {% if loop.index > 1 %}or {% endif %}table_schema = '{{ schema }}' and table_name in ('{{ ""', '"".join(current_model_locations[database][schema]) }}')
                {% endfor %}
            )
            {% endfor %}
        )

        select 'drop ' || relation_type || ' ' || relation_name || ';' as drop_commands
        from models_to_drop
        -- intentionally exclude unhandled table_types, including 'external table`
        where drop_commands is not null

    {% endset %}
    {% do log(cleanup_query, info=True) %}
    {% set drop_commands = run_query(cleanup_query).columns[0].values() %}
    {% if drop_commands %}
        {% for drop_command in drop_commands %}
            {% do log(drop_command, True) %}
            {% if dry_run == 'false' %}
                {% do run_query(drop_command) %}
            {% endif %}
        {% endfor %}
    {% else %}
        {% do log('No relations to clean.', True) %}
    {% endif %}
{%- endmacro -%}

```",7
1547,"clean-your-warehouse-of-old-and-deprecated-models","Building on the solution from @NiallRees . Here's my take for BigQuery:

To execute a dry run use: `dbt run-operation cleanup_dataset --args '{""dry_run"": True}'` 

```
-- Removes tables and views from the given run configuration
-- Usage in production:
--    dbt run-operation cleanup_dataset
-- To only see the commands that it is about to perform:
--    dbt run-operation cleanup_dataset --args '{""dry_run"": True}'
{% macro cleanup_dataset(dry_run=False) %}
    {% if execute %}
        {% set current_model_locations={} %}

        {% for node in graph.nodes.values() | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""])%}
            {% if not node.database in current_model_locations %}
                {% do current_model_locations.update({node.database: {}}) %}
            {% endif %}
            {% if not node.schema in current_model_locations[node.database] %}
                {% do current_model_locations[node.database].update({node.schema: []}) %}
            {% endif %}
            {% set table_name = node.alias if node.alias else node.name %}
            {% do current_model_locations[node.database][node.schema].append(table_name) %}
        {% endfor %}
    {% endif %}

    {% set cleanup_query %}

        with models_to_drop as (
            {% for database in current_model_locations.keys() %}
                {% if loop.index > 1 %}union all{% endif %}
                {% for dataset, tables  in current_model_locations[database].items() %}
                    {% if loop.index > 1 %}union all{% endif %}
                    select
                        table_type,
                        table_catalog,
                        table_schema,
                        table_name,
                        case
                            when table_type = 'BASE TABLE' then 'TABLE'
                            when table_type = 'VIEW' then 'VIEW'
                        end as relation_type,
                        array_to_string([table_catalog, table_schema, table_name], '.') as relation_name
                    from {{ dataset }}.INFORMATION_SCHEMA.TABLES
                    where not (table_name in ('{{ ""', '"".join(tables) }}'))
                {% endfor %}
            {% endfor %}
        ),
        drop_commands as (
            select 'drop ' || relation_type || ' `' || relation_name || '`;' as command
            from models_to_drop
        )

        select command
        from drop_commands
        -- intentionally exclude unhandled table_types, including 'external table`
        where command is not null

    {% endset %}
    {% set drop_commands = run_query(cleanup_query).columns[0].values() %}
    {% if drop_commands %}
        {% for drop_command in drop_commands %}
            {% do log(drop_command, True) %}
            {% if dry_run | as_bool == False %}
                {% do run_query(drop_command) %}
            {% endif %}
        {% endfor %}
    {% else %}
        {% do log('No relations to clean.', True) %}
    {% endif %}
{%- endmacro -%}
```",8
1547,"clean-your-warehouse-of-old-and-deprecated-models","On Redshift, the drop query wasn't throwing an error but it also wasn't dropping the relations. Once I wrapped the drop statement(s) in a begin/commit, it worked for me. 

Sharing that section of my code FWIW.

```sql
...
{% set drop_query = ['begin;'] %}
{%- for item in to_delete %}
  {%- set drop_statement %}  drop {{ item[2] }} if exists ""{{ item[0] }}"".""{{ item[1] }}"" cascade;{%- endset %}
  {%- do drop_query.append(drop_statement) %}
{%- endfor %} 
{%- do drop_query.append('commit;') %}
{%- set drop_query = drop_query|join('\n') %}
{%- do log(
    modules.datetime.datetime.now().strftime('%H:%M:%S') 
    ~ ' | Executing the following statements:',
    info=true) %}
{%- do log(drop_query, info=true) %}
{%- do run_query(drop_query) %}
...
```
```
Running with dbt=0.21.0
22:39:23 | Finding vestigial tables
22:39:24 | Executing the following statements:
begin;
  drop view if exists ""schema"".""vestigial_table_1"" cascade;
  drop view if exists ""another_schema"".""vestigial_table_2"" cascade;
commit;
```",9
1547,"clean-your-warehouse-of-old-and-deprecated-models","works perfect thanks.
One thing I have my development models in a different BQ project than my tables in production. So it looks this macro only works for me in my development project. 
How can I change the macro so it will work on my Production project as well when I run this macro in a job.
All my code is now 'transferred' to the right project via a macro: generate_database_name(), see code below.
I think I need to call  this macro somewhere from your macro to make it happen, but I don't know how.

{% macro generate_database_name(custom_database_name=none, node=none) -%}

    {{ log(""Running generate database name macro"") }}
    {% set default_database = target.database %}
    {% set production_database = 'analytics-prod' %}

    {% if custom_database_name %}

    
        {% if target.name == ""prod"" %}

            {{ production_database }}

        {% else %}

            {{ custom_database_name }}

        {% endif %}
        

    {% else %}

        {{ default_database }}

    {% endif %}

{% endmacro %}",10
1547,"clean-your-warehouse-of-old-and-deprecated-models","Awesome stuff, thanks for putting this out here!

We made one small tweak to the conditional logic to enable support for dropping BigQuery external table definitions in addition to views and tables:

```
SELECT
          CASE 
            WHEN TABLE_TYPE = 'BASE TABLE' THEN 'TABLE'
            WHEN TABLE_TYPE = 'VIEW' THEN 'VIEW'
            WHEN TABLE_TYPE = 'EXTERNAL' THEN 'EXTERNAL TABLE'
          END AS RELATION_TYPE,
```",11
1547,"clean-your-warehouse-of-old-and-deprecated-models","I created another macro that's helpful for debugging this `drop_old_relations` macro. Prints the nodes (models, seeds, snapshots) in your DBT graph:

```
{% macro print_graph() %}

    {% if execute %}
        {% set query %}

            {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""model"") | list
                            + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""seed"")  | list
                            + graph.nodes.values() | selectattr(""resource_type"", ""equalto"", ""snapshot"")  | list %}
                SELECT
                '{{node.config.schema}}' AS schema_name
                , '{{node.name}}' AS ref_name
                , '{{node.alias}}' AS alias
                , '{{node.resource_type}}' AS resource_type
                {% if not loop.last %} UNION ALL {% endif %}
            {%- endfor %}
        {% endset %}

        {%- set result = run_query(query) -%}
        {% if result %}
            {%- for node in result -%}
                {% do log(node, True) %}
            {%- endfor -%}
        {% endif %}
    {% endif %}

{% endmacro %}
```
Runnable with `dbt run-operation print_graph`",12
1547,"clean-your-warehouse-of-old-and-deprecated-models","I just adapted some things, added comments and formatting it a little bit.

Thanks to all previous contributors for making our life easier!

```

{%- macro drop_orphanate_tables(dry_run='false') -%}

    {%- if execute -%}

        -- Create empty dictionary that will contain the hierarchy of the models in dbt
        {%- set current_model_locations = {} -%}

        -- Insert the hierarchy database.schema.table in the dictionary above
        {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""]) -%}

            {%- set database_name = node.database.upper() -%}
            {%- set schema_name = node.schema.upper() -%}
            {%- set table_name = node.alias if node.alias else node.name -%}

            -- Add db name if it does not exist in the dict
            {%- if not database_name in current_model_locations -%}
                {% do current_model_locations.update({database_name: {}}) -%}
            {%- endif -%}

            -- Add schema name if it does not exist in the dict
            {%- if not schema_name in current_model_locations[database_name] -%}
                {% do current_model_locations[database_name].update({schema_name: []}) -%}
            {%- endif -%}

            -- Add the tables for the db and schema selected
            {%- do current_model_locations[database_name][schema_name].append(table_name.upper()) -%}

        {%- endfor -%}

    {%- endif -%}

    -- Query to retrieve the models to drop
    {%- set cleanup_query -%}

        WITH models_to_drop AS (
            {%- for database in current_model_locations.keys() -%}
                {%- if loop.index > 1 %}
                UNION ALL
                {% endif %}

                SELECT
                    CASE
                        WHEN table_type = 'BASE TABLE' THEN 'TABLE'
                        WHEN table_type = 'VIEW' THEN 'VIEW'
                        ELSE NULL
                    END AS relation_type,
                    table_catalog,
                    table_schema,
                    table_name,
                    concat_ws('.', table_catalog, table_schema, table_name) as relation_name
                FROM {{ database }}.information_schema.tables
                WHERE
                    table_schema IN ('{{ ""', '"".join(current_model_locations[database].keys()) }}')
                    AND NOT (
                        {%- for schema in current_model_locations[database].keys() -%}
                            {% if loop.index > 1 %}
                            OR {% endif %} table_schema = '{{ schema }}' AND table_name IN ('{{ ""', '"".join(current_model_locations[database][schema]) }}')
                        {%- endfor %}
                    )
            {%- endfor -%}
        )
        -- Create the DROP statments to be executed in the database
        SELECT 'DROP ' || relation_type || ' IF EXISTS ' || table_catalog || '.' ||table_schema || '.' || table_name || ';' AS drop_commands
        FROM models_to_drop
        WHERE relation_type IS NOT NULL

    {%- endset -%}

    -- Execute the DROP statments above

    {%- set drop_commands = run_query(cleanup_query).columns[0].values() -%}

    {%- if drop_commands -%}

        {%- for drop_command in drop_commands -%}

            {%- do log(drop_command, True) -%}

            {%- if dry_run.upper() == 'FALSE' -%}
                {%- do run_query(drop_command) -%}
                {%- do log('Executed', True) -%}
            {%- endif -%}

        {%- endfor -%}

    {%- else -%}

        {%- do log('No relations to clean', True) -%}

    {%- endif -%}

{%- endmacro -%}

```",13
1547,"clean-your-warehouse-of-old-and-deprecated-models","building on @rcoll 's version above. Added case-insensitivtity string matching

Thanks to all contributors!

```
{%- macro drop_orphanate_tables(dry_run='false') -%}

    {%- if execute -%}

        -- Create empty dictionary that will contain the hierarchy of the models in dbt
        {%- set current_model_locations = {} -%}

        -- Insert the hierarchy database.schema.table in the dictionary above
        {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""]) -%}

            {%- set database_name = node.database.upper() -%}
            {%- set schema_name = node.schema.upper() -%}
            {%- set table_name = node.alias if node.alias else node.name -%}

            -- Add db name if it does not exist in the dict
            {%- if not database_name in current_model_locations -%}
                {% do current_model_locations.update({database_name: {}}) -%}
            {%- endif -%}

            -- Add schema name if it does not exist in the dict
            {%- if not schema_name in current_model_locations[database_name] -%}
                {% do current_model_locations[database_name].update({schema_name: []}) -%}
            {%- endif -%}

            -- Add the tables for the db and schema selected
            {%- do current_model_locations[database_name][schema_name].append(table_name.upper()) -%}

        {%- endfor -%}

        {{ log(current_model_locations) }}

    {%- endif -%}

    -- Query to retrieve the models to drop
    {%- set cleanup_query -%}

        WITH models_to_drop AS (
            {%- for database in current_model_locations.keys() -%}

                SELECT
                    CASE
                        WHEN table_type = 'BASE TABLE' THEN 'TABLE'
                        WHEN table_type = 'VIEW' THEN 'VIEW'
                        ELSE NULL
                    END AS relation_type,
                    table_catalog,
                    table_schema,
                    table_name,
                    table_catalog || '.' || table_schema || '.' || table_name as relation_name
                FROM {{ database }}.information_schema.tables
                WHERE
                    LOWER(table_schema) IN ('{{ ""', '"".join(current_model_locations[database].keys())|lower }}')
                    AND NOT (
                        {%- for schema in current_model_locations[database].keys() -%}
                            LOWER(table_schema) = LOWER('{{ schema }}') AND LOWER(table_name) IN ('{{ ""', '"".join(current_model_locations[database][schema])|lower }}')
                            {% if not loop.last %} OR {% endif %}
                        {%- endfor %}
                    )

                {% if not loop.last -%} UNION ALL {%- endif %}
            {%- endfor -%}
        )
        -- Create the DROP statments to be executed in the database
        SELECT 'DROP ' || relation_type || ' IF EXISTS ' || table_catalog || '.' ||table_schema || '.' || table_name || ';' AS drop_commands
        FROM models_to_drop
        WHERE relation_type IS NOT NULL

    {%- endset -%}

    -- Execute the DROP statments above

    {%- set drop_commands = run_query(cleanup_query).columns[0].values() -%}

    {%- if drop_commands -%}

        {%- for drop_command in drop_commands -%}

            {%- do log(drop_command, True) -%}

            {%- if dry_run.upper() == 'FALSE' -%}
                {%- do run_query(drop_command) -%}
                {%- do log('Executed', True) -%}
            {%- endif -%}

        {%- endfor -%}

    {%- else -%}

        {%- do log('No relations to clean', True) -%}

    {%- endif -%}

{%- endmacro -%}
```

Execute via:
`dbt run-operation drop_orphanate_tables --args '{dry_run: ""true""}'`
(args is optional, default `false`, which will drop tables. `true` will only list tables to drop.)",14
1547,"clean-your-warehouse-of-old-and-deprecated-models","A colleague and I build @rcoll's version and configured it to work when combining dbt with Databricks. 

We added an optional `except` and used Jinja instead of SQL to build the drop statements. Furthermore, some logging is added to keep track of what is happening.

Thanks for sharing all of this. It really makes life easier. 

```python
{%- macro drop_outdated_tables(dry_run=False, except=[]) -%}

    {%- if execute -%}

    {%- set current_model_locations = {} -%}

        {%- for node in graph.nodes.values() | selectattr(""resource_type"", ""in"", [""model"", ""seed"", ""snapshot""]) -%}

            {%- set schema_name = node.schema.upper() -%}
            {%- set table_name = node.alias if node.alias else node.name -%}

            -- Add schema name if it does not exist in the dict
            {%- if not schema_name in current_model_locations -%}
                {% do current_model_locations.update({schema_name: []}) -%}
            {%- endif -%}

            -- Add table name if it does not exist in the dict
            {%- if not table_name in current_model_locations[schema_name] -%}
                {% do current_model_locations[schema_name].append(table_name) -%}
            {%- endif -%}

        {%- endfor -%}

    {%- endif -%}

    -- Query to retrieve the models to drop
    {%- set cleanup_query -%}

        {%- for schema in current_model_locations.keys() -%}

                {% set all_tables_query = ""show tables from "" + schema %}
                {% set result = run_query(all_tables_query) %}
                {% for row in result %}

                    {% set table = row[1] %}
                    {% set type = [] %}
                    {% set describe_query = ""describe extended "" + schema + ""."" + table %}
                    {% set describe_result = run_query(describe_query) %}

                    {% for type_row in describe_result %}
                        {% if type_row[0] == ""Type"" %}
                            {% do type.append(""TABLE"" if type_row[1] == ""EXTERNAL"" else ""VIEW"") %}
                        {% endif %}
                    {% endfor %}

                    {% if not table in current_model_locations[schema] and not table in except %}
                        {% do log(""Dropping "" + type[0] + "" "" + table + "" from "" + schema, True) %}
                        SELECT 'DROP {{ type[0] }} IF EXISTS {{ schema }}.{{ table }};' as drop_command
                        union
                    {% else %}
                        {% do log(""."", True) %}
                    {% endif %}

                {% endfor %}

        {% endfor %}
        SELECT 'SELECT 1;' as dummy_for_union

    {%- endset -%}

    {%- do log(cleanup_query, True) -%}
    {%- do log(""Done setting cleanup_query"", True) -%}

    -- create list of the DROP statments above
    {%- set drop_commands = run_query(cleanup_query) -%}
    {%- set commands_list = drop_commands.columns[0].values() -%}

    {%- do log(""Drop statements turned into list"", True) -%}


    -- Execute DROP statements if applicable
    {%- if commands_list -%}

        {%- for command in commands_list -%}
            {%- if dry_run == False -%}
                {%- do run_query(command) -%}
                {%- do log('Executed: ' + command, True) -%}
            {%- else -%}
                {%- do log('Not executed: ' + command, True) -%}
            {%- endif -%}

        {%- endfor -%}

    {%- else -%}

        {%- do log('No relations to clean', True) -%}

    {%- endif -%}

{%- endmacro -%}
```

Execute as follows: 


`db2 run-operation drop_outdated_tables --args '{""dry_run"": False, ""except"": [""model""]}' [--target]`",15
1591,"custom-versions-of-generate-schema-name","Hello all dbt-ers!

There has been a lot of discussion in various places about how folks set up their dbt environments, databases, schemas/datasets, etc. I would like to share here the specific changes we have made to `generate_schema_name` and the basics of how we set up our build environment in the hope that it will be useful to others, and that they will in turn share theirs. Here is our code:

```{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}

    {%- if custom_schema_name is none -%}
        {{ default_schema }}

    {# If in dbt cloud, we always want models marked to generate in custom schemas
    to generate in just that specific schema, not with the prefixed default schema#}
    {%- elif target.name in ['dev-cloud','prod-cloud'] -%}
        {{ custom_schema_name }}

    {# If running in a local non-dbt-cloud environment then use the local schema 
    name no matter what #}
    {%- elif target.name not in ['dev-cloud', 'prod-cloud'] -%}
        {{ default_schema }}

    {# Should never arrive here #}
    {%- else -%}
        {{ default_schema }}_{{ custom_schema_name | trim }}
    {%- endif -%}

{%- endmacro %}
```
So the basics of it are this:

1. We use BigQuery and have two different projects, one project for all dev datasets and data, and one for all prod datasets and data. 
2. Our developers develop using dbt Core locally in feature branches. They submit PRs for the feature branches. If approved, the PRs get merged into our `dev` branch. Periodically we then merge from `dev` to `master`. 
3. We want our dbt cloud scheduled runs between dev and prod to be as close as possible to catch any issues and we also want our devs to be able to develop in their own ""sandboxes"" without stepping on or being stepped on by what the scheduled dbt cloud jobs in Dev are doing or each other.
4. Here is the list of datasets in our Dev environment and their purposes: a) ""analytics"" - Where end-user-facing published models are made available. b) ""sandbox"" - Where all intermediate and non-user facing temporary processing and models live c) ""snapshot"" -- Where our snapshots live d) A bunch of other datasets for raw sources as they are loaded e) ""dev_name_sandbox"" - variants of ""sandbox"" where individual devs do their development and testing when running w/ dbt Core

What we want in our `generate_schema_name` macro is something that, in our dbt cloud runs for both Dev and Prod, publishes all user-facing models into the `analytics` schema and all the temporary/intermediate models into `sandbox`. But for developers working locally, it should just publish all the models, whether user-facing or not, into their own local sandbox, which is specified in their profile file. This would also include if we do any other custom schema overrides for other models in the future, which we may do. So we are using targets for now in dbt cloud to differentiate this, although definitely hoping in the future to have an environment variable or some such thing to differentiate between dbt cloud jobs and locally run jobs.

Interested to see what others have done!",1
1592,"auto-pivot-json-string-using-schema-yml-in-bigquery","Let me start by giving you some background and apologies if the background is longer than the solution.

# Background

## The scenario:

* We work with BigQuery
* Working with events data split into two data sources due to the constraints of the source system
* One source is the main set of data, let us call it TABLE_MAIN
* The other is the ancillary or supportive data, let us call this one TABLE_SUPPORT
* There is a possibility of a 1:n relationship between TABLE_MAIN and TABLE_SUPPORT
* Join these two tables into one

Simple enough? Not so much. The complication is data in TABLE_SUPPORT. Why so?

* Not all events have the same support data, one event can have few columns and others can have hundreds.
  * Column one of events *evt_123* is not necessarily the same as column one of event *evt_456*
  * A lot of empty columns if all columns are mapped to each event
* Constraints of the source system
  * Only 20 columns of data per row. Multiple rows per event
  * Only 60 characters per column so one value of support data can span multiple columns of TABLE_SUPPORT. And if this happens on column 20 then one value of support data can span two rows.
* The source system can be enhanced with additional support data at any time

To help visualize:
**TABLE_MAIN**
| Field                 | Key |
| -------------------- | :---: | 
| EVENTGUID_1 | X |
| EVENTGUID_2 | X |
| PROCESS_DATE | X |

**TABLE_SUPPORT**
| Field                 | Key | Notes |
| -------------------------- | :---: | --- |
| EVENTGUID_1 | X | |
| EVENTGUID_2 | X | |
| PROCESS_DATE    | X | |
| RECSEQ                  | X | Row sequence number if support data is across multiple rows |
| FIELDNAME 01 to 20   |    | Name |
| FIELDVALSEQ 01 to 20 |   | Sequence if value has been split |
| FIELDVALUE 01 to 20    |   | Value |

Got that? In the end, even though TABLE_SUPPORT has multiple rows for each event, it is only really one rowâ€™s worth of data per event. Which means that a 1:1 relationship between TABLE_MAIN and TABLE_SUPPORT is possible.

### The dbt Model:
I wonâ€™t go too much into how the mapping between the tables was done, suffice so say that arrays played a part. Basically it is this:

* Get TABLE_MAIN data
* Get TABLE_SUPPORT data
  * Create ARRAY
  * STRING_AGG ordering by FIELDVALSEQ
  * ARRAY_AGG(STRUCT(FIELDNAME, FIELDVALUE)
* Join
* Tadah! You have TABLE_COMBINED with an array, which is an awesome feature of BQ (Iâ€™ve got a SAPBW background, no arrays there).

However, because of the data volume you end up with TABLE_COMBINED being of similar size to TABLE_SUPPORT and any queries on table TABLE_COMBINED will be pricey. An array is not suitable so as a final step we turn the array into a JSON string using `TO_JSON_STRING()`. Table size drops dramatically

Great, now we have a usable table with a JSON string containing all my event support data. 

How do I get the data out of this JSON string when querying the table? Easy, just use `JSON_EXTRACT_SCALAR(json_string,'$.fieldname') as fieldname`. Well easy if you are versed or comfortable with SQL and if you know what you are looking. But if you are a general user, the â€œselect * and Iâ€™ll see whatâ€™s thereâ€ kind of user? Our solution to create a view with the JSON string automatically pivoted.

## Pivot Solution

You may say easy, we all know what dbt allows us to auto-generate a repeated set of code using macros. And you are right, just:

* Create a macro
* Put the JSON extract command in a FOR loop with a field list,
* Call the macro and provide the field list

But consider these questions:

* What if I do not want to manually provide field list?
* What if I do not want developer changing my model every time the source system is enhanced, especially if the model itself does not really change?
* What I need to make sure the correct data type is used as the end result of `JSON_EXTRACT_SCALAR()` is always a string?
* When should `CAST()` be used or `DATE()` be used  instead of `PARSE_DATE()` etc.

Here is how I did it:
### schema.yml
My first thought was the schema.yml for each model. This document needs to be updated every time the model gets updated. And I would rather it be changed that a model, less likelihood of breaking things.  Is it possible to use the schema to drive the pivoting and the associated rules?

One of the options in the schema is meta. I ask the question in slack about its use and Taylor Murphyâ€™s  [response](https://getdbt.slack.com/archives/C0VLZPLAE/p1595326412121300) helped me greatly. I decided to use the meta option and add three values. Note that this is the target table/view schema.

* `JSON: False or True` â€“ Mandatory for columns in JSON string
* `data_type: DATE`
* `rule: Parse` â€“ rule applied to data_type. Blank = DATE(), parse = PARSE_DATE() etc. more rules can be added if required. 

### Macro
Now that I had my driver my macro just needed to the following, with a lot of if statements:

* Get a list of columns from the schema
* Loop through columns.
  * Get JSON string columns only
  * Extract based on Data Type
  * Get conversion rule if column is a Time Dimension

I tried to keep the macro dynamic, not limited to my model or the name of the JSON string.

### Model

The hard part is done. Now I just need to call the macro passing the right arguments and it is done.

### Recap
The schema.yml file is critical. All developer now needs to do is make sure that any additional support data from the source system is added to the proper schema.yml, the macro will do the rest.

I am thinking that the meta option in schema will be used more in future.
Hope this was helpful.

## For reference:
Sample from schema.yml
```YAML
      - name: CALL_AHEAD_DATE 
        description: Call Ahead Date
        meta:
          JSON: True
          data_type: DATE
          rule: Parse
```
My macro:
```python
{%- macro json_extract_by_datetype(json_string, model_name, node_type='model', project) -%}
    {%- if execute -%}
        {# Step 1 - Get a list of columns from the schema #}
            {# Step 2 - Loop through columns, get JSON string columns only #}
                {# Step 3 - Extract based on Data Type #}
                    {# Step 4 - Get conversion rule if column is a Time Dimension #}
        {%- set anc_meta_key = 'JSON' -%}
        {%- set rule_meta_key = 'rule' -%}
        {%- set datatype_meta_key = 'data_type' -%}
        {%- set columns = [] -%}
        {%- set fqname = node_type ~ '.' ~ project ~ '.' ~ model_name -%}
        {%- set columns = graph.nodes[fqname]['columns']  -%}

        {# Step 1 - Get JSON string Columns only #}
            {# Step 2 - Extract based on Data Type #}
                {# Step 3 - Get Conversion Rule if column is a Time Dimension #}
        {%- for column in columns -%}
            {%- set anc_value = (graph.nodes[fqname]['columns'][column]['meta'][anc_meta_key]|upper) -%}
            {%- set rule_value = (graph.nodes[fqname]['columns'][column]['meta'][rule_meta_key]|upper) -%}
            {%- set dt_value = (graph.nodes[fqname]['columns'][column]['meta'][datatype_meta_key]|upper) -%}
            {%- if anc_value == 'TRUE' -%}
                {%- if dt_value == 'STRING' -%}
                    JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}') as {{column}},
                {%- elif dt_value == 'NUMERIC' -%}  
                    safe_cast(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}') as NUMERIC) as {{column}},
                {%- elif dt_value == 'INTEGER' -%}  
                    safe_cast(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}') as INT64) as {{column}},
                {%- elif dt_value == 'FLOAT' -%}  
                    safe_cast(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}') as FLOAT64) as {{column}},
                {%- elif dt_value == 'DATE' -%}  
                    {%- if rule_value == 'PARSE' -%}
                        parse_date('%Y%m%d',JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- else -%}
                        date(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- endif -%}
                {%- elif dt_value == 'TIME' -%}  
                    {%- if rule_value == 'PARSE' -%}
                        parse_time('%H%M%S',JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- else -%}
                        time(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- endif -%}
                {%- elif dt_value == 'DATETIME' -%}  
                    {%- if rule_value == 'PARSE' -%}
                        parse_datetime('%Y%m%d%H%M%S',JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- else -%}
                        datetime(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- endif -%}
                {%- elif dt_value == 'TIMESTAMP' -%}  
                    {%- if rule_value == 'PARSE' -%}
                        parse_timestamp('%c',JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- else -%}
                        timestamp(JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}')) as {{column}},
                    {%- endif -%}
                {%- else -%}
                    JSON_EXTRACT_SCALAR({{json_string}},'$.{{column}}') as {{column}},
                {%- endif -%}
            {%-else -%}
            {%- endif -%}
        {%- endfor -%} 
    {%- endif -%}
{%- endmacro -%}
```
Sample model:
```
{{config
  (
    tags         = ['testing'],
    labels       = {'created_by': 'dbt-generated'},
    materialized = 'view',
    schema       = 'dbt_demo',
    alias        = 'test_Pivot'
   )
}}

select   *
  except(JSON_STRING),  
  {{json_extract_by_datetype('JSON_STRING','my_target_model_name', 'model', 'dbt_project_name')}}
from {{ ref('my_model_name') }}     
where process_date is not null
```",1
1597,"to-replace-the-null-or-not-replace-the-null","that is my question.  When you have nulls in a databse, do you replace them with ""unknown"" or other values?  If so when and why? We've been discussing internal. 

A few notes on it we've found so far

## Kimball
[Kimball outlines 3 main areas where you'll run into nulls](https://www.kimballgroup.com/2003/02/design-tip-43-dealing-with-nulls-in-the-dimensional-model/)

and advocates for replacing null Dimensions
  
> Putting a null in these fields can be confusing to the user, as it will appear as a blank on reports and pull-down menus, and require special query syntax to find. Instead, we recommend substituting an appropriately descriptive string, like â€œUnknownâ€ or â€œNot provided.â€

But I think most BI products (atleast Chartio) handle nulls fine these days - we don't just have it ""appear as a blank"" in menus.

## Fishtown

In [How we Structure our DBT Projects](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355) the Fishtown team alludes to actually creating more nulls!

> * Light cleansing, such as replacing empty string with NULL values, has occurred.


How about you?  Any frameworks for making the call here?",1
1597,"to-replace-the-null-or-not-replace-the-null","At **Fractal River**, we do use NULL values within the data warehouse layers - at least in Staging, Prep, and Intermediate. I believe NULL values are very useful as they behave differently than sentinel values, avoid potential conflicts/bugs, and allow you to use constructs like count(), proper comparisons, etc.

The suggestion of inserting ""Unknown"" into NULL values, for example, feels like something that should be done only if necessary, and as close to the Visualization layer (Marts or better yet, within the BI tool itself - LookML, for example).",2
1597,"to-replace-the-null-or-not-replace-the-null","FWIW, As a Kimball purist, I used to always replace NULL values. However, once I started using Looker at my reporting tool, it became less necessary to do that since Looker displays everything nicely to the user.",3
1597,"to-replace-the-null-or-not-replace-the-null","In an ideal world, I would prefer no nulls in my database. The problem for me is if I see a null in a field, is it because a) its intentionally null b) something went wrong in the data engineering?  There really isn't a good way to tell from just looking at the data so you need to trace back through the lineage to the source.

I wish there were standard values for ""business null"" and ""physical null""",4
1597,"to-replace-the-null-or-not-replace-the-null","I have worked with clients that have a mixed BI tool environment so we put add a layer of report ready tables within the warehouse. And it was in that layer that we would replace dimension NULLs with '
Unknown'.  That put the same business logic in all of the tools and then if a value was NULL it was purposeful for the end user.",5
1631,"modeling-changes-to-daily-weekly-monthly-active-users","> Jump straight to the [project docs](https://www.getdbt.com/active-users/#!/overview), or to the [source code](https://github.com/fishtown-analytics/dbt-modeling-techniques/tree/main/active-users)

This article was prompted by a question from a team member :)

### Our data
Let's say we have a table with one record per user, per day they were active:
| user_id | date_day   | is_active | 
|---------|------------|-----------| 
| 1       | 2020-08-01 | true      | 
| 1       | 2020-09-01 | true      | 
| 1       | 2020-09-04 | true      | 
| 1       | 2020-09-05 | true      | 
| 1       | 2020-09-12 | true      | 
| 1       | 2020-09-17 | true      | 
| 2       | 2020-09-01 | true      | 


### The problem
We've been asked to create reports for daily, weekly, and monthly active users.

On the surface, we know that we could calculate daily active users like this:

**Daily active users**
```
select
   date_day,
   count(distinct user_id) as daily_active_users
from {{ ref('user_active_days') }}
group by 1
order by 1
```

| date_day   | daily_active_users | 
|------------|--------------------| 
| 2020-08-13 | 1                  | 
| 2020-09-01 | 2                  | 
| 2020-09-04 | 1                  | 
| 2020-09-05 | 1                  | 
| .. | ..                  |

(Note, we haven't filled in the missing days, which might be something worth doing ;) )

We can follow similar patterns for weekly and monthly active users, to generate outputs like this.
<details>
<summary>Weekly active users SQL</summary>

```
select
   date_trunc('week', date_day) as date_week,
   count(distinct user_id) as weekly_active_users
from {{ ref('user_active_days') }}
group by 1
order by 1
```
</details>


| date_week  | weekly_active_users | 
|------------|---------------------| 
| 2020-08-10 | 1                   | 
| 2020-08-17 | 1                   | 
| 2020-08-24 | 1                   | 
| 2020-08-31 | 2                   | 
| 2020-09-07 | 1                   | 
| 2020-09-14 | 1                   | 


<details>
<summary>Monthly active users SQL</summary>

```
select
   date_trunc('month', date_day) as date_month,
   count(distinct user_id) as monthly_active_users
from {{ ref('user_active_days') }}
group by 1
order by 1
```
</details>

| date_month | monthly_active_users | 
|------------|----------------------| 
| 2020-08-01 | 1                    | 
| 2020-09-01 | 2                    | 


However, wouldn't it be :cool: if we could see this changing on a daily basis? That can feel strange when thinking about ""monthly active users"" for a day in the middle of the month, but essentially we want to get to something like this â€” for each day:
- How many active users were there today?
- How many active users were there in the last week / 7 days?
- How many active users were there in the last month / 30 days?

### :sparkles:  **GOAL TABLE** :sparkles: 
| date_day   | daily_active_users | weekly_active_users | monthly_active_users | 
|------------|--------------------|---------------------|----------------------| 
| ..           | ..                  | ..                   | ..                    | 
| 2020-09-01 | 2                  | 2                   | 2                    | 
| 2020-09-02 | 0                  | 2                   | 2                    | 
| 2020-09-03 | 0                  | 2                   | 2                    | 
| 2020-09-04 | 1                  | 2                   | 2                    | 
| 2020-09-05 | 1                  | 2                   | 2                    | 
| 2020-09-06 | 0                  | 2                   | 2                    | 
| 2020-09-07 | 0                  | 2                   | 2                    | 
| 2020-09-08 | 0                  | 1                   | 2                    | 
| 2020-09-09 | 0                  | 1                   | 2                    | 
| ..           | ..                  | ..                   | ..                    | 

The ""semi-additive"" nature of these metrics means that we can't just add up our daily active users each week to get the number of weekly active users, so the way to get here may not feel immediately obvious.

### The solution
Fortunately we've seen this pattern before! 

Before we can build the summary table, we need to build a table with one record per user, per day (regardless of whether they were active that day or not), and fill in columns for:
- `active_today`
- `active_l7_days`
- `active_l30_days`



To solve this, you'll need to use a [date spine](https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265) and some window functions.

<details>
<summary>SQL to create this table</summary>

```
with user_active_days as (
    select * from {{ ref('user_active_days') }}
),

all_days as (
    select * from {{ ref('all_days') }}
),

user_first_active_day as (
    select
        user_id,
        min(date_day) as first_active_day
    from user_active_days
    group by 1
),

-- use a date spine to fill in missing days
spined as (
    select
        user_first_active_day.user_id,
        all_days.date_day
    from user_first_active_day
    left join all_days
        -- all days after the user was first active
        on all_days.date_day >= user_first_active_day.first_active_day
),

filled as (
    select
        spined.date_day,
        spined.user_id,

        coalesce(user_active_days.is_active, false) as is_active_today,

        max(is_active_today) over (
            partition by spined.user_id
            order by spined.date_day
            rows between 6 preceding and current row
        ) as is_active_l7_days,

        max(is_active_today) over (
            partition by spined.user_id
            order by spined.date_day
            rows between 29 preceding and current row
        ) as is_active_l30_days

    from spined

    left join user_active_days
        on spined.date_day = user_active_days.date_day
        and spined.user_id = user_active_days.user_id
)

select * from filled

```

</details>

| date_day   | user_id | is_active_today | is_active_l7_days | is_active_l30_days |
|------------|---------|-----------------|-------------------|--------------------|
| 2020-08-13 | 1       | true            | true              | true               |
| 2020-08-14 | 1       | false           | true              | true               |
| 2020-08-15 | 1       | false           | true              | true               |
| 2020-08-16 | 1       | false           | true              | true               |
| 2020-08-17 | 1       | false           | true              | true               |
| 2020-08-18 | 1       | false           | true              | true               |
| 2020-08-19 | 1       | false           | true              | true               |
| 2020-08-20 | 1       | false           | false             | true               |
| 2020-08-21 | 1       | false           | false             | true               |
| 2020-08-22 | 1       | false           | false             | true               |
| .. | ..       | ..           | ..             | ..               |



Once we have this, it's a reasonably simple hop from that table to our goal table (shown above):
```
with active_users as (
    select * from {{ ref('user_active_days_spined') }}
),

final as (
    select
        date_day,

        sum(is_active_today::integer) as daily_active_users,
        sum(is_active_l7_days::integer) as weekly_active_users,
        sum(is_active_l30_days::integer) as monthly_active_users

    from active_users

    group by 1
)

select * from final
order by date_day

```


### See the code
We've published some sample code for this over [here](https://github.com/fishtown-analytics/dbt-modeling-techniques/blob/main/active-users) and deployed the docs for this project [here](https://www.getdbt.com/active-users/#!/overview). Feel free to explore! (Note: the code has only been written for Snowflake)",1
1631,"modeling-changes-to-daily-weekly-monthly-active-users","What if someone logs in again? Since you're pulling the first time someone logged in, I'm missing the computation that resets the ""clock"" if they log in again.",2
1631,"modeling-changes-to-daily-weekly-monthly-active-users","[quote=""LifesGood, post:2, topic:1631""]
What if someone logs in again?
[/quote]

Do you mean multiple times in the same day? The initial table is at the grain of user and day:

[quote=""claire, post:1, topic:1631""]
### Our data

Letâ€™s say we have a table with one record per user, per day they were active:

|user_id|date_day|is_active|
| --- | --- | --- |
|1|2020-08-01|true|
|1|2020-09-01|true|
|1|2020-09-04|true|
|1|2020-09-05|true|
|1|2020-09-12|true|
|1|2020-09-17|true|
|2|2020-09-01|true|
[/quote]

This line:
[quote=""claire, post:1, topic:1631""]
`min(date_day) as first_active_day`
[/quote]
Is to facilitate the date spine - by doing an inequality join you can get a row for every day even if they didn't log in. See https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265 for a more in depth discussion",3
1658,"here-is-a-way-to-write-dbt-docs-as-sql-comment","dbt docs took out so much pain of keeping concise and fresh documentation. And it is beautiful!
But am I alone wanting to keep the documentation even closer to the code, maybe within SQL files themselves, just like Python docstring? I would give no excuse for out-of-sync docs to myself this way.

So, I just created `dbtdocstr` command. You need Python 3 to run this command.

:blue_heart: I may not be aware of any similar programs and I may be just embarrassing myself, but I found this little tool to be useful and am hoping that it does good for others, too. :slightly_smiling_face:

From [REAME.md](https://github.com/anelendata/dbt_docstring/blob/master/README.md):
## Install

```
pip install dbt_docstring
```

## How does it work?

dbtdocstr command scans .sql files under dbt's models directories and look for a block that begins with  ```` ```dbt````  and end with ````  ``` ````. Inside the block you can write the content of the models section of schema.yml corresponding to the current table as specified in [dbt document](https://docs.getdbt.com/docs/building-a-dbt-project/documentation/):

Example (<dbt_project_root_directory>/models/api_key_status.sql)

````
/*
# API key status

This table lists the API keys with the status.

```dbt
columns:
  - name: api_key
    description: API key
    tests:
      - unique
  - name: enabled
    description: True if API key is enabled status
  - name: update_datetime
    description: Last update date and time
```
*/
SELECT
   api_key,
   enabled,
   update_datetime
FROM {{ ref('my_api_key_list') }}
````

Then run:

```
dbtdocstr <dbt_project_root_directory>
```

These two files will be auto-generated from each .sql file in the dbt project:

`models/docs.md` :

```
{% docs api_key_status %}
# API key status

This table lists the API keys with the status.
{% enddocs %}
```

`models/schema.yml` :

```
version: 2
models:
  - name: api_key_status
    description: '{{ docs(""api_key_status"") }}'
    columns:
      - name: api_key
        description: API key
        tests:
          - unique
      - name: enabled
        description: True if API key is enabled status
      - name: update_datetime
        description: Last update date and time
  - name: ...
```

To see the document generation, use dbt command:

```
dbt docs generate
dbt docs serve
```

### Notes

* The doc must be a SQL comment block comment that begins with '/*' and ends with '*/'
* The first comment block will be extracted.
* The dbt block is searched within the first comment block.
* Any text after the dbt block will be ignored.
* dbt's Docs Blocks feature can be used only for table & view description. Not column descriptions.
* `dbtdocstr --backup <dbt_root_directory>`  to create backup files of schema.yml and docs.yml if they exsit.
* You should be able to add test",1
1658,"here-is-a-way-to-write-dbt-docs-as-sql-comment","I like that! I was thinking about this as well. Thank you for sharing!

Do you have plans for columns handling?",2
1771,"new-package-dbt-expectations","Hello all! I've been working on a port of the tests from the **Great Expectations** package to dbt. Turns out most of  the expectations, except those relying on more complex statistical functions, can be ported to SQL without issue. 

This allows dbt users that have been eyeing the Great Expectations Python package to use some of these tests directly in their dbt pipelines without having to deal with the potential complexities of integrating a Python-based testing workflow.

This package is currently in **alpha** release - that means ready for testing, but bound to have some changes to its API and functionality. 
I'd love it if the dbt community would check it out and provide feedback via the [Github issues](https://github.com/calogica/dbt-expectations/issues) page, or contribute directly via PRs. In particular, there are a few regex-based string matching tests I haven't had a chance to implement yet.
(The project contains a set of integration tests, similar to what the dbt-utils package is doing - happy to walk anyone through how to set that up.)
https://github.com/calogica/dbt-expectations",1
1771,"new-package-dbt-expectations","wowowow this is insanely cool. I'm really excited about where this is going!!",2
1771,"new-package-dbt-expectations","I was exactly looking for this! Fantastic",3
1771,"new-package-dbt-expectations","Great Expectations maintainer here: we're super excited about this as well. A dbt-native implementation of GE's testing semantics will be a really good thing for both communities.

We're looking forward to supporting and contributing to this effort.",4
1771,"new-package-dbt-expectations","We were just talking about testing and Great Expectations...",5
1771,"new-package-dbt-expectations","**Very cool indeed!**
Idea: for the more complex statistical calculations: it might be an idea to try and generate javascipt stored procedures for them. Both Snowflake and Bigquery support stored that language.",6
1771,"new-package-dbt-expectations","Is it possible to integrate with GE Docs for reporting or will the tests will still be reported in dbt natively?",7
1778,"state-of-testing-in-dbt","Hello!

My team and I have been thinking about testing lately and throughout this thinking I did a deep-dive into the current state of testing in dbt. In this post, I will try to motivate the importance of testing, present research I've collected about testing in dbt, make some conclusions from that research, and finally make some suggestions for advancement of testing in dbt.

I tagged a lot of folks on this post, would love to get more input, more research, and opinions here. Thanks!

## Motivation

Presently, dbt offers [two types of tests](https://docs.getdbt.com/docs/building-a-dbt-project/tests/), **schema tests** and **data tests**. For the purpose of this post, I will refer to these types of tests as dbt's **primitive tests** in the sense they are implemented in the dbt core and can be used in more sophisticated ways. _Schema tests_ are good for enforcing constraints that RDBMS systems typically offer but analytical databases don't. Meanwhile, _data tests_ can test anything with endless possibilities.

Zooming out a bit from dbt, I would like to motivate the value of [testing in software](https://en.wikipedia.org/wiki/Software_testing). The points here are applicable to all software projects including dbt projects.

### Why should I test software?

With software tests, an engineer can:

*   reduce the likelihood of an unexpected outcome or side-effect from code or system under test,
*   reduce the likelihood that a current or future code change will introduce a system error,
*   inspire confidence in the test author and others that the code or system under test works as-intended, and
*   document code and system behavior by illustrating edge-cases.

There are two parts to testing: writing tests and running tests. Both of these parts are necessary for a useful test suite to provide the above benefits.

### How should I test software?

A good test typically verifies *something:* that a piece of code behaves a certain way under certain conditions, or that a system functions in a certain way.

There are many established ways to test software, each with its own strengths and suitability. I would argue that the _best_ way to test highly depends on what is trying to be accomplished, e.g., prevent code regressions, ensure a contract, introduce a new working feature. The level of effort for testing scales depending on the situation.

I would like to highlight two testing categories that I think are relevant to dbt:

*   **[Smoke testing:](https://en.wikipedia.org/wiki/Smoke_testing_(software))** ""Where there's smoke, there's fire."" A smoke test confirms that the system under test looks and/or behaves a certain way. Smoke tests are used in blue/green deployment pipelines as a last-step check before cutover; if the smoke test fails, then the deployment pipeline fails. Smoke tests are difficult to run at development-time because it requires the entire production system to be functioning for the developer. Smoke tests are particularly effective in continuous deployment pipelines to help catch issues difficult to detect at development-time. Smoke tests are a sign of a mature system.
*   **[Unit testing:](https://www.martinfowler.com/bliki/UnitTest.html)** A unit test proves that a ""unit"" of code works as expected. A unit test commonly has three steps: setup, execute the code under test, and verification/teardown section. A unit can vary in size, from a single function, to an entire class. Unit tests are self-contained and typically do not test functionality of external systems (e.g., using HTTP calls or database connections). ""Mocking"" is a common technique used during unit tests and is often essential for unit testing. Unit tests are run at development-time and during continuous integration to detect breakages before code reaches production.

Testing can occur at a variety of points in the software development lifecycle. Unit tests are typically run at development-time (i.e., when the code is being developed) and during continuous integration (i.e., when the code is being merged into the main branch of the project). Smoke tests are typically run during continuous deployment (i.e., when the code/data is being deployed to production).

(For more on testing in software, I recommend to consult [Martin Fowler's writing](https://www.martinfowler.com/testing/)!)

## Community research

I have conducted research about dbt by reading through blog posts, GitHub issues, and dbt Discourse. Iâ€™ve found there are a number of community members who described their attempts at and/or the challenges of building advanced tests to suit their needs:

*   Petyo Pahunchev [pointed out](https://www.infinitelambda.com/post/dbt-testing-tools-gap) that primitive tests (schema tests and data tests) leave a gap in dbt development, saying ""there is no tool to let us deterministically re-create a set of conditions, execute our code and validate the output."" In his post, he suggested a way to create tests using a Python library to allow for a **behavioural-driven development** methodology in dbt.
*  @michael-kaminsky [outlined](https://kaminsky.rocks/2020/09/tdd-for-elt/) the challenges of adopting a **test driven development** approach in ELT. He made a number of good points, including ""it takes a lot of work to generate realistic test-cases"", and ""executing a pipeline and testing it can take upwards of 10 minutes â€” this is way too slow for doing real test-driven development"".
*   MichelleArk [described](https://github.com/fishtown-analytics/dbt/issues/2354) their team's attempt of hand-constructing static CSVs, building macros, and running primitive tests to assert model behavior generally and in edge cases. Ultimately, they wrap up the GitHub issue saying ""a general issue with this approach is that writing assertions for **unit tests** feels quite unnatural in SQL - it's tricky even to get the right semantics for an equality check."" Later in the post, I described my approach to **black box testing** pl/pgsql transformations using Python.
*   @fabrice.etanchaud [noted](https://github.com/fishtown-analytics/dbt/issues/2740) that with the new --defer flag, there may be a way to automate **non-regression tests** ([see diagram](https://stackoverflow.com/a/60448514)). Later in the post, @jtcohen [described](https://github.com/fishtown-analytics/dbt/issues/2740#issuecomment-717670524) how to **mock** a dbt model using macros and run-time variables.

Despite these challenges, there are successful examples of building advanced tests using dbt's primitive tests or by building scaffolding around dbt:



*   @claire's [described](https://discourse.getdbt.com/t/testing-with-fixed-data-set/564/2) how to test with dbt using a fixed dataset. Her fixed dataset consists of ""fake data consisting of all my known edge cases so that I can check that my SQL works."" She noted she only does this testing during development, and itâ€™s not clear if engineers returning to her code are able to run these tests.
*   @claus [presented](https://www.youtube.com/watch?v=jGwUonA3mDg&feature=youtu.be) how to run primitive dbt tests as a **smoke test** during a blue/green data deployment. If the smoke tests pass, then the dbt output is made available for use; otherwise the data deployment fails.
*   @gnilrets [built](https://discourse.getdbt.com/t/testing-data-transformations/733) a well-documented testing framework outside of dbt. A developer using the framework may write **testing specifications** for a data transformation, and the framework verifies the spec.
*   The [dbt-utils](https://github.com/fishtown-analytics/dbt-utils) project has a series of **integration tests** that verify project functionality across the supported database. Meanwhile [dbt-audit-helper](https://github.com/fishtown-analytics/dbt-audit-helper) has macros that can supercharge dbt data tests.

## Conclusions

Based on this research, I conclude the following:

1. Strategies for smoke testing in dbt have been discovered and documented by the community.
2. Meanwhile, there is a high barrier-to-entry to advanced development-time dbt testing strategies. To achieve advanced dbt testing, one either needs expert-level knowledge of dbt, or a high investment of individual time.
3. There's a general desire for advancing development-time testing capabilities inside or outside of dbt and community members are looking for capabilities known in the field of software testing.


## Suggestions

Here are some concrete suggestions that I think will advance testing in dbt:



*   Product: Map out the current development-time testing workflow and map out an improved development-time testing workflow.
*   Technical: Make ""mocking"" a dbt model and dbt source a well-designed and first-class construct in dbt. I think many of the community members I referenced above could benefit from a well-defined approach to mocking in dbt.
*   Documentation: Motivate testing in the dbt docs and present the trade-offs of testing. Promote testing best-practices and advanced testing strategies in the dbt docs. (What does a well-tested model look like? When should tests be run? See [React Testing](https://reactjs.org/docs/testing.html) for an example.)

Looking forward to continuing the discussion!",1
1778,"state-of-testing-in-dbt","Hi!

This is something I think about as well, as a Software Engineer coming into data and using dbt. I love dbt and how it works, it's such a great tool.

One of the things I have also thought about is testing, and how we ensure data is correct when building our pipeline:

- We use schema tests in our models to ensure not_null etc, and using dbt-utils package for helpers.
- We have a csv which has specific columns, setup like this:

```
description,table_name,where_field,where_value,test_field,test_value
A customer has this weird edge case for a field,fct_foobar,customer_id,cd8e741c-1436-4145-bf00-b3e59fbf73f7,my_awesome_field,foopytoopy
```

So this then has a python script which looks for this file under data/data_tests.csv which will build out tests in tests/generated/xxx.sql, which will have the following content:

```
{{ config(tags=['tests']) }}
{{test_where_exists('fct_foobar','customer_id','cd8e741c-1436-4145-bf00-b3e59fbf73f7','my_awesome_field','foopytoopy')}}
```

The macro looks like this:
```
{% macro test_where_exists(model, where_field, where_value, test_field, test_value) %}
select {{where_field}}, {{test_field}} from {{ ref(model) }} where {{where_field}} = '{{where_value}}' and {{test_field}} != '{{ test_value }}'
{% endmacro %}
```

This is great as we can build this against random edge cases and ""known good"" orders. If something breaks we'll know straight away.

We're also planning on using a tool like great expectations to test our data models.",2
1778,"state-of-testing-in-dbt","@joshtaylor thanks for sharing! This is quite clever, at which point in the development lifecycle do you run this test? At development-time, or at deploy-time?

I like the name `test_where_exists`, I wonder if semantic macros in a dbt package could be added to make improve the baseline of development-time testing. An idea I have for this is `exact_match` like so:

```
-- BigQuery Standard SQL:

{% macro exact_match(test_dataset_query, query_under_test) %}
(
    ({{ test_dataset_query }})
    except distinct
    ({{ query_under_test }})
)
union all 
(
    ({{ query_under_test }})
    except distinct
    ({{ test_dataset_query }})
)
{% endmacro %}
```

Then two more macros that can be used similarly:

- `is_contained_within(test_dataset_query, query_under_test)`
- `does_not_contain(test_dataset_query, query_under_test)`

Applying this in a data test (borrowing from [`jaffle_shop`](https://github.com/fishtown-analytics/jaffle_shop)):

```
{% set test_dataset_query %}
select 'mperez0@chronoengine.com' as email, 33 as customer_lifetime_value union all
select 'smccoy1@reddit.com' as email, 23 as customer_lifetime_value union all
select 'kpayne2@cargocollective.com' as email, 65 as customer_lifetime_value
{% endset %}

{% set query_under_test %}
select
    email,
    customer_lifetime_value
from {{ ref('dim_customers') }} as dim_customers
left join {{ ref('raw_customers') }} as raw_customers
    on raw_customers.id = dim_customers.customer_id
{% endset %}

{{ exact_match(test_dataset_query, query_under_test) }}
```

Now imagine being able to define which `ref`s are mocked and how in the above data test file, I think that would be a great developer experience for a self-contained ""unit test"" in a single file. @jerco curious to get your thoughts on the feasibility of that idea.",3
1778,"state-of-testing-in-dbt","We run these tests in development using filtering, but we also have a ""sample"" target set in the dbt `profiles.yml`.

We then have a filter in our models to filter by this id, as we want to filter orders that match usually :slight_smile:

So inside of our staging models, we do:
```
{{filter_if_sample_target_by('order_id')}}
```

we then have a python script which builds this macro:
```
file = open(f""./macros/join_orders.sql"", ""w"")
macro = ""{% macro filter_if_sample_target_by(source_column_name) %} {% if target.name == 'sample' %} where {{source_column_name}} in (""
macro += ""'""+""','"".join(ids)+""'""
macro += "") {% endif %} {% endmacro %}""
file.write(macro)
file.close()
```

Now our dataset in development goes down to ~150 records and is a lot faster to build against. :slight_smile:",4
1778,"state-of-testing-in-dbt","@boxysean thank you so much for pulling together this excellent literature review. You've really nailed  the capabilities and constraints of testing with dbt at the end of 2020, as they are and (even more importantly) as they're understood to be.

I'm excited to watch [two](https://www.getdbt.com/coalesce/agenda/building-a-robust-data-pipeline-with-dbt-airflow-and-great-expectations) [talks](https://www.getdbt.com/coalesce/agenda/empowering-your-data-team-through-testing) about testing + dbt over the next two days; and since I'm already overdue to responding to this, I thought I'd leave a few thoughts here now.

## If you can write it in SQL, you can test it

As I see it today, dbt testing can be quite full-featuredâ€”**the primitive tests can do _a lot_**â€”so long as you're willing to write the code for it. Data tests are a great tool for one-off assurances; you just need the SQL that selects the rows you're sure shouldn't be there. When those assurances need to become assertions, standardized and cross-applicable, rather than copy-pasting data tests and swapping out a line or two, it's simple enough to DRY up that code, parametrize the inputs (`model`, `column_name`), and wrap it in a Jinja macro prefixed `test__`. Write a bit more Jinja, and you can end up somewhere very fancy indeed:

- unit testing of one model's logic via in-file fixtures, following Sean's and Josh's examples above
- mock data in seeds, supplied by an external library like [faker](https://faker.readthedocs.io/en/master/), that stand in place of your real source data via a `source()` override, and enable full end-to-end DAG testing
- leveraging inferential statistics to identify outliers or aberrations, via [BigQuery ML](https://cloud.google.com/blog/products/data-analytics/how-to-use-bigquery-ml-for-anomaly-detection) + [dbt_ml](https://hub.getdbt.com/kristeligt-dagblad/dbt_ml/latest/), or by building on a unified multilingual platform like Databricks

All of which is to say, the fact that dbt only ships with four schema tests never struck us as a limitation: those four are the *starting points* , and the code is simple enough that users can write many, many more. There are a dozen custom schema tests in [dbt-utils](https://hub.getdbt.com/fishtown-analytics/dbt_utils/latest/). @claus wrote several dozen more in [dbt-expectations](https://hub.getdbt.com/calogica/dbt_expectations/latest/), replicating many of the great ones in [Great Expectations](https://greatexpectations.io/).

In this way and many others, dbt is a frameworkâ€”a way of writing code, wrapped in boilerplateâ€”more than it is a library. The same holds true for materializations, strategies, default macros, and the rest: the fact that a user can override, adapt, and extend built-in code to accomplish their unique use case is one of dbt's biggest features.

## Better built-ins

All of that said, I think there are a number of obvious, uncontroversial ways to make tests *more* accessible and powerful out-of-the-box. By virtue of their age, there are also a few things about tests that don't make a lot of sense, and we should fix that. We want to make those changes soon, ahead of [dbt v1.0](https://github.com/fishtown-analytics/dbt/issues?q=is%3Aopen+is%3Aissue+label%3A1.0.0).

Here are some of the things on my wishlist:

* Data tests should be configurable and documentable.
* Schema tests and data tests should both return a set of failing rows, rather than a numeric value count(*) for the former and a set of rows for the latter. Why? Because...
* dbt should make it easier to debug errant tests in development, by (optionally) writing the failing records to database tables.
* Users should have the option of setting warn and error severity via thresholds (absolute or a relative %), similar to source freshness.
* When a user creates a schema test, they should feel they are building a reusable asset, not just hacking together one more macro to maintain. Schema test blocks (!) should have optional configurations like description and human-friendly failure messages. Those assets are even more valuable when packaged up and open sourced.
* At the same time, you shouldn't need a custom schema test to do something obvious, like add a `where` filter or a `limit`. All schema tests should have the ability to apply those configs, right out of the box.

(Check the [v0.20.0 milestone](https://github.com/fishtown-analytics/dbt/milestone/62) for detailed issues on most of the above.)

## Lowering barriers

Will this solve everything? No, definitely notâ€”it doesn't even begin to touch on some of the coolest things Sean noted above. I agree first and foremost, however, that there is too high a barrier separating `unique` + `not_null` from more advanced usage, and not enough scaffolding along the way.

I'd love to end up in a place where we add native support for mock data + unit testing, following an opinionated + thoughtful approach that reflects a community-driven consensus. If we're going to get there, we need to focus on solidifying the simple stuff today: making the case that dbt testing is fully featured in its own right, and fully extensible in yours.

I want dbt developers to feel comfortable investing their time and energy in pushing the limits of tests. I know we have work to do here;  I'm reminded of it every time I hear someone suggest that dbt's built-in schema tests are fine as baby steps, on the way toward using a separate dedicated test framework. (Other frameworks, including GE, are great! If it's not already clear, they inspire a lot of my thinking here.) I hope that the technical changes I listed above are a reasonable set of steps on the way there.

At the same time, I so agree with Sean's suggestion that a big, big piece of this is less about technical change and more about a clearer narrative in the documentation:

- We should talk about schema test definitions more like models, snapshots, and seedsâ€”things you expect to *create*â€”rather than just as things you expect to *use*, and quickly grow out of.
- We should clarify the building blocks for folks who want to build more complex systems. If you're writing unit tests, should you make your equality comparisons using custom code? The widely used [dbt_utils.equality()](https://github.com/fishtown-analytics/dbt-utils#equality-source) schema test? The beloved [audit_helper.compare_relations()](https://github.com/fishtown-analytics/dbt-audit-helper/tree/0.3.0/#compare_relations-source) macro? Why not the [adapter.get_rows_different_sql()](https://github.com/fishtown-analytics/dbt/blob/dev/kiyoshi-kuromiya/core/dbt/adapters/base/impl.py#L1177-L1233) adapter method, which is used in our adapter-testing suite?
- We should encourage more community-contributed tooling that reinforces a culture of testing, tightly integrated with models, such as @tnightengale's [new package](https://hub.getdbt.com/tnightengale/dbt_meta_testing/latest/) that offers a way to enforce test + documentation coverage.
- We should tell a story about tests being expressive attributes of models, ones that can validate, document, and communicate their meaning. They are (dare I say it) rich pieces of metadata.

Wherever possible, we should have solid recommendations, use cases, and trade-offs. The most intrepid dbt developers have already done promising work here. I want to see many more community members taking measurable risks, pulling on some threads, and reporting back with their ideas and findingsâ€”without feeling like they've gone off the deep end. I promise, I'm right there with you.",5
1778,"state-of-testing-in-dbt","Hi all !

As a former software engineer, I met something called ""[design by contract](https://www.eiffel.com/values/design-by-contract/introduction/)"". The idea, proposed by Bertrand Meyer (another French citizen, sorry ;-)), implemented in the Eiffel language, is taken from [Hoare's triples](https://en.wikipedia.org/wiki/Hoare_logic), and I wonder how it could be used to validate a model. I'd like to take a little time to explain myself, and ask you to question the idea heartly !

Folks in the Eiffel world use this every day, and this reduces heavily the need for unit tests that can only test for presence of an error (In fact, there is even a way to automatically produce unit tests based on the notion below). 

The idea to test for correctness, instead of looking for defects, could be translated in our dbt world like this : 
given :

* a model M, implementation of a data transformation referencing zero or more direct upstream models as input
* a list of tests (data tests, schema tests on the direct upstream models) known as M's **precondition**
* a list of tests (data tests, schema tests on M), known as M's **postcondition**

M correctness can be express like this : 

Any run of the model started in a state where its precondition holds will end up in a state where its postcondition holds, something like `dbt test_precondition -m M; dbt run -m M; dbt test_postcondition -m M`

It's quite intuitive, think about a transformation where a numeric column of a upstream model must be non null and positive for example, because M in its transformation computes its square root.

Then, to prove correctness of all/part of your transformation chain, you only have to run it with precondition and/or postcondition enabled on all/a subset of the models. If dbt ends up correctly, that's all correct.

What do you think of this idea ?

With these glasses on, one could already see schema tests being already : 
* part of the postcondition of the model (as it constrains the underlying transformation)
* part of the precondition of all direct downstream models based on the model (as direct downstream transformations may rely on M schema tests).

So one could envision : 

New model properties : 
* `require` :  precondition tests, pretty like `tests` at model level, but with the ability to also list data tests
* `ensure` (could remain `tests`  ) :  postcondition tests, idem, but at model or column level.
In fact, may a direct upstream model have no schema tests, on could even add tests at column level in the M `require` block for needed upstream model columns.

New model configuration : 
* `assertion` : an array of possibly `'require'`, `'ensure'` values, used to configure level of correctness for that model (will test_pre or/and test_post be executed ?).
Thinking out loud, this could also be useful in jinja code, to assert that some condition holds at a given time in a macro's code. One could envision a macro like `check(boolean)`. In that case one could add a extra `'check'` value to the list, used to enable assertions in jinja.

So yes, it's all goes down to a kind of intertwined `dbt run + test` at model level !
And to follow on @jerco 's comment, these tests are more than tests, they are contracts, first class citizens of a model's metadata : *""if you run me under these preconditions, I will ensure you these postconditions""*

Of course, the new config above would be used to disable the tests in production.
To enforce eventual consistency, this mechanism should allow to defer relationships test at the end of the dbt run.

Do you think it could be valuable ?

In fact, this notion is so powerful that it could even be used to synchronize parallel executions of DAG parts (Eiffelists call that [SCOOP](https://en.wikipedia.org/wiki/SCOOP_(software))).

Imagine you partition the DAG, assigning a processor/thread to each partition. model B and model A in two distinct partitions, affected to distinct execution threads. B's precondition contains a data test T referencing A. If T is false, dbt should not fail, but instead wait until T realization, and then run B model and downstream models in the same B's partition.

So dbt run could have an ""infinite"" behaviour mode : each processor doing an endless loop of its DAG partition, applying any wait condition at each model execution. Data tests used in a model's `require` config that are referencing only same partition's models would behave as standard require tests.

We could even add source freshness to the list of waiting conditions.

I am not a graph theory specialist, and have more experience in imperative than functional programming, so there are edge cases for sure...
and easier said than done, for sure...",6
1833,"dagster-vs-prefect","We've been using dbt for a quite a while now and loving it! However as great as it is for working inside of the data warehouse, there's still a lot stuff we need to do before the data gets into the data warehouse and into domain of dbt.

We've been benchmarking the data orchestration tools, and we're considering implementing either Dagster or Prefect. Both of them seem really great and hugely popular inside the scene. And now both of them support dbt as well.

My initial thoughts:
* They both seem to have the same standard functionality and great code usability. They work very similarly. However Dagster has a bit more versatility with integrations (jupyter/papermill is appreciated)
* Dagster seems to have better UI and tools for debugging data pipelines locally. This is hugely beneficial as data pipelines grow more complex.
* Prefect has better cloud operations and less maintenance with native Prefect Cloud service, which is appreciated. We're happy to pay some premium for less work in maintenance. 

Does anybody have any hands-on experience and could give some thoughts? Or any direct recommendations? Or should we consider something else entirely?",1
1833,"dagster-vs-prefect","Also worth considering Airflow, which has hosted versions on astronomer, GCP & AWS",2
1833,"dagster-vs-prefect","Some of my teammates are pretty bullish about Argo if you're on Kubernetes.
https://argoproj.github.io/",3
1833,"dagster-vs-prefect","How finely grained are your pipelines? Are you running on K8s?

Argo works really well if they're coarsely grained. Often that'll be the case with dbt pipelines, given that dbt does much of the aggregation.",4
1833,"dagster-vs-prefect","@robmarkcole - I can't find any information on hosted Airflow on AWS. Can you send a link?",5
1833,"dagster-vs-prefect","https://aws.amazon.com/about-aws/whats-new/2020/11/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/",6
1833,"dagster-vs-prefect","Thank you. That's hot of the press!",7
1833,"dagster-vs-prefect","[quote=""yusamidas, post:1, topic:1833""]
* Dagster seems to have better UI and tools for debugging data pipelines locally. This is hugely beneficial as data pipelines grow more complex.
[/quote]

I've been at the same crossroads just weeks ago, and your thoughts are spot on. It was a tough call, but we ultimately went with Dagster, mostly due to superior tooling (Dagit) + flexible programming model + community. I honestly thing you'll make a good decision either way, but for us it just seemed like Dagster ""thinks"" holistically about the process and challenges of making data applications, whereas Prefect solves for developing and executing pipelines in a very ergonomic way, but it's not as complete.

I saw this post from Nick Schrock on the Dagster slack community, that I think gets at the core of the difference between them:

> Dagster pipelines are more structured and constrained. This allows us have a lot of additional features (a type system, a config management system, a system-managed context object that flows through the compute, among other things). By constrast Prefect pipelines are more minimal and dynamic.
> Another way of framing this difference is that dagster is very interested in what the computations are doing rather than only how they are doing it. We consider ourselves the application layer for data applications (rich metadata, type system, structured events with semantic meaning etc), whereas prefect frames their software in terms of â€œnegativeâ€ and â€œpositiveâ€ engineering. This negative/positive framing is more exclusively about the â€œhowâ€ of pipelines: retries, operational matters etc.",8
1833,"dagster-vs-prefect","It didn't come up here but Ploomber (https://github.com/ploomber/ploomber) is also a major tool in this space. It has seamless integration with Airflow, Kubeflow, Argo etc, so you can deal only with the core coding part. It's well integrated with Jupyter and papermill so you can stay in the interactive environment. We recently added monitoring and alerting for pipelines and an easy way to deploy your experiments to the cloud.

On top of that, it has seamless transition to production since some of the work behind the scene is being analyzed and cleaned into .py files.",9
1853,"new-package-dbt-meta-testing","Hey folks!

I've codified some functionality for ensuring test and documentation coverage into a dbt package called **dbt-meta-testing** ([source](https://github.com/tnightengale/dbt-meta-testing)). 

Previously, I wrote some python scripts to enforce this coverage in the CI at my prior company. I shared them with Nehil and Jonathan at SnapTravel and they made some improvements - but this package is an attempt to make this functionality easily accessible to the wider dbt community. 

This package is in **alpha** release, so there may be changes to the configuration functionality. Feedback and discussion is welcome, so please don't hesitate to open an [issue](https://github.com/tnightengale/dbt-meta-testing/issues).

May your models always be documented and the tests always pass in your favor!

Cheers,

Teghan",1
1853,"new-package-dbt-meta-testing","@tnightengale this is fantastic! I've been looking for a way to track test/documentation coverage at this granular level, and this seems to fit the bill completely. This is gonna go so well with exposures once they come out, allowing us to truly formalize data quality SLOs and make sure they're consistently met as the models underneath evolve.",2
1873,"using-dbt-artifacts-to-track-project-performance","For those who attended Coalesce, I gave a talk called â€œAnalytics on your analyticsâ€ (watch [here](https://www.youtube.com/watch?v=kqyxHCHH0d4)) where I discuss how Drizly is using dbt metadata to improve our dbt run operations and processes. I wanted to provide a more in depth guide to how we implemented using run_results with Dagster and Snowflake (more than I could cover in a 25 minute talk). Iâ€™d love to hear about others who have implemented a similar approach, or how youâ€™re accomplishing the same thing with other tools. Definitely comment below with any examples!

**Tracking dbt model performance**

As our dbt DAG became more complicated we realized that we needed a more sophisticated orchestration tool to handle the execution of our runs, macros and tests we perform with each production run. We decided to use [Dagster](https://dagster.io/) because of its flexibility, strong emphasis on parameterization and ability to support users across our data team (analysts, data scientists, data engineers). Dagster offers a [dbt library](https://dagster.io/blog/dagster-dbt) for the execution of the dbt commands, coupled with [assets](https://docs.dagster.io/overview/asset-materializations) to track dbt artifacts produced with each run. However for more detailed reporting on our runs we wanted the ability to layer our dbt run metadata with all the other data in our warehouse. To do that we needed to ingest the metadata for each run.

**dbt audits V1: Inserting records**

Our first attempt to solve the logging problem was just to include [pre and post hooks](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations/) in dbt that performed inserts into an audits table before and after each run on production (inspired by the [event-logging](https://github.com/fishtown-analytics/dbt-event-logging) package). This ensured that our models and the metadata ingested were tightly coupled, even if the run was performed by an automated process or an analyst performing an ad hoc run via the CLI.

**But there were downsides:**

* **The process was inefficient for the database**: With about 472 models, this equated to at least 944 inserts per dbt run, significantly slowing down our dbt runs. One of the reasons for this project was to be able to prioritize models for optimization so that we could speed up our runs, so this took us further away from that goal.

* **The data was not as rich as we wanted**: we were missing details about model errors, error messages, models that skipped, model tags, and more.

* **We werenâ€™t able to capture errors**: Iif the model failed, the post-hook wouldnâ€™t execute, and weâ€™d have no record of the model failure

*dbt_project.yml*
```

models:

project:

pre-hook:

- ""{{ audit_model_logging(event_type='START') }}""

post-hook:

- ""{{ audit_model_logging(event_type='END') }}""

```

*macro.sql*

```

{% macro audit_model_logging(event_type) %}

{% if target.name == ""prod"" or var('override_logging') == True %}

{% call statement('audit', fetch_result=False) %}

-- Meta insert

INSERT INTO {{target.database}}.audits.model_logging

SELECT

'{{ event_type }}',

'{{ model.name }}',

'{{ model.alias }}',

'{{ this.schema }}',

'{{ this.table }}',

'{{ model.config.materialized }}',

'{{ flags.FULL_REFRESH }}',

-- These fields should only be populated for end runs

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.table_type END),

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.is_transient END),

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.clustering_key END),

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.row_count END),

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.bytes END),

(CASE WHEN '{{ event_type }}' = 'START' THEN NULL ELSE info_schema.retention_time END),

CURRENT_TIMESTAMP,

(SELECT MAX(run_id) FROM {{ target.database }}.audits.run_logging)

-- Hack to allow for start queries if model has never been created before

FROM VALUES ('placeholder')

LEFT JOIN (

SELECT * FROM information_schema.tables

WHERE table_catalog = UPPER('{{ target.name }}')

AND table_schema = UPPER('{{ this.schema }}')

AND table_name = UPPER('{{ this.table }}')

) info_schema

ON 1 = 1

{%- endcall %}

{% endif %}

{% endmacro %}

```

**dbt audits V2: Using run_results.json**

After talking with Claire Carroll and Jeremy Cohen (teamwork!) about our process, they gave us the wonderful advice to try out using run_results.json. We moved away from individual post hooks and instead uploaded the dbt artifact to S3 after every run which would then be ingested into our warehouse with Snowpipe. Now using Dagster, it was easy to add one additional node to our DAG to run immediately after the dbt run occurred. The Dagster UI also became a better interface for analysts to perform ad hoc runs. Using presets we could define a preconfigured template for analysts to run specific models on production while still ensuring that subsequent steps such as ingesting the metadata always occurred.

![|339x528](upload://fWDkNG8bh9AOlXBVSh1fHb6ruOt.png)

*presets.py*
```

ad_hoc = PresetDefinition(

name=""ad_hoc"",

run_config=ad_hoc_run_config,

mode=""prod"",

solid_selection=[""run_model+""],

)

```

**Step 1: Upload run_results to S3**

As I described above, for this step we created an extra step (or solid) in Dagster to upload run_results.json to S3 after our dbt run is complete.

* If anyone knows how to do this with other orchestration tools, let us know in the comments.

**Step 2: Load S3 files into your data warehouse (in our case, Snowflake)**

Using Snowflake, this is accomplished using a Snowpipe that triggers when the new file hits our S3 bucket. The new data will append onto the table each time itâ€™s triggered so we can retain history of our runs.

* This should also be possible with other data warehouses, such as BigQuery and Redshift. Let us know how you accomplish this with those tools.

**Step 3: Create dbt models**

Once we had run_results in Snowflake, and taking advantage of Snowflakeâ€™s support for semi structured data, we could easily create a dbt model that flattens the JSON and parses out the data points we were interested in. run_results.json contains a lot of rich metadata that allows us to track model run times, errors, error messages, tags, and more!

*dbt_modeling.sql*
```

{{

config(

materialized='view'

)

}}

WITH run_results AS (

SELECT

  created_at::DATE AS run_date,

  parse_json(event):results AS contents

FROM {{ ref('run_results') }}

)

SELECT

  run_date,

  r.value:node:resource_type::STRING AS node_type,

  r.value:node:unique_id::STRING AS node_unique_id,

  r.value:node:alias::STRING AS model_name,

  r.value:node:database::STRING AS database_name,

  r.value:node:schema::STRING AS schema_name,

  r.value:node:full_refresh::STRING AS was_full_refresh,

  r.value:node:tags AS model_tags,

  r.value:node:refs AS model_references,

  r.value:node:depends_on:macros AS dependent_macros,

  r.value:node:depends_on:nodes AS dependent_nodes,

  r.value:node:config:materialized::STRING AS materialization,

  r.value:node:config:enabled::STRING AS is_enabled,

  r.value:execution_time::FLOAT AS model_run_time_sec,

  r.value:status::STRING AS run_status,

  run_status = 'ERROR' AS was_error,

  CASE

    WHEN was_error = TRUE

    THEN r.value:error::STRING

  END AS error_message,

  r.value:skip::BOOLEAN AS was_skipped,

  r.value:warn::BOOLEAN AS was_warning,

  r.value:timing[1]:started_at::TIMESTAMP AS execute_started_at,

  r.value:timing[1]:completed_at::TIMESTAMP AS execute_completed_at,

  r.value AS json_blob

FROM run_results,

  LATERAL FLATTEN(input => contents) r

WHERE r.value:node:resource_type::STRING = 'model'

```

Another use case for this data, is to monitor our model freshness. Either because a model did not run or cases where a model is renamed or deleted, but the table is not dropped from snowflake. We created a stale_models dbt model for this use case, which joins in the dbt_modeling model above, with some snowflake metadata from the information schema.

*stale_models.sql*
```

{{

config(

materialized='view'

)

}}

WITH model_build AS (

SELECT DISTINCT

  info.table_catalog AS database_name,

  info.table_schema AS schema_name,

  info.table_name AS table_name,

  models.execute_completed_at AS audit_timestamp,

  info.last_altered

FROM prod.information_schema.tables info --must reference PROD for accurate results

LEFT JOIN {{ ref('dbt_modeling') }} models

  ON info.table_schema = UPPER(models.schema_name)

  AND info.table_name = UPPER(models.model_name)

)

SELECT

  database_name,

  schema_name,

  table_name,

  MAX(audit_timestamp) AS model_last_built,

  MAX(last_altered) AS table_last_altered,

  DATEDIFF(DAYS, table_last_altered, NVL(model_last_built, CURRENT_DATE)) AS days_stale

FROM model_build

-- Exclude certain schemas
WHERE schema_name NOT IN (

'AUDITS',

'INFORMATION_SCHEMA'

)

GROUP BY 1,2,3

```

**Step 4: Visualize the data/build a dashboard**

To visualize this data, and make it accessible to the rest of our analytics team, we created a Looker dashboard which contains views that show our current day run stats, run errors, model run times, model run time trends, and stale models. We also complement this with some snowflake data on our dbt warehouse credit usage trends. This gives our team a one stop shop for all dbt run questions, and allows us to easily track errors as well as which models may need a reactor or optimization work. We were also able to set up alerting through the Looker dashboard, which will send an alert to our #analytics_alerts Slack channel if the model error count increases above zero, for example.

*P.S. This functionality will be coming to dbt cloud! Stay tuned for updates.*",1
1926,"what-is-the-best-practice-for-deploying-airflow-together-with-dbt","Hello : ) 

My objective is to deploy airflow and dbt on Kubernetes (more precisely, I am deploying on EKS and using a Helm Airflow chart and using Celery executor). I am trying to grasp, what are the best practices for deploying airflow together with dbt?

* Should I include both airflow and dbt in the same container? And do I have to?
* Is there a best practice in terms of where to keep the dbt models? (I am curious if I can only install dbt in the container but keep the dbt models outside the container the same way I plan to do with the airflow dags).

I am happy to provide any details if needed.",1
1926,"what-is-the-best-practice-for-deploying-airflow-together-with-dbt","A solid implementation used by many community members is to use Docker, K8 Operator and Airflow.

At a high level, the steps to do so are:

1. Build, tag and push an image with the most recent version of your dbt project on each of your deployments
2. Use the Kubernetes Operator for all tasks in Airflow that run dbt
3. Have the K8 Operator pull and run the latest tagged image, with the relevant dbt command",2
1946,"incremental-model-use-db-cluster-column","Hi. We are writing an incremental script against a very large table on Snowflake. This table needs to be clustered by date for downstream consumption. The unique key for the merge is an ID. The merge statement performs poorly unless we can include a date range. Here is a simplified example of the manual SQL we want to emulate: 
>     merge into tgt
>     using src
>     	on tgt.order_id = src.order_id
>     	and tgt.order_dt between '1992-01-01' and '1992-01-03'  -- THIS IS FILTER WE WANT TO ADD
>     when matched then update set tgt.order_status_cd = src.order_status_cd
>     when not matched then
>     	insert (order_id, order_status_cd, order_dt)
>     	values (src.order_id, src.order_status_cd, src.order_dt);

The incremental materialization doesn't seem to support this. Is the only option a custom materialization?  Any suggestions? 

Thank you!",1
1946,"incremental-model-use-db-cluster-column","Hi,
I have no idea on how to add such date logic to the generated dbt code - indeed, writing an custom materialization should be possible.
But,I would say in case of big tables you need to cluster the data for the right purpose, and if there are two purposes for the data set in mind, then split the data set in multiple tables per purpose. In this case I could imagine a following way of doing:
have  a main table containing time invariant fields maintained with inserts only, clustered by your date field (for downstream consumption reason), then have a secondary table clustered by order-id where you maintain your order-updates. Downstream consuming should then of course make use of join logic - but then again the join logic makes use of the clustering key in the secondary table.

And, now that I am writing my first idea, I come to think of another one, that might work. I conclude/assume  that your clustering key is currently the order_dt. What if you introduce a replacing unique key based on a combination of order_dt and order_id , like a concatenation of the two (date field first). Your filter key for downstream consuming will still be order_dt, but that field would be equally nicely clustered as the actual new cluster key.

Well I could not say which one would work better. The second one, I believe, comes the closest to your intended addition of a date filter logic.

As last comment, I would like to say that I personally favour a principle like 'inserts-only' DWH. So I would investigate solutions like separating time variant fields from the time invariant fields (like Data Vault does - all tables insert-only) and joining them during downstream consumption.

Regards,
Hugo.",2
1946,"incremental-model-use-db-cluster-column","Hi there,
We used BigQuery but I can't see that the basic idea would change. 
Our solution as to get a list of partition values to add to the MERGE statement.
* We didn't want to hardcode the values but rather just use what was necessary. 
* We needed the code to be dynamic, easily used by any incremental model, with a unique key. 
* We needed to know the partition field name and the source of the distinct values

This is our solution to the problem:
* We added to custom options to the config block of our model
```  python
{{config  (
        partition_filter_field  = # This isn't mandatory, it can be derived from the model's config
        partition_filter_source = # This is mandatory for the code to be 'activated'. This can point to another model or to the current model i.e. {this}
) }}
```
* We changed the changed the `get_merge_sql` macro added the lines of code below.
``` jinja SQL
{# START - Determine the value list of Partition Field. #}
{# If Partition filter is provided in config block #}
    {%- set partition_filter_field = config.get('partition_filter_field') -%}
    {%- set partition_filter_source = config.get('partition_filter_source') -%}
    {% if partition_filter_source %}
        {# 1. Get Partition by information #}
        {% if partition_filter_field %}
            {%- set partition_by_field = partition_filter_field -%}
        {% else %}   
            {%- set raw_partition_by = config.get('partition_by', none) -%}
            {%- set partition_by = adapter.parse_partition_by(raw_partition_by) -%}
            {%- set partition_by_field = partition_by.field -%}
        {% endif %}

        {# 2. Get the view/table name to uses for Partition selection #}
        {% set partition_filter_source_ref %}
            {{ref(partition_filter_source)}}
        {% endset%}

        {# 3. Get values list for Partition field #}
        {% set value_list_sql %}
            select distinct {{partition_by_field}} FROM {{partition_filter_source_ref}}
        {% endset %}
        {%- set values = run_query(value_list_sql) -%}
        {%- set value_list = values.columns[0].values() -%}

        {# 4. Build WHERE clause #}
        {%- set partition_clause = [] -%}
        {%- for value in value_list -%}
            {%- do partition_clause.append('DBT_INTERNAL_DEST.'~ partition_by_field ~ ' = ""' ~ value ~ '""') -%}
        {%- endfor -%} 

        {# 5. Add WHERE clause to predicates #}
        {% do predicates.append('( ' ~ partition_clause |join(' or ') ~ ' )') %} {# BigQuery requires ORs for multiple values in a MERGE statement #}
    {% else %}  
    {% endif %}

{# END - Determine the value list of Partition Field. #}
```
* We ended up with a MERGE statement like this
``` sql
        on ( DBT_INTERNAL_DEST.PARTITION_DATE = ""2021-01-06"" OR DBT_INTERNAL_DEST.PARTITION_DATE = ""2021-01-07"" ) and 
            DBT_INTERNAL_SOURCE.UNIQUE_KEY = DBT_INTERNAL_DEST.UNIQUE_KEY
```

Again this is the BigQuery version. You'll need to look into the Snowflake version.
Hope this helps.",3
1946,"incremental-model-use-db-cluster-column","excellent.  I think this is exactly what we're looking for.  Thank you!",4
1946,"incremental-model-use-db-cluster-column","Hello @meurant.naude ,
I'm glad to come across your post! I'm trying to implement exactly what you showed, but I'm still very new to dbt. You wrote that you added the code you listed in your post. Is it possible to share your entire ""get_merge_sql""?  I found the source file, but I'm trying to piece together how your code and this code combine, so I'm hopeful you can share your entire code, or just simply share your actual file (even better).

 https://github.com/dbt-labs/dbt-core/blob/1071a4681df91633301fdf23e34de819b66fbeb7/core/dbt/include/global_project/macros/materializations/models/incremental/merge.sql

It's a pity this functionality isn't baked-in, especially since it's the only way to improve performance in dbt-GBQ merge, aside from insert_overwrite. I see in the default code there is a predicates variable, but it's not useful for this purpose, it seems.

Thanks!
Eric",5
1946,"incremental-model-use-db-cluster-column","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
2043,"how-to-implement-dynamic-data-masking-on-redshift","Ashley Van Name from JetBlue gave an [excellent talk at Coalesce talk](https://www.getdbt.com/coalesce/agenda/securing-data-at-scale-with-dbt--snowflake) that shares how JetBlue was able to secure their data using dbt in combination with a Snowflake feature, [dynamic data masking](https://docs.snowflake.com/en/user-guide/security-column-ddm-intro.html).

The nice thing about the Snowflake feature is that the policy is applied at run time â€” regardless of whether the underlying data is stored as a table or view.

For those of us not on Snowflake, it begs the question â€” what options do we have? How can we apply column-level policies to make sure our data is secure? Here is our recommended approach for Redshift users, would love to know if the same approach can be used by BigQuery users too!

# Part 1: Understanding data masking logic

In essence, we want to run logic that masks data depending on who is running the query. We can achieve this with a case statement, the current_user() function, and the `md5` function:

```
select
 case current_user()
   when 'admin' then birthday
   else md5(birthday)
 end as masked_birthday,
 ...

from ...

```

If we encode this logic into a view definition, when a user named `admin` selects from this view, they will see someoneâ€™s birthday, whereas anyone else will see an obfuscated value.

```
create view dbt_dave.users as (
  select
   case current_user()
     when 'admin' then birthday
     else md5(birthday)
   end as masked_birthday,
   ...

  from â€¦
)
```

Itâ€™s important to note that if you were to create this model as a table instead, the value would get hard-coded depending on who ran the `create table as` statement, thereby removing the â€œdynamicâ€ aspect of this approach.

# Part 2: Implementing this logic in a dbt project

Step 1. Turn the logic into a macro

Since we will probably want to use this logic in multiple places, itâ€™s worth turning it into a macro:
```
{% macro mask_column(column_name) %}

 case current_user()
   when 'admin' then {{ column_name }}
   else md5({{ column_name }})
 end as {{ column_name }}
 
{% endmacro %}
```

Step 2, Take 1: Incorporate it into a model (thatâ€™s materialized as a view)

At first, we called the logic in models where we needed to mask data:
```
{{ config(materialized=â€™viewâ€™) }}
select
  {{ mask_column(â€˜first_nameâ€™) }} as first_name,
  {{ mask_column(â€˜last_nameâ€™) }} as last_name,
  favorite_bagel_flavor
from {{ ref(â€˜employeesâ€™) }}
```
This worked, but there was no safeguard against someone building this model as a table. We considered working around this by raising an exception if a non-view materialization was called, but that only solved half the problem - if a downstream model was materialized as a table, it would undo our good work. Further, it would be hard to manage our permissions correctly if both sensitive and masked data lived in the views and tables within the same.

It instead seemed cleaner to figure out a way to create a view with the masking logic on top of our model,  in a separate schema, and only expose that schema to our end users.

Step 2, Take 2: Views on top of models

For each model that we need to apply data masking to, we want to create a view that masks any PII columns, in a separate schema. So, we created a macro that takes two arguments â€” the schema we want to materialize the view in, and the columns we deem to be PII. We also need to first create the schema if it doesnâ€™t yet exist.

```

{% macro create_data_masked_view(schema, columns_to_mask) %}
   {% if execute %}
  
   {# get all columns in the relation #}
  
       {% set model_cols = adapter.get_columns_in_relation(this) %}
      
       {# create Relation object for masked view #}
      
       {%- set masked_view = api.Relation.create(
             database=this.database,
             schema=schema,
             identifier=this.identifier) -%}
            
       {# create schema #}
      
       {% do adapter.create_schema(masked_view) %}
      
       {# create masked view in new schema for sensitive columns #}
      
       {% set view_sql %}
 
           create view {{ masked_view }} as (
 
               select
                   {% for col in model_cols %}
                       {% if col.name in columns_to_mask %}
                       {{ mask_column(col.name) }} as {{ col.name }}
                       {% else %}
                       {{ col.name }}
                       {% endif %}
                       {{ "","" if not loop.last }}
                   {% endfor %}
               from {{ this }}
           )
 
       {% endset %}
      
       {% do run_query(view_sql) %}
      
       {% do dbt_utils.log_info(""Masked view created at: "" ~  masked_view ) %}
      
   {% endif %}
  
   select 1=1
  
{% endmacro %}

```

Since we need to run this after a model is created, post-hooks come in really handy here:

```
{{
   config(
       post_hook=""{{ create_data_masked_view(
           schema='public_analytics',
           columns_to_mask=['first_name', 'last_name']
       ) }}""
   )
}}
 
select
...
```

We put together an example project [here](https://github.com/fishtown-analytics/dbt-labs-experimental-features/tree/dynamic-data-masking-redshift/dynamic-data-masking-redshift), which you can clone and try out yourself!


Step 3: Granting Redshift permissions

The last step here is to grant permissions on the views in each schema to the users who should/should not be able to view the protected data. This would look something like:

```
-- analysts get access to the schema with our masked view
grant all on schema public_analytics to group analysts;
grant select on all views in schema public_analytics to group analysts;

-- admins get access to the private schema with unmasked PHI
grant all on schema private_analytics to group admins;
grant select on all views in schema private_analytics to group admins;
```
# Additional considerations
- This same approach could be implemented as a UDF within Redshift rather than a macro. The same case statement logic could be defined as a custom function, and referenced in the dbt code directly. More on UDFs here. 
- This should be extensible to Big Query. Rather than the current_user() variable, Big Query has a session_user() variable that can be referenced. We havenâ€™t done any testing on this, but theoretically it should work in the same way!
- We use a simple md5 function to mask the data, but there are other, more secure ways of hashing. As discussed in this related article, as well as Ashleyâ€™s talk, you can incorporate a â€œsaltâ€ value to add deterministically-random numbers to the database values prior to hashing to improve the security of the PHI data. Alternatively, you could use sha rather than md5. 
- We chose to maintain the list of users who get access to the PHI directly in the macro. For simplicity of updating, this could be maintained as a project variable referenced in the macro, and the macro itself wouldnâ€™t need ongoing maintenance. 
- Additionally, rather hardcoding the list of users with access to the protected schema, you could follow specific naming conventions for your Redshift users, and reference those in the macro. For example, all users in the `admins` user group could have the prefix `admin_` on their username, and then the logic could look like:

```
select
 case 
   when current_user() ilike 'admin_%' 
      then birthday
   else md5(birthday)
 end as masked_birthday,
 ...

from ...
```

Alternative approaches:

- This dynamic masking could also be done at the column level in your BI tool. If youâ€™re using something like Looker, you can use their user attributes functionality to dynamically allow users to see the PHI in the underlying data tables. This is a little more difficult to maintain, as youâ€™d need to write the logic directly into the column definition of each field of interest, and ensure that only views are connected to your BI tool. We feel like this is more easily managed in the transform layer, but it can certainly be done here too!",1
2043,"how-to-implement-dynamic-data-masking-on-redshift","This is awesome! Great writeup @dave.connors. 

Had you considered using the [meta property](https://docs.getdbt.com/reference/resource-properties/meta/#designate-a-source-column-as-containing-pii) on columns to tag your columns as containing PII instead of hardcoding the list of columns in each place?",2
2043,"how-to-implement-dynamic-data-masking-on-redshift","Hey @joellabes ! 

This is a fantastic question, and one we probably should have considered earlier on! :sweat_smile:

The meta property can absolutely be used to tag columns as PHI/PII, like the example in the docs site.  This column property is available in the [`column` object](https://docs.getdbt.com/reference/dbt-classes#column), and can be referenced in the macro jinja. 

@claire  took a swing at an updated version of this macro on [this branch](https://github.com/fishtown-analytics/dbt-labs-experimental-features/commit/c12401064f2dc12278fd4a69a2e12fa3562a2687). This version does not accept `columns_to_mask` as an argument in the macro, and instead relies on a PII `meta` tag in the `schema.yml` configuration. Otherwise, it works exactly the same, and doesn't require any updating of the post hook! Super elegant!

The choice between the original version and this `meta` version will depend on where you want the complexity of tagging PII columns to live. You can either have slightly more complex `post_hook` logic to directly specify the columns to mask, or slightly more complicated `schema.yml` files. 

Thanks again for the suggestion Joel!",3
2043,"how-to-implement-dynamic-data-masking-on-redshift","Another approach, assuming each user querying the database is using their own user (including on Looker, leveraging parametrized connections) rather than a service account, you could use the ABAC (attribute-based access control) pattern.

It requires maintaining a table containing users and attributes. You then join that attribute table to the table on which you want to apply security within a view, and only expose the view to the user.

Here's a SQLFiddle demonstrating row-level security AND dynamic data masking based on the user querying the table.

http://sqlfiddle.com/#!9/03c836/1",4
2079,"orchestrating-fivetran-and-dbt-with-airflow","In the dbt community, a common question that comes up is how to sync dbt runs with oneâ€™s extract and loader tool. Fivetran connectors and dbt jobs are one particularly common pairing that comes up, and Analytics Engineers typically face a choice in how to orchestrate the two. On the one hand, the load and transform steps can be scheduled and run independently. Alternatively, they can be linked into an event-driven workflow. Airflow is a great tool for creating and running this type of workflow. Setting up a system to trigger and manage events increases the complexity at first, however there are benefits that make the results worth the time spent. In particular, weâ€™ve observed several concerns raised in the [dbt Slack community](https://community.getdbt.com/):

* There are a large number of related load and transform jobs, making it difficult to manage their separate schedules. This can be particularly painful when a transformation job depends on multiple upstream data loaders. See this [thread](https://getdbt.slack.com/archives/C0VLZM3U2/p1559567039041700).
* You want to pass parameters to the DAG run based on the outcome of the data loader(s). See this [thread](https://getdbt.slack.com/archives/C0VLZM3U2/p1541412766068100).
* Itâ€™s valuable to reduce overall latency of the load + transform process due to downstream operational dependencies. These might include supplying fresh data for a machine learning model to consume, or reporting SLA requirements.

If these factors are not a high priority, then itâ€™s completely valid to use scheduled processes. However, many of the larger organizations (2,000+ employees) that @amy and I work with as members of the Solutions Architecture team at Fishtown view these as high-priority items. In that case, event-driven architectures like the one we describe in this article are key.

**Summary of Approach**

Our goal is to present a simplified, linear workflow and illustrate how to coordinate tasks using the [Fivetran](https://fivetran.com/docs/rest-api) and [dbt Cloud](https://docs.getdbt.com/dbt-cloud/api/) APIs with Airflow [XComs](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html?highlight=xcom#concepts-xcom). Note that an organization using dbt Core could accomplish a very similar workflow using the Airflow Bash Operator to trigger a dbt run. Our friends at [Astronomer](https://www.astronomer.io/) have a great series of blog posts featuring orchestration of dbt Core with Airflow. See [part 1](https://www.astronomer.io/blog/airflow-dbt-1) and [part 2](https://www.astronomer.io/blog/airflow-dbt-2). That said, there are some additional benefits to using dbt Cloud. These include the ability to run Pull Request checks with dbt Test independently of Airflow use.

**With this setup we have:**

* Isolation between the data loading (Fivetran), transformation (dbt) and orchestration (Airflow) functions in the stack
* Minimal re-work on the Airflow side as the dbt transformation and Fivetran processes grow in complexity over time
* Leveraged dbtâ€™s native ability to parallelize job runs.

**What you need to get started**

You can find the codebase we implemented in writing this article [here](https://github.com/fishtown-analytics/airflow-fivetran-dbt). To be clear, we didnâ€™t seek to implement a complete production-ready system for this task, but rather to create a starting point.

**To implement this yourself, at a minimum you will need access to the following:**

* Fivetran API Key and API Secret. These are encoded into an api token in base64. The format is api-key:api-secret. [This link](https://fivetran.com/docs/rest-api/getting-started) documents how to create this value yourself.
* dbt Cloud API key. Navigate to Profile > API Access from the dbt Cloud console to generate this.
* Snowflake database to accept data loads and run the transforms.
* The dbt Cloud Account ID, which is used in the API calls to dbt Cloud.
* Access to a Google Cloud Platform account to host the Airflow server.

**Dataset and use case**

We used a publicly available dataset on [Pokemon](https://www.kaggle.com/abcsds/pokemon) for this exercise, saved in a Google Sheet. Our use case is to use dbt to aggregate the average HP score across pokemon catches. Your dbt transformation process could evolve substantially while keeping this same architecture in place.

**Solution Architecture**

![|624x453](upload://lkUFpsKKxOR4zPhLkjfRbRGYKYP.png)

**Hosting the Airflow server**

We hosted the Airflow server on a virtual machine running in a VPC within [Google Cloud Platform](https://console.cloud.google.com/). We followed Jostein Leiraâ€™s excellent [Airflow setup guide](https://medium.com/grensesnittet/airflow-on-gcp-may-2020-cdcdfe594019) to get the Airflow server up and running.

** We recommend using a tool like [Astronomer](https://www.astronomer.io/), a managed Airflow interface, if you want the convenience of a user-friendly interface like dbt Cloud and Fivetran.

**Version controlling Airflow and dbt**

We used two git repositories - one for the [Airflow code and server setup](https://github.com/fishtown-analytics/airflow-fivetran-dbt) and one for the [dbt Project](https://github.com/fishtown-analytics/airflow-fivetran-dbt--dbt-jobs). Though it would be possible to combine them into a single repository, having separate codebases for dbt and Airflow reflects how each system is functionally separate in our architecture.

**Setting up data loads**

For this use case, we used Fivetranâ€™s [google sheet connector](https://fivetran.com/docs/files/google-sheets) to run the data loads. In the event you arenâ€™t using Fivetran for data loading, that element of the solution would change to either 1) use the API for a different data loading tool or 2) Code the data loading function into the Python Operators running in the Airflow DAG.

**Data transformation process in dbt**

Our dbt models are illustrated below. Agg_pokedex_summary is the final table we want to surface in the analytics layer of the warehouse.

![|624x292](upload://eKPckb5qZ5wFSMCCYZnKITYfWXq.png)

This is clearly a simplistic DAG. In our case, we are focused on the relationship between the data loader and the transformation more so than the complexity of the transformation itself.

**Orchestration process in Airflow**

The Airflow DAG is illustrated below.

![|624x176](upload://5pdoBfNYn24SCy7S7TmaUCqbAx0.png)

The input to this Airflow DAG is a simple mapping of the Fivetran connector_id and matching dbt job name:

> {
> 
> ""connector_id"": ""warn_enormously"",
> 
> ""dbt_job_name"": ""pokemon_aggregation_job""
> 
> }

These values are stored in the `dag_run` configuration within each Python Operator in the Airflow process. They can be accessed in code like this:

`connector_id = kwargs[â€˜dag_runâ€™].conf[â€˜fivetran_connector_idâ€™]`

**Communicating between Airflow tasks**

Communication of state across Airflow tasks can be accomplished using [XComs](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html?highlight=xcom#concepts-xcom). By default, tasks in Airflow execute in separate runtimes, and donâ€™t share variables. So, we canâ€™t declare a variable in one task and pass it to a downstream task without writing that data to a common storage location. XComs are a convenient way to accomplish this in Airflow. As your use cases grow more complex, these object types are especially useful for developing conditional logic in your pipeline. For example, you could trigger a subsection of the dbt dag using the [model selection syntax](https://docs.getdbt.com/reference/node-selection/syntax/) based on the results of prior Airflow tasks.

For example, in the â€˜dbt_jobâ€™ task we can add elements for each task like so:

> run_id = trigger_resp['id']
> 
> kwargs['ti'].xcom_push(key='dbt_run_id', value=str(run_id))

And in downstream tasks such as â€˜get_dbt_job_run_statusâ€™, we are able to extract them:

> ti = kwargs['ti']
> 
> run_id = ti.xcom_pull(key='dbt_run_id', task_ids='dbt_job')

With each of these elements in place, we have whatâ€™s needed to do things such as extracting the dbt job run status from the dbt Cloud API. The dbt run_id and account_id elements become variables in a GET request to the dbt Cloud endpoint.

https://cloud.getdbt.com/api/v2/accounts/{accountId}/runs/{runId}/

**Comparison to other approaches**

Itâ€™s important to note that the solution we present here isnâ€™t the only way to organize this type of process. Weâ€™ve spoken with a few dbt users who use Airflow to extract and rebuild the dbt DAG, *then* run each dbt model using the extracted SQL. This provides the benefit of allowing Airflowâ€™s ability to provide more sophisticated retry logic than dbt. In our opinion, isolating the system that handles orchestration (Airflow) from the transformer (dbt) makes the overall process easier to understand and maintain. Plus, as the dbt DAG grows, using Airflow to extract and build the DAG introduces additional maintenance overhead to ensure the DAG reconstruction goes smoothly and that the runs are parallelized intelligently.

**How to build this yourself**

For further details on how we implemented this, please see the README file, Airflow code, and environment setup shell scripts in [this repository](https://github.com/fishtown-analytics/airflow-fivetran-dbt). A real deployment would very likely include additional logic to enable retries and conditional execution flow in the Airflow DAG. Use this as a guide for how one could coordinate between data loading and transformation functions.

**What do you think?**

Our opinion is that this simple setup plays to the unique strengths of both Airflow and dbt:

* Airflow focuses on scheduling and process flow
* dbt - deployed as dbt Cloud in our example - handles the meat of the data transformation

What arenâ€™t we thinking about that you think should be considered here? We would love to continue the discussion with you!",1
2079,"orchestrating-fivetran-and-dbt-with-airflow","Thanks for the detailed writeup! We use a similar approach for some of our external datasets to accompany our traditional hourly/nightly/weekly runs. We tag event-driven models as ""external"".

To ensure ""external"" models are only run in the event-drive pipeline and not captured in the hourly/nightly runs, we define use selectors to generate jobs like dbt run -m tag:hourly --exclude tag:external. I've found this is helpful to ensure that the same model isn't being built and rebuilt in multiple places.",2
2079,"orchestrating-fivetran-and-dbt-with-airflow","Hi @jt_st, thanks for sharing! I like the idea to use the [model selection synxtax](https://docs.getdbt.com/reference/node-selection/syntax/) as a way to link the part that's external to dbt with the correct model(s).",3
2079,"orchestrating-fivetran-and-dbt-with-airflow","[quote=""jt_st, post:2, topic:2079, full:true""]
Thanks for the detailed writeup! We use a similar approach for some of our external datasets to accompany our traditional hourly/nightly/weekly runs. We tag event-driven models as â€œexternalâ€.

To ensure â€œexternalâ€ models are only run in the event-drive pipeline and not captured in the hourly/nightly runs, we define use selectors to generate jobs like dbt run -m tag:hourly --exclude tag:external. Iâ€™ve found this is helpful to ensure that the same model isnâ€™t being built and rebuilt in multiple places.
[/quote]

thanks for the awesome information.",4
2160,"dbt-model-think-unit-tests-poc","Hello! I recently read https://shopify.engineering/build-production-grade-workflow-sql-modelling which describes Shopify's approach to testing in dbt. 

At my day job, we've been using dbt for a couple of months now. We've been planning on providing some sort of targeted ""unit"" testing framework, which allows users to validate an individual model's logic. I took a stab at it this weekend and wanted to solicit some feedback on the approach. This is packaged as a dbt package, but it's not a package announcement. This is a barely working testing framework that just supports postgres :p 

I defined the following design constraints:
- Tests must be written as python tests (your standad test_*.py files, using python test classes). This allows end users to choose their python framework and reporting methods. It should be very familiar to anyone using python.
- Tests should support assertions using pandas dataframes. Pandas makes it easy to create relational constructs, and to transforms dataframes into sql tables and vice-a-versa
- Tests should provide any easy way to mock model dependencies, i.e. ref and source. Tests should only focus on a single model's transformations and not a dependency graph of models. 
- The test strategy should not require model definitions to change in any way. This means no conditionals, or special calls. The test framework should evaluate the model as close as possible to the definition that will be running in production.

The POC framework is available here:

https://github.com/dm03514/dbt-model-tests

Following shows an MVP of usage.

The POC initialized a new dbt project. I then created 2 tests, one for each of the ""example"" models. Each of the tests live alongside the models:

```
models/example/
  my_first_dbt_model.sql
  my_second_dbt_model.sql
  test_my_first_dbt_model.py
  test_my_second_dbt_model.py
```

The first example model looks like:

```
# my_first_dbt_model.sql
{{ config(materialized='table') }}

with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data
```
Notice that it contains no dependencies (no refs or sources).

This is a pretty simple case. The test framework uses dbts own ""adapters"" to get a connection to the database specified in the config file.

Currently, it assumes a static config file used for testing, located in the conf/ directory.

The lack of dependencies makes this test really simple:

```
import pandas as pd

from dbtmodeltest.testcase import DBTModelTestCase


class MyFirstDBTModelTestCase(DBTModelTestCase):

    def test_output_rows(self):
        df = pd.DataFrame([
            (1,),
            (None,)
        ], columns=['id'])

        out_df = self.execute_model(
            model='models/example/my_first_dbt_model.sql',
        )

        self.assertDFEqual(df, out_df)
```


```
$ python -m unittest models/example/test_my_first_dbt_model.py
Running with dbt=0.19.0
Found 2 models, 4 tests, 0 snapshots, 0 analyses, 139 macros, 0 operations, 0 seed files, 0 sources, 0 exposures

15:27:58 | Concurrency: 8 threads (target='ci')
15:27:58 |
15:27:58 | 1 of 1 START table model test.my_first_dbt_model..................... [RUN]
15:27:58 | 1 of 1 OK created table model test.my_first_dbt_model................ [SELECT 2 in 0.10s]
15:27:58 |
15:27:58 | Finished running 1 table model in 0.20s.

Completed successfully

Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
""test"".""test"".""my_first_dbt_model""
    id
0  1.0
1  NaN
.
----------------------------------------------------------------------
Ran 1 test in 1.183s

OK
```

The second test gets much more complicated.
The second test exercises the `my_second_dbt_model.sql` which looks like:

```
select *
from {{ ref('my_first_dbt_model') }}
where id = 1
```

The test framework needs to hook into the `ref` tag and replace it with a pandas dataframe. The test for this looks like:

```
import pandas as pd

from dbtmodeltest.testcase import DBTModelTestCase


class MyFirstDBTModelTestCase(DBTModelTestCase):
    def test_only_returns_id_1(self):
        my_first_dbt_model = pd.DataFrame([
            (1,),
            (2,),
            (None,)
        ], columns=['id'])

        out_df = self.execute_model_with_refs(
            model='models/example/my_second_dbt_model.sql',
            my_first_dbt_model=my_first_dbt_model,
        )

        expected_df = pd.DataFrame([
            (1,),
        ], columns=['id'])

        self.assertDFEqual(expected_df, out_df)
```

This shows the output from the test execution:

```
$ DBT_MODEL_TEST_ENABLED=1 DBT_MODEL_TEST_IDENTIFIER_PREFIX=""test1_"" python -m unittest models/example/test_my_second_dbt_model.py
creating table: test1_my_first_dbt_model
Running with dbt=0.19.0
Found 2 models, 4 tests, 0 snapshots, 0 analyses, 139 macros, 0 operations, 0 seed files, 0 sources, 0 exposures

15:32:36 | Concurrency: 8 threads (target='ci')
15:32:36 |
15:32:36 | 1 of 1 START view model test.my_second_dbt_model..................... [RUN]
15:32:36 | 1 of 1 OK created view model test.my_second_dbt_model................ [CREATE VIEW in 0.09s]
15:32:36 |
15:32:36 | Finished running 1 view model in 0.18s.

Completed successfully

Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
""test"".""test"".""my_second_dbt_model""
   id
0   1
.
----------------------------------------------------------------------
Ran 1 test in 1.137s

OK
```

The strategy for replacing ""ref"" with custom data leverages a custom macro. This macro prefixes the table with a prefix defined in the environment:

```
{% macro ref(model_name) %}
  {% set dbt_model_test_enabled = env_var('DBT_MODEL_TEST_ENABLED', False) %}
  {# set dbt_model_test_database = env_var('DBT_MODEL_TEST_DATABASE', '') #}
  {# set dbt_model_test_schema = env_var('DBT_MODEL_TEST_SCHEMA', '') #}
  {% set dbt_model_test_identifier_prefix = env_var('DBT_MODEL_TEST_IDENTIFIER_PREFIX', '') %}

  {{ log(""Running custom:ref "" ~ model_name) }}

  {% if dbt_model_test_enabled == '1' %}
    {{ log(""DBT_MODEL_TEST: model test enabled"") }}
    {% set rel = builtins.ref(model_name) %}
    {%
      set newrel = rel.replace_path(
        identifier=dbt_model_test_identifier_prefix + model_name
      )
    %}
    {% do return(newrel) %}
  {% else %}
    {{ log(""DBT_MODEL_TEST: model test disabled"") }}
    {% do return(builtins.ref(model_name)) %}
  {% endif %}
{% endmacro %}
```

This points our dbt model to the prefixed test table. The final step is to create the prefixed test table using the dataframe provide. 

```
        engine = create_engine(self._adapter_sqlalchemy_conn_string())
        with engine.connect() as conn:
            for ref_name, ref_df in ref_dfs.items():
                table_name = self.identifier_prefix + ref_name
                # drop table and cascade if exists...
                conn.execute('DROP TABLE {} CASCADE'.format(table_name))
                print('creating table: {}'.format(table_name))
                ref_df.to_sql(
                    name=table_name,
                    con=engine,
                    if_exists='replace',
                    index=False,
                )

        return self.execute_model(model)
```

This is accomplished using sql alchemy to convert the dataframe to a table using `to_sql(...)`. 



I'm wondering:
- Does replacing ref/source with a custom macro make sense? 
- Has anyone else done this?
- If so, which strategy did you use? 


I appreciate any feedback on this approach. Thank you.",1
2160,"dbt-model-think-unit-tests-poc","Cross posting an light view proposal I made in another thread, hoping to get some community feedback on the approach. 

https://discourse.getdbt.com/t/testing-with-fixed-data-set/564/9",2
2160,"dbt-model-think-unit-tests-poc","[dm03514](https://discourse.getdbt.com/u/dm03514), this is exactly what i was looking for!!

I had a very similar path to you where I disliked the jinja-heavy dbt unit tests and thought using python as the testing layer would be a much better solution.  I was glad to see shopify already implemented something and proved it out.

Have you continued with this POC?  Is your team using your current strategy?  Or continuing without unit tests?

To answer your questions:
* I think monkey-patching `ref` and `source` are totally fine in this context.
* One idea: could you put the `{% if dbt_model_test_enabled == '1' %}` around the `{% macro ref(model_name) %}`, so the `ref macro` only gets redefined in test runs?  If so, this would allow you to remove the `{% else %}` block, but more importantly your `ref` monekypatch wouldn't impact production runs at all.

Related: Trying to ""unit test dbt models within dbt"" sounds nice, but I think it makes the tests hard to write and hard to read/interpret.   Jumping up a layer to python totally makes sense.  It reminds me of in the Ruby on Rails world, we would write ""html view tests"", where we'd render our html views, then have tests which look at the output string and start making assertions against it (not in a browser, just in ruby).  These tests turned out to be extremely hard to write, hard to see what was being tested, and provided very little value.  So... the community went a layer up and tested things as ""features"" via selenium/browser.  I see a lot similarities between trying to unit test dbt models with dbt vs unit test dbt models with python in a mini pipeline.",3
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","I was recently pairing with an early-career analyst who asked me for help debugging a query she had written. The problem she had to solve was similar to this:
> A company has two newsletters that people can subscribe to â€” for the sake of this conversation, let's call them ""The Daily"" and ""The Weekly"".
>
> Each month, we want to know:
> * How many people that were members of the Daily, subscribed to the Weekly?
> * How many people that were members of the Weekly, subscribed to the Daily?

Earlier in my career, when thinking how to solve this problem, my first instinct would have been to look at the data we had available, and start writing queries â€” after all, you have to build your DAG from left to right, right?

Now, I take a backwards approach, and write the final query first.

Vid here:
https://www.loom.com/share/d77560469b0b43ba95877bcf46d89da1

and the [google sheet](https://docs.google.com/spreadsheets/d/1umsRK_3plthWX966KwKeVxhWpertQwbnkTDk50vLKUg/edit#gid=0).

## Step 1: Go to [sheets.new](sheets.new) to create a new Google Sheet
_(Did you know that this is a valid URL? RIP my Google Drive, which is now full of `Untitled Sheet`s)_

## Step 2: Mock out the final report you're trying to build, by entering values into the sheet.

For us, this looked like the following:
| subscription_month | weekly_subscribers_to_daily | daily_ subscribers_to_weekly |
|-|-|-|
| Jan 2021 | 100 | 150 |
| Feb 2022 | 120 | 160 |

^ These numbers don't have to be reflective of real numbers _at all_, and the column names don't even have to be perfect at this point.

## Step 3. Mock the query that would produce this table
You might also simultaneously mock out the table you're selecting from to write this query.

This is where things got tricky for us. At first, we thought we'd be selecting from a table with one record per subscriber:

`subscribers`: one record per subscriber
| subscriber_id |  date_subscribed_to_daily |  date_subscribed_to_weekly | 
|---------------|----------------------------|-----------------------------| 
| 1             | 2021-01-01                 | 2021-01-08                  | 
| 2             | 2021-02-07                 |                             | 
| 3             | 2021-03-18                           | 2021-03-14                  | 

But when we went to write the query, we realized we had a problem â€“ what field would we `date_trunc` on?
```sql
select
  date_trunc('month', ????) as subscription_month,
  count(???) as weekly_subscribers_to_daily,
  count(???) as daily_ subscribers_to_weekly
 
from subscribers

group by 1

```

## Step 4. Iterate
From here, we realized the model that we were selecting from needed to be one record per _subscription_ â€” a particular subscriber (human being) can appear twice. Our query was going to look something like this:

```sql
select
  date_trunc('month', subscription_date) as subscription_month,
  count(case when was_previously_subscribed_to_weekly and newsletter = 'daily' then subscriber_id end) as weekly_subscribers_to_daily,
  count(case when was_previously_subscribed_to_daily and newsletter = 'weekly' then subscriber_id end) as daily_subscribers_to_weekly
 
from subscribers

group by 1

```
^ Note that we also filled in some more of our `count` logic here, which lets us know ""ok we need to make sure our upstream model has fields like `newsletter` and `was_previously_subscribed_to_daily` etc.

Now our mock table gets updated to:
`subscriptions`: one record per subscriber, per newsletter they are subscribed to
| subscriber_id | date_subscribed | newsletter | was_previously_subscribed_to_daily | was_previously_subscribed_to_weekly | 
|---------------|-----------------|------------|------------------------------------|-------------------------------------| 
| 1             | 2021-01-01      | daily      | FALSE                              | FALSE                               | 
| 1             | 2021-01-08      | weekly     | TRUE                               | FALSE                               | 
| 2             | 2021-02-07      | daily      | FALSE                              | FALSE                               | 
| 3             | 2021-03-14      | weekly     | FALSE                              | FALSE                               | 
| 3             | 2021-03-18      | daily      | FALSE                              | TRUE                                | 


We can also take a guess as to what the SQL for those last two columns looks like, they're going to be some sort of window function, partitioned by the `subscriber_id`:
```sql
select
  subscriber_id,
  date_subscribed,
  newsletter,

  boolor_agg(newsletter = 'daily') over (
     partition by subscriber_id
     order by date_subscribed
     rows between unbounded preceding and current row
  ) as was_previously_subscribed_to_daily, 
  -- etc
from ???
```

## Step 5. Match this up with source data
So far, we've written pseudo-output and pseudocode without actually looking at our source data! At this point, we need to consider what the shape of our source data is! Do we have a table of subscriptions? Do the subscriptions to each newsletter live in separate tables (and therefore we need to `union` the data together)

## Step 6. Move the logic into your dbt project

Now that you've broken up the problem into smaller pieces, you can start moving this code into your dbt project! Sometimes the theoretical backwards-working is enough for me to start building my DAG from left-to-right. Sometimes though, it's not, and I need to work backwards in my dbt project too! In those cases, I'll often put placeholder columns in my models (especially if I'm adjusting existing models by adding a new column):
```
select
  ..., 
  null::boolean as was_previously_subscribed_to_daily

from ...
```

This approach has been met with mixed reactions internally â€” I'm curious, do you think this is a good approach, or more confusing than just writing the SQL?",1
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","I like it. It has some echoes of test-driven development which I've found to be helpful at times. Excited to try it out myself :).",2
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","""Invert, always invert"" - Carl Gustav Jacob Jacobi

Starting at the end is a very powerful problem solving approach, I'll definitely give this a go on my next transformation challenge. 

Thanks for sharing Claire!",3
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","This method of thinking also parallels the structure of many of the SQL questions on HackerRank and other sites, where the expected output table's schema is typically provided alongside the question. Which is to say - learning to think this way could help you with the SQL test in your next interview. :slightly_smiling_face:",4
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","This approach is the one proposed by Kimball in the DWH book :slight_smile: 

You start from the answer that you need to provide to the business, then you model your tables, then you figure out how to populate your model (first you look for the data, then you write the ETL :) ).

It is a good approach :)",5
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","This approach was introduced to me by my new manager last month and I'm actually finding myself struggling do to the shift to think this way (right to left).

as someone that come from web application development world to the analytics\data engineering world it feels more convenient and efficient to me to start writing code (queries) and adjust it step by step (left to right).

But I'm giving this approach a chance, I wonder how it will feel in the future :smiley:",7
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","Great writeup, Claire!

I think the most important part when starting something like this is to understand how *your* data needs to be structured in the end result for your stakeholder (i.e. having decent-enough requirements). So from that perspective, I always start at the end...but where I go from there depends on a few things. I think your Step 2 (mocking out the final report) is going to look different for people depending on where the BI calculations are done. What you currently have in there is essentially the end result of the report - but is that the end result of *your* code? It probably depends on where you're calculating the metrics - in the BI tool or the code itself. I might be wrong with this assumption, but I think most people currently have the calculations in the BI tool itself since it's more performant to e.g. apply date parameters to the dataset than it is to re-query the dataset with new date parameters. 

So for me personally, my Step 2 here would be to mock up what the end dataset is that I need to produce in order to support generating the calculated metrics that go into the report. Here are some questions I ask myself during this process (assuming the calculations are done in the BI tool):

* What are the metrics that need to go into the report? What level(s) of granularity (dimensions) are the final metrics reported at?
* For each final metric, what are the underlying fields that contribute to that metric's calculation?
* For each individual metric required to support the calculations, what dimensions are available to me and how do the dimensions at that level differ from the dimensionality needed in the final report or dataset?
* During this thought process, I also think about the operations that need to be applied to the metrics (e.g. SUM, AVG) and think about my GROUP BYs for dimensionality.

In order to answer these questions, you kinda need to start at the beginning of the DAG workflow to understand all the little pieces of data you have available.

I think where people (myself included) are potentially getting tripped up with this ""destination-first paradigm"" is writing the SQL queries at each stage. Think of it like putting together something from IKEA. You know what the end result needs to be. And if you start from the back of the instruction manual, you'll see that the final steps often require you to ~~screw~~ allen wrench something in that was assembled from individual components earlier on in the process. And so while it's great (necessary, I'd argue) to understand what those modular (assembled) components are that you need in the final steps, there's no way for me to actually build those without starting from the beginning, with the actual parts. Not to mention, the first thing you need to do with IKEA furniture is verify that you have all the necessary parts to be able to actually build the thing (aka initial data discovery).

If I were to change the process you've outlined to work better for me personally, I'd probably take out the writing SQL backwards part and instead focus on just modeling my data backwards. I'd want to understand what my data needs to look like at each stage, and once I have all the stages outlined, I'd start from the beginning (left side of the DAG) to start building towards those stages. I think there's also something to be said for failing early -- failing early doesn't only mean you fail quickly...it also relates to *where* you fail (i.e. early on in the data process). Point being, I'd rather fail in developing the initial building blocks vs. assuming I'll have certain building blocks in the end. Now, that might be a little confusing, but what I'm saying is - I can't assume the building blocks I need later on in the process will be built exactly the way I had outlined if I were writing the SQL backwards - there may be some nuances earlier on in the data process that would cause me to shift my entire approach.

Lastly, I wanted to touch on something you said early on in the article.

> Earlier in my career, when thinking how to solve this problem, my first instinct would have been to look at the data we had available, and start writing queries

First off - I think this is a testament to how much you've grown as a developer, as this, in my opinion, is the difference between junior and senior developers. Junior devs often take a problem and then sit down and start writing code, hoping that it'll play out for the best. Senior devs take a problem, then map/plan everything out before executing on their plan.",8
2287,"writing-models-backwards-an-unorthodox-approach-to-data-transformation","Thanks for sharing @Claire!

I found your post echoing with my recent experience.

We just onboarded dbt and I am now working on migrating our Postgres project with ~100 tables/views/materialised views to Redshift using dbt, with the goal of improving our schema design. As a newbie in analytics engineering (I am a data analyst), I had struggles at the beginning when I tried to design upstream models (dimension and fact tables) first as I was not so sure what we would need. 

Then I decided to start with cube models (an intermediate layer between dimensions & facts and reports), I copied the old SQL definition and start to clean up the code and logic. It was during this process that it became clear to me which parts of the old code should be decoupled, which are repeated across the project, and eventually what should be moved upstream/downstream. This approach of starting from (not the very end) helped me a lot!",9
2331,"hide-old-branches-in-dbt-cloud","Hi everyone,

I now played with DBT cloud a little and notices that merged and deleted branches still show up in the ""checkout branch"" menu.

If there a way to get rid of those permanently? This will get messy real fast.

FAQ and board came up empty.

Best

Lukas",1
2331,"hide-old-branches-in-dbt-cloud","Hi

Have you found a solution for remove deleted local branches so they dont show up in the ""checkout branch"" list?

Thanks

David",2
2331,"hide-old-branches-in-dbt-cloud","Yes, to be honest it kind of fixed itself and I have no idea why it was not working before.

So my current setup is that dbt is connected to a github repo and you can delete branches manually in github and/or set up that branches are deleted on merge.

If you delete all the branches here and then go for ""refresh git state"" in the dbt frontend, the old branches will disappear.

It also turned out to be a non issue because we are not really using the dbt webfrontend for development because it's pretty slow. Usually I just develop in visual studio code, created my branches there and push into the repo. Only some more testing is needed, I go to the dbt webfrotend, open my branch and test there.

Testing is also not optimal in the webfrontend from what I feel because there is no way to test on production data, only on your test clone which can take a good while to update.",3
2331,"hide-old-branches-in-dbt-cloud","Thanks for the info, I still find that no matter how/where I delete the branches, they remain in the available list when you use the web frontend when you select the ""checkout branch"" option, and they appear under the heading of ""removed from remote"". I thought the ""refresh git state"" would have removed them locally, but they remain in the list.

Interestingly, I exclusively uses Chrome to develop in the dbt Cloud web frontend, but I tried with Edge to see if it was quicker. The list of available ""removed from remote"" branches was available there as well, so the local git branch data must be browser agnostic.",4
2331,"hide-old-branches-in-dbt-cloud","After a big more digging/playing I logged into my DBT account from a separate machine and the old/deleted branches were there. I can only imagine that branches are also held somewhere within the DBT infrastructure somewhere.",5
2331,"hide-old-branches-in-dbt-cloud","+1 here. We are having the same issue. The local git version of the branch still exists, even after removing the branch in GitHub. I wonder if there is a way to access the CLI in DBT Cloud so we can run a command to remove the deleted branch from remote.",6
2331,"hide-old-branches-in-dbt-cloud","In the new dbt IDE, we demote branches that have been removed from remote to the bottom of the list: 

![image|512x492](upload://Adv3lderRFs5hI7j9wcfIb46ogZ.png)

We demoed the new IDE at a [recent dbt Staging event](https://www.getdbt.com/blog/staging-highlights-the-latest-from-dbt-labs/), and you can join the beta here: https://docs.google.com/forms/d/e/1FAIpQLSdlU65gqTZPyGAUc16SkxqTc50NO9vdq_KGx1Mjm_4FB_97FA/viewform",7
2331,"hide-old-branches-in-dbt-cloud","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",8
2340,"a-containerized-dbt-environment-for-your-team","Getting a new member of your team set up using dbt can be a challenge.  Did you install the same version of Python, dbt?  Is it working on your machine but not theirs?  It can also be difficult to ensure everyone on your team keeps their dbt environment up to date.  This is a common challenge with most software development environments.  Fortunately, there is a way to reduce a lot of this pain.  It involves running your dbt environment in a docker container which includes an explicit, tested recipe for setting up dbt with all the right dependencies.

I've found myself setting up containerized docker environments several times.  To make this easier in the future, I set up a [dbt container skeleton](https://github.com/gnilrets/dbt-container-skeleton) that can be used to bootstrap a manageable, secure, and containerized dbt development environment.  Once it's initially configured, updating your environment is as simple as

    inv build

and running dbt code is just

    inv dbt-shell
    $ dbt run

See the main [dbt container skeleton](https://github.com/gnilrets/dbt-container-skeleton) repo for details.",1
2340,"a-containerized-dbt-environment-for-your-team","@gnilrets this is awesome. I'm glad to see others working in the same problem space. Our team is growing quickly and dev env set up is probably our largest impediment right now?

Follow up question:

> Would you recommend this process for all new team members?

I ask because this solution seems to ensure a stable environment at the expense of initial set up and overall complexity. Is a stable environment what you see new users struggle with the most. Does this solution assume that the new team members are already familiar with anaconda environments?

Even teaching anaconda to someone with only Data Viz and SQL experience is quite a heavy lift already. It was validating too see @aescay's post last week rationalizing virtualenv over conda envs:

[quote=""aescay, post:1, topic:2353""]
### Why `venv` ? Why not `pyenv-virtualenv` or `conda` ?

While itâ€™s true that the other two alternatives (and many more) are likely better, all other alternatives require installing another package whereas `venv` comes shipped with the latest versions of Python out of the box. This makes it easy for us to use this across all your machines without having to worry about installing other bloatware that might not be useful to us. Keeping this as simple as possible will also help maintainability!
[/quote]

Please don't read this as critical, I'm just struggling a lot with  happy path to ohelping new team members get set up right now. Perhaps we can set up a working group for figuring this out!",2
2340,"a-containerized-dbt-environment-for-your-team","Hi @data_ders, glad you liked this!  No criticism taken (although accepted if needed).

Unless your team uses dbt-cloud 100%, I would recommend this process (or one like it) for all team members and projects.

I would also argue that this process is actually supposed to make initial set up easier, rather than being an expense.  Before I started using containerized development environments (on non-dbt projects), I would spend half a day with a new team member just getting their local environment set up with the right version of python, pip packages, homebrew recipes, etc.  There was always a few things that worked on my machine a few months ago that no longer works with more recent packages, and that only gets worse as the project complexity and dependencies grow.  And later, making any sort of upgrades to our environment was just as difficult, and we would rarely be willing to go through the pain of upgrading our packages and end up missing out on nice new features or important bug fixes, and our code would just end up rotting.

I personally don't think setting up miniconda is too big of a lift.  After the initial setup, there's only one command you have know: `conda activate myenv`.  But I can see how virtualenv might be slightly simpler, especially if you're only going to be managing a single dbt project.",3
2340,"a-containerized-dbt-environment-for-your-team","Do you currently walk new users through the set up described in the repo? Or do you ask them to do it independently? Do we assume that new users know how to open up a shell into a Docker container?

Also, dumb question -- is the idea that the container is hosted on same host as the database itself, to have sort of a remote development environment? Or is this just meant to build a layer of abstraction for end users' local set up?

Again asking from the perspective of someone also struggling with helping new team members onboard. I wrote [a thing](https://discourse.getdbt.com/t/onboarding-w-o-dbt-cloud/2367) about our pain points yesterday


Last thing -- 
[quote=""gnilrets, post:3, topic:2340""]
just getting their local environment set up with the right version of python, pip packages, homebrew recipes, etc.
[/quote]

We don't have any homebrew recipes, so for us, conda does all this for us. The big challenge I find is helping ensure that the conda env will activate for users automatically when they open the project in VSCode. Fortunately now the Python extension auto-activates if there's a requirements.txt file in the repo. The challenge is helping new users to set their `python.PythonPath` variable in the `settings.json`",4
2340,"a-containerized-dbt-environment-for-your-team","Some new users are able to follow directions in our repo `README.md` without any help, which is pretty similar to what is in the dbt-container-skeleton.  Others who are less familiar with working on the command line may need some handholding at first.  Most of the complexity is meant to wrapped up into simple invoke tasks (via `tasks.py`).

This setup is meant to be for _local_ development, so the container is built and run on the developer's laptop.  (Since the image is built on the developer's machine, there is a risk that the build process could differ slightly from dev to dev -- if that becomes an issue, then it would be prudent to build a workflow that involved hosting a main image on dockerhub and share that with your team, but I haven't found that to be really necessary).",5
2340,"a-containerized-dbt-environment-for-your-team","@data_ders I think another important caveat to note with our current solution, which I failed to mention in the main post, was that our workflow centers mostly around data model development and we rarely delve into custom python workflows and scripts. That said, for our team, a light environment handler did the trick because it was rare that we had python (and other programming) dependencies floating around and in flux in our local machines. However, prior to working at Fishtown I was previously in a broader data team and we had a lot of Python and R projects that we were working on simultaneously. In that sort of environment, which is common for a lot of data teams whose resources are pulled in all different directions, I would highly recommend using something like conda (because it is more robust and worth the effort of having fully discrete Python environments), or even dockerizing environments (which gives you full flexibility and isolation, and was what we were using in our organization). Hope this helps you figure out the best environment setup for your team!",6
2340,"a-containerized-dbt-environment-for-your-team","Updates!  The container skeleton now includes dtspec testing, SQLFluff linting, and a minimal GitHub Actions setup for CI!",7
2340,"a-containerized-dbt-environment-for-your-team","Hi @gnilrets and followers of this post! @gnilrets, your description of the pain points data teams encounter is so spot on :tada:

Our team at Palmetto Energy has also thought a lot about this problem this year. I looked through your repo and found that your approach to containerizing dbt is very similar to the approach my team has taken!

I wanted to share our package, [`palm`](https://github.com/palmetto/palm-cli), which we just open sourced (and my coworker Emily spoke about at Coalesce last week, the talk called ""Data Paradox of the Growth Stage Startup"").  Palm is a CLI for your CLIs,  that is to say a standardized interface for your multitudes of projects using a common set of customizable commands, which you can implement with custom logic for each project you work in, or share the same logic across projects with similar workflows via plugins.

We wrote a plugin for palm called [`palm-dbt`](https://github.com/palmetto/palm-dbt) which has the standard dbt commands built out, along with the ability to generate all the files necessary to containerize your project with a simple command (`palm containerize`). 

Our team also uses palm in 10+ other projects, including Airflow, Great Expectations, Terraform, and more. We don't have plugins for these written yet because dbt is by far our biggest use-case and our other projects have company-specific logic, but we hope the community at large will help us build out these plugins going forward as well! 

The `containerize` command is included with palm core, so it is available for use in other projects besides dbt. This has helped us tremendously with the Python version juggling issues that others have mentioned in this post. It has also enabled us to forego Python or other environment managers, virtual envs, etc. Just containerize and list dependencies for your project in the requirements file!

You have done great work providing this template for containerization to the community. We would love your contributions to palm if there is something we missed!",8
2340,"a-containerized-dbt-environment-for-your-team","I just wanted to mention VSCode's devcontainer feature which would play very nicely with this",9
2353,"setting-up-your-local-dbt-run-environments","As an analytics engineer on Fishtown's professional services team, I spent the past year working on 10 different dbt projects for customers. I wrote the following guide to help our team easily switch back and forth between dbt versions. There were times when I was working on 2-4 different projects at once and it was cumbersome to operate in an ""all or nothing mentality"" regarding which dbt version I could work on. I also wanted to help make it easier to test out beta releases of dbt without having to fully commit to using the beta for *every* project I was actively working on. 

I wanted to share this document with you all since it might help your team if you're managing multiple dbt projects as well, or you'd like to easily test beta releases and keep up with the latest and greatest that dbt has to offer!

## TLDR

- If you've ever wanted to get multiple versions of dbt running in your machine (specifically the latest release and maybe the latest pre-release - beta or rc) then you've come to the right place!
- This guide will help you get set up with 2 virtual environments (using `venv`):
    1. dbt - latest official release
    2. dbt-beta - latest pre release

- This will help your team:
    1. **standardize development experience** - no more ""this works on mine but idk why it doesn't work on yours"" problem
    2. **easily upgrade and manage local dbt versions** - this should make it easier to both beta test new releases and easily swap back to existing stable releases when bugs are discovered

## Why should I bother?

### What are Virtual Environments?

Real Python does a great job at explaining what virtual environments are and why we need them in this primer (feel free to loop back to this later):

[Python Virtual Environments: A Primer - Real Python](https://realpython.com/python-virtual-environments-a-primer/#managing-virtual-environments-with-virtualenvwrapper)

For now, the most important takeaway here is the concept of **project isolation**. Being able to isolate your projects allows you to install and handle multiple, potentially conflicting, package versions in one machine at the same time. 

### How will using Virtual Environments help your team?

In the same vein as having code style guides, organizing project directories consistently (using a `~/dev/` directory), and version controlling analytics, this is just another way to aid in improving the developer experience and preventing unexpected code breakage. Here are a couple ways it could benefit your team:

1. **Everyone will have the exact same local installation of dbt**, in an isolated environment, guaranteeing consistency across all your machines. No more ""I don't understand why it doesn't work on your machine, it works on mine ðŸ¤·â€â™‚ï¸""
2. **You can now easily hot swap between dbt versions.** If you want to run the latest stable release, you can do so, if you want to easily try the same project on beta, you could do so also! All this with much fewer commands than those required to upgrade or downgrade dbt.
3. **We can have a script manage your environments and make updating easy!** If everyone has the same environments set up, it makes it really easy for people to build scripts to help manage that across all your machines. (And this is what we did here!) No need to remember to upgrade both the dbt stable release and also the beta, just run a simple command (like `dbt-update`) and voila, you have both environments updated!

### Why `venv`? Why not `pyenv-virtualenv` or `conda`?

While it's true that the other two alternatives (and many more) are likely better, all other alternatives require installing another package whereas `venv` comes shipped with the latest versions of Python out of the box. This makes it easy for us to use this across all your machines without having to worry about installing other bloatware that might not be useful to us. Keeping this as simple as possible will also help maintainability! 

## Count me in! Where do I get started?

If you're convinced this is the right solution, here's a quick guide to get you set up!

1. Copy the contents of this gist:
    - dbt-update.sh

        ```bash
        #!/usr/bin/env bash

        DBT_ENV=~/.virtualenvs/dbt
        DBT_BETA_ENV=~/.virtualenvs/dbt-beta

        process_environment() {
          env=$1
          release=$2

          if [[ -d ""$env"" ]]; then
            echo """"
            echo ""There is an existing dbt environment in: $env""
            echo -n ""Would you like to update(u) or reinstall(r)? [u/r]: ""
            read ans

            if [[ $ans == ""r"" ]]; then
              echo """"
              echo ""Reinstalling!""
              echo ""Deleting existing environment""
              rm -rf $env && echo ""Successfully deleted existing environment""
            elif [[ $ans = ""u"" ]]; then
              echo ""Updating!""
            else
              echo """"
              echo ""Exiting script""
              exit 1
            fi

          fi

          echo """"
          echo ""Creating dbt environment in: $env""
          python3 -m venv $env && echo ""Successfully created dbt environment!""

          echo """"
          echo ""Activating your dbt environment and installing the latest dbt version""

          if [[ $release == ""stable"" ]]; then
            source $env/bin/activate && pip install dbt -q -U && echo ""Successfully installed dbt:""
          elif [[ $release == ""pre"" ]]; then
            source $env/bin/activate && pip install dbt -q -U --pre && echo ""Successfully installed dbt:""
          else
            exit 1
          fi

          dbt --version
          deactivate

        }

        echo """"
        echo ""Initializing dbt environments""

        echo """"
        echo ""=== main dbt environment ===""

        process_environment $DBT_ENV stable

        echo """"
        echo ""=== dbt beta environment ===""

        process_environment $DBT_BETA_ENV pre

        echo """"
        echo ""If you would like to get the commands to set your aliases""
        echo -n ""to easily activate the environments respond with your shell or skip, [bash/zsh/skip]:""
        read ans_alias

        if [[ $ans_alias == ""bash"" ]]; then
          profile=~/.bash_profile
        elif [[ $release == ""zsh"" ]]; then
          profile=~/.zshrc
        else
          exit 1
        fi

        echo """"
        echo ""If you don't already have these aliases to quickly activate the dbt environments""
        echo ""run the following commands in your terminal then restart your terminal:""
        echo ""echo \""alias dbt-activate='source $DBT_ENV/bin/activate'\"" >> $profile""
        echo ""echo \""alias dbt-beta-activate='source $DBT_BETA_ENV/bin/activate'\"" >> $profile""
        ```

2. Paste the contents in a file in your `~/.dbt/` directory called: `dbt-update.sh`. This is the same folder where you'd find your `profiles.yml` file!
3. Run the following commands in your terminal:

    You can figure out your shell by checking the top of your iTerm2 window to see what's written, should be either `zsh` or `bash`

    - If you're using `zsh` (the new macOS default since macOS Catalina)

        ```bash
        chmod +x ~/.dbt/dbt-update.sh # this makes the file executable
        echo ""alias dbt-update='~/.dbt/dbt-update.sh'"" >> ~/.zshrc # this allows dbt-update to be run from anywhere
        ```

    - If you're using `bash` (disclaimer, I haven't tested this, let me know if you're trying it, would love to pair)

        ```bash
        chmod +x ~/.dbt/dbt-update.sh # this makes the file executable
        printf ""\nalias dbt-update='~/.dbt/dbt-update.sh'"" >> ~/.bash_profile # this allows dbt-update to be run from anywhere
        ```

4. Restart your terminal by closing the window and opening a new one (you could also run the `source` command against your config file)
5. Run the following command:

    ```bash
    dbt-update # yes it's that simple!
    ```

    You should see some printed status messages letting you know what's going on underneath. The script will now proceed to set up the `dbt` and `dbt-beta` environments for the first time, as well as install fresh dbt versions in each environment. At the end of each install, you should see the script print out the version of dbt that was installed. It should look something similar to this:
![Untitled|424x500](upload://hOiFUXP513u2y2kjTxwRwIzWPEy.png) 

6. Once your environment is set up, run the two commands that the script suggests you run in your terminal after it's been installed.
7. Restart your terminal one last time and you now should be able to run:

    ```bash
    dbt-activate
    ```

    and you will get something that looks like this:
![Untitled 1|472x478, 50%](upload://b4IClG1JdzORCqN6q0a5eYDg5ob.png) 

    After running the activate command, you should be able to see your terminal prefixed with the environment name (in this case `dbt`). You can do the same for the `dbt-beta` environment, just run:

    ```bash
    dbt-beta-activate
    ```

8. To deactivate your environments, just run:

    ```bash
    deactivate
    ```

9. The next time you need to update dbt, just re run: `dbt-update`. It should detect that you already have the environments set up. It will then prompt you to choose whether you'd like to update or reinstall (type any other character to cancel out).

## Improving this workflow

I'm looking to see if anyone is familiar with how this could be deployed in a smoother way? Right now it requires that people have some familiarity with creating custom bash scripts and setting up aliases on their command line tool of choice. I would love to see some way to just distribute this across multiple machines with some simple install and maybe add some sort of flexibility so that users could potentially manage versions by just using some configurable options, instead of making edits to the raw [`dbt-update.sh`](http://dbt-update.sh) file. Feel free to reach out here and write some thoughts if this is interesting to you or if you've tackled the same problem differently in your organization!

## Other Helpful Links

[An Effective Python Environment: Making Yourself at Home - Real Python](https://realpython.com/effective-python-environment/#virtual-environments)",1
2353,"setting-up-your-local-dbt-run-environments","I really like this approach @aescay, particularly that `venv` does not _require installing another package_. Thanks for sharing!

I've been working on establishing a consistent development environment for our Analytics Engineering team at Surfline as well. While `venv` is the simplest solution, we've settled on `conda` over `venv`, `pyenv-virtualenv`, `poetry`, and others for a few reasons:
1. Analytics Engineering falls under our Engineering team and they use `conda`.
2. Our Data Science team also uses `conda`. So all of our technical teams company wide use the same solution.
3. Other projects we work on at times require non-python packages (e.g., `gcc`) and `conda` really excels here.

It seems to me that knowing what the other technical teams in the org are using as a virtual environment manager is _likely_ the best indicator of what Analytics Engineering should use. 

In terms of ""improving this workflow"", I'd suggest checking out [`direnv`](https://direnv.net/). We use it so that anytime we navigate inside a repo with a `conda` environment, the `conda` env is automatically activated and when you navigate outside the repo it is automatically deactivated. This guards us against installs in the wrong environment (:scream:) and makes switching between projects super easy!

I imagine the same can be achieved with `venv` enviroments!",2
2353,"setting-up-your-local-dbt-run-environments","Got and fixed an error with this script, so I wanted to drop a note here! When I ran `dbt-update`,  it created the environment but something went wrong on the step to install dbt. The error message was quite long, but this was listed towards the end:
```
ERROR: Failed building wheel for pyarrow
  Failed to build pyarrow
  ERROR: Could not build wheels for pyarrow which use PEP 517 and cannot be installed directly
```
I was also getting a warning about pip:
```
WARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.
You should consider upgrading via the '/Users/user/.virtualenvs/dbt-0.20.2/bin/python3 -m pip install --upgrade pip' command.
```

A pip update fixes the issue. I changed the dbt-update.sh file to include pip installs right before the dbt installs in the script.

```
#!/usr/bin/env bash

DBT_ENV=~/.virtualenvs/dbt
DBT_BETA_ENV=~/.virtualenvs/dbt-beta

process_environment() {
  env=$1
  release=$2

  if [[ -d ""$env"" ]]; then
    echo """"
    echo ""There is an existing dbt environment in: $env""
    echo -n ""Would you like to update(u) or reinstall(r)? [u/r]: ""
    read ans

    if [[ $ans == ""r"" ]]; then
      echo """"
      echo ""Reinstalling!""
      echo ""Deleting existing environment""
      rm -rf $env && echo ""Successfully deleted existing environment""
    elif [[ $ans = ""u"" ]]; then
      echo ""Updating!""
    else
      echo """"
      echo ""Exiting script""
      exit 1
    fi

  fi

  echo """"
  echo ""Creating dbt environment in: $env""
  python3 -m venv $env && echo ""Successfully created dbt environment!""

  echo """"
  echo ""Activating your dbt environment and installing the latest dbt version""

  if [[ $release == ""stable"" ]]; then
    source $env/bin/activate && python3 -m pip install --upgrade pip && pip install dbt -q -U && echo ""Successfully installed dbt:""
  elif [[ $release == ""pre"" ]]; then
    source $env/bin/activate && python3 -m pip install --upgrade pip && pip install dbt -q -U --pre && echo ""Successfully installed dbt:""
  else
    exit 1
  fi

  dbt --version
  deactivate

}

echo """"
echo ""Initializing dbt environments""

echo """"
echo ""=== main dbt environment ===""

process_environment $DBT_ENV stable

echo """"
echo ""=== dbt beta environment ===""

process_environment $DBT_BETA_ENV pre

echo """"
echo ""If you would like to get the commands to set your aliases""
echo -n ""to easily activate the environments respond with your shell or skip, [bash/zsh/skip]:""
read ans_alias

if [[ $ans_alias == ""bash"" ]]; then
  profile=~/.bash_profile
elif [[ $release == ""zsh"" ]]; then
  profile=~/.zshrc
else
  exit 1
fi

echo """"
echo ""If you don't already have these aliases to quickly activate the dbt environments""
echo ""run the following commands in your terminal then restart your terminal:""
echo ""echo \""alias dbt-activate='source $DBT_ENV/bin/activate'\"" >> $profile""
echo ""echo \""alias dbt-beta-activate='source $DBT_BETA_ENV/bin/activate'\"" >> $profile""
```

Hopefully  this helps if anyone runs into something similar :)",3
2536,"enabling-jinja-syntax-support-in-sql-files-in-pycharm","It turns out that you can add Jinja syntax support for dbt model files (.sql) and yaml files in PyCharm Professional Edition or other paid JetBrains products that have multi-language support (Sorry DataGrip users). Here's how to do it.


**Disclaimers** 
1. Unfortunately this is not available in the *Community Edition* of PyCharm. 
2. There is currently a bug in JetBrains IDE that shows a valid SQL with Jinja as having syntax errors if a SQL Dialect is selected to something other than ""Generic SQL"" (e.g Snowflake) [Link to ticket](https://youtrack.jetbrains.com/issue/PY-48894). The workaround is to either set the global or project SQL Dialect setting under Languages & Frameworks to ""Generic SQL"".
3. This is not possible to do in DataGrip 

**Steps**

1. Preferences > Languages and Frameworks > Template Languages.
2. Select â€œJinja2â€ for the Template Language dropdown at the top,
3. Click on the â€œ+â€ icon to add SQL (and YAML) to the list. Hit Apply or OK and you are done!

![Screen Shot 2021-05-14 at 9.04.49 AM|690x462](upload://pMRiEJsRtnihAe48Gq5iHD3qVBi.png)",1
2536,"enabling-jinja-syntax-support-in-sql-files-in-pycharm","I just recently rolled out a dbt workstation environment to a team of 15 and I have been asked if this sort of configuration is possible using VSCode. The VSCode python extension behaves properly and locates the dbt virtualenv, but I ly by manually initiating the extension. Since it is only set to auto initialize on .py files...not .sql

Does anyone know if this is possible in VSCode?",2
2536,"enabling-jinja-syntax-support-in-sql-files-in-pycharm","Re VS Code you might want to check this out to see if it suits your use case. 
https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user",3
2536,"enabling-jinja-syntax-support-in-sql-files-in-pycharm","Thanks, I will take a look at it.",4
2536,"enabling-jinja-syntax-support-in-sql-files-in-pycharm","[quote=""makotonium, post:1, topic:2536""]
on the â€œ+â€ icon to add SQL (
[/quote]

I've just commented on the ticket you raised as it has been conflated with the [DataGrip ticket](https://youtrack.jetbrains.com/issue/DBE-12416) and flagged as a duplicate which it patently isn't.",5
2573,"triggering-a-dbt-cloud-job-in-your-automated-workflow-with-python","Say you want to create an automated workflow that includes a dbt Cloud Job. Maybe the workflow is within an orchestration tool like Airflow, Azure Data Factory, or AWS Step Functions. Or perhaps you want a dbt Cloud Job to be part of a git change management process powered by GitHub Actions, CircleCI, or Azure DevOps. How can you do that?

Well, that's where the [dbt Cloud API](https://docs.getdbt.com/dbt-cloud/api) comes in! The [`trigger job to run`](https://docs.getdbt.com/dbt-cloud/api#operation/triggerRun) endpoint can be accessed to run a Job of your choosing. Then, you can use the [`get run`](https://docs.getdbt.com/dbt-cloud/api#operation/getRunById) endpoint to implement a basic and effective API polling technique to check the Job Run status.

I prefer a python script for this solution as it's a powerful language understood by many developers, with exceptional error handling capabilities. Below is a basic script I recommend starting with:

```
import enum
import os
import time

# Be sure to `pip install requests` in your python environment
import requests


ACCOUNT_ID = <your-dbt-cloud-account-id>
JOB_ID = <your-dbt-cloud-account-id>

# Store your dbt Cloud API token securely in your workflow tool
API_KEY = os.getenv('DBT_CLOUD_API_TOKEN')


# These are documented on the dbt Cloud API docs
class DbtJobRunStatus(enum.IntEnum):
    QUEUED = 1
    STARTING = 2
    RUNNING = 3
    SUCCESS = 10
    ERROR = 20
    CANCELLED = 30


def _trigger_job() -> int:
    res = requests.post(
        url=f""https://cloud.getdbt.com/api/v2/accounts/{ACCOUNT_ID}/jobs/{JOB_ID}/run/"",
        headers={'Authorization': f""Token {API_KEY}""},
        json={
            # Optionally pass a description that can be viewed within the dbt Cloud API.
            # See the API docs for additional parameters that can be passed in,
            # including `schema_override` 
            'cause': f""Triggered by my workflow!"",
        }
    )

    try:
        res.raise_for_status()
    except:
        print(f""API token (last four): ...{API_KEY[-4:]}"")
        raise

    response_payload = res.json()
    return response_payload['data']['id']


def _get_job_run_status(job_run_id):
    res = requests.get(
        url=f""https://cloud.getdbt.com/api/v2/accounts/{ACCOUNT_ID}/runs/{job_run_id}/"",
        headers={'Authorization': f""Token {API_KEY}""},
    )

    res.raise_for_status()
    response_payload = res.json()
    return response_payload['data']['status']


def run():
    job_run_id = _trigger_job()

    print(f""job_run_id = {job_run_id}"")

    
    while True:
        time.sleep(5)

        status = _get_job_run_status(job_run_id)

        print(f""status = {status}"")

        if status == DbtJobRunStatus.SUCCESS:
            break
        elif status == DbtJobRunStatus.ERROR or status == DbtJobRunStatus.CANCELLED:
            raise Exception(""Failure!"")


if __name__ == '__main__':
    run()

```


### Ideas for using this script

- Triggering a Continuous Integration job when you open a pull request in BitBucket or Azure DevOps. (Note: with GitHub and GitLab, [this can be done in dbt Cloud with the `Run On Pull Request` feature!](https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-enabling-continuous-integration-with-github#enabling-continuous-integration-ci))
- Triggering a Production job when you merge a pull request in to the `main` branch.

### Related topics

- @matt.winkler [describes](https://discourse.getdbt.com/t/orchestrating-fivetran-and-dbt-with-airflow/2079) how to use Airflow to orchestrate dbt Cloud and Fivetran.
- @DAVOS [shows us](https://discourse.getdbt.com/t/trigger-a-dbt-cloud-job-on-merging-to-master-branch/1278) how to use `curl` commands to trigger the dbt Cloud API within CircleCI.",1
2573,"triggering-a-dbt-cloud-job-in-your-automated-workflow-with-python","Thanks for this @boxysean! I've created a [Google Colab notebook](https://gist.github.com/ernestoongaro/657689941acbf46d71f77b88627317bb) from your code for quick testing.",2
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","**Quick disclaimer**: *We use GCP and the solution is based on GCP only*

We've been using dbt for a while now and have had a few deployment choices. I'd like to explain how we went about getting the Airflow using a `KubernetesPodOperator` choice working and also give a brief explanation as to why. 

Let's start with the why. 
# Cloud Build Triggers and Cloud Scheduler
This was our first choice and we have a few models running using this strategy. This works well if you have no idea about Composer or Airflow. 
This works well if you have a set schedule to run your dbt models but can't trigger loads based non-dbt ingestion. This works well if you don't need any smarts in the scheduling process, such as checking that the previous load has completed before you start another load. This works well if you don't exceed the limitation of Cloud Scheduler. 
Plan B...
# Composer using a virtual environment 
Why run in a virtual environment and not install as a PyPI package? The dependencies of dbt PyPI package have conflicts with Composer's. Direct installation of dbt in Composer causes failure. And depending on the version of Composer the conflicts vary.
The choice bypassed all the issues that Cloud Scheduler had. We can trigger the dbt DAG via pub/sub message or schedule it. Current implementation is to create a temporary virtual environment with dbt installed for each dbt job. The virtual environment is destroyed when dbt job finished. 
Why stop using this option?  Running dbt job in virtual environment created in Composer is causing security threats. This Again the answer is simple, security. Google support is void installing software into an virtual environment bypassing security check.

And that is how we ended up with Plan C.
# Composer using a KubernetesPodOperator
Since the dbt models are docker containerised, dependency conflict issue will not be a problem.
As the KubernetesPodOperator will automatically spin up the pods to run the docker image, we do not need to manage the infrastructure.
This option will require monitoring on the Composer resource usage. And we need to scale it up or scale it out if it is needed. However, we have already been doing this as part of support.

## To-Do List:
1. Create a base Docker image Artifact of containing dbt, python, git and gcloud.
2. Create a Docker image Artifact of your dbt repo.
3. Set up Workload Identity.
4. Use the KubernetesPodOperator in your DAG.

## Build a base dbt Artifact
We decided to use a separate docker image that contains all the â€œinstallsâ€ to execute a dbt command. This means that there is a central image for updating versions and also compilation time for docker image using this dbt docker image is much faster.

To enable this, we created a base_dbt_docker repo with the following files:
1. A docker file
2. A requirements.txt file
3. A yaml file to create a docker image of your repo in the Artifact repository

## Docker file
*Feel free to try alpine instead of ubuntu. I had issues with getting gcloud and python installed in alpine.*
```make
FROM ubuntu:20.04
COPY . /app/

RUN apt-get update 
# Install gcloud
RUN echo ""deb [signed-by=/usr/share/keyrings/cloud.google.gpg]   cloud-sdk main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
RUN apt-get install apt-transport-https ca-certificates gnupg curl -y
RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg  | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
RUN apt-get update && apt-get install google-cloud-sdk -y
# Install git
RUN apt-get install -y git
# Install Python
RUN apt-get install --no-install-recommends -y python3.8 python3-pip 
# Install dbt
RUN python3 -m pip install -r /app/requirements.txt
```

## Requirements file

This is a simple file containing the version of dbt to install `dbt>=0.18.1`

## YAML file

The naming format for an Artifact is LOCATION-docker.pkg.dev/PROJECT-ID/REPOSITORY/IMAGE. For more information, have a look at this [link](https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling#tag). Our location is australia-southeast1, please adjust accordingly.
```yaml
steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker build -t ${_ARTIFACT_NAME} .']

- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker tag ${_ARTIFACT_NAME} australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_ARTIFACT_NAME}/${_IMAGE_NAME}']
  
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker push australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_ARTIFACT_NAME}/${_IMAGE_NAME}']

substitutions:
  _PROJECT_ID: $PROJECT_ID
```
Create a Cloud Build trigger to use this yaml to build the Artifact. 
![image|546x198](upload://kAHSE9MaqXy3beIgklF2q61Dfgi.png)

![image|457x500](upload://lYmQYWyuYFheNw9lVICs6eH5Jii.png)

**Remember** that each time there is a change in the repo, you need to rebuild the Artifact, just run the Cloud Build trigger again. You'll need to rebuild the dbt project Artifact too.

# Build a dbt project Artifact
You will need the following files in your repo:
1. A docker file
2. A shell script that will execute your dbt commands
3. A yaml file to create a docker image of your repo in the Artifact repository


## Docker file
Note that the docker file points to the base dbt Artifact you had just created, see the FROM.
```
ARG project_id
ARG base_artifact
ARG base_image

FROM australia-southeast1-docker.pkg.dev/${project_id}/${base_artifact}/${base_image}
COPY . /app/
RUN echo ""dbt via docker""

RUN chmod +x /app/dbt_run.sh
WORKDIR /app/
ENTRYPOINT [""/app/dbt_run.sh"" ]`
```
## Shell script
The .sh file executes the dbt commands and currently caters for dbt run and dbt debug.
```sh
#!/bin/bash
mode=$1
dbt_target=$2
dbt_models=$3
dbt_vars=$4
full_refresh=$5

cd /app/

if [ $mode = ""run"" ]; then
    echo ""dbt Mode is run""
    if [ $full_refresh = ""True"" ]; then
        echo ""Doing a full refresh""
        dbt run --target=""$dbt_target"" --models=""$dbt_models"" --vars=""$dbt_vars"" --profiles-dir=/app/profiles_dir --full-refresh
    else
        echo ""Not doing a full refresh""
        dbt run --target=""$dbt_target"" --models=""$dbt_models"" --vars=""$dbt_vars"" --profiles-dir=/app/profiles_dir
    fi
elif [ $mode = ""debug"" ]; then
    echo ""dbt Mode is debug""
    dbt debug --profiles-dir=/app/profiles_dir --target=""$dbt_target""
else   
    echo ""Incorrect dbt Mode. Nothing to do""
fi

# capture the exit code from the dbt run command
# so that the final exit code form removing virtualenv cmd doesn't get used by KubernetesPodOperator 
exit_code=$?

# rethrowing the exit code to KubernetesPodOperator
exit $exit_code 
```
Note the correlation between the `COPY . /app/` from the docker file and the `cd /app/` in the .sh file

### YAML file
```yaml
steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker build --build-arg ""_PROJECT_ID=${_PROJECT_ID}"" --build-arg ""base_artifact=${_BASE_ARTIFACT}"" --build-arg ""base_image=${_BASE_IMAGE}"" -t ${_REPO_NAME} .']

- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker tag ${_REPO_NAME} australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}']
  
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker push australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}']

  substitutions:
    _PROJECT_ID: $PROJECT_ID
```
Create a Cloud Build trigger to use this yaml to build the Artifact.
![image|539x185](upload://qh13oH9EvAcV3eMORKMt1piiih5.png)

![image|403x500](upload://nVzWVthPYRBYeoVgSWkzaOQvdUD.png)

**Remember** that each time there is a change in the repo, you need to rebuild the Artifact, just run the Cloud Build trigger again. 

## Workload Identity
What is Workload Identity? It is basically a way to make a Kubernetes service account have the same credentials as a Google service account. Or as Google puts it, â€œ*Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its improved security properties and manageability.*â€
Here is the [link](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) to Googleâ€™s documentation. It gives an in-depth explanation of Workload Identity and has an easy-to-follow set of steps to get Workload Identity up and running. Below are the steps we took, a condensed version.

Steps (these steps can be done with Terraform too):
1. [Update a cluster](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_on_cluster) with Workload Identity enabled. (Another option is creating a new cluster)
`gcloud container clusters update <cluster_name> --zone australia-southeast1-a --project=<project_id> --workload-pool=<gcp_project_id>.svc.id.goog`
2. [Add a new node pool](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#option_1_node_pool_creation_with_recommended) to the cluster with Workload Identity enabled. (Another option is to modify an existing node pool but that it not recommended)
`gcloud container node-pools create <node_pool_name> --zone australia-southeast1-a --project=<project_id> --cluster=<cluster_name> --workload-metadata=GKE_METADATA --machine-type=<pick_a_type default is e2-medium>`
3. [Authenticating to Google Cloud](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#authenticating_to)
-- Configure kubectl to communicate with the cluster
`gcloud container clusters get-credentials <cluster_name> --zone australia-southeast1-a --project=<project_id>`
-- Create the namespace to use for the Kubernetes service account
`kubectl create namespace <k8s_namespace>`
-- Create the Kubernetes service account
`kubectl create serviceaccount --namespace <k8s_namespace> <k8s_sa>`
-- Allow the Kubernetes service account to impersonate the Google service account by creating an IAM policy binding between the two
`gcloud iam service-accounts add-iam-policy-binding --role roles/iam.workloadIdentityUser --member ""serviceAccount:<gcp_project_id>.svc.id.goog[<k8s_namespace>/<k8s_sa>]"" <gcp_sa> --project=<project_id>`  
-- Add the iam.gke.io/gcp-service-account annotation to the Kubernetes service account
`kubectl annotate serviceaccount --namespace <k8s_namespace> <k8s_sa> iam.gke.io/gcp-service-account=<gcp_sa>`

# KubernetesPodOperator
 Here is the [link](https://cloud.google.com/composer/docs/how-to/using/using-kubernetes-pod-operator) to Googleâ€™s documentation. 
We use the same mechanism that trigger and schedule the Virtual environment DAG. I'm not explaining that mechanism but just as a final step I'm giving you the minimum requirements for your `KubernetesPodOperator` DAG.
```python
from airflow import DAG
from airflow import models
# Make sure to add below
from airflow.contrib.operators.kubernetes_pod_operator import KubernetesPodOperator
from airflow.utils.dates import days_ago

args = {
    'owner': 'dummy',
    'retries': 0,
}

docker_dag = DAG(
    dag_id='dbt_docker_test',
    default_args=args,
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=['dbt run model'],
)

quay_k8s = KubernetesPodOperator(
    # k8s namespace created earlier
    namespace='kube-pod', 
    # k8s service account name created earlier
    service_account_name='ap-kube-pod-sa', 
    # Ensures that the right node-pool is used
    nodeSelector={'http://cloud.google.com/gke-nodepool ': 'k8sworkload-pool'}, 
    # Ensures that cache is always refreshed
    image_pull_policy='Always', 
    # Artifact image of dbt repo
    image='australia-southeast1-docker.pkg.dev/<gcp-project-id>/<artifact>/<artifact-image>', 
    # links to ENTRYPOINT in .sh file
    cmds=['/app/dbt_run.sh'], 
    # matches sequence of arguments in .sh file (mode,dbt_target,dbt_models,dbt_vars,full_refresh)
    arguments=['run', 'projectzone', 'tag:test2', '{from_date: """", to_date: """"}','False'], 
    name=""run-dbt-in-pod"",
    task_id=""run-dbt-in-pod"",
    get_logs=True,
    dag=docker_dag,    
    )
```
The key to making sure that the DAG use the k8s service account is the `service_account_name` argument. The documentation suggests using kubernetes secrets but that doesnâ€™t work for our purposes.
We also found that the Artifact image isnâ€™t always updated and that a cached version is used. Using argument `image_pull_policy='Always'` solves this issue.

And there you have it. Follow these steps, put the DAG in the correct folder in Composer and you'll have dbt running using a KubernetesPodOperator.

Questions?
Feel free to ask and I'll respond as best I can.

# Thank you!
I'd like to thank my fellow contributors in getting this solution to work. 
@ryanyuan - Composer, Airflow and general GCP maestro
@Phil_Xu - kubernetes maestro",1
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","Thank you so much for writing this! I'm new to KubernetesPodOperator so this is so so helpful! I've been struggling to get my head around how authorisation should be done so thanks so much for including the Workload Identity section.",2
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","Great walk through and activation of workload identity.

However, it seems theres a slight step missing in getting dbt to actually run. Where would the keyfile be located so that `profile.yml` can point to the correct keyfile?

Thanks",3
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","Yes. This. I'm trying to figure out what to do here given that access to BQ is controlled by IAM in GCP while running dbt in Composer.",4
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","For the first docker image I had to add curl and gnupg, 
and also replace the gcloud installation row as apt-key was not supported.
```
FROM ubuntu:20.04
COPY . /app/

RUN apt-get update 
RUN apt-get install -y gnupg
RUN apt-get install -y curl
# Install gcloud
RUN apt-get install apt-transport-https ca-certificates gnupg -y
RUN echo ""deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | tee /usr/share/keyrings/cloud.google.gpg && apt-get update -y && apt-get install google-cloud-sdk -y
# Install git
RUN apt-get install -y git
# Install Python
RUN apt-get install --no-install-recommends -y python3.8 python3-pip 
# Install dbt
RUN python3 -m pip install -r /app/requirements.txt
```
Was my final version. 

And in the second `.yaml` the `_PROJECT_ID` had to be replaced with `project_id` on row `4`, to match the Dockerfile's ARG. 
```
steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker build --build-arg ""project_id=${_PROJECT_ID}"" --build-arg ""base_artifact=${_BASE_ARTIFACT}"" --build-arg ""base_image=${_BASE_IMAGE}"" -t ${_REPO_NAME} .']

- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker tag ${_REPO_NAME} australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}']
  
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args: ['-c', 'docker push australia-southeast1-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}']

  substitutions:
    _PROJECT_ID: $PROJECT_ID
```
I'm not completely through the post yet. Posted this in the meantime. I might edit this  :)",5
2590,"running-dbt-in-composer-using-a-kubernetespodoperator","Hi,

Am seeing an error with the gcloud-sdk packages, while building the base dbt docker image. Anyone else seeing this or knows how to resolve this?
 

```
Reading package lists...

Step #0: W: GPG error: https://packages.cloud.google.com/apt cloud-sdk InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB
Step #0: E: The repository 'https://packages.cloud.google.com/apt cloud-sdk InRelease' is not signed.
Step #0: The command '/bin/sh -c apt-get update && apt-get install google-cloud-sdk -y' returned a non-zero code: 100
```",6
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","_Edit, October 2021: I wrote this before joining the dbt Labs team, while I was the Head of Data at Education Perfect. Please interpret_ ""we"" _accordingly!_

The core of dbtâ€™s value proposition is that analytics code is an asset, like any other piece of code, so it should be developed and deployed with the same rigorous processes as any other piece of code. A key principle behind this rigour is Continuous Integration--as you modify code, automatically check that your intended changes donâ€™t break the rest of the project. Sounds great, right?

Well, during a [recent dbt Staging event](https://youtu.be/2i1rVK50K64?t=1991), we heard that only about a third of dbt Cloud customers have CI runs configured. This means that everyone else is either finding their bugs in production, or they are manually running tests that a robot could handle for them! ^[They could also be writing perfect code first time.] Neither of these are great options.

I want to share our CI history as we expanded from a tiny project in early 2020 to several hundred models and 1000+ automated tests today. As our project has scaled up, weâ€™ve taken advantage of more advanced features in dbt Core and Cloud to keep a tight feedback loop and help our team be more confident in their work.

*NB: we use dbt Cloud at Education Perfect, so this article focuses on that use case. Everything I describe here can also be done via dbt Core - the Cloud product just makes some of the setup easier (especially artifact management).*

## The start of our dbt journey
Early on, we only had a few models, so doing a full run of everything was pretty quick.
![we'll-take-the-lot|400x180](upload://d7MSHNCYbF6ucTFoCxsBFlJRjfa.gif)

If we made a mistake, it didnâ€™t take long to find out, AND it didnâ€™t hurt anyone! This was a much happier place to be than our pre-dbt paradigm of pushing to live, then frantically refreshing the changed report so that if weâ€™d forgotten a comma we could revert it before anyone noticed. 
![first-ci-job-slack|597x447, 75%](upload://9BVjBUxJR9ND6Zz6Y6VmTC6rRjL.png)

## Saving time with target-aware configuration
As our project matured, we started to build complex models which didnâ€™t perform well as views. Every time someone hit refresh in Mode, Redshift ran the whole query from scratch, which meant our users had to wait several minutes for results. We changed to materializing these large models as tables, but that made our CI jobs take much longer. Another easy fix: dbt lets you provide a different materialization config for CI or dev environments vs prod.

We added a `config` block to the top of relevant files (you can also do this in your `dbt_project.yml` file ^[Spoilers!]) like this:
![config-unrendered|537x100](upload://nrsubkQLfTdWjBUQbAYv1udYcyc.png)
which turns into either 
![config-rendered-table|218x114](upload://fWeVxMYzSxnZQUR7riYvdEqvjoP.png) or ![config-rendered-view|210x111](upload://nEYHNjLjK9A2djVoLDVR7DcDDhl.png).
It was a huge win to avoid wasting time generating redundant tables over and over again.

Despite these optimisations, by the six month mark our CI jobs tipped over 50 minutes--an inevitability given the projectâ€™s size and complexity but much too slow for an effective feedback loop. 

## Sidebar: How tight is tight enough?

â€œTight feedback loopâ€ is one of those sneaky phrases that gets people to nod along without thinking about the specifics. Having experienced run times between 3 minutes and 2 hours over the last year, I think that *if you canâ€™t see the results of your changes within a 5-10 minute window, itâ€™s too slow to be effective.*

Ironically though, this goal can discourage testing! I noticed that when I should have been adding tests, I was so concerned about our ballooning runtime that I tried to convince myself that new ones werenâ€™t necessary. This is a false economy! Testing our assumptions is critical. We must be notified when our assumptions no longer hold. It saves hours of debugging and ensures that we always have the opportunity to correctly read our data.

So, if we want to maintain solid test coverage, but also want to get results in less than 10 minutes, we have to find another solution.

An obvious question is: â€œif weâ€™re only changing a web sessions model, why are we bothering to re-test the other 99% of the project?â€ Most models arenâ€™t directly related to one another. Wouldnâ€™t it be great if we could just tell dbt to ignore the other models and focus on what has changed?

## We discover Slim CI
Enter [Slim CI](https://docs.getdbt.com/guides/legacy/best-practices#run-only-modified-models-to-test-changes-slim-ci)! dbt can now detect the things that actually need to be tested by comparing each modelâ€™s code to the version that was used in the last successful run, and only testing those that have changed in this PR. When we implemented this by [picking the job to compare to in the Cloud UI](https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-enabling-continuous-integration-with-github#defer-to-a-production-job) and adding `state:modified` to our `--models` selector, we immediately saw a 2.5x increase in performance - CI jobs that once took 50 minutes were complete in 20.
![dbt-cloud-deferral-dropdown|690x134, 75%](upload://mVcogPeVDMCjf47iTbTO0zscn0w.png)

## Didn't you say 10x in the title?
There was one significant downside remaining: dynamic materialization config, our saviour introduced above, was now our most significant source of wasted effort. This was a problem because dbt only compares the CI version of the model file to the production version *after* rendering Jinja. Ironically, our slowest models were the only ones that were still built and tested on every run, because their CI materialization config was different to production.

Fortunately, thereâ€™s an improvement in dbt 0.19.0: if you set your config in your `dbt_project.yml` file instead of inline the *unrendered* config is stored for comparison. When that launched, we moved our configurations and got down to 5 minute runs - a 10x improvement compared to where we were before Slim CI. Historically, [best practice](https://docs.getdbt.com/reference/model-configs#configuring-models) has been to put folder-wide settings in the project file, and deviations from the norm inline with the model. In this case, the performance gains are so substantial that Iâ€™d recommend adding all dynamic configuration in `dbt_project.yml`.

## Where to from here?

Weâ€™ve started to spend our savings on a more robust pipeline which tests modified models *and* their descendants, after being bitten a couple of times by a model which passed on its own but broke something downstream.

We initially did this in the most naive way (`dbt test -m state:modified+`) but found that to be overkill. Instead, weâ€™re validating the first-level children as well as anything that powers an exposure (`dbt test -m state:modified+1 1+exposure:*,state:modified+`). This gives a good balance between catching the most likely failure points while not wasting too much time.
*Not sure what all the pluses and stars and commas mean? Check out the docs on [graph](https://docs.getdbt.com/reference/node-selection/graph-operators) and [set](https://docs.getdbt.com/reference/node-selection/set-operators) operators, or the appendix below.*

The parsing improvements in [0.19.1](https://discourse.getdbt.com/t/faster-dbt-startup-in-v0-19-1-beta/2142) are another bonus - by increasing parsing performance by 2-3x, thereâ€™s less fixed cost to commands like `dbt seed` which parse the whole project even if thereâ€™s nothing to do (as is often the case with seeds). 

With a combination of Slim CI, moving our materialization configs outside of the model file, and the big speed boosts in the latest versions of dbt, itâ€™s not uncommon for us to now see runs completing in 3-4 minutes, while still maintaining full confidence that weâ€™ll know if something unexpected happens. If youâ€™re not already using CI, start there! But once youâ€™re ready to step it up, itâ€™s very achievable to get even bigger wins in minimal time. 

Huge thanks to @Mila for reading early drafts of this post, and @jerco for finding a bug in my job selectors ðŸ˜¬


## Appendix: Our full CI job definitions
âš ï¸ *dbt Cloud automatically adds the `--defer` and `--state` flags. If you're using something else to orchestrate CI, you'll need to include them yourself.*
### seed
**`dbt --warn-error seed --select state:modified sales__currencies --full-refresh`**

We include `--warn-error` in our `seed` step to ensure that any project-level warnings are resolved prior to merge. For example, we might move models whose configuration is defined in `dbt_project.yml` into a different folder. This would normally only be raised as a warning ^['Configuration paths exist in your dbt_project.yml file which do not apply to any resources', to be precise.], but now we'll be forced to fix it! 

Amusingly, there's also a warning if your selector returns no results ^['Nothing to do. Try checking your model configs and model specification args', to be precise.], so we have to explicitly rebuild a single tiny seed (`sales__currencies`) every time.

Finally, we `--full-refresh` every time to ensure that any table changes (new columns, changed types) are applied.

### run
**`dbt run -m state:modified+1 1+exposure:*,state:modified+`**

`state:modified+1` refers to the modified model and its [first-order children](https://docs.getdbt.com/reference/node-selection/graph-operators#the-n-plus-operator). `1+exposure:*,state:modified+` uses the [intersection operator](https://docs.getdbt.com/reference/node-selection/set-operators#intersections) to get any models which are referenced in an [exposure](https://docs.getdbt.com/docs/building-a-dbt-project/exposures) and depend on a modified model.

### test
**`dbt test -m state:modified+1 1+exposure:*,state:modified+ --exclude test_name:relationships test_name:relationships_where tag:fragile`**

The `-m` selector is the same as the `run` job above. We exclude all relationship tests ^[Looking forward to getting rid of `relationships_where` in dbt 0.20.0!] to avoid [oddities from the deferral process](https://docs.getdbt.com/reference/node-selection/defer#test) or pipelines being out of sync, as well as a handful of explicitly-tagged fragile tests who cause more trouble than they're worth in this context.",1
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","This is excellent, thanks a lot!

How do you manage multiple branches running on CI? Does each get its own schema?",2
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","[quote=""max-sixty, post:2, topic:2603""]
Does each get its own schema?
[/quote]

Yes that's right! dbt Cloud creates a schema for each PR, something like dbt_cloud_TEAM-ID_PR-ID, then automatically drops the schema once the PR is merged. More info here: https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-enabling-continuous-integration-with-github/#understanding-ci-in-dbt-cloud",3
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","What are your thoughts on using the state:modified on production runs?  Is that even possible and is the juice worth the squeeze?  Perhaps running only views that have changed plus all physical tables?  Views don't take a ton of resources from our warehouse db (snowflake), but when you have a few hundred views to run that starts to add up.",4
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","Excellent question! I think it should work - during the beta process, the ability to defer to the same job was added for exactly this use case. I never got around to doing it myself at EP before leaving so can't give any specific guidance off the top of my head. 

If you get it going, please come back and pop anything interesting you learned on the thread!",5
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","@joellabes are you holding back on me?!  the solution to all our problems is dbt 0.21.0! https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models#what-if-the-columns-of-my-incremental-model-change",6
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","[quote=""yoitsbenc, post:6, topic:2603""]
are you holding back on me?!
[/quote]

I don't think so!

[quote=""yoitsbenc, post:6, topic:2603""]
the solution to all our problems is dbt 0.21.0!
[/quote]
Normally I'd agree with you - 0.21.0 is very good! I don't know how incremental models help here though? I don't recommend moving things to incremental models unless driven by performance needs, and I definitely wouldn't change all my views to incrementals..",7
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","Agreed, there was some hyperbole in my comment.  But the `on_schema_change` definitely helps avoid errors in the slim CI job run.",8
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","> Iâ€™d recommend adding all dynamic configuration in `dbt_project.yml`

@joellabes could you please elaborate on how to add dynamic config to the `dbt_project.yml` file? It would be awesome if you could share a snippet as an example, thank you!",9
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","[quote=""ecalzo, post:9, topic:2603""]
@joellabes could you please elaborate on how to add dynamic config to the `dbt_project.yml` file? It would be awesome if you could share a snippet as an example, thank you!
[/quote]

Sure! Borrowed from [FAQ: Configuring models based on environments](https://discourse.getdbt.com/t/faq-configuring-models-based-on-environments/463):
```
models:
  materialized: ""{{ 'table' if target.name == 'prod' else 'view' }}""
```",10
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","We found a mix of both works well, as sometimes we want a table, view or incremental.

We wrote a macro that we put at the top of the models that aren't views (we use views by default) and then have environments like you did (sample/ci/full).

Sample: only a very small subset of data (like <= 200 rows)
CI: Last few weeks of data, uses tables/views. Sometimes a table is much faster for an incremental table depending. You pay for the cost in either building the table or when running a test against that view, it has to build anyway, but now it builds twice (once for the test, and again for the downstream model(s)).
Full: full dataset, uses incremental views, tables, views as needed. :)",11
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","I have also been setting the environment where the CI runs target name to 'ci' using an environment variable combined with an `if` block in the .sql files specifically looking for the target where the dev CI check runs:
```
with 

long_running_model as (
  select * from {{ ref('fct_model_with_billion_rows')}}

  {% if target.database == 'ci' %}
    limit 1000
  {% endif %}
)
...
```
which then will only result in 1000 rows in the CI check, rather than all of them, which will speed up the process considerably.",12
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","Hi @joellabes , 
In a Slim-CI setup in dbt cloud, does state comparison run properly if we defer to a partial prod run instead of a deferring to a job that runs all the models? Since a manifest file gets generated for the entire dbt project instead of just the models that were run, does it matter if we defer to a run that only executed a subset of models from the project, are there any known caveats?",13
2603,"how-we-sped-up-our-ci-runs-by-10x-using-slim-ci","@padmavathy it doesn't matter - as you correctly note, dbt generates a full manifest no matter what, so a partial prod run would be fine.",14
2626,"how-and-why-use-exposures","I'm fairly new in DBT and trying to explore how to exposures. I've already read the documentation ( https://docs.getdbt.com/docs/building-a-dbt-project/exposures ), but I do not feel that I get the answers to my questions.

I'm well aware of the concept that you create an exposures file in your models' folder, then you declare the table name and the other tables/sources that it depends on.

Q1 - Should I state the whole downstream of tables or just the direct tables that it depends on?

Q2 - What exact benefit does it do? Can you come up with a specific scenario?

Q3 - what the purpose of dbt run -m exposure:name and dbt test -m exposure:name? Is it testing the model or the exposure?

I've done exactly what they say in the documentation, I just do not get how I can use it.

Thank you in advance :-)",1
2626,"how-and-why-use-exposures","Hey @MadsS, good questions! Some answers:

[quote=""MadsS, post:1, topic:2626""]
Q1 - Should I state the whole downstream of tables or just the direct tables that it depends on?
[/quote]
You only need to state the final model(s) in your exposure - it will automatically detect all parents from there to display in the DAG.

[quote=""MadsS, post:1, topic:2626""]
Q2 - What exact benefit does it do? Can you come up with a specific scenario?
[/quote]
A couple of use cases:

1. If you're using dbt Cloud, you can generate a metadata tile which shows data freshness and test success in your BI tool. Check out [this video from earlier in the year](https://youtu.be/5QvdNYPQofE?t=46m22s) for a demo 
2. We used exposures recently to know which reports would be impacted by a series of source tables being deprecated. We could identify all of them at once and make decisions about whether to upgrade them or replace them. Without exposures, we would have done our best to find them but probably missed one which would have impacted our business users. 
3. It can add confidence in your CI runs to know that proposed changed won't impact dashboards etc 

[quote=""MadsS, post:1, topic:2626""]
Q3 - what the purpose of dbt run -m exposure:name and dbt test -m exposure:name? Is it testing the model or the exposure?
[/quote]
On their own, running and testing an exposure won't do anything. However, when you use the [+ operator](https://docs.getdbt.com/reference/node-selection/graph-operators#the-plus-operator), it will run/test the models that feed into an exposure. [Here's our selectors for CI jobs, for example](https://discourse.getdbt.com/t/how-we-sped-up-our-ci-runs-by-10x-using-slim-ci/2603#run-9)",2
2626,"how-and-why-use-exposures","Thank you very much @joellabes!
I'll give it all a go! :)",3
2694,"limiting-the-not-null-test","This question was [posed in Slack in 2019](https://getdbt.slack.com/archives/C2JRRQDTL/p1568186744302000):

> When running schema tests, such as `not_null`, the SQL is executed over the entire table. As the table grows in size the cost of running such tests increases....  Is there a way to limit the schema tests to the partition of data that was inserted on the current `run` ?

One proposed solution was ""write a custom test that only filters on the latest [data]."" 

What's the best practice?",1
2694,"limiting-the-not-null-test","As of dbt 0.20.0, you can include a `where` clause in your test config! Here's an example from the docs: https://docs.getdbt.com/reference/resource-configs/where

If you're not able to upgrade yet, earlier versions of dbt-utils have a [not_null_where](https://github.com/dbt-labs/dbt-utils/tree/0.6.6#not_null_where-source) test you can use instead.",2
2694,"limiting-the-not-null-test","Great. Thanks @joellabes",3
2694,"limiting-the-not-null-test","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
2727,"seed-data-column-type-changes-arent-being-applied","I followed the instructions described in https://docs.getdbt.com/reference/seed-configs to customize a couple of columns' data type, but seems dbt did pick up the config, but did nothing, column types were still inferred wrong.  Is this a known issue or is there a better way to do this?  Thanks",1
2727,"seed-data-column-type-changes-arent-being-applied","Hi,

This does work, I do it for some RegionCode seed data we use. In the dbt_project.yml file:


```
seeds:
  schema: STATIC_DATA
  {my_project_name}:
    regions:
      alias: region_codes
      +column_types:
        RegionCode: varchar(16)
        Description: varchar(255)
```
Resulting log entries when a dbt seed is run:

```
2021-07-08T07:27:08.370458Z: On seed.{my_project_name}.regions: /* executed by dbt cloud */

    create table DEMO_DB.dbt_dbeaty_STATIC_DATA.region_codes (RegionCode varchar(16),Description varchar(255))
2021-07-08T07:27:08.654076Z: SQL status: SUCCESS 1 in 0.28 seconds
```",2
2727,"seed-data-column-type-changes-arent-being-applied","Thank you for the response! Could you share your seed data file path? Does the file path have to be strictly reflected in the configuration nested hierarchy?",3
2727,"seed-data-column-type-changes-arent-being-applied","I think I found the problem, I ran once and the data type and table already there. I need to drop the schema to start from scratch. Now it works.",4
2727,"seed-data-column-type-changes-arent-being-applied","[quote=""yduan, post:4, topic:2727""]
I need to drop the schema to start from scratch
[/quote]

You're right - dbt truncates and re-inserts into the table during a `dbt seed` step. You can use the `--full-refresh` flag to force it to drop your seed table, which saves you from having to manually drop it: https://docs.getdbt.com/reference/resource-configs/full_refresh#seeds",5
2727,"seed-data-column-type-changes-arent-being-applied","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
2782,"advanced-schema-tests-thresholds-exclusions-and-date-limits","At Rezdy weâ€™ve got our fair share of failing dbt tests and it goes without saying that when you have a failing test you should always address the root cause of the problem. 

That being said itâ€™s a lot faster just to sweep non-critical failing tests under the rug and get on with your day. These ""advanced"" schema tests are like the trap door under the rug.

Inspired by a [post on creating error thresholds for schema tests](https://discourse.getdbt.com/t/creating-an-error-threshold-for-schema-tests/966), we set out to create a malleable dbt schema test for our most troublesome tests(unique and not_null)

The test handles 3 cases:

1. Date scoping the dataset before running the test
2. Implementing either a absolute or relative error threshold for row failure
3. Explicitly exclude dodgy rows from tests

Date scoping has been covered a fair bit already and is [likely to be made redundant by either dbt_utils or the new capability of the not_null schema test in dbt v0.20.0](https://discourse.getdbt.com/t/migrating-away-from-dbt-utils-not-null-where-and-unique-where-tests/2773) and thresholds have received a fair bit of attention so this post is mostly about the more idiosyncratic test exclusions option. 

The test exclusion functionality allows us to test a dataset as normal but put in a small clause such as:

*Even though all customers who sign up should have a trial event before they have a purchase event(`trial_id` not null in `fct_purchase`), there was that one time back in 2018 where a customer managed to somehow do it because of a bug. We'll make a note of the customers ID and the reason why they don't have a trial in our exclusion file (a seed file or a google sheet if you can swing it **Hail BigQuery**) then move on with our lives.*


*Note:*

* There are separate test macros for not_null and unique but they are identical except for the logic that captures duplicates or nulls in the `perform_test` CTE.
* These models are currently running on a BigQuery warehouse, you might need to tweak some syntax to get it to run on other warehouses.* 
* The two supporting macros; `get_model_ref `and `exclude_rows_from_test` are also included down the bottom.
* You can name the macros test_not_null and test_unique without the adv prefix and they will just replace the ones that come with dbt, but then you have to go and update your whole project.
* `tbl_test_exclusions` is the name of the table where we store both the rows that will be excluded and the reason for doing so you can call this whatever you like.
* If you've been doing some black magic with condition blocks and ref statements and encountered the below test ERROR, it might be worth checking out the `get_model_ref` macro down the bottom.

```
Compilation Error in test some_test_name (models/staging/something/something.yml)
  dbt was unable to infer all dependencies for the model ""some_test_name"".
  This typically happens when ref() is placed within a conditional block.

  To fix this, add the following hint to the top of the model ""some_test_name"":
```




**Not Null**

```

/* -----------------------------------------------------

This is an advanced not_null schema test.
It is used the same way the normal not_null schema test is used inside the .yml file
however instead of:
              - not_null
use
              - adv_not_null:

The test has 3 functions:
 - Thresholds (absolute and relative)
 - Date Limits (by day)
 - Row Exclusions (nit picking out bad rows)

Usage Notes:
  Thresholds are calculated after date scoping.
  When testing relative thresholds use values 1 to 100 indicating the % of bad rows expected.
  The error output for threshold fields indicates the number of fails beyond the threshold.
    not the absolute number of fails. (The absolute number can be obtained from a normal test.)
  A single test can use all 3 functions.
  For specifcs on the row exclusions functionality check the exclude_rows_from_test macro


config for .yml file
            - adv_unique:
                model:                          # Taken from the model the test is on
                column_name:                    # Taken from the column the test is on
                event_timestamp_column_name:    # [The date column for date limit tests]
                number_of_days_to_check:        # [The number of days for date limit tests]
                threshold_type:                 # relative / absolute (relative in %)
                threshold_value:                # integer to indicate accepted errors for abs/rel
                column_to_exclude_on:           # if the column has exclusions on a field, enter that column name here

*/ -----------------------------------------------------
{%macro test_adv_not_null(
  model,
  column_name,
  event_timestamp_column_name = None,
  number_of_days_to_check = None,
  threshold_type = None,
  threshold_value = 1,
  column_to_exclude_on = None ) %}

/* -----------------------------------------------------
Scope to valid test rows.
*/ -----------------------------------------------------
WITH valid_test_rows AS (
SELECT *
  FROM {{ model }}
 WHERE 1 = 1

   {% if column_to_exclude_on !=  None %} --This will check your test exclusion
    AND {{ exclude_rows_from_test( model.name, column_to_exclude_on ) }}
   {% endif %}

   {% if number_of_days_to_check != None and event_timestamp_column_name != None%}
    AND {{ event_timestamp_column_name }} > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {{number_of_days_to_check}} DAY)
   {% endif %}
 )
/*-----------------------------------------------------
Set Threshold if applicable.
*/ -----------------------------------------------------

, threshold_limit AS (
SELECT

    {% if threshold_type ==  'relative' %}
      ROUND(COUNT(*) * ({{ threshold_value }} / 100),0) AS max_errors  --find the maximum amount of rows that can fail the test
      FROM valid_test_rows --This allows relative tests to be effective on date range tests

    {% elif threshold_type ==  'absolute' %}
      {{ threshold_value }} AS max_errors  --find the maximum amount of rows that can fail the test

    {% else %} --ELSE
      0 AS max_errors  --find the maximum amount of rows that can fail the test
    {% endif %}
)

/*-----------------------------------------------------
Perform the Test.
*/-----------------------------------------------------

, perform_test AS (

        SELECT *
        FROM valid_test_rows
        WHERE {{ column_name }} IS NULL


)
/*-----------------------------------------------------
Count the Errors.
*/ -----------------------------------------------------
, validation_errors AS (
SELECT
COUNT(*) AS n_errors
FROM perform_test
)
/*-----------------------------------------------------
Check if the number of errors is greater than the threshold.
*/ -----------------------------------------------------
SELECT
    CASE WHEN ve.n_errors > tl.max_errors THEN ve.n_errors - tl.max_errors
         WHEN ve.n_errors < tl.max_errors THEN 0
    ELSE 0
    END AS result
FROM validation_errors ve
CROSS JOIN threshold_limit tl


   {% endmacro %}


```


**Unique**

```
/* -----------------------------------------------------
This is an advanced unique schema test.
It is used the same way the normal unique schema test is used inside the .yml file
however instead of:
              - unique
use
              - adv_unique:

The test has 3 functions:
 - Thresholds (absolute and relative)
 - Date Limits (by day)
 - Row Exclusions

Usage Notes:
  Thresholds are calculated after date scoping.
  When testing relative thresholds use values 1 to 100 indicating the % of bad rows expected.
  The error output for threshold fields indicates the number of fails beyond the threshold.
    not the absolute number of fails. (The absolute number can be obtained from a normal test.)
  A single test can use all 3 functions.
  For specifics on the row exclusions functionality check the exclude_rows_from_test macro


config for .yml file
            - adv_unique:
                model:                          # Taken from the model the test is on
                column_name:                    # Taken from the column the test is on
                event_timestamp_column_name:    # [The date column for date limit tests]
                number_of_days_to_check:        # [The number of days for date limit tests]
                threshold_type:                 # relative / absolute (relative in %)
                threshold_value:                # integer to indicate accepted errors for abs/rel
                column_to_exclude_on:           # if the column has exclusions on a field, enter that column name here

*/ -----------------------------------------------------
{%macro test_adv_unique(
  model,
  column_name,
  event_timestamp_column_name = None,
  number_of_days_to_check = None,
  threshold_type = None,
  threshold_value = 1,
  column_to_exclude_on = None ) %}

/* -----------------------------------------------------
Scope to valid test rows.
*/ -----------------------------------------------------
WITH valid_test_rows AS (
SELECT *
  FROM {{ model }}
 WHERE 1 = 1

   {% if column_to_exclude_on !=  None %} --This will check your test exclusion
    AND {{ exclude_rows_from_test( model.name, column_to_exclude_on ) }}
   {% endif %}

   {% if number_of_days_to_check != None and event_timestamp_column_name != None%}
    AND {{ event_timestamp_column_name }} > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {{number_of_days_to_check}} DAY)
   {% endif %}
 )
/*-----------------------------------------------------
Set Threshold if applicable.
*/ -----------------------------------------------------

, threshold_limit AS (
SELECT

    {% if threshold_type ==  'relative' %}
    ROUND(COUNT(*) * ({{ threshold_value }} / 100),0) AS max_errors  --find the maximum amount of rows that can fail the test
      FROM valid_test_rows --This allows relative tests to be effevtive on date range tests

    {% elif threshold_type ==  'absolute' %}
      {{ threshold_value }} AS max_errors  --find the maximum amount of rows that can fail the test

    {% else %} --ELSE
      0 AS max_errors  --find the maximum amount of rows that can fail the test
    {% endif %}
)

/*-----------------------------------------------------
Perform the Test.
*/-----------------------------------------------------

, perform_test AS (

        select
            {{ column_name }}

        from valid_test_rows
        where {{ column_name }} is not null
        group by {{ column_name }}
        having count(*) > 1


)
/*-----------------------------------------------------
Count the Errors.
*/ -----------------------------------------------------
, validation_errors AS (
SELECT
COUNT(*) AS n_errors
FROM perform_test
)
/*-----------------------------------------------------
Check if the number of errors is greater than the threshold.
*/ -----------------------------------------------------
SELECT
    CASE WHEN ve.n_errors > tl.max_errors THEN ve.n_errors - tl.max_errors
         WHEN ve.n_errors < tl.max_errors THEN 0
    ELSE 0
    END AS result
FROM validation_errors ve
CROSS JOIN threshold_limit tl


   {% endmacro %}

```


**The Test Exclusion Macro**

```
/* -----------------------------------------------------
This is the macro that allows the advanced schema tests to access the tbl_test_exclusions 
table.

Arguments:
- model_name: The model you wish to exclude from
- column_name: The column name you wish to exclude on

Output:
A SQL clause that will exclude rows from a table.

Usage:
  Example use case:
  In 2019 a bug allowed a single customer (dim_customer.customer_id =123)
  to be able to sign up without a last name. Then the not_null schema test on
  dim_customer.last_name would always fail on a single row. How Annoying!

  Usage of this macro depends on the configuration of the tbl_test_exclusions
  table schema. It should have the following columns:
    id:               surrogate key for the exclusion table.[N]
    exclusion_value:  unique key of the model that you wish to exclude a row from for testing.[123]
    model_name:       The name of the model that you wish to exclude a row from. [dim_customer]
    column_name:      The name of the column you wish to exclude on. [customer_id]
    owner:            String field to indicate who decided to exclude the row from testing.[your_name]
    description:      String field to explain why this row is special. [Bug back in 2019 that allowed a single customer to sign up with no last name.]

Notes:
The exclusion table can be any table in the database. dbt seed files and Gsheets
work well.
The macro should be placed inside a SQL WHERE clause, either:
- after WHERE
- or after AND
Because of the way get_mode_ref() works, if you choose to create a base table for your
tbl_test_exclusions model you will need to set the schema=none


*/ -----------------------------------------------------
{% macro exclude_rows_from_test(model_name, column_name) %}
CAST({{ column_name }} AS STRING) NOT IN
(
SELECT CAST(exclusion_value AS STRING) AS exclusion_value  -- eg, id
   FROM {{ get_model_ref('tbl_test_exclusions') }} RIRE
  WHERE
  RIRE.model_name =  '{{ model_name }}'
  AND
  RIRE.column_name =  '{{ column_name }}'
)
{% endmacro %}


```

**The base table for the test exclusions macro**

```

{{
  config(
    materialized='table',
    schema=none
  )
}}
/* -----------------------------------------------------

Scope: Identifying data for rows in models that we wish to exclude from certain tests because
they will cause the test to constantly fail.

Grain: id(surrogate key from source table which is a gsheet.)

Purpose: to create a base model for the test exclusion dataset.

Config:
This model has to be a table because the source data is a a gsheet which has strange errors
when it is repeatedly queried.

The schema must be set to none because it is referred to using the get_model_ref macro
which is designed to get around a strange issue with using refs in conditional blocks.
https://github.com/dbt-labs/dbt/issues/1077.


Notes:

*/ -----------------------------------------------------

SELECT rite.id,
       rite.exclusion_value,
       rite.model_name,
       rite.column_name,
       rite.owner,
       rite.description
  FROM {{ source('internal_datasets','tbl_test_exclusions') }} rite
 WHERE rite.id IS NOT NULL
   AND rite.exclusion_value IS NOT NULL
   AND rite.model_name IS NOT NULL
   AND rite.column_name IS NOT NULL


```


**Handy macro ""get_model_ref""**

```
/*
Purpose:
to provide an alternative for {{ ref('model_name') }}
that will allow dbt to compile:
- when using a diabled model
- when there is a {{ ref('model_name') }} inside a conditional block.

Arguments:
    model_name: the model name that might be disabled but needs to be referenced

Output:
project_id.dataset_name.table_name

Notes:
This doesn't include any custom schemas.
*/

{% macro get_model_ref(model_name) %}
`{{target.project}}.{{this.schema}}.{{model_name}}`
{% endmacro %}

```",1
2782,"advanced-schema-tests-thresholds-exclusions-and-date-limits","This is cool! We've historically filtered out handfuls of misbehaving records by adding their IDs to the where clause of the standard tests, but if you've got a lot of them and want more observability this is more scalable once it's properly set up.",2
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","<!-----
â„¹ï¸ Are you looking for troubleshooting help?

If so, please post your question to Stack Overflow:
https://stackoverflow.com/questions/ask?tags=dbt

We are reserving Discourse for more long-form discussions, and ""How to"" posts. Check out more details here:
https://discourse.getdbt.com/t/welcome-to-dbt-discourse/8 
---->

Hi,

I am new to dbt and we are trying to implement scd 2 using dbt. I learned about Snapshot, however, is there nay other way to implement scd 2 in dbt?

Thanks",1
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","Why are you looking for an alternative to snapshots? Are you concerned that they won't solve a specific use case you have in mind?",2
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","Thanks for reply Joellabes. There are couple of things, we want to check how we can handle scd 1 and scd 2 in same table. Also, we have multiple teams doing simultaneous development and has their own folder structure for modeling and we observed that, we cannot have snapshots in different folders as per each team. 

Hence, we want to explore new avenues to handle scd 2 using dbt",3
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","Generally you should snapshot your source data and bring it in with zero transformations.    Let other teams model off that raw data if they need to.  I'd be curious of the use case that requires each team to have their own snapshot of raw data.  Sounds like you want historical data and also the current status, to do that just create your type 2 dim and then a view on top that filters to the most current record.",4
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","Thanks for reply. The reason we are thinking out of box here is, snapshot creates table in DB while our policy states that, tables shall be created using scripts, Git check ins and Dev Opps programs. No out side program should create tables automatically. 

We are looking into macros but no luck so far. If you have any specific suggestion, we are all ears :slight_smile:

Appreciate your help.",5
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","dbt snapshots' columns are knowable in advance (it's just the columns of your existing snapshot table plus the four bookkeeping columns `dbt_scd_id`, `dbt_updated_at`, `dbt_valid_from` and `dbt_valid_to`), so you could always pre-generate your tables and have dbt snapshot populate them. 

With that said, keep in mind that unless you're building everything as views, `dbt run` will also create tables... 

I agree with @yoitsbenc - build a SCD2 table and then filter to current records (those where `dbt_valid_to` is null) in a downstream model to get the SCD1 equivalent.",6
2872,"can-i-implement-scd2-in-dbt-without-using-snapshots","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","If you have a MacBook with an M1 chip and have tried to get dbt up and running from the command line, you may have run into the error below:

![Screen Shot 2021-08-16 at 2.21.12 PM|690x116](upload://rlDZwmhUWE2LNTdCrWaKkCJmp0Q.png)

After some searching of the interwebs, I found a solution that worked for me, tucked away in a GitHub issue, in a [comment](https://github.com/dbt-labs/dbt/issues/3162#issuecomment-799767694) by from Mike Lanza (thanks Mike! none of the other mentioned workarounds worked for me :partying_face:).

Here's how you do it :arrow_down:

**Step 1: Set up your Terminal to use Rosetta.**
Go to Applications > Utilities > Terminal (or go to whatever other Terminal app you use), right click, select â€˜Get Infoâ€™ and then select â€˜Open using Rosettaâ€™ under the General options. Close the Get Info window.

**Step 2: Open your Terminal and uninstall any versions of dbt you may have tried to install before.**
Iâ€™m team Homebrew  - run `brew uninstall dbt` in the Terminal*. Then, if you run `which dbt` you should get `dbt not found`.

*Per @claire - you may also consider uninstalling Homebrew completely and reinstalling brew via Rosetta first.

**Step 3: Create a python virtual environment and install dbt.**
Full instructions [here](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/), but run these are the exact commands I ran in the Terminal:

`python3 -m pip install --user --upgrade pip`
`python3 -m pip install --user virtualenv`
`python3 -m venv env`
`source env/bin/activate`
`pip3 install dbt==0.19.0`
`pip3 install --upgrade pip`
`dbt --version`

**Step 4: Change your working directory, if necessary.**

**Step 5: Do whatever you need to do in dbt!**

**Step 6: Deactivate your virtual environment.**
Run `deactivate` in the Terminal.

And that's it! Hope this saves some time for anyone struggling through the same situation :blush:",1
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","Chiming in here with my suggestions! (Thanks for the tag @roliepoly!)

First of all, if dbt is the *only* command line program you are having issues with, use Olya's approach! It's great!

I personally was finding that I had M1 issues on other programs installed via homebrew (pyenv, ruby, everything, gah!) that a python env wouldn't solve, so I had to do something a little more drastic!

Here's what I did:
1. Completely uninstalled homebrew, and all brew-installed programs
I can't remember the exact instructions here, but think this was pretty google-able

2. Relauched iTerm with Rosetta.
This is a way to use programs as though they are operating on an non-M1 Mac
To do this: right-click on iTerm, click 'Get Info', and check the 'Open using Rosetta' option.

3. Reinstalled homebrew using my (Rosetta) iTerm
This let me install dbt without issues, and all my other programs :tada:

Granted, using this approach unilaterally means that I lose some of the speedups from the M1 chip, and some programs *are* compatible with the M1 chip.

If you want to take advantage of the M1 software where possible, some solutions on the internet also suggest that it's possible to run a dual setup, so that you can install M1-compatible software on the M1 architecture, and then have a backup way to install non-compatible software (like dbt).

There are two main approaches to a dual setup, neither of which I could get to work, but you may be able to! I've included a conceptual overview of both for context

**Option 1: Using two versions of iTerm (one using the M1 architecture, one using Rosetta):**
To create your second iTerm app, copy your iTerm app, name it ""iTerm rosetta"" version, and launch it using Rosetta.
Then install homebrew from the iTerm Rosetta app, and any incompatible programs.
Default to using your standard iTerm app when installing programs, and use the iTerm Rosetta app when installing programs that have compatibility issues.
I copied the app successfully, but couldn't get the two versions of homebrew to coexist.

**Option 2: Using one iTerm application, but two versions of homebrew**
To install the second version of brew (using Rosetta) use `arch -x86_64` as a prefix to the installation command ([source](https://stackoverflow.com/questions/64882584/how-to-run-the-homebrew-installer-under-rosetta-2-on-m1-macbook)).
Then use that prefix again when installing programs with the old version of homebrew: `arch -x86_64 brew install <package>`.
Default to using the standard brew command when installing programs, and use the prefixed brew command when installing programs that have compatibility issues.
Again, I couldn't get this to play nicely, mostly due to `pyenv` issues.

The issues I hit were to do with `$PATH` variables, sourcing commands (esp. on `pyenv`, a tool I use a lot) and all kinds of things that I hate debugging. A more patient person than me could probably get a dual architecture working! For now, I'm sticking with my Rosetta-only approach.",2
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","Hey folks! Adding in my solution which worked for dbt 0.20.1, Python 3.9.6 and M1 Mac.

This will require using Pycharm as the IDE for building and running dbt models.


**Step 1** : Use Pycharm IDE ( latest version) for development & terminal emulator within the IDE for running dbt commands

**Step 2**:  Create virtualenv with Python 3.9.* installed in it.


**Step 3** . Add the location of above as the interpreter in PyCharm



**Step 4**. Install dbt inside the interpreter through PyCharm.  Latest version of Pycharm has started supporting M1 ARM architecture without any explicit configurations, see the blog post [here](https://blog.jetbrains.com/pycharm/2021/01/the-transition-to-apple-silicon/).

![Screen Shot 2021-08-31 at 1.32.28 PM|578x500](upload://iX0twHeo00ul9UbLUmV5l8nF0wM.png)


**Step 5** . *dbt run* inside the terminal emulator might throw the error ""command not found"". Solution to this is to alias '*dbt*' in bashrc/zshrc to point to dbt installation location.

`alias dbt=/Users/aathresh/.pyenv/versions/dbt-env/bin/dbt`",3
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","Chiming in with my experience dealing with this...

I ended up with the same solution as you, Claire. I tried the ""both"" thing (using your option 1, two versions of iTerm). Some of my projects were in the M1 setup and I switched them over to Rosetta as-needed if I ran into M1 compatibility issues. I was able to mostly get it to work although there were a lot of hiccups, especially around my .zshrc/.bashrc files. 

After a few weeks I decided it was just too annoying/confusing/complex. I do everything w a Rosetta terminal now and basically just pretend I'm not on an M1. I have the base Macbook Air model and everything feels pretty snappy despite being in the emulator.",4
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","Just a quick note on the original code in the article that tripped me up. dbt requires psycopg2 (or in the M1 case, psycopg2-binary) dependency. When following these steps, a critical error is to not upgrade pip AFTER you activate your virtual environment.

I spent a solid amount of time trying to figure out why nothing would load, thinking the pip upgrade outside of the virtual environment would propogate through - that is not the case.


```
source env/bin/activate
pip3 install --upgrade pip
pip3 install psycopg2-binary 
pip3 install dbt
```

Then you're good to go",5
2908,"get-dbt-up-and-running-from-the-command-line-on-a-macbook-with-an-m1-chip","Just to add on to the last comment.

`pip3 install dbt`  is no longer valid

 Use the below instead



```
     As of v1.0.0, `pip install dbt` is no longer supported.
      Instead, please use one of the following.
      
      **To use dbt with your specific database, platform, or query engine:**
      
          pip install dbt-<adapter>
      
          See full list: https://docs.getdbt.com/docs/available-adapters
      
      **For developers of integrations with dbt Core:**
      
          pip install dbt-core
      
          Be advised, dbt Cores python API is not yet stable or documented
          (https://docs.getdbt.com/docs/running-a-dbt-project/dbt-api)
      
      **For the previous behavior of `pip install dbt`:**
      
          pip install dbt-core dbt-postgres dbt-redshift dbt-snowflake dbt-bigquery
`
```",6
2909,"the-exact-privileges-we-grant-to-set-up-redshift","> :sparkles: **2021 update** :sparkles:
This article originally appeared as part of an article that covered grant statements for both Redshift and Snowflake (Snowflake version [here](https://discourse.getdbt.com/t/the-privileges-we-grant-to-set-up-snowflake/439)). The original was written in 2019, when I was an employee of dbt Labs. Now, Iâ€™m freelancing (among other things), and actually got the opportunity to set up some new Snowflake databases recently. I took the opportunity to update some of my advice over on the Snowflake version of this article, and figured it was more logical to have two separate articles. Note that I haven't used Redshift lately, but I think this advice still stands (until they implement cross-database queries for everyone, not just those using `ra3` nodes).

Before reading through these exact instructions, take a moment to read some prior art:

1. @jercoâ€™s [article](https://blog.getdbt.com/how-we-configure-snowflake/) on setting up the concepts behind sending up Snowflake for clients â€” we're trying to apply a lot of the same ideas on Redshift, but with less flexibility (namely, no databases or warehouses)
2. My article on [database administration](https://blog.getdbt.com/five-principles-that-will-keep-your-data-warehouse-organized/)

While these article go deep into the â€œwhatâ€ and â€œwhyâ€ of database admin respectively, it was pretty light on the very specific â€œhowâ€ of setting this up, i.e. the implementation steps.

In this guide, Iâ€™ll run through the exact privileges it takes to set up your Redshift cluster the way these articles intended.

# 1. Set up the database
On Redshift, you can't run cross-database queries, so we'll do everything in one single database. By default, Redshift creates a database named `dev`, so let's create our own one with a better name
```sql
create database analytics
```

# 2. Set up groups
We'll grant privileges to these groups rather than to users directly
```sql
create group loader;
create group transformer;
create group reporter;
```

# 3. Create users, assigning them to their groups
Every human being and application gets a separate user, and is assigned to the correct shared role.
```
create user stitch -- or fivetran
    password '_generate_this_'
    in group loader;

create user claire -- or amy, jeremy, etc
    password '_generate_this_'
    in group transformer; 

create user dbt_cloud
    password '_generate_this_'
    in group transformer;

create user looker -- or mode or periscope
    password '_generate_this_'
    in group reporter;
```


# 4. Let `loader` load data
By granting a user the privilege to create a schema within a database, they will also then be able to create tables and views within that schema. 

```
grant create on database analytics to group loader;
grant select on all tables in schema information_schema to group loader;
grant select on all tables in schema pg_catalog to group loader;
grant select on all tables in schema information_schema to group transformer;
grant select on all tables in schema pg_catalog to group transformer;
```
^ I've also snuck in some statements to give privileges to information schemas here â€“ these are required by both your ETL tools (i.e. members of your `loader` group) and by dbt users (i.e. members of your `transformer` group).

# 5. Wait...
If you've just set up your database from scratch, you'll have to wait a while here for data to start flowing before you can grant privileges for members of your `transformer` group to read it.

# 6. Let `transformer` transform data
Once your raw data is in your warehouse, you'll need to give privileges for your `transformer` group to read it!

There are three parts to granting the privilege to read all relations within a schema:
* Granting usage on a schema
* Granting select privileges on all existing relations within a schema
* Granting select privileges on all relations created in this schema in the future (default privileges), so that you don't need to rerun grant statements for every new object in the schema.

On Redshift, this looks like:
```
-- for each schema:
grant usage on schema raw_stripe to group transformer;
grant select on all tables in schema raw_stripe to group transformer;
-- note that the `for user` clause should be adjusted based on the user who will be creating
-- objects in the schema (which should also be the schema owner).
alter default privileges for user stitch in schema raw_stripe
  grant select on tables to group transformer;
```
You'll need to rerun these whenever you add a new schema to your database via Fivetran or Stitch

This group also needs to be able to create in the database:
```sql
grant create on database analytics to group transformer;
```
And needs some permissions to generate dbt docs
```sql
grant select on all tables in schema information_schema to group transformer;
grant select on all tables in schema pg_catalog to group transformer;
```

# 7. Let `reporter` read the transformed data

The `reporter` role should be able to read anything that dbt creates.

In Redshift, we implement these via an [on-run-end hook](https://docs.getdbt.com/docs/using-hooks) â€“ check out [this](https://discourse.getdbt.com/t/the-exact-grant-statements-we-use-in-a-dbt-project/430) article about the exact hooks we run!

# 8. Maintain!
When new users are added, make sure you add them to the right shared role! And as new schemas of raw data are added, ensure that privileges are granted to the `transformer` role to be able to read them.",1
3291,"setting-up-vscode-to-use-with-the-dbt-cli","> :information_source:  For the most-up-to-date version, you might want to go here: https://dbt-msft.github.io/dbt-msft-docs/docs/guides/vscode_setup/

## Intro

When our team first started using the dbt CLI, we started with Claire's well-loved discourse post, [How we set up our computers for working on dbt project](https://discourse.getdbt.com/t/how-we-set-up-our-computers-for-working-on-dbt-projects/243). The post details how the dbt team uses Atom and iTerm 2 on macOS for an improved workflow. Many folks commented on how they acheived similar productivity using VSCode. I thought I'd consolidate some of this into a single article, and expand on it given the recent developments. I'm also going to add things to make it easier for working with Azure databases such as the Azure CLI and Azure Data Studio.

### Goals

Following this guide will enable the following behavior in VSCode (some points lifted from Claire's guide -- linked above)

- a stable, reproducible Python environment for running dbt
- syntax highlighting for SQL files with jinja in them
- graying out the text of files that have compiled code in them, to help prevent you from editing compiled SQL (as opposed to your actual model)
- quick switching b/w a model file and it's `compiled` and `run` counterparts 
- allow  


### Prerequisite

If you've never used VSCode with Python, I strongly recommend at least the first half of  Dan Taylor's [Get Productive with Python in Visual Studio Code](https://www.youtube.com/watch?v=PnOPp4DsY2w) talks. It covers a lot of the basics like installing Python, the Python extension, and the command pallette.

You should also have the following installed:

- Git
- VSCode
- Python 3 (via anaconda, brew or [python.org](https://www.python.org/downloads/) )

In VSCode you'll also need to install the Python extension

### If you already know VSCode

Here's [a gist for an example .vscode directory](https://gist.github.com/swanderz/5cf876d88c7c8d268d8c1e1e5d05bffd) that contains a `settings.json` and an `extensions.json`

### Getting started

To get started, we'll use the [jaffle_shop repo](https://github.com/dbt-labs/jaffle_shop), a self-contained project.

You can use the Git CLI or the VSCode Git extension to Git Clone command in VSCode
```bash
git clone https://github.com/dbt-labs/jaffle_shop.git
```

Then, open the `jaffle_shop/` directory in VSCode.

## Python environment


### Goal

The goal of this section is to ensure that the right version of Python and dbt are always available right away when you open your dbt project in VSCode. Sounds simple, but below is a one-time setup guide on how to make it work. More context is that some folks have bundled this set up process into [bash scripts](https://discourse.getdbt.com/t/setting-up-your-local-dbt-run-environments/2353) and [Docker containers](https://discourse.getdbt.com/t/a-containerized-dbt-environment-for-your-team/2340).

Some folks deem this problem so difficult as to justify having users use Docker containers, but I have yet to be convinced of that yet.

### Walkthrough

Python can be tricky get working in VSCode (and trickier to work on Windows). You OS likely already has a version of python installed, but this can be troublesome because you don't control it's version.

It's better practice to have a dedicated dbt environment. Three popular tools are `venv`'s, `virtualenv`'s and `conda` environments. Our team uses `conda` envs because we have many different projects with different sets of package requirements, but if dbt is 1) your only use case for Python, or 2) your first Python-based use case, you'll likely have a better time with `virtualenvs`. I'm going to only talk about venv because it comes built-in with Python

Open a terminal with `CTRL+`` (which should open within the jaffle_shop directory) and do the following steps:

```bash
# make sure you have Python at least 3.6 and less than 3.10
# Create and activate virtual environment
python3 -m venv .dbtenv
source .dbtenv/bin/activate

# install the dbt package you want
pip install dbt-synapse # or dbt-sqlserver or whatever

# make Git ignore all these newly created files
echo '.dbtenv/' > .gitignore
```

Once you've done this you should now be able to:
1. bring up the command pallette (`CMD+SHIFT+P`)
2. search for ""Python: Select Interpreter"", and
3. Pick the `.dbtenv` environment (should be the first result)

Those three steps will:

1. activate the Python extension if it hasn't been already
2. ensure that all new terminals opened in VSCode will auto-activate your `.dbtenv` environment

This is huge because now all your terminals in the VSCode will always have your dbt package available. However, this behavior will not persist the next time you open this repo in VSCode. To make this auto-env selection persist, you must do two things:
1. add a `requirements.txt` to you the top level of the repo ([pip docs on `requirements.txt` files](https://pip.pypa.io/en/stable/user_guide/#requirements-files))
2. (optional) add to the `requirements.txt` what packages w/ versions you plan to do in this project (example below)
3. create a new file `.vscode/settings.json` and add the Python path to the `settings.json` (more on VSCode settings later!)

#### `requirements.txt`
```
dbt-synapse==0.19.2
sqlfluff==0.7.1
```
#### `.vscode/settings.json`
```json
{
    // change this to your desired path!
    ""python.pythonPath"": ""./.dbtenv/bin/python"",
}
```

Now that you've done these two things, everytime you open the `jaffle_shop/` dir, in VSCode two things should happen:
1. the Python extension activates right away (do you see the Python version listed alongside your environment name on the bottom info bar?)
2. any terminal you open will auto-activate your `.dbtenv` and each line should begin with `(.dbtenv)`

You should test this by closing VSCode, then opening the `jaffle_shop` repo

## Extensions

In this section, I'll go over some of the extensions that our team uses. Each extension requires that you install it from within VSCode, and most will require adding additional settings to your `.vscode/settings.json`

### vscode-dbt

the [vscode-dbt extension](https://marketplace.visualstudio.com/items?itemName=bastienboutonnet.vscode-dbt) is great because it provides a few things:
1. syntax highlighting for SQL with jinja in it (also for `.md`'s and `.yml`s), and
2. helpful jinja snippets will save you a lot of time


To get this working you should add the following to your `.vscode/settings.json`

There's an optional addition I strongly recommend `""**/target/**"": """",`, which will not do any syntax highlighting/colorization to any file in the `target/` folder. This prevents me from making the classic mistake where I start editing a compiled model file, instead of the original model file. Then when I call `dbt run` my changes aren't incorporated, but instead overwritten by the unchanged logic of the model file. with this setting you know something is wrong then the sql has no coloring.

```json
""files.associations"": {
    // the pattern on the left side can be whatever you want: e.g.
    ""**/jaffle_shop/**/*.sql"": ""jinja-sql"", // just the .sqlfiles inside of jaffle_shop, or
    ""*.sql"": ""jinja-sql"", // all .sql files

    // optional: don't format models in `target/` dir
    ""**/target/**"": """",
    // I don't personally use these, but you can also have jinja work for `yaml` and `md` files
    ""**/<dbt-project-dir>/**/*.yaml"": ""jinja-yaml"",
     ""**/<dbt-project-dir>/**/*.yml"": ""jinja-yaml"",
    ""**/<dbt-project-dir>/**/docs/**/*.md"": ""jinja-md""

    // the vscode-dbt docs say you may need this
    ""editor.quickSuggestions"": {
    ""strings"": true
    }
}
```

You'll know it is working when you open a `.sql` model and, in the bottom toolbar on the right it says now says ""Jinja SQL"" instead of ""SQL"".

### Find Related

the [find-related extension](https://marketplace.visualstudio.com/items?itemName=amodio.find-related) allows you to use regular expressions to correspond a `.sql` file in your `models/` directory to it's `compiled` and `run` counterparts in the `target/` folder. I find this a huge timesaver compared to manually naviagting the `target/` dir in the explorer sidebar.

After you install the `find-related` extension, you can enable it by adding the following to your `.vscode/settings.json`. There's no dbt or jinja magic going on here, just regex. So you may need to tweak these settings if they're not working for you.

Once it is set up, you can type `Option+R` on any model file to jump to it's compiled version. While on a compiled model file, `Option+R` will take you to it's `target/run` counterpart.


```json
{
    // this is so you can easily jump to your compiled SQL files
    ""findrelated.workspaceRulesets"": [
        {
            ""name"": ""sql"",
            ""rules"": [
                {
                    ""pattern"": ""^(.*/)?models/(.*/)?(.+\\.sql)$"",
                    ""locators"": [
                        ""**/compiled/**/$3""
                    ]
                },
                {
                    ""pattern"": ""^(.*/)?compiled/(.*/)?(.+\\.sql)$"",
                    ""locators"": [
                        ""**/run/**/$3""
                    ]
                },
                {
                    ""pattern"": ""^(.*/)?run/(.*/)?(.+\\.sql)$"",
                    ""locators"": [
                        ""**/models/**/$3""
                    ]
                }
            ]
        }
    ],
    ""findrelated.applyRulesets"": [
        ""sql""
    ]
    }
```

### Rainbow CSV

the [rainbow-csv extension](https://marketplace.visualstudio.com/items?itemName=mechatroner.rainbow-csv) just highlights csvs where each column is it's own color. It's great to use when you have a csv where character-width varies greatly within a column. You can also hover over a value to see what column it belongs to. Very helpful for seeds!

### SQL Fluff

Our team has recently implemented sqlfluff linting for our dbt projects, especially because versions `0.6.5` and greater now support TSQL. There's also a great VCcode extenstion.

If you already have a `.sqlfluff` and `.sqlfluffignore` configured and working, it is enough to install [vscode-sqlfluff](https://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff) and add the following to your `settings.json`

```json
    // you get this by calling `where sqlfluff` after calling `pip install sqlfluff`
    ""sql.linter.executablePath"": ""<PATH_TO_YOUR_SQLFLUFF_FROM_WHICH_SQLFLUFF_COMMAND"",
    ""sql.linter.run"": ""onType"" // alternatively ""onSave"" if you'd like it less frequent 
```

### dbt Power User

I personally haven't had time to use the [dbt Power User](https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user) extension, but folks have good things to say. I hope to try it out soon and folks are more than welcome make a Pull Request to this doc as they see fit.

## Settings

### Extra settings
Here's some other settings that I recommend:


```json
        // easier to see if there are unsaved changed
        ""workbench.editor.highlightModifiedTabs"": true,
        ""workbench.editor.labelFormat"": ""medium"",
        // make Command Prompt the default shell for Windows instead of Powershell
        ""terminal.integrated.shell.windows"": ""C:\\Windows\\System32\\cmd.exe"",
        
        // make a vertical line so I don't make lines too long
        ""editor.rulers"": [80],
        // show whitespace as dots
        // (easier to count out indentation and spot trailing whitesapce)
        ""editor.renderWhitespace"": ""all"",
```

### Workspace-level settings files

Sometimes it isn't convenient to have a `.vscode/settings.json`, such as when you:
1. have a subset of settings under source control that you'd like all users to be using (it doesn't make sense to source control your specific Python path)
2. you prefer [multi-root workspaces](https://code.visualstudio.com/docs/editor/multi-root-workspaces) a.k.a. more than one repo open at at time (great for when you also want your `.dbt/profiles.yml` close at hand)

A worksapce settings file has the extension `.code-workspace` and encapsulates all the configuration you might find in a `.vscode/` dir into a single file. This file also works as a shortcut that you can double click or navigate to to bring up all your settings.

If someone wants more info on this free free to open an issue. For now I'll leave this as as stub.",1
3291,"setting-up-vscode-to-use-with-the-dbt-cli","[quote=""data_ders, post:1, topic:3291""]
`""editor.quickSuggestions"":`
[/quote]

This seems to be on the wrong nesting level. It gave en error till I moved it outside of `""files.associations""` block",2
3291,"setting-up-vscode-to-use-with-the-dbt-cli","Search rules for related files seemed too limited for me:
* if I am in `models`, why can't I look for related files in `run` folder?
* more often I am looking for `.yml` counterparty of the model rather than its compiled version => new locators and changed precedence
* going back from `.yml` to the model also ultra-useful => new set of rules
* I rarely do an error of editing compiled files, but I like them to be formatted as what they are - SQL with dialect specification

+ There are some more settings of the plugin at the end

```
        ""findrelated.workspaceRulesets"": [
            {
                ""name"": ""sql"",
                ""rules"": [
                    {
                        ""pattern"": ""^((?!.*target).*/)?(analysis|data|macros|models|tests)/(.+/)?([^/]+)\\.sql$"",
                        ""locators"": [
                            ""**/$4.yml"",
                            ""**/compiled/**/$2/$3$4.sql"",
                            ""**/run/**/$2/$3$4.sql"",
                        ]
                    },

                    {
                        ""pattern"": ""^(.*/)?compiled/([^/]+)/(.+/)?([^/]+)\\.sql$"",
                        ""locators"": [
                            ""$3$4.sql"",
                            ""**/run/$2/$3$4.sql"",
                            ""**/$4.yml"",
                        ]
                    },
                                        {
                        ""pattern"": ""^(.*/)?run/([^/]+)/(.+/)?([^/]+)\\.sql$"",
                        ""locators"": [
                            ""$3$4.sql"",
                            ""**/compiled/$2/$3$4.sql"",
                            ""**/$4.yml"",
                        ]
                    },
                ]
            },

            {
                ""name"": ""yml"",
                ""rules"": [
                    {
                        ""pattern"": ""^(.*/)?(analysis|data|macros|models|tests)/(.+/)?([^/]+)?\\.yml$"",
                        ""locators"": [
                            ""$2/$4.sql"",
                            ""$2/**/$4.sql""
                        ]
                    }
                ]
            },
        ],
        ""findrelated.applyRulesets"": [
            ""sql"",
            ""yml"",
        ],
        ""findrelated.autoOpen"": false,
        ""findrelated.autoPreview"": false,
        ""findrelated.openPreview"": false,
```

NB: in `(analysis|data|etc)`list all subfolders of the project that can contain buildable SQL. E.g. ""data"" was common before dbt version 1.0.0 and would likely be substituted to ""seed"" in newer versions",3
3291,"setting-up-vscode-to-use-with-the-dbt-cli","Thank you for the great article.

## Making sqlfluff work
To make sqlfluff work I also had to do a few extra things in addition to what you described.

Sqlfluff has multiple ways deal with jinja-sql:

1. A dbt based templater that uses dbt to compile your jinja-sql. This generates more accurate sql and automatically uses all your macros and variables, but is fairly slow.
2. Their own jinja templater, which requires extra work to make dbt work, but is very fast

In addition to that, the [vscode sqlfuff extension doesn't work with the dbt templater](https://github.com/sqlfluff/vscode-sqlfluff/issues/41).

But since the dbt templater is quite slow, it's probably not the best choice for use inside of vscode anyways.

But it also means you have to use some workarounds to make your dbt variables, macros and packages work.

### Configure sqlfluff
To configure sqlfluff I had to create two files:

#### .sqlfluff:
```ini
[sqlfluff]
# Run `sqlfluff dialects` to get a list of all dialects
dialect = tsql

# Use the dbt templater
# templater = dbt

# Use jinja templater instead of dbt
templater = jinja

[sqlfluff:templater:jinja]
# Make dbt default macros like available like `ref`
apply_dbt_builtins = true

# Load custom macros from the macro directory
load_macros_from_path = ./macros/

# Allow mocking dbt_utils and other packages through
# python files placed in this dir
library_path = ./sqlfluff_libs/

# Mock variables that are used in dbt
[sqlfluff:templater:jinja:context]
target = prod
```

#### .sqlfluffignore
```
target/
dbt_modules/
dbt_packages/
macros/
```

### Make dbt_utils work
The above config worked, but when I ran sqlfluff against a model that used `dbt_utils.surrogate_key()`, I received this error:

```
L:   1 | P:   1 |  TMP | Unrecoverable failure in Jinja templating: 'dbt_utils'
                       | is undefined. Have you configured your variables?
                       | https://docs.sqlfluff.com/en/latest/configuration.html
L:  17 | P:  12 |  TMP | Undefined jinja template variable: 'dbt_utils'
```

I was able to fix it by creating a python library that mocks dbt_utils package macros:

#### sqlfluff_libs/dbt_utils.py:
```python
def surrogate_key(field_list):
    return ""'0xMockKey'""
```

To mock more complex and nested libraries, you can refer to:
https://docs.sqlfluff.com/en/stable/configuration.html#library-templating


### A broken link...
I also noticed the link `a gist for an example .vscode directory` doesn't seem to work, because you changed your github name. 

The working link is: https://gist.github.com/dataders/5cf876d88c7c8d268d8c1e1e5d05bffd",4
3291,"setting-up-vscode-to-use-with-the-dbt-cli","Hi all, 

Its been a while since this thread has been updated, and I just wanted to say that the best way (IMHO) to setup dbt to work with vscode has been to install the [dbt-power-user extension](https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user) along with some other extensions like sqlfluff, vscode-dbt, rainbow-csv etc. 

**[self promo warning]** For those using Big Query, however, I'd like to present my vscode extension, [vscode-dbt-bigquery-power-user](https://marketplace.visualstudio.com/items?itemName=butchland.vscode-dbt-bigquery-power-user) which integrates an sql results preview pane 
![query-runner-screenshot|664x500, 75%](upload://ht2VawvhgVbDXmilH88izKQy7d.png)


Its actually a fork of the dbt-power-user extension plus some other extensions ([vscode-bigquery](https://github.com/google/vscode-bigquery) and [vscode-query-runner](https://github.com/tadyjp/vscode-query-runner) which I integrated into this plus some of my own bag of tricks.

Some of the highlights of this extension are:
* A **Show Compiled SQL** menu icon that opens the compiled sql version of your models

* An **Open Query Runner** menu icon that runs the compiled sql and displays the results in another panel

* The **Query Runner** panel that displays the results of the query.
    - The **Rerun Query** button which allows you to update the model in your original panel and rerun the query (including the compilation) as well as the **Results navigation** buttons to see the next/prev page of results.
   - The **Download** buttons which allow to display the results  (with a choice of JSON, text, or csv formats)  as well as download the data into the `logs/results` folder of the dbt-project.

Its still in _alpha release_, meaning it hasn't been tested extensively but I've been dogfooding it in my own side projects, and I'm now looking for more feedback. 

For those vscode users using dbt with bigquery , I hope you find it useful and [give me some feedback](https://github.com/butchland/vscode-dbt-bigquery-power-user/issues).",5
3291,"setting-up-vscode-to-use-with-the-dbt-cli","I read the entire thing still clueless about how to set up VScode to use with dbt...",6
3291,"setting-up-vscode-to-use-with-the-dbt-cli","Can you provide a bit more context? Which parts specifically caused you difficulty?",7
3291,"setting-up-vscode-to-use-with-the-dbt-cli","A post was split to a new topic: [Debugging run-operation](/t/debugging-run-operation/5322)",8
3291,"setting-up-vscode-to-use-with-the-dbt-cli","`""python.pythonPath""` setting is not supported anymore. Use `""python.defaultInterpreterPath""` :+1:",9
3432,"configuration-change-for-incremental-model-on-dbt","On one our previously created incremental models, I added `partition_by` and `partition_expiration_days` parameters to the configuration to set the table partition and retention in place.

```
{{ config(
        materialized='incremental',
        unique_key='record_id',
        on_schema_change='append_new_columns',
        partition_by={
              ""field"": ""row_ts"",
              ""data_type"": ""timestamp"",
              ""granularity"": ""day""
        },
        partition_expiration_days = 365
        )
}}
```

I observed on the next run that the configuration didn't applied to the table.

It seems a `full-refresh` operation needed here. Yet we have strict retention on the data source for this table which some of the data would be lost with `full-refresh` operation.

Could anyone please let me know how this issue can be addressed with a solution?",1
3432,"configuration-change-for-incremental-model-on-dbt","Hi guz!
I would clone that table ->clone_table
Delete the original table
Insert the clone_table aterialized='table' in the ordinal_table
and the activate the incremental model again",2
3432,"configuration-change-for-incremental-model-on-dbt","Is it one time activity?",3
3432,"configuration-change-for-incremental-model-on-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
3451,"how-to-add-observability-to-your-dbt-deployment","This post describes the system we (@kevinc and @jt_st) built at Snapcommerce to get more observability out of our dbt deployment. It serves as a companion piece to the Coalesce 2021 talk ""[Observability Within dbt](https://coalesce-2021.heysummit.com/talks/observability-within-dbt/)"".

tl;dr: Use tools in your existing stack (MDS), dbt artifacts, and your query log to build reporting and alerts on dbt model/job runs and performance

## Introduction
We undertook this project to answer questions analytics engineers have about their dbt models, tests, and pipelines. The questions we often heard were: 
- Why isn't my model up to date?
- How long did my model take to run?
- When is the last time my model ran?
- Is my data accurate? Did my tests pass?
- Why are my models taking so long to build?

There wasn't an easy way to answer these questions without going into several services or SaaS tools, or painstakingly debugging logs. 

We decided to build a system that would help answer these questions in an automated and proactive manner. Applying the ""[jobs to be done](https://hbr.org/2016/09/know-your-customers-jobs-to-be-done)"" framework, with analytics engineers were our customers, we set out to create a system that would perform the following jobs:
- Send alerts to individual model owners based on custom criteria (e.g. run or test failures)
- Surface key information about model and job-level executions, like model runtimes and freshness
- Identify models in need of optimization and bottlenecks in dbt pipelines
- Reliably collect metadata for *all* production pipelines in quasi-realtime, including after failures

Our guiding principles for this project were making the system:
1. **Lightweight** â€” deploy the system easily using tools in the modern data stack
2. **Flexible** â€” explore, report, and alert on metadata using SQL
3. **Exhaustive** â€” support all dbt resources, artifacts, and command types (e.g. run, test, build)

## Architecture

Our system comprises four distinct parts:
1. **Orchestration**
    - dbt pipelines are orchestrated using Airflow and KubernetesPodOperator tasks
        1. Example: `dbt run -s tag:hourly`
2. **Metadata**
    - Artifacts are loaded into Snowflake at the end of every pipeline, even if the dbt pipeline resulted in a failure
        - Example: `dbt run -s tag:hourly; dbt run-operation upload_artifacts â€”args run_results`
3. **Modelling**
    - Artifacts are modelled and joined to the manifest using dbt
        - Example: `stg_dbt_run_results`
4. **Reporting and Alerting**
    - dbt artifacts are brought into Looker and displayed in dashboards. Alerts are set up to fire directly to specific people based on Slack user groups.
        - Example: Alert @finance-domain on all model run and test failures with `tag:finance` in the last 15 minutes

### Orchestration

Our dbt deployment consists of three types of models: Hourly, nightly, and external. Each model in our project can only have one of these **deployment tags**. Hourly and nightly models are managed in their respective pipelines, which look like this:

![image|690x62](upload://4FzGTd6yi0P2gAXUVdJeLW7SIzK.png)
![image|690x42](upload://r8tdHYsoIXUK07OpcNENhIIsRyC.png)

External dbt models refer to models managed outside of the hourly and nightly jobs. An external pipeline might be structured like this:
```copy_myreport_to_s3 >> load_myreport_to_snowflake >> run_myreport_dbt_models```

We ensure that there is no duplication across hourly/nightly and external pipelines by using intersection selectors. For example, an ""external"" dbt job runs models at the intersection between the source's downstream models and `tag:external`, e.g.:

 `dbt run -s source.myreport+, tag:external`

In Airflow, we use the KubernetesPodOperator to execute these dbt commands in a single task, rather than splitting up into separate tasks as [some teams do](https://www.astronomer.io/blog/airflow-dbt-1).

```bash
dbt_nightly_run = KubernetesPodOperator(
  **snaptravel_defaults,
  **dbt_defaults,
  task_id='dbt_nightly_run_task',
  name=""dbt-nightly-run-task"",
  arguments=dbt_run_commands,
  dag=dag
  )
```

### Metadata
We created a macro to upload dbt artifacts directly to Snowflake from the Airflow worker node's local disk. We first load data to a Snowflake stage using a `PUT` command:

```sql
{% set put_query %}
    PUT file://target/run_results.json @RAW.DBT.DBT_LOAD auto_compress=true;
{% endset %}
{% do run_query(put_query) %}
```

Then, we copy the file from the stage the respective dbt artifacts table using `COPY` :

```sql
{% set copy_query %}
    BEGIN;
    COPY INTO RAW.DBT.RUN_RESULTS FROM
        @RAW.DBT.DBT_LOAD
        file_format=(type='JSON')
        on_error='skip_file';
    COMMIT;
{% endset %}
{% do run_query(copy_query) %}
```
Then we remove data from the stage:

```bash
{% set remove_query %}
    REMOVE @{{ database }}.DBT.DBT_LOAD pattern='.*.json.gz';
{% endset %}

```
We run this macro after every dbt job in Airflow, using a command structure like the following:

```bash
dbt run -m tag:hourly; ret=$?;
dbt run-operation upload_dbt_artifacts --args 'run_results'; exit $ret
```
This ensures that artifacts are uploaded even if the first command returns a failure, as a result of a failing test, for example.

### Modelling

Once the artifacts land in Snowflake, they're modelled using dbt so they can be used for reporting, analysis, and alerting on our dbt pipelines and model performance. 

Much of this can be done automatically now using the [Tails.com](http://Tails.com) [dbt_artifacts](https://github.com/tailsdotcom/dbt_artifacts) package, which didn't exist when we began this project. We did use the Gitlab data team's [open source analytics repo](https://gitlab.com/gitlab-data/analytics/-/tree/master/transform/snowflake-dbt/models/workspaces/workspace_data/dbt) for a lot of our initial code, and we are very grateful to them!

The full dbt DAG for artifacts looks like this:
**![|938px;x269px;](upload://9jV1BLCHvR3xbcii8M71m1Flwtm.png)**

The most relevant model joins the  `run_results` artifact, `manifest` artifact, and Snowflake query history. The query history allows us to calculated the estimated cost of every dbt model and pipeline run. It also gives us key query performance metrics like **byte spillage** and **partitions scanned** at the model level, which helps us identify poorly performing models.

We join artifacts to the query log using a query comment:

```bash
query-comment:
  comment: ""{{ query_comment(node) }}""
  append: true
```

The query comment macro is taken directly from the [dbt documentation](https://docs.getdbt.com/reference/project-configs/query-comment#advanced-use-a-macro-to-generate-a-comment), with small modifications. The fields `node_id` and  `invocation_id `allow us to join artifact instances to the underlying queries.

```bash
{% macro query_comment(node) %}
    {%- set comment_dict = {} -%}
    {%- do comment_dict.update(
        app='dbt',
        dbt_version=dbt_version,
        profile_name=target.get('profile_name'),
        target_name=target.get('target_name'),
        invocation_id=invocation_id
    ) -%}
    {%- if node is not none -%}
      {%- do comment_dict.update(
        file=node.original_file_path,
        node_id=node.unique_id,
        node_name=node.name,
        resource_type=node.resource_type,
        package_name=node.package_name,
        relation={
            ""database"": node.database,
            ""schema"": node.schema,
            ""identifier"": node.identifier,
	        ""materialized"": node.config.get('materialized')
        }
      ) -%}
    {% else %}
      {%- do comment_dict.update(node_id='internal') -%}
    {%- endif -%}
    {% do return(tojson(comment_dict)) %}
{% endmacro %}
```

We use four types of models in our dbt project. Views and tables are single CREATE statements and can be mapped 1:1 with a query in the query history. Insert by period and incremental models have multiple queries associated with them, so for these we aggregate relevant metrics in the query history (e.g. `bytes_spilled` becomes `total_bytes_spilled` across all model queries). 

We use our snowflake contract rate and the estimated credits used per query to calculate our estimated cost per query:

```bash
CASE
  WHEN group_queries.warehouse_size = 'XSMALL'
    THEN 1
  WHEN group_queries.warehouse_size = 'SMALL'
    THEN 2
  WHEN group_queries.warehouse_size = 'MEDIUM'
    THEN 4
  WHEN group_queries.warehouse_size = 'LARGE'
    THEN 8
  WHEN group_queries.warehouse_size = 'XLARGE'
    THEN 16
  WHEN group_queries.warehouse_size = 'XXLARGE'
    THEN 32
  WHEN group_queries.warehouse_size = 'XXXLARGE'
    THEN 64
  WHEN group_queries.warehouse_size = 'XXXXLARGE'
    THEN 128
END                                                        AS warehouse_credits_hourly,
total_elapsed_time_mins * warehouse_credits_hourly / 60    AS est_credits_used,
{{ var('snowflake_contract_rate') }} * est_credits_used    AS est_cost
```
### Reporting and Alerting

dbt artifacts are brought into Looker and displayed in dashboards. The first use case is alerting on model run and test failures. 
#### Failures

One of our main objectives was sending alerts directly to individual model owners. To accomplish this, we require every model to be tagged with a single domain tag â€” finance, growth, product, etc. We construct charts for each domain's model runs like the one below, which shows the most recent runs of every model tagged `finance`
![Untitled|660x361](upload://n97g5uW7Tx2HArQgmV4voEY23FM.png)

A Looker alert on this chart runs every 15 minutes, and if there's a failed run, a notification is sent to our #data-alerts slack channel tagging the slack user group @finance-domain. Once an alert fires, a domain member goes to the dashboard, fetches the compiled SQL, and starts debugging. The same occurs for tests, snapshots, source freshness failures, etc.:
![Untitled|690x214](upload://zF8J5vQNnTXvvxnLeaZT2AcW2y1.png)

#### Model Performance
We also show data relevant to model performance. We can easily see the models which take the longest along with their key query performance metrics. These metrics help us determine when a model's materialization should be updated, or when the warehouse size should be increased. For example, let's look at a subset of our models:
![Untitled|690x186](upload://xwKSDjaY7jsPt5EcrvD4uzyqmKH.jpeg)

There are three main action items one can take from this view, beyond simple optimization and SQL refactoring:
1. **Materialization:** If the model is taking a long time to build, it may be worth exploring a new materialization strategy, like an incremental or insert_by_period
2. **Clustering**: If the model is taking a long time to build and a high percentage of total partitions are being scanned, it may be worthwhile to explore new cluster keys
3. **Warehouse**: If the model tends to have a lot of spillage and bytes sent over the network , it may be worthwhile to simply increase the size of the warehouse, once other opportunities for optimization have been exhausted.

Analytics engineers can also view models individually across time:
![dbt_supply_bucketed_lmp|690x187](upload://XQ58YuK24xWWMnDoqAg9UVQI42.gif)
![Untitled|690x321](upload://hPDh3tMfAzNcqHxcKThPOeYkcWn.png)
![Untitled|690x346](upload://lLDCJp7Ytl8XgvJDi7k5YvlPXcr.jpeg)
#### Job Performance
Finally, we can evaluate job-level performance and identify pipeline bottlenecks using Gantt charts, inspired by @claire's [post](https://discourse.getdbt.com/t/analyzing-fishtowns-dbt-project-performance-with-artifacts/2214) about the dbt project at dbt labs.

When the hourly or nightly jobs begin to take longer than expected, this view can help identify areas for optimization to keep them within SLA. Here are examples of our core hourly and nightly jobs:
![Untitled|690x242](upload://3pmv0tjSpwVsTeY1oE0rDILj6D2.jpeg)
![Untitled|690x205](upload://e6Kd0ZLfN2HEm32XyeCnLJ11BWj.jpeg)
![Untitled|690x447](upload://tJJRxQjSVPyZSm7gnb5PnoptZsV.jpeg)

### Conclusion
Setting this up on your stack should be doable even if you have to swap out pieces, like a different orchestration tool or BI platform. Community projects like the dbt_artifacts package and Gitlab's open source analytics project help a lot. We are happy to share our code too -- just reach out to either of us on the dbt slack.",1
3451,"how-to-add-observability-to-your-dbt-deployment","Very nice write up on how you've implemented this! We are looking to implement similar monitoring, particularly for fine-grained error notifications as well as performance and bottleneck analyses.",2
3558,"forcing-dependencies-depends-on-example-code","Hello,

Could you please help with an example for Forcing Dependencies  -- depends_on
I checked ""Forcing Dependencies"" section in the below DBT documentation link, but it doesn't have a clear example.

https://docs.getdbt.com/reference/dbt-jinja-functions/ref

Please help.

Thanks,
Gauresh.",1
3558,"forcing-dependencies-depends-on-example-code","Hi Gauresh,

The example shown in [dbt Docs](https://docs.getdbt.com/reference/dbt-jinja-functions/ref) as a comment is actually the way the dependency is hinted to the compiler. That is, in order to let dbt know an implicit dependency in the model you're creating, you simply have to add the following line somewhere at the top of your model sql file.

` -- depends_on: {{ ref('upstream_parent_model') }}`

I hope that helps!",2
3558,"forcing-dependencies-depends-on-example-code","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Since becoming an analyst and learning SQL, I have formatted my SQL queries in every possible way. After meeting the folks at dbt Labs (nee Fishtown) four years ago, I was inspired by their SQL style guide; after spending more time writing python and using black, I desperately wanted an auto formatter like black for dbt SQL.

Over the past few months, I have been hacking away at that tool, and today it works with dbt jinja and creates well-formatted, modern-looking SQL. It's not a linter (use SQLFluff for that) -- it just does one thing and does it well.

You can try it at [sqlfmt.com](http://sqlfmt.com/), or check out the [repo on Github](https://github.com/tconbeer/sqlfmt). Or if you like to live dangerously, `pipx install shandy-sqlfmt` 

It's still really early innings. There are more features and integrations I'd like to build (and I'm sure plenty of bugs yet to be found). I would really, really value any contribution from this community, whether that's feedback, an issue, or a PR. Thanks for checking it out; please let me know what you think!",1
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Installed and started playing around with it over the weekend. Great work.

I love an opinionated formatter: we've been using `black` for a while in our Python libraries, and it really reduces the cognitive (and conversational) load. So I'm excited for an equivalent for our SQL files. (And I like that it starts from the dbt Labs style guide.)",2
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Thanks for contributing!",3
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Hi,

Is it possible to force the `and`s to align here? 

Autoformatted:

```sql
select *
from historic_answers ha
where
    subject = 'Math' and learningobjective = ""Addition"" and dt_utc > date(
        '2020-10-01'
    ) and testschool = false
```

Desired:

```sql
select *
from historic_answers ha
where 1=1
    and subject = 'Math' 
    and learningobjective = ""Addition"" 
    and dt_utc > date('2020-10-01') 
    and testschool = false
```

Or does this go against some of the fundamental rules of your formatter?",4
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","I agree your solution is better. It's been on my mind to improve the formatting of chained boolean operators with parentheses for some time. Can you [open an issue](https://github.com/tconbeer/sqlfmt/issues/new/choose) for this?",5
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Good that you mention it, should I open two issues:

1. Force separate `where` clauses on single lines (even if they don't exceed max width)
2. Improve formatting of chained boolean operators with parentheses

Or do you see these two issues as one?

P.S. it would be awesome if there was a way to change the line length in the interactive web version :) I use it a lot when showcasing it to people but the default line length is a bit short for most people",6
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Sorry, my notifications for discourse started going to Spam! #2 is what I had in mind, it basically implies #1... the real issue today is the interaction between the parentheses-driven line splitting and merging and the boolean operator-driven line splitting and merging.

Re: line-length online, sure thing! I'll create an issue for that one on the sqlfmt.com repo",7
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","Just wanted to say thanks for making this! I'm using it on on a project at work and so far it's been great!",8
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","@janmeppe Line length config is now available on sqlfmt.com: click ""Configure Formatting"" next to the blue button for the available options.",9
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","No way you can hack this or sqlfluff into dbt cloud... any option to have decent formattation on dbt cloud as we speak or coming soon?",10
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","[quote=""obar1, post:10, topic:3687""]
hack this or sqlfluff into dbt cloud
[/quote]

Not at the moment, but hopefully one day :crossed_fingers:",11
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","The new dbt Cloud IDE has a format button that is powered by sqlfmt! Itâ€™s in Beta right now, GA early next year",12
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","great feature - I hope it can be customised to match your/my formatting preference",13
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","hey on the http://sqlfmt.com/ website you are able to configure the max line length.  Is this config (and other settings) possible in dbt?",14
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","No, formatting in dbt Cloud is currently not configurable. You can configure line length if you download the sqlfmt CLI and use it with dbt Core. 

See: https://docs.sqlfmt.com/getting-started/installation
And: https://docs.sqlfmt.com/getting-started/configuring-sqlfmt",15
3687,"introducing-sqlfmt-an-auto-formatter-for-dbt-sql","shame... I hope this comes in future releases of dbt Cloud",16
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","Our group has more than 400 source schemas that we use for various data downstream into snowflake. We're in the process of migrating to dbt and are trying to think through how to structure the project at this scale. One question came up with how to handle maintaining schema-specific definitions in the project config file.

Do I have to make an entry for every schema that we have? Is there a dynamic way of doing this or a way to not bloat the config file?

Curious to see how others have solved this problem.",1
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","At our company, we have defined what we call a `client_schema` table. We also deal with 1000s of schemas(the number is growing exponentially). Every time a new client comes in, we store their data in a specific schema and create a record in that `client_schema` table containing the client name, a client unique identifier(client_id) and the relevant schema for the client,  through the backend. If this is a solution you are interested in, we can discuss how we do it using Snowflake external functions, AWS Lambda and AWS API.",2
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","Yes! I'd love to hear how you've set this up. I haven't used snowflake external functions yet so this could be a great use case.",3
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","How do you consolidate data all the clients data with dbt?",4
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","We also deal with hundreds of schemas but we use BigQuery â€“ so we don't need to define the schemas in `dbt_project.yml`. :thinking: 

All schemas contain the same set of tables and columns and they all follow the pattern `schema_ORG`.

We combine ""raw"" tables from all ""schemas"" (aka ""organizations"") into one table under the `raw` dataset. We created a dbt macro to combine them all into one view. Ex:

```sql
# models/raw/raw_users.sql
{{
combine_organization_tables(
  'users',
  ['id', 'team_id']
)
}}
```

The macro:
1. pulls the list of ORG from `public_organizations`
2. pulls the list of columns from one of the `ORG`. Ex: `acme.users`
3. builds a view that UNION ALL the tables from all ORGs:
3.1 we add one extra column `org` to be able to scope queries down to a specific organization
3.2 we prefix all `id` columns (here: `id` and `team_id`) with the `org` to prevent conflicts. Ex: `acme/123`

So for example, `raw_users` is:

```
org    | id        | name        | team_id        |
acme   | acme/123  | angela      | acme/300       |
acme   | acme/124  | chris       | acme/400       |
foo    | foo/123   | carl        | foo/350        |
```

I hope that helps!",5
3821,"managing-hundreds-of-schemas-in-the-dbt-project-yml","Ha ha! I just came across this post: https://discourse.getdbt.com/t/unioning-identically-structured-data-sources/921

It's a much better write up! :slight_smile:",6
3868,"compare-two-models-schemas","Hi All,
working on dbt test cases, is there any way we can do schema check? like compare schema between existing model and new source model schema? 

Thanks 

alex",1
3868,"compare-two-models-schemas","@xzheng 
Unfortunately I have not seen this before in the ecosystem but it might be possible to create by doing something with the  [dbt-expectations](https://github.com/calogica/dbt-expectations) packages' helper functions. 
Something in the structure of [expect_table_row_count_to_equal_other_table](https://github.com/calogica/dbt-expectations/blob/main/macros/schema_tests/table_shape/expect_table_row_count_to_equal_other_table.sql) where you compare the [_get_column_list.sql](https://github.com/calogica/dbt-expectations/blob/main/macros/schema_tests/table_shape/_get_column_list.sql) from each side? 

Just an idea. Good luck.",2
3868,"compare-two-models-schemas","It sounds like you might want something like our audit helper package: https://hub.getdbt.com/dbt-labs/audit_helper/latest/

It lets you compare two Relations (e.g. tables and views) to check that a refactored model still works the same way. Check out [How to not lose your mind when auditing data](https://discourse.getdbt.com/t/how-to-not-lose-your-mind-when-auditing-data/445) for a deeper dive on how it works!",3
4055,"what-is-upstream-and-downstream","why do we use upstream and downstream ??",1
4055,"what-is-upstream-and-downstream","Great question! When you look at the dbt DAG, you can consider a stream of data which flows from source tables (upstream) through staging and intermediate layers until you end with marts at the end of the flow (downstream). 

This is well-illustrated when you look at the DAG that dbt docs creates: 
![image|690x166](upload://5uJLrlclKXUbmN7YYLEfK8YBLop.png)

In this example, `jafflegaggle_contacts` is highlighted in purple. Relative to it, the 5 event and user models are upstream, and `jafflegaggle_facts` and `jafflegaggle_corporate_accounts` are downstream. 

If we instead use `stg_users` as our reference point, then only `raw_user` is upstream. Now `jafflegaggle_contacts` is considered downstream (along with its descendants).

You might also consider BI tools that query your final marts to be ""downstream tools"" or ""downstream consumers"", and data sources such as your CRM to be an ""upstream source"".",2
4174,"running-sqlfluff-fix-on-prs-as-a-github-action-with-inline-suggestions","Here's a secret: SQLFluff doesn't need to be specifically configured to use the dbt templater. Using the default Jinja templater, SQLFluff can fix inconsistent capitalisation or indentation without any fancy database connections.

With that in mind, I created this basic GitHub Action file in `my-repo/.github/workflows/sqlfulff.yml`. This action uses the new (as of this post) [**action-sqlfluff**](https://github.com/yu-iskw/action-sqlfluff) project to make inline suggestions on the pull request.

```
name: sqlfluff with reviewdog
on:
  pull_request:
jobs:
  test-check:
    name: runner / sqlfluff (github-check)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      # only run when a comment requests linting
      - uses: khan/pull-request-comment-trigger@master
        id: check
        with:
          trigger: '/lint'
          reaction: rocket
        env:
          GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}'
      - uses: yu-iskw/action-sqlfluff@v3
        if: steps.check.outputs.triggered == 'true'
        id: lint-sql
        with:
          github_token: ${{ secrets.github_token }}
          reporter: github-pr-review
          sqlfluff_version: ""0.11.1""
          sqlfluff_command: ""fix"" # Or ""lint""
          config: ""${{ github.workspace }}/.sqlfluff""
          paths: '${{ github.workspace }}/models'
      - name: 'Show outputs (Optional)'
        shell: bash
        run: |
          echo '${{ steps.lint-sql.outputs.sqlfluff-results }}' | jq -r '.'
          echo '${{ steps.lint-sql.outputs.sqlfluff-results-rdjson }}' | jq -r '.'
```

Here are the results, when creating a PR with the comment `/lint` and a missing explicit alias keyword.
![blur-demo|576x500](upload://cbaiJ2Tt8maXtfS29BoedfzKmaG.jpeg)

P.S. Here's my SQLFluff configuration file, `my-repo/.sqlfluff`...

```
[sqlfluff]
dialect = bigquery

# L031 - Exclude rule requiring fully-qualified table names in joins
# should have been disabled for BigQuery already
# L036 - multiple SELECT columns can appear on the SELECT line
# L051 - Exclude rule requiring INNER JOIN rather than just JOIN
# L003 - Ignore whitespace problems for now
# L034 - Don't move around columns
# L016 - Long lines are fine
# L009 - No need for a single newline at end of file
# L008 - No need for spaces around commas
exclude_rules = L051,L031,L036,L003,L034,L016,L009,L008

[sqlfluff:rules:L010]
# Keywords
capitalisation_policy = upper
```",1
4174,"running-sqlfluff-fix-on-prs-as-a-github-action-with-inline-suggestions","Nice! Are your rule exclusions for your preference, or must those rules be excluded for it to work without dbt templater?",2
4174,"running-sqlfluff-fix-on-prs-as-a-github-action-with-inline-suggestions","SQLFluff can provide basic linting of dbt models using the jinja templater without any rule exclusions. These exclusions have more to do with BigQuery (L031), dbt Cloud (L009), and our company's code preferences (L010).",3
4174,"running-sqlfluff-fix-on-prs-as-a-github-action-with-inline-suggestions","@martinburch Thanks a lot for using my action.",4
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","We have a long-running model that performs a complex set of geospatial comparisons to get point-to-point distances between every person in the US and a list of around 300K points of interest. We are using snowflake, and using snowflake's geospatial data types and functions. We also do restrict the comparisons, so we are not (for instance) calculating distances between people in New York with POI in Los Angeles. Regardless, the first pass at the model took over 24 hours to execute on a large snowflake warehouse. This first pass was not implemented in dbt. We then tried essentially partitioning the query into multiple queries, each running a separate region of the country, executing in parallel. This reduced the overall execution time by around 50%, with approximately the same credit burn on snowflake. This also was not in dbt. The orchestration was simply a python script that fired off the regional queries in parallel.

We want to move this model into dbt, but are a little stuck on how to accomplish the parallelism bit. Based on my understanding of dbt, since this is technically a single model, resulting in a single table, it would not ""natively"" be able to be parallelized the way described. One thought we have is to create separate models for different geographic regions, which can execute in parallel (via dbt threads), with downstream union to get at the final data set. But, if we were to decide that we wanted change our geographic ""partitioning"", that then would mean creating or removing intermediate models. Depending on how small we determined our geographic regions needed to be, it also could require quite a lot of intermediate models. Are there other options we are not thinking of?",1
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","Not simple, but you could also implement the partitioning logic as a custom materialization. See https://docs.getdbt.com/guides/legacy/creating-new-materializations

The custom materialization would encapsulate the logic for performing the partitioning,  and then unioning, ultimately outputing just 1 table, with downstream dependencies being none-the-wiser about how this came to be.",2
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","Are you re-calculating the entire dataset on every invocation? How much of it changes between two runs and how often do you run it? 

Where I'm going with this is that you might consider an [incremental model](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models). By doing this, you would pay the large calculation cost once, but would then only have to calculate the changes from there. 

If you have a table showing users' locations and when they last changed, it might look something like 

```sql
{% config(materialized='incremental', unique_key='user_location_key') %}

with user_locations as (
    select * from {{ ref('user_locations') }}
),
points_of_interest as (
    select * from {{ ref('points_of_interest') }}
),

final as (
    select 
        {{ dbt_utils.surrogate_key(['user_locations.user_id', 'points_of_interest.poi_id']) }} as user_location_key,
        user_locations.user_id,
        points_of_interest.poi_id,
        calculate_distance(user_locations.location, points_of_interest.location) as distance,
        user_locations.modified_date as user_location_changed_at
    from user_locations
    cross join points_of_interest using (user_id, state_id)
    {% if is_incremental() %}
        where user_locations.user_location_changed_at >= select(max(user_location_changed_at) from {{ this }})
    {% endif %}
)

select * 
from final
```",3
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","I should check back for replies to my questions more often :slight_smile: . In our current use case, we do have to fully rebuild the data set on each execution, as this model gets re-run when we get fully new underlying data sets. We might be able to implement an incremental approach, but I think the returns on investment there would be limited because the two input base data sets can change drastically between runs. 

As for the idea of a custom materialization, I am very interested in looking in to that. Actually started looking in to it today, taking the existing implementation for the table materialization and starting to fiddle with it. So far, approach is to derive the partition values from the source (or hard code them in, for dev purposes), use the model sql as a basis to define a temporary relation for each partition (suffixed with the partition value), populate it (applying the partitioning on the data while populating its specific temp relation), and then union the list of temp relations returning the target final relation. However, what I cannot figure out is how to get the temp partitions to execute in parallel to get the scalability I'm really looking for. Anyway, only a couple of hours put in to it, will post back if I end up making it work.

Incidentally, I'm very excited to see if the python models in beta right now will enable this type of behavior more naturally.",4
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","That approach sounds reasonable! I'm _pretty sure_ that you'll find that although dbt is multithreaded and can do parallel processing, its granularity is at the model level. So the temp partitions likely won't execute in parallel unless you can find a way to fire off multiple builds in some sort of other task in Snowflake (and then you'd need some way to poll to know whether they're all finished or not). 

I don't know enough about Snowflake to know if that sort of thing is possible; interested to hear how you get on!

[quote=""nadelman, post:4, topic:4481""]
Incidentally, Iâ€™m very excited to see if the python models in beta right now will enable this type of behavior more naturally.
[/quote]
Also an interesting idea! Going to send this to our Python PM as a fun nerd snipe if nothing else",5
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","LOVE this thinking on Python models. If you experiment more on this will you let us know how it goes? Very interestedâ€¦",6
4481,"breaking-a-large-query-into-parallelized-partitioned-queries","Really interesting thought about whether a Python model could help here...

As I understand it, parallel processing is *the* reason folks go through the trouble of writing PySpark code, rather than just single-node Pandas.

Snowpark also supports this explicitly via its ""batch API,"" by adding the `@vectorized` decorator to a UDF. It will apply the function over batches of rows/series (in parallel): https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-batch.html. Though I'm not sure if you can tell it *how* to batch the data (by region), to restrict the comparisons â€” beyond just specifying the batch size. If the source table is clustered on country, though...?

I'd *hope* that the execution engine is smart enough to handle that parallelism itself. I would feel less good about users needing to muck with, like, asyncio within their Python model definition (if such a thing were even possible). I shouldn't even say such words for fear of wishing them into existence",7
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","Hi, 

I would like to run dbt through Cloud Composer (which is the GCP's managed service for Apache Airflow) and I am struggling to find a proper and secure way in order to authenticate dbt so that it can perform operations on Google Cloud BigQuery. 

So here's my profile in `profiles.yaml` file:

```
my-profile:
  outputs:
    dev:
      dataset: my-bigquery-dataset
      job_execution_timeout_seconds: 300
      job_retries: 1
      keyfile: /path/to/keyfile.json
      location: EU
      method: service-account
      priority: interactive
      project: my-gcp-project
      threads: 4
      type: bigquery
  target: dev
```

Where am I supposed to store the service account JSON key in order to be visible to `dbt` when running it through Cloud Composer?
 
One option is to place the JSON key file on Cloud Storage, under `gs://<cloud-composer-bucket>/data` which is mounted into every Airflow worker and simply use the `/home/airflow/gcs/service-account-key.json` for `keyfile` on `dbt`'s profile but I don't think this is a secure enough approach (i.e. storing the service account key on an object storage such as GCS). 

PS: Note that I am not using Secret Manager on GCP, but self-hosted HashiCorp Vault instead. 

Thanks in advance.
Giorgos

<!-----
â„¹ï¸ Are you looking for troubleshooting help?

If so, please post your question to Stack Overflow:
https://stackoverflow.com/questions/ask?tags=dbt

We are reserving Discourse for more long-form discussions, and ""How to"" posts. Check out more details here:
https://discourse.getdbt.com/t/welcome-to-dbt-discourse/8 
---->",1
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","Hi Giorgos,

We run dbt via ""GKEStartPodOperator"" on Cloud Composer. We can then leverage workload identity of the service account running the node pool. This has the benefit of separating the scheduling of Airlfow from the execution of dbt that will be on a separate node pool. And also no need anymore to create a json key for the service account.

https://cloud.google.com/composer/docs/composer-2/use-gke-operator

Best regards,

Charles",2
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","Thanks a lot for your reply Charles! 
May I also ask what method/keyfile do you specify in your `profiles.yml` file when using workload identity?",3
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","  type: bigquery
  method: oauth

dbt will then automatically use the credentials of the service account of the node pool.

I have written a series of articles on how to run dbt at best on Google Cloud. Here is the link of part 1 if you are interested:

https://medium.com/astrafy/dbt-at-scale-on-google-cloud-part-1-54f8655443a7",4
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","A post was split to a new topic: [Equivalent of GKEStartPodOperator in Anthos Cloud Run with Workload Identity Federation](/t/equivalent-of-gkestartpodoperator-in-anthos-cloud-run-with-workload-identity-federation/4958)",5
4646,"how-to-run-dbt-on-cloud-composer-and-authenticate-the-service-account","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
4708,"why-does-jinja-parse-the-commented-out-macros-in-sql-queries","I tried to comment and  keep my old macro code, but dbt failed to compile the macro. 
The problem is that  **Jinja** parses the commented-out lines and finds `{{}}` in the query so shows an error message.

```
{% macro markup(column1, column2) %}
({{column1}} - {{column2}})/{{column2}}
{% endmacro %}


/*
{% macro markup(x, y) %}
({{x}} - {{y}})/{{y}}
{% endmacro %}
*/
```

The same problem happens if I comment out the macro in the main SQL query and don't keep any commented code in the macro.sql file.
```
select *
--, {{ markup('ordersellingprice', 'ordercostprice') }} as markup
from {{ ref('raw_orders') }} as o
```

**Jinja must not parse the commented lines**.",1
4708,"why-does-jinja-parse-the-commented-out-macros-in-sql-queries","Jinja context should be commented out with {# ... #}",2
4727,"avoiding-collisions-during-md5-surrogate-key-creation","Hi all,

I searched the forum but did not find a thread specific to my topic, but only closely related ones as per the following:
For surrogate key creation, dbt utils implements md5 hashing [[1]](https://www.getdbt.com/blog/the-most-underutilized-function-in-sql) 
Auto incrementing has other downsides and is generally not supported [[2]](https://discourse.getdbt.com/t/generating-an-auto-incrementing-id-in-dbt/579/2). 

My question now is: how do you handle collisions? 
While you can and should definitely test for uniqueness in your keys - as you cannot enforce it - what do you do if your test fails and you face a collision? Do you add some salt or something?

Appreciate your answers!",1
4727,"avoiding-collisions-during-md5-surrogate-key-creation","How much data do you have?

MD5 has its flaws, but random collisions are [extremely rare](https://stackoverflow.com/questions/201705/how-many-random-elements-before-md5-produces-collisions). On average, you would need 2^64 records before you get a single collision -- that's 18 billion billion records (1.8 * 10^19).

You could use a hash with even more entropy, like SHA2, but if you have enough data to be concerned about MD5 collisions, then the performance penalty of a different hash is probably also going to be a concern. But I suppose if you have a quintillion records in your table, you could build up an incremental model that uses SHA512, which would scale up without collisions until you have more records than there are atoms in the universe (~10^80)",2
4727,"avoiding-collisions-during-md5-surrogate-key-creation","we use UUIDs, and Snowflake's UUID_STRING() specifically to generate most of our unique ids.",3
4767,"data-compare-in-if-condition-in-dbt-model","I have to compare the result of query with hardcoded integer value in if condition

{%- set query %}
        select cast(1 as integer) 
    {% endset -%}
    {%- set main = run_query(query) -%}


{% if execute   %}

{% if main > 2 %}

Getting below error...
```
'>' not supported between instances of 'Table' and 'int'
```",1
4767,"data-compare-in-if-condition-in-dbt-model","Hi @Bhrat123, the result of `run_query` is an Agate table, not an integer. 

https://docs.getdbt.com/reference/dbt-jinja-functions/run_query

The [Getting Started Guide](https://docs.getdbt.com/guides/getting-started/learning-more/using-jinja#dynamically-retrieve-the-list-of-payment-methods) has a section on using `run_query` effectively - I would recommend reviewing that as well!",3
4767,"data-compare-in-if-condition-in-dbt-model","Hi Bhrat123,

'main' variable in your code is of 'table' type, it doesn't hold the value of your query result. Check https://docs.getdbt.com/reference/dbt-jinja-functions/run_query for more details, especially 'columns' property. 

Cheers, 
Tomaz",5
4767,"data-compare-in-if-condition-in-dbt-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
4780,"repeated-ctes-dry-and-intermediate-models-good-practice","Hi everyone,

I am refactoring some legacy SQL codebase at my company and after reading ""[How we structure our dbt projects](https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview)"", something does not click for me regarding intermediate models.

I have a case where 2 marts models have a big portion of repeated code in a CTE. Actually, the person who wrote the code even created a temporary table (we're using BQ) for this CTE. So this same TEMP table is created 2 times in 2 different models.

To me, the logical thing to do was to move this repeated piece of code in an intermediate model that would be ref'ed by the 2 marts models. However, the [chapter of the guide regarding Intermediate Models](https://docs.getdbt.com/guides/best-practices/how-we-structure/3-intermediate) advocates for a 1 to 1 relationship between intermediate and mart models.

I understand that if I would move my CTE into an ""ephemeral"" intermediate model (as it is recommended), we would compute the same thing twice. However, according to the guidelines, I don't see where else this code should live.

Of course guidelines are just guidelines but I'm curious as to  what is the dbt way to handle this. Moving it to a mart model even if it's not really aimed to end users? And what about CTEs that are reused in many different models?

Thanks!",1
4780,"repeated-ctes-dry-and-intermediate-models-good-practice","Create a new folder and call it 
Intermediate
Put there what is shared on the downstream and use table as materialization 
Just avoid cycle between intermediate and final models",2
4780,"repeated-ctes-dry-and-intermediate-models-good-practice","I'm in a similar situation as the OP and this does not seem to solve the problem as I understand it: two pipelines share code (e.g. a big CTE), how do you adhere to the DRY principle in this case?  Do you have to resort to a macro? That seems a bit unwieldy.",3
4780,"repeated-ctes-dry-and-intermediate-models-good-practice","Hi @Thmsrey! Winnie here, writer of the ""How we structure"" Guide. First off, thank you for reading it thoughtfully! Secondly, this is a *really* good question.

This is the squishiest part of the guide, and the set of recommendations I'm the least satisfied with how I communicated. It needs more examples.

My biggest overarching response is: as you said, guidelines are just guidelines, and everybody breaks this one when needed. Almost every dbt installation will have some instance where they need to reuse intermediate models in multiple marts, so don't feel like you're necessarily breaking the rules if this particular group of models is that area for y'all.

All that said, let me elaborate a bit on the goal of this recommendation and how I think about it:
- I've seen a lot of people re-use all of their intermediate models in like 10 models each, this creates this really gnarly set of dependencies that makes your DAG difficult to reason about **and** duplicates tons of data in your warehouse. Storage is cheap so that's not the end of the world, but if it's expensive compute-wise you could be needlessly computing transformations over and over again.
- What I prefer to do is *layer my marts*. So you mention these tables reuse this big complex CTE for example. I would consider importing it into one of the marts. Then importing that mart into the other mart that needs those transformations, and just select the columns that overlap. This keeps the dependency chain in the DAG really clear, and means I'm only processing that set of transformations one time.

Let me know if that makes sense! Again, there are always scenarios where this isn't ideal, but I push people to think through this option first before they start plugging intermediate models all over the place and tangling up their DAG.

Happy modeling!",4
4795,"accessing-external-schemas-in-redshift","All of my raw sources are located in external schemas that I fetch into Redshift. Redshift schema called information_schema does not store metadata on those sources. My dbt macros that get use of table metadata (like get_relations_by_pattern) trying to read from information_schema but finding nothing in there so my models are not being built properly. How do you handle this? I will be grateful for any advice.",1
4795,"accessing-external-schemas-in-redshift","@plotnikov.sseu I'm not 100% sure if this will solve your problem, but have you looked at the [external tables package](https://hub.getdbt.com/dbt-labs/dbt_external_tables/latest/)?",2
4801,"pass-a-parameter-to-dbt-from-the-command-line","hi there,

I'm trying to run a ""dbt run"" command while passing a parameter (number of rows to return from a select query) to my .SQL file in model.
The command looks like ""dbt run -mypara 100
Then in the SQL:
select * from table {if mypara !=''} limit mypara {endif}

Is it possible do that using default dbt? Thanks.",1
4801,"pass-a-parameter-to-dbt-from-the-command-line","Yes, you can pass parameters to SQL models in DBT

dbt run --vars{""var"":""value""}

and use 'var' in SQL model",2
4801,"pass-a-parameter-to-dbt-from-the-command-line","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
4812,"dbt-dev-prod-data-pipelines","Hello,

First time adopter of dbt in the last couple months. I had a question around best practices in handling dbt runs for dev and prod. I'd like to:

1. Have a daily job that runs `dbt run` to target a dev schema
2. Run `dbt test` to make sure all the unit tests pass
3. Run `dbt run --target prod` 

A couple of questions:

* Is this order of operations typical?
* We're using big query. Is it possible to have the the dev target in one GCP ""dev"" project and the prod target in a GCP ""prod"" project?",1
4812,"dbt-dev-prod-data-pipelines","Hello there,

You can definitely target two different GCP projects.

Ex:

```
# profiles.yml
templates:
  bigquery: &bigquery
    # run the following command to authenticate:
    # $> gcloud auth application-default login --scopes=https://www.googleapis.com/auth/bigquery
    dbname: analytics
    type: bigquery
    method: oauth
    threads: 10
    timeout_seconds: 300
    priority: interactive
    retries: 1

default:
  outputs:
    dev:
      <<: *bigquery
      project: acme-dev
    prod:
      <<: *bigquery
      project: acme-prod
  target: dev
```",2
4812,"dbt-dev-prod-data-pipelines","[quote=""pcreux, post:2, topic:4812""]
```
      <<: *bigquery
```
[/quote]

@pcreux I haven't seen this `<<:` syntax before - does this let you take all of the contents of an anchor and then append extra keys? Thatâ€™s very exciting because it has always annoyed me that I couldn't make that work!",3
4812,"dbt-dev-prod-data-pipelines","@joellabes Yes! You're totally right!",4
4812,"dbt-dev-prod-data-pipelines","Watch this then ðŸ˜€ https://www.youtube.com/watch?v=2cxBftrVc2Y&t=247",5
4827,"unusual-undefinedmacroexception-error","I am trying to create histogram bins in dbt using jinja. This is the code I am using.
```
{% set sql_statement %}
    select  min(eir) as min_eir, floor((max(eir) - min(eir))/10) + 1 as bin_size from {{ ref('interest_rate_table') }}
{% endset %}

{% set query_result = dbt_utils.get_query_results_as_dict(sql_statement) %}

{% set min_eir = query_result['min_eir'][0] %}
{% set bin_size = query_result['bin_size'][0] %}

{% set eir_bucket = [] %}

{% for i in range(10) %}
    {% set eir_bucket = eir_bucket.append(min_eir + i*bin_size) %}
{% endfor %}

{{ log(eir_bucket, info=True) }}

select 1 as num
```

The above code returns dbt.exceptions.UndefinedMacroException.
Below is the error log.

```
dbt.exceptions.UndefinedMacroException: Compilation Error in model terms_dist (/my/file/dir)
  'bin_size' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with ""dbt deps"".
```

Now, I haven't written the SQL yet. I want to build an array containing the historical bins, that I can use in my code. (edited)",1
4827,"unusual-undefinedmacroexception-error","Hi datachef,

Have you tried to output or log `bin_size` immediately after you set it (and temporarily remove subsequent code just to see the outcome)? 

Cheers,
Tomaz",2
4858,"how-to-specify-distribution-sort-style-and-column-encoding-for-dbt-seeds-in-redshift","How to specify distribution and sort style for dbt seeds?

There are no .sql files associated with seeds. How do we define the config? And how do we define column encoding algorithms?",1
4858,"how-to-specify-distribution-sort-style-and-column-encoding-for-dbt-seeds-in-redshift","Hey @evgeniy, as far as I know you can't define dist/sort style for seeds. In general, you probably shouldn't need to though as they're small files, so even if the seed isn't on the same node it would be trivial for Redshift to copy it over. 

With that said, if you do need to configure this then there's a workaround: you could make a model that just passes the seed through:
```sql
-- models/my_seed_with_dist_key_set.sql
{{ config(materialized='table',  sort='id',  dist='received_at') }}

select * from {{ ref('my_seed') }}
``` 

and when that gets materialized it will behave as you'd expect. It's a bit suboptimal because you'd have two copies of the table in your warehouse, but you can [materialize your seeds into a different schema](https://docs.getdbt.com/docs/faqs/seed-custom-schemas) that your BI tools etc can't access if you want to.

This would also be a good thing to add as an issue on the [dbt-redshift repo](https://github.com/dbt-labs/dbt-redshift/issues).",2
4858,"how-to-specify-distribution-sort-style-and-column-encoding-for-dbt-seeds-in-redshift","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4864,"about-the-help-category","Get help using dbt from the community of practitioners.",1
4874,"about-the-discussion-category","For questions that might not have a single correct answer, and long-lived discussions about dbt or analytics engineering at large. Share best practices, something cool you learned or a new way you're thinking about something.",1
4876,"about-the-show-and-tell-category","This is the category for sharing quick snippets, patterns, tools, and techniques to solve specific analytics engineering problems.

Do you have a dbt macro you use to set up Snowflake permissions on a new instance? A quick window function technique for getting a customer's first order? A general pattern you follow for crafting revenue metrics? A code snippet for making sure the last comma in a Jinja `for` loop gets dropped? These are all fantastic things to share in this category!

Feel free to expand on them over time, and we can always develop them into Guides or Articles on the Developer Hub!",1
4877,"casting-column-datatypes-based-on-a-seed","Old thread, I know, but I ran into this recently. We have a simple data dictionary that lists source column name, target column name and data type. At the same time, we are pivoting key/value pairs to a wide format. 

To do that, I created a macro that queries the column metadata then uses cases statements to pivot, rename and set the data type (source data is read in as all varchars). It works well. 

Here's the macro (recordset identifies source data, record_pip identifies the target):

    {% macro get_field_definitions(record_set, record_pipe) %}
        {% set query %}
           select
               field_key,
               field_type,
               target_name
           from {{ ref('stg_field_definitions') }}
           where record_set = '{{record_set}}'
           and record_pipe = '{{record_pipe}}'
        {% endset %} 

        {% set query_results = run_query(query) %}

        {% if execute %}
            {% set results = query_results.rows %}
            {{ return(results) }}
        {% endif %}
    {% endmacro %}


In SQL:

    {{
        config(
            materialized='view'
        )
    }}

    {#- Get the field definitions that exist  -#}
    {% set field_definitions = get_field_definitions(
                                                                 record_set = 'SomeRecordset',
                                                                 record_pipe = 'SomeTargetEntity')
     %}


    select
        identifier,
            {% for row in field_definitions %}
                cast( max(case when field_key = {{ row[0] }} then field_value end) as {{row[1]}} ) as ""{{ row[2] }}"",
            {% endfor %}
        source_file,
        sum(is_changed) as num_changed
    from {{ ref('stg_some_long_key_value_table') }}
    group by
        dentifier,
        source_file -- We have daily files coming in with changes


A few notes:

* Tried dbt_utils.pivot, but it doesn't support using a second column as the value (that I could find)
* This is my first attempt at creating a macro, I'd love any feedback
* I was surprised how easy this was. It really sold me on DBT as a great solution to dynamic SQL",1
4881,"how-to-use-zip-list-a-list-b-in-a-loop","I'm trying to do the following
```
{% set my_list_a = ['alice', 'bob'] %}
{% set my_list_b = [ref('table_a'), ref('table_b')] %}
{% set my_zip = zip(my_list_a, my_list_b) | list %}
{% for (a, b) in my_zip %}
   do something...
{% endfor %}
```
But it seems that only the last element was picked up in the loop.. Am I using it correctly? TIA!",1
4881,"how-to-use-zip-list-a-list-b-in-a-loop","Hey @topo.lora, it's hard to know based on your summarised code. I tried using a variant: 

```python
{% set my_list_a = ['alice', 'bob'] %}
{% set my_list_b = [2, 3] %}
{% set my_zip = zip(my_list_a, my_list_b) | list %}
{% for (a, b) in my_zip %}
   {{ a }} {{ b }}
{% endfor %}
```

which compiled to:
```
   alice 2

   bob 3
```

which seems to be what you'd expect! 

Does `ref('table_a')` exist? Can you post the code that is in your `do something...` block?",2
4891,"creating-a-separate-database-for-datasets-created-by-python-pipelines","Hello! 

Please let me know if this is the wrong category for post this!

A bit of background on our current database set up. We have a staging database for preparing datasets from external provider databases, a development database that mirrors the production database. We also have a number of Python pipelines deployed on a server producing datasets, such as forecasting results and web scraping data.  

We are currently trying to create a proper staging to data mart data flow. Should we 1) create a separate database for these pipeline-produced datasets that then are brought into the staging database, treating them as a raw data source or 2) put them into a ""pipelines"" schema directly in the staging database because we have control over the formats and quality of these datasets (this is referencing a [post](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355/11) by Claire about putting seeds directly in the staging database since they can already be put into a ""staging"" format).

Any thoughts would be greatly appreciated! :) 

Nancy",1
4891,"creating-a-separate-database-for-datasets-created-by-python-pipelines","Hey @nancy.chelaru, this is a really interesting question! I think my answer depends on the specifics of

[quote=""nancy.chelaru, post:1, topic:4891""]
we have control over the formats and quality of these datasets.
[/quote]

How much transformation and cleaning is happening in Python before they're being loaded to the database, and does your team control that process directly? If the data is already in a perfect format for analysis, and that format isn't going to change <s>ever</s> for a long time, you could probably get away with loading it directly into a `staging` db. 

""Could probably get away with"" is not exactly inspiring though! On the whole, I'd recommend still loading it into a separate `raw` database. Your staging layer might wind up being `select * from ""raw"".""pipelines"".""web""` for now, but it maintains optionality if your data format changes in the future - you can make changes in your staging layer to maintain the interface/contract that your marts rely on.",2
4891,"creating-a-separate-database-for-datasets-created-by-python-pipelines","Thank you so much for your reply, @joellabes !

Great to hear your take on it! Our team does have complete control over these pipelines. I too was leaning towards having a separate `raw` database for the resultant datasets. It just didn't quite feel right for them to land directly in the `staging` database, which is supposed to, as you said, a layer to maintain data contracts that downstream models depend on. Having them in a separate database also makes the lineage graph clearer.",3
4900,"using-snapshots-while-avoiding-the-meta-fields-dbt-scd-id-dbt-valid-from-dbt-valid-to","I am using dbt snapshots with the check-strategy to transform data and fill a set of pre defined tables in my db. The structure of the database is set in a way that I do not have the  ""dbt_scd_id"", ""dbt_valid_from"" and ""dbt_valid_to""  in the table I am targeting, neither will those columns be needed in any way so there is no reason to have them. Since snapshots require these columns to be present I run into a bit of a problem that I have not managed to solve. 
Does someone know of a way to get a snapshot to target a table that lacks these three fields?",1
4900,"using-snapshots-while-avoiding-the-meta-fields-dbt-scd-id-dbt-valid-from-dbt-valid-to","[quote=""arvidp, post:1, topic:4900""]
I do not have the â€œdbt_scd_idâ€, â€œdbt_valid_fromâ€ and â€œdbt_valid_toâ€ in the table I am targeting, neither will those columns be needed in any way so there is no reason to have them.
[/quote]

How are you keeping track of which version of a snapshot row is the one to use for a given date, if you don't collect these dates? 

[quote=""arvidp, post:1, topic:4900""]
fill a set of pre defined tables in my db
[/quote]
Could you add the columns to your existing table?

I don't have enough context to fully answer your question, but perhaps you should consider an incremental model instead of snapshots.",2
4904,"how-can-i-use-packages-documentation-only","Hi community!

I am struggling with the docs generation, with my particular setup.

There are two dbt projects: data_quality and main.
The `data_quality` project is only used by some DataEngineers, with special access to some tables. No one else is able/allowed to run this project.

The `main` project, is what the rest of the company uses to build their models. All the sources used in the `main` project come from the `data_quality` project. In fact, all the tables are listed in `sources.yml` file pointing to the output of `data_quality` project .

Both projects describe the models and tests, but when I generate the `main` docs the sources don't include the documentation!
I need to list `data_quality` as a package and change the `source` to `ref` pointing to the package to get the documentation in place, but this new config breaks the permissions separation we have in place!


Is there a way to utilise a package _without_ running the package models?
Or even better, is there a way to ""export"" only the docs of a package to use in another project?


Thanks!",1
4904,"how-can-i-use-packages-documentation-only","This is a great question @emancu! It's not possible (yet) but is an area of development that is extremely in the zeitgeist. 

We're currently discussing this on GitHub as a potential area of development in dbt - check out https://github.com/dbt-labs/dbt-core/discussions/5244 if you want to weigh in!",2
4904,"how-can-i-use-packages-documentation-only","",3
4909,"how-can-i-upload-csv-file-in-seeds","Hi, 
Can you help me how to upload a CSV file in seeds from local system.",1
4909,"how-can-i-upload-csv-file-in-seeds","[This guide](https://docs.getdbt.com/guides/getting-started/add-a-seed) should help you out!",2
4909,"how-can-i-upload-csv-file-in-seeds","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
4911,"processing-source-tables-with-differing-arrival-times-throughout-day","Interested to know what approach one would use with Snowflake when you have multiple sources arriving at different times of the day that you want to process as they come in.
Teradata would allow you to create a view that could be inserted into so no problem, but without being able to create views to alias I dont see how to get around this.

THis is a very common scenarios in corporate data warehouses, account balance tables for example that get load from many disparate sources.

Will DBT move away from the one table one transform model soon?  After all conforming data from mulitple sources is sort of the point of data analytics in the first place.",1
4911,"processing-source-tables-with-differing-arrival-times-throughout-day","[quote=""MatthewMunn, post:1, topic:4911""]
Teradata would allow you to create a view that could be inserted into so no problem
[/quote]

I'm unfamiliar with Teradata so this could be ignorance on my part, but generally you don't insert into views; do they use different terminology? I also don't know what you mean by ""create views to alias"", could you clarify those points? 

[quote=""MatthewMunn, post:1, topic:4911""]
Will DBT move away from the one table one transform model soon?
[/quote]
dbt supports (and strongly encourages!) taking multiple source tables and turning them into a single transformed mart table. It doesn't support building multiple output tables from a single model file, but I don't _think_ that's what you're referring to. 

[quote=""MatthewMunn, post:1, topic:4911""]
multiple sources arriving at different times of the day that you want to process as they come in.
[/quote]

If you use a traditional view where the underlying tables are accessed live on every query, then dbt would work great for that. If you want to materialize them as a table, then your data would obviously only be as up to date as the last `dbt run` which built that table. (Keep in mind that if you have a very very large table, you can use [incremental models](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models) to only process the new data).",2
4915,"local-variable-set-variable-usage-in-config-block","Hi Team, 

Is there any way we can use variable which value through set function can be used in config pre_hook or post_hook section ? 

eg: 
{%- set empDelId   = get_emp_id_to_delete() %}

{{
    config(materialized = 'incremental',
            pre_hook = [ "" DELETE FROM {{ source('myWIsource', 'Employees') }} where employee_di = empDelId"" 
                                    ]
}}





<!-----
â„¹ï¸ Are you looking for troubleshooting help?

If so, please post your question to Stack Overflow:
https://stackoverflow.com/questions/ask?tags=dbt

We are reserving Discourse for more long-form discussions, and ""How to"" posts. Check out more details here:
https://discourse.getdbt.com/t/welcome-to-dbt-discourse/8 
---->",1
4915,"local-variable-set-variable-usage-in-config-block","No, this won't work -- `set` creates a local variable that is inaccessible outside the model's context.

Instead, I would create a macro and call the macro in the pre_hook:

```
{{
config(materialized = â€˜incrementalâ€™,
pre_hook = [ ""{{ delete_users() }}""]
}}
```

```
{% macro delete_users() %}
{%- set empDelId = get_emp_id_to_delete() %}
DELETE FROM {{ source(â€˜myWIsourceâ€™, â€˜Employeesâ€™) }} where employee_di = {{ empDelId }}
{% endmacro %}
```
Or you could just call the `get_emp_id_to_delete` macro from your pre-hook:
{{
config(materialized = â€˜incrementalâ€™,
pre_hook = [ ""DELETE FROM {{ source(â€˜myWIsourceâ€™, â€˜Employeesâ€™) }} where employee_di = {{ get_emp_id_to_delete() }}""
]
}}",2
4915,"local-variable-set-variable-usage-in-config-block","Thank you @ted.conbeer (y)",3
4915,"local-variable-set-variable-usage-in-config-block","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4916,"how-to-map-binary-data-to-variable-in-macro","I have use case where I have pass last query id to another script but when I tried to use binary data, DBT through an error. Please let me know if you face similar issue

<!-----
â„¹ï¸ Are you looking for troubleshooting help?

If so, please post your question to Stack Overflow:
https://stackoverflow.com/questions/ask?tags=dbt

We are reserving Discourse for more long-form discussions, and ""How to"" posts. Check out more details here:
https://discourse.getdbt.com/t/welcome-to-dbt-discourse/8 
---->",1
4916,"how-to-map-binary-data-to-variable-in-macro","@shivaramp I can understand the use case of wanting to pass one query ID to another, but I don't see how that leads to having trouble with binary data. Can you share the code you're using and the error messages dbt is returning?

Please also explain the actual problem you're trying to solve by passing one query ID to another, as there may be a more dbt-standard way to do this.",2
4922,"snowflake-cluster-keys-in-dbt-learnings-and-thoughts","My data engineering team has recently been working on converting the generation/maintenance of some very large data sets that our data science team has been managing. We use snowflake, and for many of our large data sets we define custom cluster keys on the tables. As we have been working on these conversions, the query times for producing some of these large models (dozens of billions of rows) were far longer than we had expected. Examination of the query plans showed that the longest step was a sort on the full data set, which is not something that we had in our dbt model definition. After some digging, we discovered that this is a product of how dbt decided to handle snowflake clustering. 

References:

https://github.com/dbt-labs/dbt-core/issues/634
https://docs.getdbt.com/reference/resource-configs/snowflake-configs#configuring-table-clustering

Basically, dbt takes advantage of the fact that if you insert sorted data to an empty table (or as part of a ctas), and then apply clustering on the same sort keys, then the data is already clustered by default. So if you have added the ""cluster by"" config to your model, when the model gets compiled, dbt adds a sort on the cluster key(s) to the end of the sql. This decision appears to have been made partially in response to the fact that snowflake's normal clustering mechanism involves a background process that reorganizes the underlying data for clustered data sets until they are close to optimal, and that process does have expense ($$) associated with it. 

Another way to think about it is that dbt enforces immediate consistency on clustered tables, while normally you would have eventual consistency. This makes sense in the context of a dbt process on its own: presumably you have clustered your data to make it more efficient to use it downstream, and in a dbt process presumably your downstream use is right away, so you want immediate consistency.

However, in our use case, we do actually want eventual consistency...we want to leverage snowflake's background process. The trade off we are making is that we want the dbt process to run faster, with the knowledge that at some point in time later on people will be leveraging the outputs, and they want it clustered.

Anyway, we thought those were some really interesting ""down in the weeds"" details on how dbt snowflake clustering actually works, and wanted to share with the community.

Also, our solution for our particular case is now to not the ""cluster by"" config to apply the clustering to our models, but instead to apply them via a post-hook. That way dbt does not compile in the sort on the data set, but we still get that ""eventual consistency"" behavior.",1
4925,"if-statement-not-evaluating-to-true-when-comparing-a-model","Hello

I am trying to create a sql template using a macro with one parameter, the if condition does not evaluate to true when passed TABLE1 or TABLE2

> {% macro cloud_test_results_get_standard_columns(modelName) %}
>         result,
>         Length,
>         estimatedLength             as estimatedLength, 
>         {% if '{{modelName}}' == 'TABLE1' %}
>                 TABL1_COL1,
>                 TABL1_COL1,
>                 TABL1_COL1,
>         {% elif '{{modelName}}' == 'TABLE2' %}
>                 TABL1_COL1,
>                 TABL1_COL1,
>                 TABL1_COL1,
>         {% else %}
>                 TABL_DEFAULT1,
>                 TABL_DEFAULT2,
>                 TABL_DEFAULT3,       
>       {% endif %}
> {% endmacro %}",1
4925,"if-statement-not-evaluating-to-true-when-comparing-a-model","please disregard, had to use modelName instead of â€˜{{modelName}}â€™ inside if block",2
4925,"if-statement-not-evaluating-to-true-when-comparing-a-model","[quote=""jks2k, post:2, topic:4925""]
had to use modelName instead of â€˜{{modelName}}â€™ inside if block
[/quote]

Yes you got it! When you're inside of curly braces, you don't need another set of curlies around your variables. 

Related docs:
https://docs.getdbt.com/docs/building-a-dbt-project/dont-nest-your-curlies",3
4925,"if-statement-not-evaluating-to-true-when-comparing-a-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4935,"run-dbt-between-separate-snowflake-databases-in-the-same-account","I have a snowflake account with my raw data in database RAW, and a separate database ANALYTICS where I would like to store the output of transformation models done by dbt, e.g.

run model abc.sql ""select 1 from RAW.schema.table"" -> should end in ""ANALYTICS.dev.abc""

how do I set up my project to allows for a transformation between different databases?

I have tried granting dbt-user/role OWNERSHIP on both databases, but still run into ""schema does not exist or not authorized.""",1
4935,"run-dbt-between-separate-snowflake-databases-in-the-same-account","Hey @Vince, this is definitely [best practice](https://www.getdbt.com/blog/how-we-configure-snowflake/) - you're on the right track! 

I've never set up Snowflake permissions myself, but we have an example in our docs: https://docs.getdbt.com/reference/snowflake-permissions 

If this doesn't get you unstuck, can you share more about the specific dbt code you're using which is resulting in that error?",2
4935,"run-dbt-between-separate-snowflake-databases-in-the-same-account","The Snowflake permissions script you linked what was I was missing, that worked!

Ran that over both databases for the TRANSFORMER role I created for the dbt-user, and no more permission conflicts.
Previously I had only granted OWNERSHIP to the TRANSFORMER role, which apparently did not suffice; seems like the individual (future) USAGE-grants on the databases from the script were what was necessary.

Thank you for your kind & helpful reply @joellabes 
all the best to you!",3
4935,"run-dbt-between-separate-snowflake-databases-in-the-same-account","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4936,"creating-dynamic-queries-with-jinja-and-model-yml-files","### Problem Context:
We have various fields that we do/donâ€™t want to surface to end users in Looker for any given transformation. However, the fields are necessary for transformation logic. In a gross sense, our query relationships goes `staging` â†’ `transformation` â†’ `surface only selected fields to visualization tools ('Clean')`. However, manually maintaining our `Clean` queries is growing into a bit of a pain, and weâ€™ve run into some frustrating situations doing so. A key part of this is that we don't want to actually maintain the SQL in the `Clean` queries.
We also have limited resources to implement this from a BigQuery or Infra side of things, so we had to do this only with what DBT has to offer.

### Solution
We have added a property to every column entry, that we can read via Jinja. This greatly simplifies things, as we already document each column, both as a matter of good practice, and to centralize documentation in DBT--all of our descriptions are written to BQ, and Looker now has the capability to import the descriptions into LookML when creating view files. Part of the design of this is to operate on an allow-list basis--the column must have the property, and the property must = `true`. At some point, a column must be defined as being allowed/not allowed, and we decided that dbt is right place for us, given our constraints.

I had avoided using the `tags` hierarchy, as the .yml files were getting pretty big.

An example of the `model.yml`:
```
version: 2

models:
  - name: upstream_query
    description: ""blablabla""
    columns:
    - name: field_1
      description: ""blablabla""
      is_allowed: true
    - name: field_2
      description: ""blablabla""
      is_allowed: false
    - name: field_3
      description: ""blablabla""
      is_allowed: true
```
  
The macro:
```
{% macro get_allowed_columns_macro(tablename) -%}
	{% if execute %}
		{%- set column_list = [] -%}
		{%- for node in graph.nodes.values() -%}
			{%- if node.name == tablename -%}
				{%- for column, properties in node.columns.items() -%}
					{%- do column_list.append(column) if properties.get('is_allowed') == true -%}
				{%- endfor -%}
			{%- endif -%}
		{%- endfor %}
		{{ return(column_list) }}
	{% endif %}
{%- endmacro %}
```

An example query:
```
/* Define the table you are referring to */
{% set tableref = 'upstream_query' -%}
/* Create a [list] of columns that are flagged as field_name == true */
{% set allowed_columns = get_allowed_columns_macro(tableref) -%}

SELECT
{%- for columns in allowed_columns -%}
{%- if not loop.first %}
, {{ columns }}
{%- else %}
{{ columns }}
{%- endif -%}
{%- endfor %}

FROM {{ ref(tableref) }}
```

Which will compile to:
```
SELECT
field_1
, field_3

FROM `database.dataset.upstream_query`
```",1
4942,"using-python-to-update-model-schema-yml-files-and-create-staging-files-bigquery-specific","Howdy folks, this is a long post--I didn't want to make multiple posts. Also, happy for input on the Python, in case you think it could be improved :slight_smile: 

# 1. Updating model_schema.yml Files Using Python

### Background
Analysts are required to be on top of updating `model_schema.yml` files. I wrote a Python script that will update the `model_schema.yml` for the analyst, after they have modified & run a query. This is very helpful, especially for documenting queries that are dynamically generated using Jinja loops or similar, where one can end up with a large number of relatively-dynamic columns. I have used `ruamel.yaml`--I found that the common `PyYAML` to not be sufficient for my purposes, as I found the ruamel round-trip formatting and export to be much superior.

This codes uses ruamel v0.17.21, found here: https://pypi.org/project/ruamel.yaml/0.17.21/
1. `pip install -U pip setuptools wheel`
2. `pip install ruamel.yaml==0.17.21`

### Some Important Notes
A few notes about the limitations of this code:
1. This code requires the analyst to have run the query recently--it operates off of the columns it finds in the database from the analyst's development run environment.
2. This code is specific to BigQuery,
3. This code assumes that all `model_schema.yml` files follow the same naming structure, ex: `models_schema__invoices.yml`, `models_schema__customers.yml`, .....
4. This code preserves any existing `description` or other parameters present in YAML entries
5. It assumes that all `description` fields are double-quoted ex. `description: ""Hello world!""`

### How the code works
1. Prompt user for the name of query they modified
2. Search across `model_schema.yml` files across the project
3. Identify where the entry exists: Note: there must be an entry for the query, and at least 1 column defined
4. Query against the analystâ€™s development output, to identify columns that will be seen in production
5. Check if it should keep, drop, or add existing column entries
6. Write the updated query+column fields to the appropriate YAML file

As a side note, updating YAML in-place was a massive pain, but it didnâ€™t seem to be a common thing on the internet. This is my hacky solution--happy to hear if folks have a better way of approaching this.

At the moment, this code must be manually triggered by the analyst. In a nice future state, this would be triggered by git-commit actions, and it would automagically search for modified queries, update the relevant yaml, and prompt the user to update relevant documentation parameters in the yaml.

### Minimum amount of yml necessary for the code to successfully execute
This code works for both net-new and existing yml entries. However, it needs the analyst to start the net-new entry--the intent of this is that it focuses more on dynamically updating existing entries, so the analyst doesn't have to go digging through folder structure as often.

```
version: 2

models:
  - name: query_you_modified
    columns:
      - name: need_at_least_1_field
        description: ""If net-new, just put in a fake field name, as it will be dropped and replaced with the proper fields.""
```

### The Python code:
Any variables specific to your use case are flagged as `### UPDATE ME!!!` in the code
```
import collections
import os
import pandas
import pathlib
import sys

from google.api_core.exceptions import BadRequest
from google.cloud import bigquery
from ruamel.yaml import YAML
from ruamel.yaml.scalarstring import DoubleQuotedScalarString # Used in our production environment, but not used in this demo code

########## USER DEFINED CUSTOM CONFIGS FOR USE IN SCRIPT ##########
### NOTES ###
# This script requires that dbt be installed in the top-level folder of your OS ex. ~/dbt, and that your profiles.yml lives in ~/.dbt/profiles.yml
# This script requires that your model_schema.yml files are all quoted for descriptions
# This script works for BigQuery connections--not sure how it works with other databases
# This script requires that you already have the query defined in the model_schema.yml, with at least 1 column defined
### END NOTES ###

# Define your dataset name--current query works with Google BigQuery
client = bigquery.Client(location=""US"")
project_name = ""your-project-name""  ### UPDATE ME!!!

# Requires that all of your model_schema.yml files to start with the same naming schema
yaml_file_prefix = ""models_descr""  ### UPDATE ME!!!
# If you are adding new fields, what are the default dbt column fields that are always added. This ensures that the fields are always added in order
additional_default_fields = [(""description"", ""No description set""), (""allowed_field"", True), (""other_field"", ""yay"")]  ### UPDATE ME!!! This is a dictionary of default fields that you expect for every column entry. 
additional_default_fields = collections.OrderedDict(additional_default_fields)

########## END USER DEFINED CONFIGS ##########

# RUAMEL.YAML CONFIGS
# ruamel.yaml v.0.17.21
yaml = YAML()
# control indentation of hierarchy in yaml
yaml.indent(sequence=4, offset=2)
# Preserve quotes in outputs
yaml.preserve_quotes = True
# Keep descriptions from line wrapping
yaml.width = 4096

# Pull table and column data from current dev output
def table_pull(project_name, dbt_dev_folder, query_name):
    query = f""""""
        SELECT
        cols.column_name
        , cols.description AS column_description
        , tables.option_value AS table_description

        FROM `{project_name}.{dbt_dev_folder}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS` AS cols

        LEFT JOIN `{project_name}.{dbt_dev_folder}.INFORMATION_SCHEMA.TABLE_OPTIONS` AS tables
        ON cols.table_name = tables.table_name
        AND tables.option_name = ""description""

        WHERE cols.table_name = ""{query_name}""

        ORDER BY 1
        """"""

    print(f""Querying {query_name} \n"")

    query_job = client.query(query)

    try:
        query_job.result()
    except BadRequest:
        for e in query_job.errors:
            print(f""QUERY ERROR: {e['message']} \n"")
        sys.exit(f""Beep boop query failed, exiting script... \n"")

    columns = query_job.to_dataframe()
    print(f""The query used was: \n\n{query} \n"")
    print(f""Tables loaded into dataframe \n"")

    if columns.empty:
        sys.exit(""Your query returned no result, exiting script \n"")

    return columns

def main():
    # Define paths for use across script
    current_path = pathlib.Path().resolve()
    home_path = pathlib.Path.home()

    dbt_path = f""{home_path}/dbt/""
    dbt_profile_path = f""{home_path}/.dbt/profiles.yml""
    dbt_yml_locations_path = f""{home_path}/dbt/models/""

    actual_path = os.path.relpath(home_path, current_path)

    contyn = False

    # User must have run the modified query
    print(""You need to have run the query you are trying to generate .yml for before running this script \n"")

    has_run_query = input(""Have you run the query in question? Enter y or n:  \n"")

    if has_run_query == ""n"":
        sys.exit(""You need to run your query before using this script. We will now exit. \n"")

    # Prompt user to enter file name
    while contyn == False:
        modified_file_name = input(""Enter the name of the file that you have modified, and want to generate .yml entries for: \n"")
        yesno = input(f""You entered \n{modified_file_name}\nDo you want to use this? Enter y or n: \n"")
        contyn = True if yesno == ""y"" else False

    # Grab the dev dataset for the analyst
    with open(dbt_profile_path, ""r"") as profiles:
        data = yaml.load(profiles)
        dbt_dev_dataset = data.get(""default"").get(""outputs"").get(""dev"").get(""dataset"")

    # Query output of modified dbt query
    dbt_query_output = table_pull(project_name, dbt_dev_dataset, modified_file_name)

    # Identify all .yml files in dbt/models
    all_yml_files = []
    for (dirpath, dirnames, filenames) in os.walk(dbt_yml_locations_path):
        for file in filenames:
            if file.startswith(yaml_file_prefix) and file.endswith("".yml"") and "".ipynb_checkpoints"" not in dirpath:
                all_yml_files.append(os.path.join(dirpath, file))

    # Check if the query already has a entry in a model description .yml file
    query_yml_file_location = None

    for index, ymlfile in enumerate(all_yml_files):
        with open(ymlfile, ""r"") as models:
            read_data = yaml.load(models)

            # We only want to search across keys inside of the ""models"" key entry in the description yaml files
            # We need to search 1 level down (models -> ""name"" keys of queries)
            # dbt model_schema.yml is a dictionary(version/models) then a list (each query is a list) then a dictionary (name/description/columns) then a list for columns, then a dictionary for each column entry

            # Access version/models in .yml -- dictionary
            for models_key, models_values in read_data.items():
                # Only want to look at ""models"" dictionary
                if models_key == ""models"":
                    # Need to access list of queries in .yml
                    for queries_index, queries_values in enumerate(models_values):
                        # Need to access top-level entries in each query (name/description/columns)
                        for query_key, query_values in queries_values.items():
                            if query_key == ""name"":
                                if query_values == modified_file_name:
                                    query_yml_file_location = ymlfile

    # Exit script if query not located in model_schema.yml files
    if query_yml_file_location is None:
        sys.exit(f""We couldn't find an entry for {modified_file_name} , the script will now exit :( \n"")

    # Create list of columns
    dev_column_list = dbt_query_output[""column_name""].tolist()
    yaml_index = None

    # This code only works if you have created a yaml entry, with a dummy column.
    # Need to identify if existing column+values needs to be removed
    # Need to identify if column needs to be added
    # Write new columns, and assume PII flagging based off of column names
    with open(query_yml_file_location, ""r"") as descriptions:
        yaml_entries = yaml.load(descriptions)

        # Need to identify index position of query yaml entry we care about
        for models_key, models_values in yaml_entries.items():
            if models_key == ""models"":
                # Need to access list of queries in .yml
                for queries_index, queries_values in enumerate(models_values):
                    # Need to find index of list so that we can access other keys inside list
                    for query_key, query_values in queries_values.items():
                        if query_key == ""name"" and query_values == modified_file_name:
                            yaml_index = queries_index

        ####### THIS ONLY WORKS FOR EXPLICIT DBT MODEL SCHEMA YAML ENTRIES!!!!

        # Pull all columns from current yaml file
        prod_yaml_columns = []
        for index, value in enumerate(yaml_entries[""models""][yaml_index][""columns""]):
            prod_yaml_columns.append(yaml_entries[""models""][yaml_index][""columns""][index][""name""])

        # Create list of all unique values from dev output + current yaml
        all_columns = []
        for i in dev_column_list + prod_yaml_columns:
            if i not in all_columns:
                all_columns.append(i)

        # Identify which columns need to be added or removed
        add_keep_remove_entries = {}
        for all_index, all_value in enumerate(all_columns):
            dev_appears = 0
            prod_appears = 0

            # Check if prod or dev appears in all entries, which lets us know whether to keep, add, or remove
            # We create a dictionary that will allow us to update the actual yaml entries
            # Performance here is probably pretty garbage, but alas, that is a problem for another day
            for dev_index, dev_value in enumerate(dev_column_list):
                if all_value == dev_value:
                    dev_appears += 1
            for prod_index, prod_value in enumerate(prod_yaml_columns):
                if all_value == prod_value:
                    prod_appears += 1

            # Add to dictionary, and decide if we keep, add, or remove an entry
            if dev_appears == 1 and prod_appears == 1:
                add_keep_remove_entries[all_value] = ""keep""
            elif dev_appears == 1 and prod_appears == 0:
                add_keep_remove_entries[all_value] = ""add""
            else:
                add_keep_remove_entries[all_value] = ""remove""

        # Now we compare our keep/add/remove dictionary vs the dictionary of columns, and update the yaml

        prod_yaml_entries = yaml_entries[""models""][yaml_index][""columns""]
        new_query_yaml = []

        for key, value in add_keep_remove_entries.items():
            # Going to check if the prod yaml entries appear in the total list. Then, we can decide if we remove the item in prod_yaml_entries, or add the new thing
            # Find index of yaml entry
            location = None
            for index, v in enumerate(prod_yaml_entries):
                for yk, yv in v.items():
                    if yk == ""name"" and key == yv:
                        location = index

            mini_dict = {}
            if value == ""keep"":
                mini_dict = prod_yaml_entries[location]
                new_query_yaml.append(mini_dict)
            elif value == ""add"":
                mini_dict[""name""] = key
                if len(additional_default_fields) > 0:
                    for key, value in additional_default_fields.items():
                        mini_dict[key] = value
                new_query_yaml.append(mini_dict)

        # Insert column info back into all yaml dictionary
        yaml_entries[""models""][yaml_index][""columns""] = new_query_yaml

    # Write output to yaml file
    with open(query_yml_file_location, ""w"") as descriptions:
        yaml.dump(yaml_entries, descriptions)

    print(f""Yay, we updated your yaml!! You can find the yaml file at \n {query_yml_file_location}\n"")

if __name__ == ""__main__"":
    main()
```

# 2. Importing Staging Files Using Python
### Background
Creating staging files can be a pain in the neck, when you have a lot to import, especially when you're creating a new project, connecting to a new dataset, or exposing a bunch of new tables from an existing dataset. I know that there are some Jinja-based utils to streamline this, but it still means you have to manually create the files. We also run into the situation where you have a number of staging files already imported, but you need to add some new ones due to a new product feature release, but you don't want to overwrite the existing staging files.

### How it works

Note that this script is specific to our idiosyncratic DBT setup--we have our staging files set up in a particular way to deal with some data concerns regarding PII/PHI, due to our work with healthcare-related data.
Our specific design choices:
1. Each underlying table is initially declared as a staging file
2. Each field is explicitly declared
3. We only perform minor name and datatype changes in staging files
4. Each staging file follows a particular pattern: `stg_datasourcefolder__table_name.sql` ex. `stg_salesforce__user.sql` or `stg_internal_api_replica__user.sql`. You can feel free to modify the code to match your naming strategy.


### Python Code
My design patterns/baked-in assumptions for this code:
1. The user should be able to decide if they're overwriting existing staging files, or only importing net-new ones
2. It should ensure that BigQuery reserved keywords are modified.
3. Any `id` fields are PKs, and should be renamed to `table_name_id`
4. We only want to allow specific datatypes
5. We have a recognizable pattern to how your staging folders are structured
6. Source YAML files will have to be manually edited--dynamically editing these files was out of scope for this project, but future iterations will include concepts from the first part of this document for that.

This code assumes that your folder structure follows something similar to this:
* models
  * staging
    * fivetran
        * fivetransource_1
           * `stg_fivetransource_1__table1.sql`
              `stg_fivetransource_1__table2.sql`
         * fivetransource_2
           .....
    * internal_api
        * internal_apisource_1
           * `stg_internal_apisource_1__table1.sql`
              `stg_internal_apisource_1__table2.sql`
         * internal_apisource_2
          .....


I know that this is perhaps not 100% the favored DBT structure, but this is what we have ended up with, due to some internal design and resource constraints. 

I also had this code with user prompts, but that got a little complicated, so I have trimmed it down for this exercise--you have to manually update the variables.

Any variables specific to your use case are flagged as `### UPDATE ME!!!` in the code

```
import os
import pandas
import pathlib
import sys

from google.cloud import bigquery
from google.api_core.exceptions import BadRequest
from google.api_core.exceptions import NotFound

########## USER DEFINED CUSTOM CONFIGS FOR USE IN SCRIPT ##########
### NOTES ###
# This script requires that dbt be installed in the top-level folder of your OS ex. ~/dbt, and that your profiles.yml lives in ~/.dbt/profiles.yml
# This script requires that your staging folders follow consistent structure, as well as your queries having consistent naming structure
# This script works for BigQuery connections--not sure how it works with other databases
### END NOTES ###

# Define your dataset name--current query works with Google BigQuery
client = bigquery.Client(location=""US"")
project_name = ""your-project-name""  ### UPDATE ME!!!

dataset = ""dataset_name"" ### UPDATE ME!!!-- Name of actual BigQuery Dataset. Required.
table_prepend = None ### UPDATE ME!!! -- Naming pattern of tables in your dataset--do they all start with a common text pattern? If you don't have it, replace with None.
table_wildcard = None  ### UPDATE ME!!!-- Do the tables you want to import have a wildcard common text pattern, ex. you want all tables that match '%invoice_%'? If you don't have it, replace with None.
datasource = ""internal_api""  ### UPDATE ME!!! -- Name of parent folder that you want staging files to appear in in DBT folder structure. Corresponds with say, Fivetran, your internal company data, etc. Required.
folder = ""internal_apisource_1"" ### UPDATE ME!!! -- Name of folder you want staging files to appear in in DBT folder structure. Corresponds with 'dataset' in BigQuery. Required.
# Flag if we only want to add non-existing staging files, or add new + overwrite existing staging files
import_type = ""new""  ### UPDATE ME!!! -- Otherwise = ""overwrite"" Required.

########## END USER DEFINED CONFIGS ##########

# Some columns have BQ reserved keywords. We need to handle these when writing the queries
bq_reserved_keywords = [""all"", ""and"", ""any"", ""array"", ""as"", ""asc"", ""assert_rows_modified"", ""at"", ""between"", ""by"",
                        ""case"", ""cast"", ""collate"", ""contains"", ""create"", ""cross"", ""cube"", ""current"", ""default"",
                        ""define"", ""desc"", ""distinct"", ""else"", ""end"", ""enum"", ""escape"", ""except"", ""exclude"", ""exists"",
                        ""extract"", ""false"", ""fetch"", ""following"", ""for"", ""from"", ""full"", ""group"", ""grouping"", ""groups"",
                        ""hash"", ""having"", ""if"", ""ignore"", ""in"", ""inner"", ""intersect"", ""interval"", ""into"", ""is"", ""join"",
                        ""lateral"", ""left"", ""like"", ""limit"", ""lookup"", ""merge"", ""natural"", ""new"", ""no"", ""not"", ""null"",
                        ""nulls"", ""of"", ""on"", ""or"", ""order"", ""outer"", ""over"", ""partition"", ""preceding"", ""proto"",
                        ""qualify"", ""range"", ""recursive"", ""respect"", ""right"", ""rollup"", ""rows"", ""select"", ""set"", ""some"",
                        ""struct"", ""tablesample"", ""then"", ""to"", ""treat"", ""true"", ""unbounded"", ""union"", ""unnest"", ""using"",
                        ""when"", ""where"", ""window"", ""with"", ""within""]

# Standardize datatypes to only certain kinds, for consistency across the project
allowed_datatypes = [""INT64"", ""STRING"", ""TIMESTAMP"", ""DATE"", ""FLOAT""]

# Define paths for use across script
current_path = pathlib.Path().resolve()
home_path = pathlib.Path.home()
actual_path = os.path.relpath(home_path, current_path)

dbt_path = f""{home_path}/dbt/""
dbt_stg_locations_path = f""{home_path}/dbt/models/""

# Continue constant for user input loops
contyn = False

# Function that returns list of tables that we will create staging files for
def query_data_pull(project_name, dataset, table_prepend, table_wildcard):
    # table type
    if dataset is None:
        print(f""You did not enter a dataset, exiting script...\n"")
        sys.exit()

    if table_prepend is None:
        table_cleanup = """"
    else:
        table_cleanup = f""{table_prepend}_""

    if table_prepend is None:
        table_prepend_filter = ""AND 1 = 1""
    else:
        table_prepend_filter = f""AND table_name LIKE '{table_prepend}%'""

    if table_wildcard is None:
        table_wildcard_filter = ""AND 1 = 1""
    else:
        table_wildcard_filter = f""AND table_name LIKE '%{table_wildcard}%'""

    query = f""""""
        SELECT
        table_name
        , REPLACE(table_name, '{table_cleanup}', '') AS condensed_table_name
        , column_name
        , data_type

        FROM `{project_name}.{dataset}.INFORMATION_SCHEMA.COLUMNS`


        WHERE column_name NOT IN ('_airbyte_ab_id', '_airbyte_normalized_at')
        AND column_name NOT LIKE '_airbyte%_hashid'
        AND table_name NOT LIKE '%__dbt_tmp'
        AND table_name NOT LIKE '_airbyte_%'
        {table_prepend_filter}
        {table_wildcard_filter}

        ORDER BY 1, LENGTH(column_name)
    """"""
    query_job = client.query(query)

    # Run the query, exit if we have a non-functional query
    try:
        query_job.result()
    except BadRequest:
        for e in query_job.errors:
            print(f""QUERY ERROR: {e['message']}\n"")
        print(f""The query used was:\n\n{query}\n"")
        sys.exit(f""Beep boop query failed, exiting script...\n"")

    print(f""Query complete"")

    # Load query into dataframe
    tables = query_job.to_dataframe()
    print(f""The query used was:\n\n{query}\n"")
    print(f""Tables loaded into dataframe\n"")

    if tables.empty:
        sys.exit(""Your query returned no result, exiting script\n"")

    return tables

def main():

    # Get list of existing queries in folder
    existing_staging_queries_dict = {}
    existing_staging_tables_list = []
    entrycount = 0
    for (dirpath, dirnames, filenames) in os.walk(dbt_stg_locations_path):
        for file in filenames:
            if file.startswith(f""stg_{folder}"") and file.endswith("".sql"") and "".ipynb_checkpoints"" not in dirpath:
                if entrycount == 0:
                    # Directory path to get us to staging folder
                    existing_staging_folder_path = dirpath
                # Dictionary of staging file names and their path
                existing_staging_queries_dict[file.replace("".sql"", """")] = os.path.join(dirpath, file)
                # List of current tables used in staging queries
                existing_staging_tables_list.append(file.replace("".sql"", """").replace(f""stg_{folder}__"", """"))

    # Create source yml output
    source_yml_path = f""{existing_staging_folder_path}/src__{folder}_temp_source_yaml.yml""
    actual_path_source_yml = os.path.relpath(source_yml_path, current_path)

    # Get query output
    table_column_df = query_data_pull(project_name, dataset, table_prepend, table_wildcard)

    # Need list of tables that we might want to create tables for
    unique_tables = table_column_df[""condensed_table_name""].sort_values().unique().tolist()

    # Create list of tables to iterate over--decide if we're going to do only net-new tables, or existing+net-new
    if import_type == ""new"":
        create_table_list = list(set(unique_tables).difference(existing_staging_tables_list))
    else:
        create_table_list = unique_tables

    table_count = len(create_table_list)

    if len(create_table_list) == 0:
        sys.exit(f""No tables to create, exiting script\n"")

    create_table_list = sorted(create_table_list)

    # Write source yaml to a temporary file--we don't feel like navigating yaml at the moment
    # To be replaced with yaml writing code from model schema yaml code for dynamic updates
    with open(actual_path_source_yml, ""w+"") as source_f:
        output = f""version: 2\n\nsources:\n  - name: {dataset}\n    tables:""
        source_f.write(output)
        for index, table in enumerate(create_table_list):
            if table_prepend is None:
                output = f""\n      - name: {table}""
            else:
                output = f""\n      - name: {table_prepend}_{table}""
            source_f.write(output)

    print(f""Created yaml source declaration file--go to this file and copy-paste the source declarations to your official source.yml file.\n You can find this at: \n {actual_path_source_yml}\n"")

    # Decide if we're over-writing existing staging files, or only adding in new non-existent ones
    # For table names in tables, create a SQL query and print to output file that matches table name
    # Depending on the prior logic, we will either replace existing+add new queries, or just add new
    for index, table in enumerate(create_table_list):

        # Pull in column name rows for the table we are creating a file of
        table_index = index + 1
        truncated_dataframe = table_column_df.loc[table_column_df[""condensed_table_name""] == table]
        truncated_dataframe.reset_index(drop=True, inplace=True)

        staging_file_path = f""{dbt_stg_locations_path}/staging/{datasource}/{folder}/stg_{folder}__{table}.sql""
        actual_path_stg_file = os.path.relpath(staging_file_path, current_path)

        with open(actual_path_stg_file, ""w+"") as f:
            # Start SQL SELECT command
            output = ""SELECT\n""
            f.write(output)

            # Generating field calls in main part of SELECT statement
            for row in truncated_dataframe.itertuples(index=True):

                # Identify if we need to have a comma in front of the field call
                commas = "", ""
                if row[0] == 0:
                    commas = """"

                # Pulling out column values so things are a little more human-readable
                full_table_name = row[1]
                condensed_table_name = row[2]
                column_name = row[3]
                data_type = row[4]

                # We get fields coming back that match BQ reserved keywords or is just called 'id'--need to handle these
                if column_name in bq_reserved_keywords or column_name == ""id"":
                    if data_type == ""DATETIME"":
                        output = f""{commas}TIMESTAMP({column_name}, \""UTC\"") AS {condensed_table_name}_{column_name}""
                    else:
                        output = f""{commas}{column_name} AS {condensed_table_name}_{column_name}""
                elif data_type == ""DATETIME"":
                    output = f""{commas}TIMESTAMP({column_name}, \""UTC\"") AS {column_name}""
                else:
                    output = f""{commas}{column_name}""

                f.write(f""{output}\n"")

            # Once we write all field calls, now we need to write the table call. Use a bunch of curly braces cause things are silly
            output = f""\nFROM {{{{ source('{dataset}', '{full_table_name}') }}}}""

            # Write the query
            f.write(output)

        percent_done = table_index / table_count * 100
        print(f""Processed {percent_done:.1f}% of tables"")

    # Exit script after finishing
    sys.exit(f""\nFinished updating staging files! You can find the new source YAML entries at\n {actual_path_source_yml}"")

if __name__ == ""__main__"":
    main()
```",1
4944,"pass-table-column-values-to-macro","How can we pass column values to Macro for processing.

Example - in below model, we would like to pass column (col1,col2) values to macro (concat_macro) to process using these 2 columns .

with test as
(
    select 'abc' as col1, 'xyz' as col2 union
    select 'ggg' as col1, 'xxx' as col2
)select col1,col2,
{% if execute %}
'{{ concat_macro(""col1"",""col2"") }}' as column1 from test
{% endif %}

Regards,

___________________________________________________________________
Thanks  @ [joellabes](https://discourse.getdbt.com/u/joellabes) for your reply...
Kindly find model and macro definition
___________________________________________
macro
{% macro concat_macro(col1,col2) %}
  {% set tmp = col1+col2 %}  
  {{ tmp }}

{% endmacro %}
_________________________________
Model
with test as
(
    select 'abc' as col1, 'xyz' as col2 union
    select 'ggg' as col1, 'xxx' as col2
)select col1,col2,
{% if execute %}
'{{ concat_macro(""col1"",""col2"") }}' as column1 from test
{% endif %}
____________________________________________
I tried to remove the quotes from model but its giving error like invalid identifier 'COL1COL2'

But Anyways if I just put quotes in model, out put model is giving like 
![image|690x117](upload://3M7CSCxk06LzNYB2PiiUrUkXUFq.png)

COL1     COL2    COLUMN1
abc        xyz         col1col2
ggg        xxx         col1col2

Also Can you Please help me to understand when to use Macros and when to use Snowflake UDFs
Can you Please share some link/documentation for learning macros/jinja coding.

@joellabes - Appreciate your help :slight_smile: 
Regards,",1
4944,"pass-table-column-values-to-macro","Hi @dinesh.rupchandani, the code you've posted looks basically correct, however you don't need to wrap your concat_macro call in the `'` at the start/end of the line - that will result in a quoted string like `'col1 || col2'`. With the quotes removed, I would expect your code to render as 
```
with test as
(
select â€˜abcâ€™ as col1, â€˜xyzâ€™ as col2 union
select â€˜gggâ€™ as col1, â€˜xxxâ€™ as col2
)
select col1,col2,
col1 || col2 as column1 
from test
```

If that's not what you're seeing, can you share the `concat_macro()` that you've written? Perhaps there is an issue in your macro code instead of in the model you posted. 

Or, are you asking whether there is a sample concatenation macro already written? You can use [`{{ dbt.concat() }}`](https://docs.getdbt.com/reference/dbt-jinja-functions/cross-database-macros#concat).",2
4944,"pass-table-column-values-to-macro","Hi @dinesh.rupchandani thanks for updating the post - it's easier to click the <kbd>Reply</kbd> button and add an additional post though! 

Looking at the macro you provided:
 ```
{% macro concat_macro(col1,col2) %}
    {% set tmp = col1+col2 %}
    {{ tmp }}
{% endmacro %}
```

by creating a `tmp` variable you are creating a string which is just the two inputs combined, instead of creating a command for Snowflake to concatenate the two columns.  

Since you're using Snowflake, the correct syntax for concatenation is `concat()` or `||`: https://docs.snowflake.com/en/sql-reference/functions/concat.html. This doesn't necessarily need to be a macro as it's pretty standard behaviour: 

```
select     
    'abc' as col1, 
    'def' as col2, 
    col1 || col2 as combined
```

But if you wanted to do it anyway as an experiment then this would be better: 
```
{% macro concat(val1, val2) %}
    {{ val1 }} || {{ val2 }}
{% endmacro %}
```
Note that this macro would only support two arguments, you wouldn't be able to concatenate 3 columns together (which is why [the native one linked above](https://docs.getdbt.com/reference/dbt-jinja-functions/cross-database-macros#concat) is better)

[quote=""dinesh.rupchandani, post:1, topic:4944""]
Also Can you Please help me to understand when to use Macros and when to use Snowflake UDFs
Can you Please share some link/documentation for learning macros/jinja coding.
[/quote]
We have an on-demand course for learning Jinja here 
https://courses.getdbt.com/courses/jinja-macros-packages",3
4944,"pass-table-column-values-to-macro","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4954,"what-can-i-use-to-load-monthly-updating-excel-files-to-database","Hello everyone!

Each month, we have a number of Excel files that have ~20-60K rows that need to be uploaded to the database as tables that are then used in downstream data models. They require some light data cleaning before loading in, which are currently done using Python but can be done using SQL when we stage them. I'm conflicted as to whether to use seeds for this:

Reasons against using seeds

* These files are larger than the ~1k row recommended in the seeds docs
* While these Excel files can be saved as CSVs before uploading, this sometimes leads to loss of column data type
* These files are updated monthly, which might not be ""infrequent"" as recommended in the seeds docs
* These files need to be renamed to give the proper table name, which losses the ""lineage"" of what the name of the original file was that created this data table

Reasons  for using seeds
* It would be great to version control the data files that created the tables, which seeds would allow us to do

If seeds are not the appropriate method, what would be the recommended workflow for loading Excel files into the database?

Really appreciate any thoughts on this! Thank you very much!",1
4954,"what-can-i-use-to-load-monthly-updating-excel-files-to-database","Hey @nancy.chelaru, this is a great question and you're definitely thinking about it in the right way! 

I agree that you shouldn't use seeds. When you say you need to version control the data files, what part of the version control process is most important to you? 
- If you need to track changes to the file over time, using [dbt snapshots](https://docs.getdbt.com/docs/building-a-dbt-project/snapshots) might be a good approach. 
- If you need to be able to block changes to the file, subject to a process similar to code review, we'd have to think about it a bit more and it would depend on the specific tool you ultimately choose. 

[quote=""nancy.chelaru, post:1, topic:4954""]
what would be the recommended workflow for loading Excel files into the database?
[/quote]
Which database platform and ingestion tools do you already use? Ingestion tools like Fivetran and Stitch have the ability to ingest data from a Google Sheet or a folder in Google Drive/Box/Dropbox, or to upload a file directly. They tend to need to be csv files so the issue you raised about column datatypes remains true, although [Fivetran's browser uploader](https://fivetran.com/docs/files/browser-upload) supports changing the column type inline. 

If you're using Snowflake, it also has a [web-based loader](https://docs.snowflake.com/en/user-guide/data-load-web-ui.html); BigQuery can [read from Google Drive or Google Cloud Storage](https://cloud.google.com/bigquery/docs/external-data-drive); Redshift can [copy data over from S3](https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html) (you'll probably want to use our [Redshift package](https://github.com/dbt-labs/redshift) to help with this). 

That's a lot of options and the advice is pretty generic - if you let me know what tools you're already using, we can get a bit more specific.",2
4954,"what-can-i-use-to-load-monthly-updating-excel-files-to-database","Since you say that you use Python for data cleaning, it sounds like you already have a working way to get the Excel data into Python. How do you do this in Python - do you use Pandas, PyArrow, or something else? You can probably just write the data pretty directly to your data warehouse from your Python code.",3
4954,"what-can-i-use-to-load-monthly-updating-excel-files-to-database","I have a Power Automate Desktop solution that will split large files and load this to a staging table.
From Excel, save the data as csv, From Power Automate Desktop (PAD) CSV Upload dialog, you pick the file, set the few options such as split file, create format, create stage, create table, then upload. This will create all the objects in snowflake and upload the data very quickly. Then call or create another script to do the ETL in snowflake.
It uses the snowflake cli from Windows machine.
See
[paddelia1/csvToSnowflake: CSV to Snowflake in 3 steps and 3 seconds (github.com)](https://github.com/paddelia1/csvToSnowflake)

![image|395x500](upload://u1XeYQwvs1V7XqTCu2uwyUwYWv8.jpeg)",4
4958,"equivalent-of-gkestartpodoperator-in-anthos-cloud-run-with-workload-identity-federation","Charles,

I see above that ""GKEStartPodOperator"" is used with Composer. Do you know how to make it work in Anthos Cloud Run with Workload Identity Federation? The keyfile will not be deployed with the container running on Cloud Run.

Regards,
Dan",1
4959,"select-models-from-multiple-directories","Is it possible to do all of the models in both blue folders with a structure is like this?
```
models
    colors
        blue
            some_blue.sql
        red
            some_red.sql
    new_colors
        blue
            new_blue.sql
        red
            new_red.sql
            
```",1
4959,"select-models-from-multiple-directories","@Laminator 
From the syntax that @drew suggested above, to run all files in the model in your both blue folders, it would requires 2 steps for dbt version of 0.21.0 or later.

dbt run --select colors.blue
dbt run --select new_colors.blue",2
4959,"select-models-from-multiple-directories","Jade is correct, but you could also combine them into a single command (space-separated commands in dbt will be treated as a union): 

`dbt run --select colors.blue new_colors.blue`",3
4959,"select-models-from-multiple-directories","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
4960,"use-dbt-to-deploy-changes-to-a-liquibase-table-without-dropping-and-recreating-it","Hi @fabrice.etanchaud,

do you know if there is any support in dbt to deploy changes to a tables data model without dropping and re-creating the table? I would like to have an automated comparison of the table's DLL in dbt with the existing table structure in the database that generates ""alter table add / modify / drop column"" statements.

Best regards,
Jens",1
4960,"use-dbt-to-deploy-changes-to-a-liquibase-table-without-dropping-and-recreating-it","Jens, hi.

You may want to use dummy dbt objects to have manual control over your legacy/unsupported/etc. underlying object.

"" for example if we want to manage a DBT table with the name ""not_supported_sql_server_temporal_table"" we make an auxiliary DBT model ""not_supported_sql_server_temporal_table_dbt"" and do all the DML and DDL and other stuff in hooks, even SP and exec SP. In hooks correspondingly we use [{{this.schema}}].[{{this.table!replace('_dbt','')}}] reference.  Despite having the overhead of having these dummy _dbt objects the approach delivers tremendous management relief in several areas. Previously we considered using SSDT/
redgate/flyway/liquibase as well""

https://getdbt.slack.com/archives/CMRMDDQ9W/p1614637447031700?thread_ts=1614637447.031700&cid=CMRMDDQ9W",2
4960,"use-dbt-to-deploy-changes-to-a-liquibase-table-without-dropping-and-recreating-it","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
4981,"using-grants-in-a-target-specific-way","Hello ,
Dbt version :1.2
We have 3 git branches 
develop - pointing to GCP dev project
uat - pointing to GCP uat project
prod - pointing to GCP prod project.
If we use the Grant in models/schema.yml file as below , when the models gets executed in dev,uat and prod the required access will granted in all the 3 environments.

- name: student
      description: ""student table""
      config:
        grants:
          roles/bigquery.dataViewer: ['user:someone@yourcompany.com']

I want this role to be granted in specific target  . For eg: i want above grant to be applied in uat environment only.

Any thoughts on this and how to achieve the same.

Thanks in Advance
mp",1
4981,"using-grants-in-a-target-specific-way","[quote=""minhajpasha, post:1, topic:4981""]
For eg: i want above grant to be applied in uat environment only.
[/quote]

Do you mean that there are certain users who should only have `dataViewer` access to the UAT versions of the tables?",2
4981,"using-grants-in-a-target-specific-way","Yes, User should have view access to UAT environment only. The same user should not have access to dev and prod environment even if we ran the models in dev and prod.",3
4981,"using-grants-in-a-target-specific-way","OK I spent some time looking at the documentation today and I don't think that's currently supported with the grants out of the box, so you have a few options: 
- continue to use post hooks to do grants as described in this blog post: https://docs.getdbt.com/blog/configuring-grants#is-there-still-a-place-for-hooks
- open an issue on the dbt core repo describing your use case, as it could be a future enhancement (you should do this no matter what) 
- consider creating a custom version of the macro that handles the grant logic. You can override almost all of dbt's functionality (see [Did you know? dbt ships with its own project!](https://discourse.getdbt.com/t/did-you-know-dbt-ships-with-its-own-project/764)), so if you made a custom version of the grants macro you could make it do different things based on the target. Hand-wavy but an option to consider.",4
4981,"using-grants-in-a-target-specific-way","Hello , 

Is this option possible in sql model??

{{ config(materialized='table' }}

{% if target.name == 'uat' %}
{{ config(grants = {'roles/bigquery.dataViewer': ['user:someone@yourcompany.com']}) }}
{% endif %}

select *
from ...


I think we can multiple config in sql model. Please correct me if am wrong.? if yes, think Above should should work?

Regards",5
4981,"using-grants-in-a-target-specific-way","That's a really good idea! I don't know if it would work offhand, but please give it a go and report back!",6
4981,"using-grants-in-a-target-specific-way","Hello,

It work's , I have defined it as below.
```python
{% if target.name == 'dev' %}
    {{ config(grants = {'roles/bigquery.dataViewer': ['user:someone@yourcompany.com']}) }}

{% elif target.name == 'uat' %}
    {{ config(grants = {'roles/bigquery.dataViewer': ['user:sometwo@yourcompany.com']}) }}

{% elif target.name == 'prod' %}
    {{ config(grants = {'roles/bigquery.dataViewer': ['user:somethree@yourcompany.com']}) }}
{% endif %} 
```
But we don't want this to be added in SQL Models.

We need it to be part of models/schema.yml file.

Can you please share the Git dbt core repo details so that i can raise ticket.


Regards
Minhaj Pasha",7
4981,"using-grants-in-a-target-specific-way","[quote=""minhajpasha, post:7, topic:4981""]
It workâ€™s , I have defined it as below.
[/quote]

This is good to know! Thanks for reporting back. 

[quote=""minhajpasha, post:7, topic:4981""]
But we donâ€™t want this to be added in SQL Models.

We need it to be part of models/schema.yml file.

Can you please share the Git dbt core repo details so that i can raise ticket.
[/quote]
Understood - the issues page for the Core repo is https://github.com/dbt-labs/dbt-core/issues. 

I'm going to mark your post as the solution - although it doesn't work for your use case, it might be useful for other people who come across this in the future :star2:",8
4981,"using-grants-in-a-target-specific-way","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
4990,"canceling-slim-ci-jobs-if-a-new-pr-is-made-on-the-same-codebase-or-if-a-new-commit-is-made-to-a-pr-using-github-actions","I noticed some dbt Cloud users wanted a way to cancel running CI jobs when a new PR is opened against the same code base in order to always have the latest PR changes running without having to wait for stale ones to finish.

I created a [Github Action](https://github.com/marketplace/actions/dbt-cloud-cancel-running-ci-job-runs-action) that checks if a dbt cloud CI job (based on job ID) is running after a new or updated PR commit is made. If there is a job running or there are job runs that are queued - this action will cancel these running and queued runs since they are likely stale as a new commit to the PR has been made. This allows to always have the newest code commit running in the CI job without having to wait for the stale job runs to finish.

Loom video on using this Github Action can be found [HERE](https://www.loom.com/share/f2f10e61d40d4d3c9fc2e06abdbf8207)",1
4990,"canceling-slim-ci-jobs-if-a-new-pr-is-made-on-the-same-codebase-or-if-a-new-commit-is-made-to-a-pr-using-github-actions","This is really cool! I love that the actions page also has a bunch of example workflows, including interactions with other actions :star-struck:

_Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1664842104481339?thread_ts=1664816099.844239&cid=CCKAGDE0M). It might not have transferred perfectly._",2
4990,"canceling-slim-ci-jobs-if-a-new-pr-is-made-on-the-same-codebase-or-if-a-new-commit-is-made-to-a-pr-using-github-actions","You are my personal hero.",3
4999,"dbt-cloud-cannot-reference-to-first-data-model","Hi,
I'm struggling with getting the second data model query to read the output from the first data model file.
The first data model sql ran without issue; the only parameter I used was ```{{ config(materialized='table') }}``` no alias assigned. 
But when I tried 
```
select *
from {{ ref('my_first_dbt_model') }}
where id = 1
```

I got 
```
Server error: Database Error in rpc request (from remote system)
Could not find parent table for alias ""segment.dbt_pfan.my_first_dbt_model"".
```

I tried assigning the first data model sql an alias and reference that in the second file, but still doesn't work.

Please help?",1
4999,"dbt-cloud-cannot-reference-to-first-data-model","Hey @pfan, can you post the full content of your two models? 

I don't think this is the error message I'd expect, but just checking that you saved the first model before running `dbt run`? (this caught me out when I first started using dbt Cloud)",2
4999,"dbt-cloud-cannot-reference-to-first-data-model","By the way - the error you're seeing is from your data warehouse (I think it's Redshift?) not dbt itself. Could you also check whether the table/view `segment.dbt_pfan.my_first_dbt_model` exists in the warehouse?

If it does exist, then it could be a permissions error for your dbt user",3
4999,"dbt-cloud-cannot-reference-to-first-data-model","Hi,
Completely new to dbt here.
How do I know if the first model has been saved?

And also how do I check if `segment.dbt_pfan.my_first_dbt_model` exists in the warehouse?

I don't see warehouse as an option on the left panel

![Screen Shot 2022-10-05 at 12.50.33 PM|690x349](upload://3bd1zFKOwbkk1HO2HXL0pRQPrFq.png)",4
4999,"dbt-cloud-cannot-reference-to-first-data-model","[quote=""pfan, post:4, topic:4999""]
How do I know if the first model has been saved?
[/quote]

If it's unsaved, there would be a blue-green dot beside the file name, like this. Your screenshot makes it look like you have saved :+1:  
![image|690x183](upload://yv6kk83VDXT9mRMYUC8s5dPkeef.png)

[quote=""pfan, post:4, topic:4999""]
how do I check if `segment.dbt_pfan.my_first_dbt_model` exists in the warehouse?
[/quote]
Do you have access to another database query tool, e.g. DataGrip or even the [AWS Redshift Console](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor.html)? If so, you can check there. The reason I suggest these is that if you have a more powerful account you can use to access those then you can confirm whether the model exists even if your dbt user doesnâ€™t have access.

My other suggestion is that I noticed in your screenshot you have a lot of custom code. It might be worth going back to the default code ([you can find it here if you need it](https://github.com/dbt-labs/dbt-starter-project)) and making sure that works before continuing on. That way you can check whether the problem is your database connection etc or a mistake in your model itself.",5
4999,"dbt-cloud-cannot-reference-to-first-data-model","Hi, Thank you for the reply.

Well I wanted to test this with my own DB, so the query is pulling data from my data warehouse and to see if I can easily create a materialized view.

I'm not sure how to know this is feasible at this point.",6
4999,"dbt-cloud-cannot-reference-to-first-data-model","[quote=""pfan, post:6, topic:4999""]
so the query is pulling data from my data warehouse
[/quote]

Does this mean you managed to make it work? I'm unclear on whether you still need help. If your connection is still not working, I recommend you reach out to the dbt Cloud support team using the chat bubble in-app, as they will have the ability to review logs etc to see what's happening. 

[quote=""pfan, post:6, topic:4999""]
see if I can easily create a materialized view.
[/quote]
dbt does not currently have native support for materialized views. The different warehouses/data platforms don't have consistent implementations for materialized views yet so it's not practical for us to build an abstraction over them. There is a [sample package here with demo code](https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/materialized-views), but it's not something I'd look into until I was already more familiar with dbt.",7
4999,"dbt-cloud-cannot-reference-to-first-data-model","[quote=""joellabes, post:7, topic:4999""]
sample package here with demo code
[/quote]

So I was able to pull data from my redshift database with the first `my_first_dbt_model.sql`. It returns a list of user_ids.
However, when I tried to refer to the output from that sql file, that's why I got the error I showed in my original message.",8
4999,"dbt-cloud-cannot-reference-to-first-data-model","[quote=""pfan, post:8, topic:4999""]
refer to the output from that sql file
[/quote]

This sounds like you tried to use the `user_id`s directly in a next query, something like this 

```sql
--This won't work
select * 
from {{ source('schema', 'table') }}
where user_id in ({{ ref('my_first_dbt_model') }}
```

Am I understanding you correctly? If so, that won't work - `ref` returns the full table name, not the list of data it contains.

If not, can you please post the full code of `my_first_dbt_model.sql` and whatever second model you were writing when you got the error message? You can post code by typing three backticks (```) at the start and end of a code block.",9
4999,"dbt-cloud-cannot-reference-to-first-data-model","`my_first_dbt_model.sql`:
```
{{ config(materialized='table') }}
select distinct user_id 
from public.pages
```

`my_second_dbt_model.sql`:
```
select *
from {{ ref('my_first_dbt_model') }}
where user_id = 1
```
So I think my understanding of what `{{ ref('my_first_dbt_model') }}` is wrong. Because I did think it would return the output of the query from the first model. If it is simply returning the `name` of the first dbt model, which isn't a materialized view in redshift, that explains why the second query failed because it doesn't exist in my database.

So I'm a little confused as to how to use dbt models and build materialized views.
My objective is using dbt to create intermediate tables that are easier to query than using a long and complex query to pull data directly.

Thank you so much for your help thus far!",10
4999,"dbt-cloud-cannot-reference-to-first-data-model","Those two queries are correctly written (the first one should really use a [source](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources) to reference `public.pages`, but we can set that aside for a moment). 

When you invoke `dbt run`, it should compile the `ref`s into actual table names and run code along the lines of 

```
create table something.something.my_first_dbt_model as (
  select distinct user_id 
  from public.pages
)
```
and 
```
create or replace view something.something.my_second_dbt_model as (
  select *   
  from something.something.my_first_dbt_model
  where user_id = 1
)
```

You'll see that the second one doesn't return the _output_ of the first query per se (i.e. it's not returning an array of `[1, 2, 3, 4, ... 99999]`), just the name of the first model. This does let you build intermediate tables that are easier to query, so you're on the right track conceptually.

Have you gone through the [getting started tutorial](https://docs.getdbt.com/guides/getting-started)? It looks like you might have dived straight into trying to build your own project given that you're referencing a custom table (`public.pages`). The tutorial covers all the best practices and has Redshift-specific guidance.",11
4999,"dbt-cloud-cannot-reference-to-first-data-model","Ah I didn't run `dbt run` on `my_first_dbt_model.sql` so that's why it didn't work.
After I did that, I was able to run `my_second_dbt_model.sql`:
```
select *
from public.pages
where user_id IN
(select user_id from {{ ref('my_first_dbt_model') }})
limit 10
```
I was able to run this query using `compiled SQL` and it returns result as expected.

But when I ran `dbt run` on this, it shows an error:
```
2022-10-10T23:11:00.314388Z: 23:11:00  On model.my_new_project.my_second_dbt_model: /* {""app"": ""dbt"", ""dbt_version"": ""1.2.2"", ""profile_name"": ""user"", ""target_name"": ""default"", ""node_id"": ""model.my_new_project.my_second_dbt_model""} */


  create view ""segment"".""dbt_pfan"".""my_second_dbt_model__dbt_tmp"" as (
    -- Use the `ref` function to select from other models

select *
from ""segment"".""dbt_pfan"".""my_first_dbt_model""
where id = 1
  ) ;

2022-10-10T23:11:00.320278Z: 23:11:00  Postgres adapter: Postgres error: column ""id"" does not exist in 
my_first_dbt_model
```

I'm not sure what is the issue here?",12
4999,"dbt-cloud-cannot-reference-to-first-data-model","Hurrah!!! :tada: :tada: :tada: :tada: 

It looks like you might not have saved `my_second_dbt_model` before running - the compile step in dbt Cloud doesn't need you to save first, but doing a `dbt run` does only look at work you've saved. (It's a common pain point, and we're looking into ways to make this clearer on the revamped IDE we're building at the moment).",13
4999,"dbt-cloud-cannot-reference-to-first-data-model","Thank you! So I need to save the model.sql file before executing `dbt run`. 
One more question - for the tables created from the `my_first_dbt_model.sql` and `my_second_dbt_model.sql`files, are they temporary tables or materialized views in redshift? If other people want to query those tables from another BI tool, can they? and I supposed they can be refreshed via running the models at a scheduled cadence?",14
4999,"dbt-cloud-cannot-reference-to-first-data-model","[quote=""pfan, post:14, topic:4999""]
are they temporary tables or materialized views in redshift?
[/quote]
By default, they're built as standard views (note: *not* materialized views). You can also change them to build as permanent tables. Check out https://docs.getdbt.com/docs/building-a-dbt-project/building-models/materializations for more info. 

[quote=""pfan, post:14, topic:4999""]
If other people want to query those tables from another BI tool, can they?
[/quote]
Yes, but you'll need to make sure the database user account you make for your BI tool has access to the tables. Check out https://www.getdbt.com/blog/five-principles-that-will-keep-your-data-warehouse-organized/ 

[quote=""pfan, post:14, topic:4999""]
I supposed they can be refreshed via running the models at a scheduled cadence?
[/quote]
Absolutely! dbt Cloud comes with a schedule runner built in, since you're already using the IDE you have access to it: [Schedule a job](https://docs.getdbt.com/guides/getting-started/building-your-first-project/schedule-a-job) and [Running dbt in production](https://docs.getdbt.com/docs/running-a-dbt-project/running-dbt-in-production)",15
4999,"dbt-cloud-cannot-reference-to-first-data-model","with dbt core, can I schedule dbt run jobs? or is it only available in dbt cloud and I have to look for a workflow orchestrator to run the jobs?",16
4999,"dbt-cloud-cannot-reference-to-first-data-model","dbt Core doesn't come with its own scheduler, so you'll need a separate workflow orchestration tool. We list some options here https://docs.getdbt.com/docs/deploy/deployments 

You're using the dbt Cloud IDE though, so you have access to dbt Cloud. Why would you not use its scheduler?",17
4999,"dbt-cloud-cannot-reference-to-first-data-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",18
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","
![Wrong_code|485x281](upload://ndOptwTtxgiUQphQb0nug50YXcL.png)

In the correct_code file, I am just hard coding the macro name under sql_text.

But in the wrong_code file, I am trying to pass the macro name to a variable and then using the variable name with the select command under sql_text block. So it should ne read as a macro (which is : select {{poc1_macro()) instead it is reading the whole thing as a variable itself and throwing an error:
```
syntax error line 1 at position 8 unexpected '{'.
  syntax error line 1 at position 19 unexpected '('.
```",1
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","Hey @ashishk9, let me check I'm understanding you correctly: 
You want to dynamically call a macro, by taking the name of a macro and surrounding it with curly braces (and parentheses for an empty function signature)? 

This won't work because dbt only goes through one rendering pass - as you've seen, the compiled result will just be the string `{{poc1_macro()}}` instead of whatever `poc1_macro` does. 

Can you explain why you're trying to do this? There might be a different approach you can take.",2
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","Hi @joellabes, I have three macros one which inserts data, one updates the table, and the last one is the main macro where I want to pass any of the two macros name as a parameter at a time. So that it performs only task which is insert or update.",3
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","dbt itself takes responsibility for doing inserts and updates, you shouldn't need to be writing macros to be executing DDL etc. 

Have you gone through the [getting started tutorial](https://docs.getdbt.com/guides/getting-started) or [read the dbt introduction](https://docs.getdbt.com/docs/introduction)? It may be helpful to get an indication of what dbt can do for you",4
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","I have gone through dbt fundamentals. My requirement here is to see whether I can call a macro in another macro or not? insert or update is just an option not my end result.

But when I try to call the macro as parameter, it reads that as a variable.",5
5002,"i-am-facing-issue-when-trying-to-call-a-macro-as-a-parameter-and-then-passing-it-to-a-variable-can-anyone-guide-me-where-i-am-going-wrong","You can call macros from other macros, but it can't be totally dynamic. 

For example you could do 
```python 
{% macro macro_1() %}
  {{ macro_2('hello') }}
{% endmacro %}

# in a separate file 
{% macro macro_2(msg) %}
  {{ msg }}
{% endmacro %}
```

and call macro_1 from a sql file. But you can't set which macro you want to call in a variable or pass that in as another argument. 

That's why I want to know your actual goal, so we can find an approach that *does* work.",6
5007,"package-installation-fails-on-windows-when-directory-paths-are-very-long","Posting for awareness in case anyone else runs into a similar issue.

**Problem**
Environment summary:
* Azure DevOps Self-Hosted Agent
* Windows Server 2019
* Python 3.9.10 and 3.10
* dbt_utils v0.9.2
* Running locally with no Python virtual environment (works fine in venv and local developer machines)
* Running `dbt deps` command using PowerShell 2.0

Error below occurs on agent when attempting to install dbt packages.

```
09:08:52  Running with dbt=1.2.1
09:08:52  Installing dbt-labs/dbt_utils
09:08:53  Encountered an error:
[Errno 2] No such file or directory: 'dbt_packages\\dbt-utils-0.9.2\\integration_tests\\data\\schema_tests\\data_test_mutually_exclusive_ranges_with_gaps_zero_length.csv'
09:08:53  Traceback (most recent call last):
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\main.py"", line 129, in main
    results, succeeded = handle_and_check(args)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\main.py"", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\main.py"", line 238, in run_from_args
    results = task.run()
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\task\deps.py"", line 67, in run
    package.install(self.config, renderer)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\deps\registry.py"", line 74, in install
    connection_exception_retry(download_untar_fn, 5)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\utils.py"", line 616, in _connection_exception_retry
    return fn()
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\deps\registry.py"", line 86, in download_and_untar
    system.untar_package(tar_path, deps_path, package_name)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\site-packages\dbt\clients\system.py"", line 478, in untar_package
    tarball.extractall(dest_dir)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\tarfile.py"", line 2040, in extractall
    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\tarfile.py"", line 2081, in extract
    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\tarfile.py"", line 2154, in _extract_member
    self.makefile(tarinfo, targetpath)
  File ""D:\AzureDevOps\_work\_tool\Python\3.10.0\x64\lib\tarfile.py"", line 2195, in makefile
    with bltn_open(targetpath, ""wb"") as target:
FileNotFoundError: [Errno 2] No such file or directory: 'dbt_packages\\dbt-utils-0.9.2\\integration_tests\\data\\schema_tests\\data_test_mutually_exclusive_ranges_with_gaps_zero_length.csv'
```",1
5007,"package-installation-fails-on-windows-when-directory-paths-are-very-long","**Solution**

The error may be caused by Windows not allowing long file paths by default. You can resolve this on the agent with the below PowerShell command.
`Set-ItemProperty -Path HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem -Name LongPathsEnabled -Value 1;`",2
5007,"package-installation-fails-on-windows-when-directory-paths-are-very-long","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5009,"run-selected-code-only-in-dbt-cloud-ide","It appears in dbt cloud using scratchpad
there are some issues
1) selecting a portion of a query it just runs whathever is on the pad not only the selected one

![image|690x314](upload://seukTQeuUdYFpCgR2rjv4PoDtJO.png)

2) if you use a jinja macro and use  -- to comment it out still it's parsed in compile and it screw the query",1
5009,"run-selected-code-only-in-dbt-cloud-ide","Hey @obar1! Thanks for the feedback. 

[quote=""obar1, post:1, topic:5009""]
1. selecting a portion of a query it just runs whathever is on the pad not only the selected one
[/quote]
Our rebuilt IDE, currently in beta, resolves this! You can sign up to join the beta here https://docs.getdbt.com/docs/dbt-cloud/cloud-ide/ide-beta

[quote=""obar1, post:1, topic:5009""]
2. if you use a jinja macro and use â€“ to comment it out still itâ€™s parsed in compile and it screw the query
[/quote]
You can't comment out a Jinja command with a SQL `--` comment. See this:
https://discourse.getdbt.com/t/why-does-jinja-parse-the-commented-out-macros-in-sql-queries/4708/2",2
5009,"run-selected-code-only-in-dbt-cloud-ide","I was aware about the jinja comment syntax
hope the nex ide will be smart to handle automatically that 

ex
select col1
from {{ref('table_a')}}

/*
select col1
from {{ref('table_a')}}
*/

your workaround would be

{#
select col1
from {{ref('table_a')}}
#}",3
5009,"run-selected-code-only-in-dbt-cloud-ide","It's not really a ""smartness"" thing - Jinja comments and SQL comments are useful in different contexts. Sometimes you want to include the result of a commented out jinja statement in your sql, e.g. [forcing a dependency in `ref`](https://docs.getdbt.com/reference/dbt-jinja-functions/ref#forcing-dependencies) or something like this:

```sql
--table created at {{ modules.datetime.datetime.now() }}

select * 
from {{ ref('my_model') }}
```

It is a bit confusing that there's two comment styles, but it's because there's actually two languages in use. 

FWIW, if you did 
```sql
/*
select col1
from {{ref(â€˜table_aâ€™)}}
*/
```

then you'd be OK as long as the ref resolved. If the model doesn't exist then you will get an error. 

Compare these two:
```sql
select id,
--{{ dbt_utils.pivot() }}   --this won't work
/*{{ dbt_utils.pivot() }}*/ --this will
from {{ ref('my_model') }}
```
The former won't work because only the first line of _compiled_ SQL will be commented out, which is multiple lines. Whereas the block comment will comment out the multiple lines of generated SQL.",4
5009,"run-selected-code-only-in-dbt-cloud-ide","I think /* will be my std */ 
it helps with ctes in cascade to  break down and rerun things by piece ...",5
5009,"run-selected-code-only-in-dbt-cloud-ide","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5016,"dbt-cloud-permanent-link-to-the-latest-project-documentation-after-each-dbt-run","Hello,
Each time dbt jobs are rerun on dbt Cloud, project documentation is updated. However, to access that documentation, you have to go each time via login page. It is because on each dbt job run documentation URL changes with different run ID. 
Example URL which keeps changing
```
https://cloud.storebrand.getdbt.com/accounts/1/jobs/26/docs/#!/overview/sam
```

It would be nice to have permanent URL which always points to the latest version of project documentation in dbt cloud. This would allow to permanently link to model documentation from outside sources, like wiki pages, intranet resources.
Good example is GitHub URL to download latest software release. Internally its a new commit, but that latest version is accessible via permanent URL:
```
https://github.com/dbt-labs/dbt-core/releases/latest
```",1
5016,"dbt-cloud-permanent-link-to-the-latest-project-documentation-after-each-dbt-run","Hey @scanbix, 

There's two ways you can get a dbt Docs URL from dbt Cloud: 
1. A run-specific link, like this: `https://cloud.getdbt.com/accounts/1/runs/87163747/docs`
2. A job-specific link, like this: `https://cloud.getdbt.com/accounts/1/jobs/940/docs`

The first one does change from run to run, but the second stays static (unless you change which job is responsible for generating documentation [in your Artifacts settings](https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/artifacts#create-dbt-cloud-artifacts)). 

What's surprising to me is that the example link you provide _is_ a job-focused one, so I'm not sure why you're seeing that the run ID keeps changing. Could you double-check that you pasted the right type of link?",2
5016,"dbt-cloud-permanent-link-to-the-latest-project-documentation-after-each-dbt-run","Hello,

Your explanation might resolve this small problem for now. Until the job, refreshing documentation is recreated/redeployed.
However, I would like put an improvement, that each project would have permanent, user-friendly like in GitHub URL to the latest documentation independent of job ID ir run ID.",3
5016,"dbt-cloud-permanent-link-to-the-latest-project-documentation-after-each-dbt-run","Agreed! I'll bump that suggestion internally.",4
5016,"dbt-cloud-permanent-link-to-the-latest-project-documentation-after-each-dbt-run","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5017,"running-two-dbt-cloud-jobs-back-to-back","I have job1 and job2
I need job1 to  be scheduled and it takes 3H to finish
after job1 completes I want job2 to  be executed and it take 2H

if I just schedule them and they run in parallel I have problem on TABLE looking

how can I do that in dbt cloud without external scheduler?

job1 -> job2 -> job1 etc for ever",1
5017,"running-two-dbt-cloud-jobs-back-to-back","Hey @obar1, dbt cloud jobs don't have dependencies between one another by default. We rely on the dependency graph that dbt builds for itself internally to make sure things run in the correct order and without conflict.  

Why do they need to be two jobs? Could you add the steps from job 2 into job 1's definition? 

Before `dbt build` was released, my old team did have a job for snapshots which ran separately to the rest of the project. We only ran it once a day and scheduled it far enough in advance of the other runs that they never overlapped, but it doesn't sound like that solves your use case. 

I'd be interested to see the specific problem you're trying to solve",2
5017,"running-two-dbt-cloud-jobs-back-to-back","hi  @joellabes  I have 1 model that is big let's call it  *big_job* so the idea was to split the original  1 job we had in 2 jobs like so

JOB_Y
```
dbt build --select +big_job+
```

and another
JOB_OTHERS
```
dbt build --exclude +big_job+
```

but there are issues with the intermediate layer now:
as JOB_OTHERS will not materialize the intermediate that in the lineage graph are ancestor on  big_job (exclude cmd) so  I need to run first JOB_Y to build them and then JOB_OTHERS 
Having 1 in all take too much time ...  to get some big_job done and we cannot increate AWS spending for new :stuck_out_tongue: so in this way we prioritize big_job over the rest ...
any suggestion how to do it better ?",3
5017,"running-two-dbt-cloud-jobs-back-to-back","I'm a bit confused - you said that having them all in a single job takes too much time, and so you want to prioritise the `big_job` over the rest, but in your original message you asked for them to run in series. Splitting it out won't reduce the total time to run if you're only going to start job 2 after job 1 is complete anyway. 

[quote=""obar1, post:1, topic:5017""]
job1 â†’ job2 â†’ job1 etc for ever
[/quote]

How many threads do you have in your job definition? You said AWS spend so I assume you're using Redshift. Too many threads in Redshift can lead to resource contention and the job actually running *slower*. I would aim for 4 (maybe try with up to 8 and see where you get a decreasing return). 

You might be able to create a single job that contains several steps, something like this: 
```
dbt build -s +big_job --exclude big_job --threads 4
dbt build -s big_job --threads 1
dbt build -s big_job+ --exclude big_job --threads 4
```

The one thing to keep in mind here is that any nodes that aren't related to `big_job` won't be built using this selector. If that's a problem, you might be able to express it with a [YAML selector](https://docs.getdbt.com/reference/node-selection/yaml-selectors) but I haven't spent enough time using those to have any useful guidance.",4
5017,"running-two-dbt-cloud-jobs-back-to-back","**tagging** can be handy as well when I have more the once big job :)",5
5017,"running-two-dbt-cloud-jobs-back-to-back","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5019,"dynamically-create-models-using-schema-files","Hi, 
We have a requirement to create models (sql) dynamically using schema files. We have like 10 schema files and want to generate models using those schema files. These schema files keep changing. I tried using generate_base_model but it works for analyses and it look for DB tables. Any suggestion to handle this scenario.",1
5019,"dynamically-create-models-using-schema-files","Can you post an example of the schema file you're trying to input, and the sort of output you would like to see?

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1665090191585469?thread_ts=1665058505.567239&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",2
5019,"dynamically-create-models-using-schema-files","Hi, 
Below is the example version 1 *person.json* schema file in aws s3 location (assuming in some location where we maintain all schemas)
```
{
  ""$schema"": ""http://json-schema.org/draft-04/schema#"",
  ""type"": ""object"",
  ""properties"": {
    ""firstName"": {
      ""type"": ""string""
    },
    ""lastName"": {
      ""type"": ""string""
    },
    ""age"": {
      ""type"": ""integer""
    }
  },
  ""required"": [
    ""firstName"",
    ""lastName"",
    ""age""
  ]
}
```
I want to create a model dynamically person.sql like below
```
with source as 
(
 select * from person
)

select   
    firstName,
    lastName,
    age
from source
```
In future schema may change to version v2 like below (adding dob column)
```
{
  ""$schema"": ""http://json-schema.org/draft-04/schema#"",
  ""type"": ""object"",
  ""properties"": {
    ""firstName"": {
      ""type"": ""string""
    },
    ""lastName"": {
      ""type"": ""string""
    },
    ""age"": {
      ""type"": ""integer""
    },
    ""dob"": {
      ""type"": ""string""
    }
  },
  ""required"": [
    ""firstName"",
    ""lastName"",
    ""age"",
    ""dob""
  ]
}
```
I want to recreate a model dynamically person.sql like below (with dob column)  
```
with source as 
(
 select * from person
)

select   
    firstName,
    lastName,
    age, 
    dob
from source
```",3
5019,"dynamically-create-models-using-schema-files","OK that makes sense! The first problem you're going to have is that dbt runs inside your warehouse, and can only do things that you could do in straight SQL. 

Which warehouse are you using? You're going to need some sort of function that can ingest the file from S3 and make it available in SQL. Perhaps a UDF that returns a string of json?

If you can do that, then you should be able to parse a json blob relatively easily and create a model using jinja templating. You'd need to make a file for each model you want to generate. 

Once you've done that, you will still have the problem that you won't be able to automatically attach tests to the models because yaml files can't be dynamically generated. 

Honestly if you only have 10 files then it sounds like a lot more effort than it's worth for it to be 100% dynamic. How often do they change that the additional functionality is worth the overhead in complexity?

If you really want to go down this path though, an alternative approach might be to template the sql and yaml files in Python and then run dbt as normal on the results of that work.",4
5019,"dynamically-create-models-using-schema-files","Thank you for your input. 
We are using snowflake warehouse. We have multiple customers and Schema files are slightly different for each, roughly 10 - 12 schema files for each customers. These schemas will change once in six months or even more depend on customers. So thought of taking dynamic model creation approach.",5
5019,"dynamically-create-models-using-schema-files","OK - dbt requires a `.sql` file for each model that you want to have materialized in your database, so you have two options:

â€¢ If each customer needs their own set of tables (`customer_1/person.sql`, `customer_2/person.sql`...), you'd want some sort of codegen tool to make the template files that each reference a macro. Up to you whether you dynamically generate those from scratch at the start of each run, or just re-trigger them when you get a new customer or their schema changes. This can't be done directly in dbt.
â€¢ If your requirements allow a single table with each customer's data unioned together (`person.sql`, `'address.sql`...), you could check out <https://discourse.getdbt.com/t/unioning-identically-structured-data-sources/921/1>

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1665336555490129?thread_ts=1665126780.782259&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",6
5019,"dynamically-create-models-using-schema-files","We have thousands of customers and planning to load data to in separate databases.

We are seeing below options now:
- Have one project for each customer with their own schemas and loading data to separate DB and tables.
- Have one project for all customers with similar schema",7
5019,"dynamically-create-models-using-schema-files","I'll be interested to hear what you ultimately decide on!",8
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","Hi,

Can we actually alter warehouse size in snowflake from dbt based on the table count('XSMALL', 'LARGE', etc...) ?. If yes then should I make changes in dbt_project.yml or need to write a macro code.


Thanks in advance.",1
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","You can change which warehouse size you use, see this package for an example: https://github.com/Montreal-Analytics/dbt-snowflake-utils#snowflake_utilswarehouse_size-source


[quote=""ashishk9, post:1, topic:5020""]
based on the table count
[/quote]

Note sure what you mean by this though?",2
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","Hi @joellabes, I don't want to use snowflake_utils library for altering warehouse size. Is there any alternative to it other than snowflake_utils library!!!?",3
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","[quote=""ashishk9, post:3, topic:5020""]
donâ€™t want to issue to use snowflake_utils library for altering warehouse size
[/quote]

Why not? dbt packages don't have the same dependency issues as Python libraries etc - they are just a reference to a set of normal dbt macros. See https://docs.getdbt.com/docs/building-a-dbt-project/package-management for more info.


[quote=""ashishk9, post:3, topic:5020""]
Is there any alternative to it other than snowflake_utils library
[/quote]

You could make a copy of the macro from the package and incorporate it into your own project, or use it as inspiration for your own macro.",4
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","I don't want to use it because of data theft reasons. The client has denied using snowflake_utils library. So I am searching for an alternate solution.",5
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","OK then the code is open source so I would recommend making a copy of it - you can independently verify its behaviour and then you will control its provenance by keeping it in your client's project.",6
5020,"can-we-alter-warehouse-size-in-snowflake-from-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
5027,"should-dbt-run-its-tests-against-the-tmp-table-before-committing","using test is a must  but looking into  the debug log of the jobs  where  I added some tests to some models in the related yml I see something I would like to have confirmation on:
```
create  table
    ""dwh"".""data"".""traffic__dbt_tmp""
...
alter table ""dwh"".""data"".""traffic__dbt_tmp"" rename to ""traffic""
...
test.dwh.not_null_traffic_user_id
```
why does not do test on the tmp and exit in case of failure but it does on the final model?",1
5027,"should-dbt-run-its-tests-against-the-tmp-table-before-committing","@obar1 what command did you use to invoke dbt? 

If you used `dbt run`, then it will create the models without testing them. If you used `dbt build`, it will create the models and then test them, and if they fail they will prevent downstream models from being built. 

Running tests against the temp model and only committing if they pass is an interesting idea; I suspect the answer to your question is that the majority of dbt's models are views instead of tables so don't go through this interstitial state. 

As noted above, `dbt build` will stop downstream models from being built if the test fails, but that still means that the failing model is in prod. 

From what you're describing, you might want to look at [Performing a blue/green deploy of your dbt project on Snowflake](https://discourse.getdbt.com/t/performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake/1349); a blue/green deploy is where you ""swap out"" the production version of your tables only after all the tests have passed.",2
5027,"should-dbt-run-its-tests-against-the-tmp-table-before-committing","Coming back to this - check out https://github.com/dbt-labs/dbt-core/discussions/5687 for the discussion around whether this is added to dbt Core in the future!",3
5027,"should-dbt-run-its-tests-against-the-tmp-table-before-committing","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5028,"dbt-debug-log-analysis","any tip/tool to get some insight from the log details
![image|690x289](upload://1LeYEifOqvNjhAMIhrUHLsn4NWo.png)

ex 
- longest running one
etc 

yes I can parse with some bash script or even import somehow in sqlite and play with it .. open to suggestion",1
5028,"dbt-debug-log-analysis","Hey @obar1, have you seen the [dbt_artifacts](https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest/) package on the dbt Package Hub? It contains macros which will materialise your job timing etc, including [how long it takes for a model to build](https://brooklyn-data.github.io/dbt_artifacts/#!/model/model.dbt_artifacts.fct_dbt__model_executions). 

Also, if you're using dbt Cloud, you can use the [Model timing tab](https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-model-timing-tab) to visualise your longest-running nodes.",2
5028,"dbt-debug-log-analysis","cool and handy feature would be adding to the  model timing tab a export log kind of button to have it as text ... I need to mouse over to see details as it is :P thanks",3
5028,"dbt-debug-log-analysis","hi @joellabes have you seen this [Using dbt artifacts to track project performance - Show and Tell - dbt Community Forum (getdbt.com)](https://discourse.getdbt.com/t/using-dbt-artifacts-to-track-project-performance/1873) playing with `run_results.json` looks a practical solution",4
5028,"dbt-debug-log-analysis","I have! The first version of dbt_artifacts parsed run_results.json (which is how it got its name) but it changed to using the run's context variables as its data source so it was usable by other warehouses than just Snowflake",5
5031,"subquery-containing-correlated-aggregate-function-can-only-appear-in-having-or-select-clause","If you are just diving into doing incrementals using Snowflake as your DB of choice, here is a quick learning that I recently had (thank you @jerco for the assist).

If you copy and paste the example code from the [incremental models page](https://next.docs.getdbt.com/docs/build/incremental-models) , then you are likely to get this error message.

The feedback from dbt is that snowflake is struggling with the potential recursive relationship here of a field having the same name both inside and outside of that correlated aggregate function.

the solve --> aliases

from : 
```
where date_day >= (select max(date_day) from {{ this }})
```

to:
```
where *[main query table alias]*.date_day >= (select max(this.date_day) from {{ this }} as this)
```

Best of Luck !",1
5031,"subquery-containing-correlated-aggregate-function-can-only-appear-in-having-or-select-clause","That's a tricky one! Thanks for sharing ðŸŒŸ",2
5031,"subquery-containing-correlated-aggregate-function-can-only-appear-in-having-or-select-clause","This saved me! Thank you!",3
5040,"how-to-design-and-structure-metrics","Based on the blog post released during Coalesce, this discourse Topic will serve as the central hub for feedback on how to design and structure your metrics! It is our hope that this Topic will continue until we feel that we have enough information and consensus gathered to convert the blog into a Guide of its own.",1
5040,"how-to-design-and-structure-metrics","My team is using dbt in sync with Metabase and Metrics are one big feature we are all waiting for",2
5040,"how-to-design-and-structure-metrics","One use case that I am interested in would be to define derived metrics [like here](https://docs.getdbt.com/blog/how-to-design-and-structure-metrics#option-1-putting-everything-in-the-semantic-layer) where the component-metrics can be of a different time grain. 

**Example:**
I would like to defined a metrics that calculates user stickiness as *WAU/MAU*.
I imagine something like this:

```
metrics:
  - name: active_users
    label: Active Users
    model: ref('fct_sign_ins')
    description: ""The number of active users""
    calculation_method: count_distinct
    expression: dim_user_id

    timestamp: dim_date_day
    time_grains: [day, week, month, year]

    dimensions:
        - user_role
        - organization_type

  - name: stickiness
    label: stickiness
    description: ""The stickiness as defined as WAU/ MAU""
    calculation_method: expression
    expression: ""{{ metric('active_users(time_grain=week)')  / metric('active_users(time_grain=month)') }}""

    time_grains: [month]

    dimensions:
        - user_role
        - organization_type

```

I realise this brings about some difficulties involved with something like this (like there is not really the possibility for time_grains in the derived metric).

But do you reckon metric definitions like this would become possible in the future?",3
5040,"how-to-design-and-structure-metrics","Super helpful article team! We were waiting for this kind of best practices to validate our implementation and start spreading the metrics within our company.

One point that would suggest is to have something in between the single metrics file, and the one metric per file. Our team is going to have a yaml file for each mart, within the metrics folder. The rationale behind it is that this folder could end with a huge number of files, leading to some difficulties to find related/derived metrics. In the way we implement it, we actually need to associate a metric with a business unit, keeping the ownership easy to track, but maintaining the folder neat. 

Our setup is something like:

```
- models
     - marts
        - metrics
            core.yml
            finance.yml
            marketing.yml
```",4
5042,"profiles-dir-fails-for-dbt-test-and-dbt-compile","I have a `profiles/profiles.yml` under my dbt project. I am able to do `dbt run --profiles-dir ./profiles`, it'll successfully create tables at the specified location. However, when I run `dbt test --profiles-dir ./profiles` or `dbt compile --profiles-dir ./profiles`, it fails with the following error message.
```
14:33:58  Encountered an error while reading the project:
14:33:58    ERROR: Runtime Error
  Could not find profile named 'avr_heor'
```
I also tried running `dbt debug --profiles-dir ./profiles`, everything is ok. 

My dbt version:
```
Core:
  - installed: 1.2.1
  - latest:    1.2.2 - Update available!

  Your version of dbt-core is out of date!
  You can find instructions for upgrading here:
  https://docs.getdbt.com/docs/installation

Plugins:
  - bigquery:  1.2.0 - Up to date!
  - snowflake: 1.2.0 - Up to date!
  - redshift:  1.2.1 - Up to date!
  - postgres:  1.2.1 - Update available!
```",1
5042,"profiles-dir-fails-for-dbt-test-and-dbt-compile","I'm not sure - the fact that it works in run and debug but not other commands is very strange. I'd suggest opening an issue about this: https://github.com/dbt-labs/dbt-core/issues",2
5051,"how-do-i-temporarily-make-an-archive-copy-of-a-schema","Thanks @joellabes .

For what concerns option 1: what is your suggested approach to temporarily archive a schema we'd like to keep for disaster recovery as we test for Blue / Green Deployment?

I couldn't find a post specifically on this topic so far.",1
5051,"how-do-i-temporarily-make-an-archive-copy-of-a-schema","A couple of options:
1. You could temporarily [set a custom schema](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas), configure everything to [build as tables](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/materializations#table) and then do a standard dbt run. You would wind up with a copy of every model in a separate schema
2. You could do something with jinja, e.g. use [`dbt_utils.get_relations_by_pattern`](https://github.com/dbt-labs/dbt-utils/tree/utils-v1#get_relations_by_pattern-source) and then loop over its results with handwritten sql statements: 
```sql
--pseudocode
{% set relations = dbt_utils.get_relations_by_pattern(schema_pattern='original_schema', table_pattern='%') %}

{% for relation in relations %}
  create table original_schema_archive.{{ relation.identifier }} as (select * from {{ relation }})
{% endfor %}
```

I would go with option 1 personally!",2
5052,"insert-by-period-materialization-failing-on-last-hour","Hello,

I'm running backfill job using 

I'm using insert_by_period_materialization  for backfill incremental load. 
Im incrementaly loading historical data per hour and all hours of increment works nice except last one which fails with statement:

```
 Tried to run an empty query on model 
   sql, eg. in a model hook, make sure your `else` clause contains valid sql!
```

I googled but it seeme like this is not really error it's just that dbt fails in case it has to run empty query. 
This is causing problems since model stops execution on this error. 
Any help apriciated, 
Thanks!",1
5052,"insert-by-period-materialization-failing-on-last-hour","Hey @hrvoje, I'd suggest opening an issue on the repo: https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/insert_by_period

Note that it's not an officially-supported macro so we probably won't be able to dig into it very far, but potentially another community member will be able to contribute to a workaround. 

In the meantime, I would ensure that you always have a record for the final period - perhaps you can widen the size of your windows?",2
5052,"insert-by-period-materialization-failing-on-last-hour","Thank you @joellabes , do you know any simillar materialization that works well for incremental load in Snowflake?",3
5052,"insert-by-period-materialization-failing-on-last-hour","I think another community contributor has expressed interest in making it usable in Snowflake but I don't think it's come to fruition yet.",4
5054,"dbt-pro-resources-to-lookup","hello guys, we have:

here
discourse.getdbt.com/tag/best-practice

this
github.com/dbt-labs/dbt-core/discussions/categories/community

slack
app.slack.com/client/T0VLPD22H/C0VLZM3U2
[Slack | advice-dbt-for-power-users | dbt](https://app.slack.com/client/T0VLPD22H/C2JRRQDTL)
something else ?",1
5054,"dbt-pro-resources-to-lookup","We just launched the dbt Developer hub, which has links out to everything:
docs.getdbt.com",2
5054,"dbt-pro-resources-to-lookup","That's great I noticed in slaw you have even a channel that mirrors contents here...",3
5057,"grant-revoke-in-big-query","Hello @joellabes 

dbt core:1.2
dwh: Bigquery
Model type: Incremental

 The access is not getting revoked via DBT.Below are the steps which i have followed and could see issue in the dbt.

**case one:**
I have applied the grants as below so that user someone can access the table.

{%if target.name =='dev'%}
   {{ config(grants = {'roles/bigquery.dataViewer': ['user:someone@yourcompany.com']}) }}
{%endif%}

**result**

 user (someone@yourcompany.com) is able to access the table. I can see grant statement in the log

table to check metadata of roles/principles.
.INFORMATION_SCHEMA.OBJECT_PRIVILEGES
    where object_schema = ""dataset_name""
      and object_name = ""table_name""

**Case two**

Now i need to revoke access to user :someone@yourcompany.com and provide access to user :sometwo@yourcompany.com. I have updated the code in model as below.

{%if target.name =='dev'%}
   {{ config(grants = {'roles/bigquery.dataViewer': ['user:sometwo@yourcompany.com']}) }}
{%endif%}

**result**
after dbt run the revoke got  triggered for someone@yourcompany.com and grants got triggered someone@yourcompany.com. The user:someone is unable to access the table and user:sometwo is able to access the table. I can see the revoke and grants statements in log.

**use case three**
I need to revoke access to the user sometwo@yourcompany.com and as per dbt documentation (https://docs.getdbt.com/blog/configuring-grants) I have left it blank as below in the model.


{%if target.name =='dev'%}
   {{ config(grants = {'roles/bigquery.dataViewer':  **[]**}) }}
{%endif%}


**Result**
 **user sometwo@yourcompany.com is still able to access the tables.** Also i can't find revoke is getting triggered in logs. 

**Expectation:** The dbt should trigger revoke to remove access to user:sometwo@yourcompany.com 

When i ran below query(taken from log) i **can't** find the same.
 **select privilege_type, grantee    from `gcp-project`.`region-EU`.INFORMATION_SCHEMA.OBJECT_PRIVILEGES**
**where object_schema = ""dataset_name"" and object_name = ""table_name""**
**-- filter out current user**
**and split(grantee, ':')[offset(1)] != session_user()**

I have changed the query as below and i could find the entry in metadata table.

**select privilege_type, grantee from `gcp-project`.`region-EU`.INFORMATION_SCHEMA.OBJECT_PRIVILEGES where object_schema = ""dataset_name"" and object_name = ""table_name""** 

Reference to my previous topic for easy understanding: https://discourse.getdbt.com/t/using-grants-in-a-target-specific-way/4981

Please help me on this? 

Regards
Minhaj Pasha",1
5057,"grant-revoke-in-big-query","Hi @minhajpasha, thanks for the extensive writeup! 

I'm not super familiar with grants, but since you're following the instructions described on the documentation site and the permissions aren't being revoked as expected, I suspect you're running into some sort of bug. 

Could you instead open an [issue on GitHub](https://github.com/dbt-labs/dbt-core/issues) so the maintainers can look into this for you?",2
5057,"grant-revoke-in-big-query","Perhaps the best practice to handle your case @minhajpasha, is to use Google Groups instead of users, granting access to your models to specific groups.
Therefore, if you need to revoke the access of someone to a model, you just remove them from the group without changing the code of your dbt project.",3
5057,"grant-revoke-in-big-query","@hans.lemm , Hey agree with you, we are currently using groups to provide access, but the issue persistent if you are using groups too..

Regards
Minhaj",4
5070,"materialized-views-in-rs","how much this is experimental [dbt-labs-experimental-features/materialized-views at main Â· dbt-labs/dbt-labs-experimental-features (github.com)](https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/materialized-views) meaning stable? I see the commit history stopped in march - any experience of usiong it ? I saw a ref to it in [dbt CLI and Amazon Redshift (workshops.aws)](https://catalog.workshops.aws/dbt-cli-and-amazon-redshift/en-US/lab-8)",1
5073,"set-grants-on-dataset-level-in-bigquery","Hi, i am trying to set specific viewer-access in BigQuery on a dataset through dbt and dbt_project.yml file. 

```
a_folder_in_models:
  +dataset: some_dataset
  +grants:
     roles/bigquery.dataViewer: ['group:group@company.com']
```
But when i execute dbt run it will set it on table/view level. At my company we wan't to set these type of view-access to different ad-groups on dataset level. We wan't to control these through the file dbt_project.yml and not inside every model.

Is there a solution to this? Maybe a macro that will solve it? Or am i just doing it wrong in our dbt_project.yml.",1
5073,"set-grants-on-dataset-level-in-bigquery","Hello, If you are creating datasets via terraform, then tf would be best place for this use case . If you are looking via dbt please use post hooks (defined in dbt_project.yml) or define macro with list of grant SQL statements and call macro on-run-start or on-run-end based on your use case.

I have done the extensive analysis around grants for bq. You can go thru in below link

https://discourse.getdbt.com/t/grant-revoke-in-big-query/5057",2
5073,"set-grants-on-dataset-level-in-bigquery","No we are creating the datasets via dbt. Looking into creating a macro for setting the grant on dataset level instead.",3
5076,"grouping-sets-macro-for-redshift","Good evening!

As it is known Redshift does not support ROLLUP, GROUPING SETS and CUBE aggregate functions. Do you know about any macros made by DBT community to programmatically create UNION ALL expressions to mimic GROUPING SETS behaviour?

Thank you.",1
5076,"grouping-sets-macro-for-redshift","I had never heard of GROUPING SETS, but I just read up about them here: https://www.sqltutorial.org/sql-grouping-sets/

This is very cool! I would love to support something like it in dbt utils - could you open an issue? https://github.com/dbt-labs/dbt-utils/issues

It'd be sort of like how there is native width_bucket on Snowflake but not others, so we have made [an equivalent](https://github.com/dbt-labs/dbt-utils#width_bucket-source).

It looks like BQ at least doesn't support it either: https://dwgeek.com/google-bigquery-grouping-sets-alternative-example.html/",2
5076,"grouping-sets-macro-for-redshift","Hello!
Thank you for your reply!

I've opened the issue: https://github.com/dbt-labs/dbt-utils/issues/709",3
5087,"is-it-possible-to-customize-incremental-strategy","As per topic, is it possible to roll your own incremental_strategy? For example if the unique_key already exists, do nothing so that the original record is kept as is.",1
5087,"is-it-possible-to-customize-incremental-strategy","Yes, it's possible to make your own strategy, check out https://docs.getdbt.com/guides/legacy/creating-new-materializations#materialization-configuration for an introduction. 

Before you go down that path, there's another option which I haven't tried but feels like it has decent odds of working: 

There are [strategy-specific configs](https://docs.getdbt.com/docs/build/incremental-models#strategy-specific-configs) available when configuring incremental models, and one of them is `merge_update_columns` if you use the `merge` strategy on adapters which support it (including Snowflake, BigQuery, Apache Spark, and Databricks). I wonder whether passing an empty list to that would mean nothing would be updated if the key already exists?


```
{{
  config(
    materialized = 'incremental',
    unique_key = 'id',
    merge_update_columns = [],
    ...
  )
}}

select ...
```",2
5087,"is-it-possible-to-customize-incremental-strategy","thanks for the reply @joellabes. I did arrive at the same thinking with merge_update_columns after reading other discussions here.

unfortunately  we are using postgres, which does not support merge strategy.

I will have a read on [Creating new materializations](https://docs.getdbt.com/guides/legacy/creating-new-materializations#materialization-configuration)",3
5087,"is-it-possible-to-customize-incremental-strategy","If you get it working, I'd love to read about it in #show-and-tell! Feel free to open another topic if you run into any implementation issues along the way",4
5087,"is-it-possible-to-customize-incremental-strategy","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5093,"how-to-migrate-mysql-cursors-to-dbt","How to migrate Mysql Cursors to Dbt?
share some references to migrate",1
5093,"how-to-migrate-mysql-cursors-to-dbt","Can you share what the cursor does?
Are you migrating to a new warehouse (away from MySQL)?
What type of database is the target?

If you need iterate over rows, potentially a Python Model would be the cleanest way.
https://docs.getdbt.com/docs/building-a-dbt-project/building-models/python-models

Otherwise you could use macros + hooks to create procedures.",2
5093,"how-to-migrate-mysql-cursors-to-dbt","One of dbt's solutions architects was working on migrating cursors to dbt statements. I don't know if she's succeeded yet or not though! If she has I'll ask her for an update. 

In general, cursors and stored procedures are not part of the dbt mindset. In general you should try to work out how to express your models as simple select statements that describe the way you want your data to be at the end, not worry about how exactly it gets there. 

As an example, we have a [blog post about migrating stored procedures](https://docs.getdbt.com/blog/migrating-from-stored-procs) - what was originally an enormous block of code with upsert logic backed in turns into a handful of tightly defined modules which are unioned together. This isn't a 1:1 mapping with cursors but it has the same thought pattern. Cursors and loops generally reflect the fact that the warehouse couldn't process everything at once - with modern tooling that can be less true. 

It would be helpful if you could share an example query you're trying to convert.",3
5093,"how-to-migrate-mysql-cursors-to-dbt","is there any way to share you privately?",4
5093,"how-to-migrate-mysql-cursors-to-dbt","It would be better if you could share an example (even a cut-down example) publicly - I am not familiar with cursors myself so *probably* won't be able to help (but am willing to try!), and it would be useful for anyone else who comes along with more experience to be able to see what you're trying to do.",5
5093,"how-to-migrate-mysql-cursors-to-dbt","```
USE [temp_Staging]
GO
  
SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO

ALTER procedure [Process].[Set_Fact]
as
Begin
Begin try

DECLARE @min int = 0,
        @max int = 0,
		@DID int,
		@columnlength int,
		

Select @columnlength = Max(len([scid])) from [xyz].[kbo].[ref_cus] as cx (nolock)


declare @ref_cus TABLE(

[scid] varchar( 20 ) NOT NULL UNIQUE CLUSTERED,
[CIDP] int not NULL
)

Insert into @ref_cus ( scid, CIDP )
Select 
	cx.[scid],
	cx.[CIDP]

from [xyz].[kbo].[ref_cus] as cx (nolock)
order by 	cx.[scid]



--Select count(*) from @ref_cus


Select 
	@FSDID_Min = Min(DID),
	@FSDID_Max = Max(DID)
from [xyz].[kbo].[FSD] as s (nolock)

DECLARE @DCur as CURSOR;
SET @DCur = CURSOR FOR
Select DID
from [xyz].[kbo].[Dim_Date] as d (nolock)
where [DID] >= @FSDID_Min
and   [DID] <= @FSDID_Max

open @DCur

FETCH NEXT FROM @DCur INTO  @DID

	WHILE @@FETCH_STATUS = 0
	BEGIN
	    Select @DID as 'DEV'

		update s
		set s.CID = cx.[CIDP]
		-
		from       [xyz].[kbo].[FSD] as s
		inner join @ref_cus as cx
			 on ( s.[scid] = cx.[scid] )
		where s.[DID] =  @DID
       and s.[CID] <> cx.[CIDP]
	   FETCH NEXT FROM @DCur INTO  @DID
	end

	CLOSE @DCur;
	DEALLOCATE @DCur;
	

End try
Begin Catch

exec kbo.usp_GetErrorInfo

End Catch

End
```",6
5093,"how-to-migrate-mysql-cursors-to-dbt","Thanks for sharing - unfortunately I don't have any specific tips to give as I've never worked with cursors and only have a rough grasp of what it's doing there, but it looks like it's updating a table and/or inserting blocks of rows into the table. 

The closest equivalent in dbt is an incremental model, and again I think you might find some useful stuff in the stored proc migration guide to help you stop thinking about things in terms of updates and move to a more declarative approach. 

If anyone with more specific context comes along, I'd love to read their take!",7
5098,"question-on-using-dbt-with-parquet-in-s3-via-trino","I am a quite n00b at DBT.. have used it briefly in Postgresql via Meltano project... but there is no DBT-Trino meltano transformation plugin.. which led me to ask.. how am I supposed to use DBT when I move from Postgresql to Trino and S3 + Hive + parquet data ? 

there's this https://towardsdev.com/trino-dbt-excellent-fit-for-cross-database-elt-and-data-connectors-e36126e2d121

But its a bit advanced/unclear for me how to just ""get started""",1
5098,"question-on-using-dbt-with-parquet-in-s3-via-trino","dbt has an adapter (a database-specific plugin) for trino: https://docs.getdbt.com/reference/warehouse-setups/trino-setup

If Meltano doesn't support Trino, I assume you'd need to install dbt-trino directly with the instructions above. 

It may be useful for you to go through the getting started tutorial as well, to understand how dbt works:  https://docs.getdbt.com/docs/get-started/getting-started/overview",2
5100,"hi-everyone-how-to-dynamically-rename-the-table-name-based-on-data-changes"," Example: 
Currently we have 2020 year data so table created as xxx_2020 but later the data in the table changes and contains 2021 data then table should be renamed to xxx_2021

Table 2020 should be renamed to 2021.",1
5100,"hi-everyone-how-to-dynamically-rename-the-table-name-based-on-data-changes","In dbt, the table name is the model name. We don't dynamically rename models. (Nor can I imagine wanting to split tables up by years!)

Can you share more about your use cases?",2
5100,"hi-everyone-how-to-dynamically-rename-the-table-name-based-on-data-changes","It appears like you are trying to shard your data based on `year`. In that case you should look into table partitioning based on your datawarehouse.",3
5100,"hi-everyone-how-to-dynamically-rename-the-table-name-based-on-data-changes","In my case, I am reading data from oracle where month wise tables are created 
e.g : tbl_name202301

I want to create incremental dbt model where it should dynamically change the model name as per the oracle table names .

Help will be appreciated.",4
5107,"node-named-my-python-model-was-not-found","This is the model structure:

![image|418x362](upload://bd2iYoHlu3o69AlRBMJ9JBVDXun.png)

Python model defination:
ã€ã€ã€

def model(dbt, sesion):
    dbt.config(
        materialized='table'
    )

    # these are DAG-aware, and return dataframes
    dim_all_learners = dbt.ref(""cq_temperature"")
    # source_thinkific_users = dbt.source(""year"", ""temp"")

    philly_sample = dim_all_learners
                        .filter(col(""temp"">0))
                        .limit(1000)

    import numpy as np
    from sklearn.linear_model import LinearRegression
    ...
		
    # your final 'select' statement
    df = philly_sample.select(""*"")

    return df
ã€ã€ã€

Then it's running into error: 

*Server error: RPC server failed to compile project, call the ""status"" method for compile status: Compilation Error in model demo (models/demo/demo.sql)*
*Model 'model.my_new_project.demo' (models/demo/demo.sql) depends on a node named 'my_python_model' which was not found*

So, how can I define and refer to a python model?   I followed the document and failed.  THanks !",1
5107,"node-named-my-python-model-was-not-found","@zmoxie are you using dbt Core v1.3? If not, then dbt doesn't know about python models and will ignore your `my_python_model.py` file when evaluating `ref`s.",2
5107,"node-named-my-python-model-was-not-found","Thanks @joellabes  .  It's working well.  

I am new to dbt and  got a confusion on the env setting.   I configed the dbt core1.3 for the wrong env.",3
5107,"node-named-my-python-model-was-not-found","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5110,"handling-translations-in-models","Good morning ! FYI, quite new in dbt thanks to the Coalesce conference ðŸ”¥, so sorry if I'm doing it wrong.

I was playing around with models and I have a question about data refactoring. I work in a company that deals with customers that have different languages, and depending on that language I wan't to update my query followingly.

As instance I have a column called **state** that either container the string `accepted` or `not accepted` values by default, but I want to change them to `acceptÃ©`  and `non acceptÃ©` if the customer is french.

I was wondering what was the best practice to do so. I was thinking of using a variable that specifies the customer's language, but I wonder how to switch values in the query smoothly.

A first approach would be to have a if/else condition but if I would like to add more languages in the future (which will surely happen), I fear it would became a mess like this
`{% if var('language') == 'en' %} 'accepted' {% elif var('language') == 'fr' %} 'acceptÃ©' {% elif ... %} 'akzeptiert' ... { % endif %}.`

What I was thinking of was to read value from an external YAML file, the way we can use the directive `doc` to load markdown files for documentation.
The final result would look like :
`{{ yaml('my_yaml_file')['state'][var('language')] }}` which is much better.

Any thoughts about how to implement this ?
Thanks !",1
5110,"handling-translations-in-models","Found myself an first answer by reading the Jinga documentation of dbt about the [fromyaml directive](https://docs.getdbt.com/reference/dbt-jinja-functions/fromyaml). #RTFM_FTW

Now stays the question of moving in a separate file these translations, which does not seem to be possible right now reading this issue : https://github.com/dbt-labs/dbt-core/issues/1790",2
5110,"handling-translations-in-models","use a seed and put all the details then
macro lookup on it as parameter ... so you pass the right dict sort of",3
5110,"handling-translations-in-models","@Dynnammo Can you say more about your use case? If you did want to do this I'd agree with @obar1 that a seed is a good place to store a lookup table, but I'm not convinced that dbt is the place to do this at all. 

dbt is normally used for analytics workloads, where instead of changing the display text you would be normalising it by flattening the status down to a single `state_id` column or something. 

From your description, it's unclear to me whether you're changing the display for inclusion in rollup reports to stakeholders from multiple languages, or planning to use this to control the UI that's shown to end users. Either is possible I guess, but the latter is not a common use case given the latency that normally exists in ETL workloads.",4
5110,"handling-translations-in-models","I didn't know dbt as such feature, which appears to completely fulfill the use case.
I'm in the second use case. Since I'm not concerned (yet) by performances issues (I usually work with tables of less than 10000 lines), it should do the trick. 

Warm thanks for your answers @obar1 and @joellabes  !",5
5110,"handling-translations-in-models","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5117,"can-we-implement-insert-only-with-dbt-incremental-load","I don't want the older data to be deleted or updated. rather I just want to insert data everyday",1
5117,"can-we-implement-insert-only-with-dbt-incremental-load","Hello @sheetal.goyal ,

Based on the database ,we can achieve the APPEND only mode. Below is the case for big query database.
```
{{
    config(
        materialized='incremental'
    )
}}
select *
...
```
if we are not defining unique_key in config will result in append-only behavior

More Information @ : https://docs.getdbt.com/docs/build/incremental-models


Regards
mp",2
5117,"can-we-implement-insert-only-with-dbt-incremental-load","Yes.  If you do not specify `unique_key` it will not attempt to update or delete anything.",3
5117,"can-we-implement-insert-only-with-dbt-incremental-load","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5120,"dbt-packages-dbt-external-tables","Hello Everyone, I'm trying to create an external table in my snowflake environment using the dbt_external_tables package, I've created the .yml into my tree.

![image|292x490](upload://lX82z7zP1MeGsfVyqueIK5smP6B.png)


but seems like when I run dbt run-operation stage_external_sources it is not reading my yml, even including a lot of typos, It doesn't return an error, what I'm missing, help me, please!!

yml example:

```
version: 2

sources:
- name: mydb
  database: mydb
  schema: myschema
  loader: S3
  tables:
    - name: desired_externaltable_name
      ext_full_refresh: true
      description: >
      external:
      location: ""@stage_example"" ##reference an existing external stage 
      file_format: >
        (TYPE = CSVdawdawd COMPRESSION = AUTO FIELD_DELIMITER = ',' SKIP_HEADER = 1)
      columns:
        - name: ID ## you can input any name
          data_type: varchar ## best to use varchar for all
```

Let me know if you need any further information, thanks in advance.",1
5120,"dbt-packages-dbt-external-tables","Nvm, just figured out that it was my YML file that was incorrect, the indentation! dbt should have an error message or warning for that, wasted a long time before figure out!",2
5120,"dbt-packages-dbt-external-tables","Some things that jump out at me: 
- `location` and `file_format` should be further indented
- You had a `>` as your description, with no other content. I don't know if that's legal YAML - you could just leave out description if you wanted. 
- what is the `dawdawd` after `CSV` in the `file_format` block? 

Try this - I haven't used the external tables package myself so don't know if this is going to work, but it is more cleaned up:
```yaml
version: 2

sources:
- name: mydb
  database: mydb
  schema: myschema
  loader: S3
  tables:
    - name: desired_externaltable_name
      ext_full_refresh: true
      description: A description of the table
      external:
        location: ""@stage_example"" ##reference an existing external stage 
        file_format: >
          (TYPE = CSVdawdawd COMPRESSION = AUTO FIELD_DELIMITER = ',' SKIP_HEADER = 1)
      columns:
        - name: ID ## you can input any name
          data_type: varchar ## best to use varchar for all
```",3
5120,"dbt-packages-dbt-external-tables","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","I am using snowflake and I need to create a temporary table from dbt and inserting that table using another table and using that temporary table i am updating another table and at the  need to drop",1
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","You can materialize the table as an `ephemeral` model and it won't actually create the model in the database, but you'll be able to use it to populate other tables. Up the top of your model

```
{{
    config(
        materialized='ephemeral'
    )
}}
```",2
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","thank you Fraser :+1:",3
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","[dbtÂ® deeper concepts: materialization | The Big Data Tools Blog (jetbrains.com)](https://blog.jetbrains.com/big-data-tools/2022/02/22/dbt-deeper-concepts-materialization/)",4
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","Fraser's description of ephemeral models is correct, and they can help to provide a temporary holding area for code, but they can also get complex over time (especially if you stack multiple ephemeral models on top of each other). 

In general, the dbt mindset encourages you to move away from thinking about inserting data, then updating other data and dropping a temp table at the end. Instead, try to think of it as writing a select statement that describes the way your table should look at the end, regardless of how it gets there. 

By default, dbt will drop and re-create a table during each `dbt run`, which might feel counterintuitive but dramatically reduces the amount of mental overhead you have to deal with while building models. At the point that this becomes a performance problem, you can look at creating incremental models, but even then you will still describe your model in terms of a `select` statement and just provide a bit of extra metadata around which rows to delete/insert/update. 

If you are coming from a stored procedure world, you may find this article from our developer blog useful: https://docs.getdbt.com/blog/migrating-from-stored-procs",5
5128,"what-should-i-do-instead-of-creating-a-temporary-table-in-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5131,"how-much-incrementalism-should-i-use-in-my-project","```
# aug_customers.sql
select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders

from jaffle_shop.customers

left join customer_orders using (customer_id)
```

If we were doing a normal materialized table it doesn't matter if both sources get updated as we will recreate the entire table each run.

But can we convert the above transform to incremental if both `customer_orders` and  ` jaffle_shop.customers` are incremental? I.e, changes in both `customer_orders` and `jaffle_shop.customers` should be reflected in the final table.

The issue is that the above query converted to incremental will only cover changes from `jaffle_shop.customers`.

The only way I can see pushing both sources as incremental is to write two queries and then do a union. Is there a better way to do implementing this scenario with dbt? Even with the union it's not simple as we will need to clear duplicates before dbt can execute incremental updates and this becomes really complex as the number of sources increases.",1
5131,"how-much-incrementalism-should-i-use-in-my-project","This is a good question, and it comes up frequently! 

it doesnâ€™t matter whether the upstream tables are incrementals, standard tables, or views - dbt will build them in the right order and won't start on your `aug_customers` model until `customers` and `customer_orders` are complete. 

In your case, I would make it incremental like this: 

```sql
# aug_customers.sql
select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders

from jaffle_shop.customers

left join customer_orders using (customer_id)

-- below this line is the new stuff 

{% if is_incremental() %}
  where user_id in (
    select user_id 
    from customer_orders
    where most_recent_order_date >= (
      select max(most_recent_order_date) 
      from {{ this }}
    )
  )
{% endif %}
```

This would capture any customers who have ordered since the model was last built.",4
5131,"how-much-incrementalism-should-i-use-in-my-project","Hi @joellabes ,

Thanks a lot for the reply.

I understand now why the materialization type won't matter with dbt as it's doing the transformations with all the data and applying a filter on top of the result and then pushing to the sink(At least in this case).

Is there a good way to move the incremental filter to the sources and not the transformed data?",5
5131,"how-much-incrementalism-should-i-use-in-my-project","[quote=""hari1, post:5, topic:5131""]
Is there a good way to move the incremental filter to the sources and not the transformed data?
[/quote]

When you say sources here, I'm not sure if you mean `source` in the way dbt refers to it ([a raw, totally untransformed table which isn't controlled by dbt](https://docs.getdbt.com/docs/build/sources)). If you do, then no there's no way to move incremental filters into the source table, because dbt doesn't control that table, it just reads from it.  

You can make your initial staging (bronze or silver) models which read from and tidy those sources incremental if you want to, but keep in mind that each model is standalone, there's not a way to push logic into one model from another.

As a heads up, we strongly recommend holding off from making models incremental until the performance demands it. Although incrementals can improve warehouse performance, that comes at the cost of more complex model logic and a more brittle project whose tables are harder to modify as you develop. 

In general, we prefer to optimise for humans' brains!",6
5131,"how-much-incrementalism-should-i-use-in-my-project","Hi @joellabes ,

Thanks for the reply and the recommendation.

We were trying to build a fully incremental pipeline which works on only change data, But the pipeline is becoming really complex and hard to manage.

Was wondering if dbt could help with reducing the complexity. But it seems that it's more of a problem with the requirements we have set for ourselves.",7
5131,"how-much-incrementalism-should-i-use-in-my-project","[quote=""hari1, post:7, topic:5131""]
But the pipeline is becoming really complex and hard to manage.
[/quote]

Yep! Reliably getting just the modified data, across all your models, is very fiddly. 

A way that dbt can help to reduce the complexity is to handle all of your dependencies etc automatically, so you can break your code up into small modular pieces but be confident that they'll be built in the right order (to say nothing of automatic quality testing, documentation, etc) 

The good news is that if you build everything as views at first, you can change some of the slow ones to tables with a single line of config later, instead of having to rewrite lots of DDL. And then if you do decide to turn some of those tables into incremental tables, you're taking on the complexity in the right places at the right time",8
5131,"how-much-incrementalism-should-i-use-in-my-project","We already have dependency and ordering working properly in our pipeline, We are using databricks jobs/tasks and airflow to manage orchestration.

But quality control,  testing and documentation are something we really like in dbt.

Anyways, Thanks again for the replies",9
5131,"how-much-incrementalism-should-i-use-in-my-project","Also on the same thread, there are high chances  incremental model works fine with two sources table for any dbt model , but what about the requirement where the source tables are more than 2 and needs to be imcremented each time .However it is very hard to track all changes in source separately! 
just wondering to see the same topic /thread to get the appropriate result if any",10
5131,"how-much-incrementalism-should-i-use-in-my-project","This is another great question - it's hard to answer in the abstract, can you share an example of the data you're working with, and the shape you want it to be in when you're finished? As long as you have a consistent set of timestamps (or similar) accessible on all the source tables, it can be achievable, but it really depends.",11
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","Environment:
dbt 1.3, dbt could beta, snowflake, windows

```
packages:
  - package: dbt-labs/dbt_utils
    version: 0.9.2
```

Issue:
doing this: `{{ dbt_utils.surrogate_key(dbt_utils.get_filtered_columns_in_relation( source('source',  'table'))) }}`
Produces this Warning: 

`the `surrogate_key` macro now takes a single list argument instead of multiple string arguments. Support for multiple string arguments will be deprecated in a future release of dbt-utils. The dbt_.foo_bar model triggered this warning.`

The warning makes a lot of sense to me.  But I'm having trouble fixing it... 

Adding square brackets like this: `{{ dbt_utils.surrogate_key(dbt_utils.get_filtered_columns_in_relation([ source('source',  'table'))]) }}`
produces SQL which is not what I'd expect.
By that I mean:
instead of `md5(cast(coalesce(cast(field1 as TEXT), '') || '-' || coalesce(cast(field2 as TEXT), '') as TEXT))`
I'm getting `md5(cast(coalesce(cast(['field1', 'field2'])))`

More detailed example:

print get_filtered_columns_in_relation to see what it looks like:

```
{% set column_list = dbt_utils.get_filtered_columns_in_relation( source('source',  'table')) %}

-- print the column list to see what it looks like
{{column_list}}
-- it prints something that looks like a list, surrounded in square brackets. example ['field1','field2']

--  manually add some square brackets as another test
{% set column_list2 = [dbt_utils.get_filtered_columns_in_relation( source('source',  'table'))] %}

-- print the column list 2 to see what it looks like
{{column_list2}}
-- it prints something that looks like a list, with one list in it.  example [['field1','field2']]
```

Try to use it in SQL

```
with example as
(
select *,
{{ dbt_utils.surrogate_key(column_list) }},
-- the above line produces the new warning
{{ dbt_utils.surrogate_key(column_list2) }}
-- the above produces the strange sql which does not coalesce nulls, etc
from
source('source',  'table')
)
select * from example
```

I hope this makes sense!",1
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","I'm surprised that you're seeing that warning! `get_filtered_columns_in_relation` definitely returns a list; the you showed with `column_list2` is a list of lists which is not the right input shape. 

Could you experiment with a couple of things: 

- Try making a list by hand: `{% set custom_list = ['item1', 'item2'] %}` and pass that into `surrogate_key`.
- Try using the `get_filtered_columns_in_relation` macro to create a variable which you pass into the `surrogate_key` macro. There's no reason it should care, but it might help draw out the cause instead of having so much happening on a single line. 

You said that it's a new warning message; what version of dbt_utils were you using prior to this? This warning has been around for a loooong time, but we recently changed the macro and I wonder whether we are incorrectly triggering it.",4
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","> * Try making a list by hand: `{% set custom_list = ['item1', 'item2'] %}` and pass that into `surrogate_key`.

This works as expected.

> ( Try using the `get_filtered_columns_in_relation` macro to create a variable which you pass into the `surrogate_key` macro.

If I'm understanding you correctly, I did show that I tried that in my original post?

> select *,
{{ dbt_utils.surrogate_key(column_list) }}, ... ...

> You said that itâ€™s a new warning message; what version of dbt_utils were you using prior to this?

I just looked back at my old projects where I *thought* I was was doing this, but I was doing something slightly different.  I was actually looping over `adapter.get_columns_in_relation` and passing in a custom list of that result.  So in fact I have not done **exactly** this before. 


Brass tacks:  If we know `get_filtered_columns_in_relation` is returning a list, then I can safely ignore this warning / my code will not break in future releases... correct?",5
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","Hey @joellabes , do you think I'm future proof here?
We know `get_filtered_columns_in_relation` returns a list, so it feels like I'm good.  But I'm embedding this in a lot of places and the warning is scary.

Can you reproduce it?  Or, I'm happy to open a support ticket if that is more appropriate.",6
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","Sorry for the silence here! It feels like this is a bug, so an issue on utils would be the thing to do: https://github.com/dbt-labs/dbt-utils/issues

I am out of office for the next 2.5 weeks - if you wanted to do some exploration on your own, I would poke around in here, and see whether you could work out why it's triggering unnecessarily:

https://github.com/dbt-labs/dbt-utils/blob/0.9.2/macros/sql/surrogate_key.sql#L10-L18

You could also try the [utils 1.0 beta](https://github.com/dbt-labs/dbt-utils/releases/tag/1.0.0-b2) which replaces `surrogate_key()` with `generate_surrogate_key()` and doesn't do this checking any more (if you give it a single string it'll break).",7
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","I'm happy to report that utils 1.0 beta (generate_surrogate_key) does not have the issue!  I get no warnings or errors when running the same code with the new generate_surrogate_key macro.",8
5132,"combing-dbtutils-surrogate-key-with-get-filtered-columns-in-relation-produces-new-warning","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
5139,"customize-target-table-in-incremental-models","Hello! I need to insert data from multiple incremental models in one table. Can I customize target table in config not to use table as file name? Of course I can do it by union all, but we have large data and it makes one big query instead several small, and in it critical for our dwh.",1
5139,"customize-target-table-in-incremental-models","What warehouse are you using?
In any modern warehouse that uses column store, a union is one of the most efficient operations you can do.  It would be much more efficient than looping over the ref tables and inserting them one at a time.",2
5139,"customize-target-table-in-incremental-models","U can customise ur target table name using alias and config in the model file or yml file 

```
Model File:
{{ config(alias= 'target_table_name') }}
Yml file:
models:
  - name: tatget_table_sql_file_name
    config:
      alias: target_table_name
```

For more info
https://docs.getdbt.com/reference/dbt-jinja-functions/config",3
5139,"customize-target-table-in-incremental-models","dbt believes very strongly in one file :arrow_right: one object in the database. @Surya is right that you can use alias to change the name of the created object, but you can still only have one object with that name. 

I agree with @jeffsk that your best bet is to make the incremental models separately, and then create a view on top that unions them.",4
5142,"profile-in-dbt-init-command-being-ignored","Hi there,

I am trying to create a dbt project using an existing profile `default` that is already defined in `profiles.yml`. Running dbt init --help shows this ""--profile PROFILE     Which profile to load. Overrides setting in dbt_project.yml."", so accordingly I ran this command to create a new project ""proj1"" using my `default` profile.
```
dbt init --profile default proj1
```
Expected behaviour:
proj1 created with `dbt_project.yml` showing `profile: 'default'` 

Actual behaviour:
proj1 created with `dbt_project.yml` showing `profile: 'proj1'` which is the default name for a new profile.

What am I doing wrong?

Thanks in advance for any help.",1
5142,"profile-in-dbt-init-command-being-ignored","I see you've made an issue: https://github.com/dbt-labs/dbt-core/issues/6154

This feels like the right place for that to be handled so I'm going to close this. Sorry for the delay in getting back to you!",2
5142,"profile-in-dbt-init-command-being-ignored","",3
5144,"how-do-views-get-called-with-a-where-clause","Hi I am new to dbt and using views in general, never really found a good use case for them, but reading the docs for dbt I get it. 
Having said that, how do they get called with a where clause? 
I have a view set up right now that has all the data for the past year with no filter. 
If I call that view on an incremental model, does the whole view run and then get filtered? 
Can the is_incremental() macro be added to the view code? 
Should I just make my view a partitioned table instead? 
I feel like I only think in partitioned tables and can't fully understand a good use case for a view that can handle being backfilled on a full resync. Or am I supposed to unfilter - resync - add the filter back in?

Thank you in advance!",1
5144,"how-do-views-get-called-with-a-where-clause","I am unfamiliar with BQ, so we're going to go on an adventure together! 

[quote=""strimboli, post:1, topic:5144""]
If I call that view on an incremental model, does the whole view run and then get filtered?
[/quote]

This depends on the specific optimisations of the warehouse, but it should be able to push the filter into the view instead of processing the whole query and then throwing away the unwanted data. Understanding query optimisers is a dark art, but I found this write up really interesting: https://teej.ghost.io/understanding-the-snowflake-query-optimizer/


[quote=""strimboli, post:1, topic:5144""]
Can the is_incremental() macro be added to the view code?
[/quote]

No, it's only available inside of incremental models. But, as discussed above if you use the is_incremental macro inside of an incremental model to filter the results of a view, the optimiser should take that into account. 

[quote=""strimboli, post:1, topic:5144""]
Should I just make my view a partitioned table instead?
[/quote]
Only if performance demands it - I try to leave everything as views until that gets slow, and then strategically change some things to be tables to speed them up. 

[quote=""strimboli, post:1, topic:5144""]
I feel like I only think in partitioned tables
[/quote]

You might find it useful to think about views as a named, saved query that gets run on demand. It only exists when summoned, it's not so much that you create it with filters and then dismiss it afterwards",2
5147,"how-to-debug-an-issue-occurring-within-threads","Hey folks!
I have built a custom macro (post-hook) for incremental models that does the following if `dbt run -m <incremental_parent>+`:
1. Check the checksum  of the` <incremental_parent>` .
2. If the checksum  of the model has changed, drop all of the children of that model that are also built incrementally.
3. Because the `incrementally` built `children` have been dropped, running `dbt run -m <incremental_parent>+` should result in the children being built from scratch - since they should now not exist in dbt.

I was able to get 1. and 2. working but I end up with a database error for the third point, where the thread that builds out the children doesn't seem to identify that the child has been dropped.
I'm not sure why this is happened as if I run the command `dbt run -m <incremental_parent>+`  again (after the first time which produces the error), the children do indeed get built from scratch.
I would genuinely appreciate any advice on why this might be happening so I can materialize this effort, and thanks in advance for the same!
Kindly find the macro here:

```
{% macro drop_modified_incremental_downstream(modified_incrementals) %}

    {%- set default_schema = target.schema -%}
    {% set models_to_drop = [] %}

    {%- for node in graph.nodes.values()
     | selectattr(""resource_type"", ""equalto"", ""model"")
     | selectattr(""config.materialized"", ""equalto"", ""incremental"") %}

     {% if modified_incrementals[0] in node.refs[0] %}
        {% do models_to_drop.append(node.name) %}
     {% endif %}

    {% endfor %}

    {% for rel in models_to_drop %}
        {% set drop_query %}
            DROP TABLE IF EXISTS {{ default_schema }}_{{ env_var(""DBT_USER"") }}.{{ rel }};
        {% endset %}
        {% do run_query(drop_query) %}
    {% endfor %}
    {% do return(['Done']) %}

{% endmacro %}

{% macro is_incremental_modified(model_name) %}

    {%- set modified_incrementals = [] %}
    {% set foo = false %}

    {% if execute %}
    {%- for node in graph.nodes.values()
     | selectattr(""name"", ""equalto"", model_name) %}

            {% set last_stored_checksum = 'ad0b6192abb7548da303224afb8663544b25e18e213c756117aaa3685d6cc19a' %}
            {%- set current_model_checksum = node.checksum %}

        {%- if current_model_checksum != last_stored_checksum %}
            {% do log(""Checksum modified since last run"", info=true) %}
            {% do log(""Modified incrementals: "" ~ node.name, info=true) %}
            {% do modified_incrementals.append(node.name) %}
        {% endif %}

    {% endfor %}

    {%- if modified_incrementals | length > 0 %}
        {% do log(""Modified Incremental models found in run."", info=true) %}
        {% do log(""Dropping downstream models"", info=true) %}

        {% set status = drop_modified_incremental_downstream(modified_incrementals) %}
    {% endif %}
    {% endif %}

{% endmacro %}
```",1
5147,"how-to-debug-an-issue-occurring-within-threads","This is a very interesting idea! I think you're having trouble because you are dropping a table after the dbt invocation started. I'm pretty sure that during its setup/compilation phase, dbt goes through and finds all the models it's going to need to build, checks if they exist and generates the appropriate sql. 

Because you are dropping the downstream models in a post hook, dbt is expecting to find them and then panics when it can't. 

I don't think this is a threading error, but you could conclusively prove that by doing `dbt run ... --threads 1`. If that succeeds (I don't think it will), then I would open a bug on GitHub. 

Instead I think you'd need to do this as two separate dbt invocations - firstly build the new one and drop the downstream children, and then a second job which will rebuild the children. 

Another option could be to override the `should_full_refresh()` macro, which has the final say on whether a model should be dropped and rebuilt or not. Not 100% sure if that'll work, but it feels more dbtonic than manually dropping things!",2
5148,"waiting-time-tab-appeared-in-model-timing-tab-for-long-running-spikes","Hello,
I am using RedShift



usually is timing tab looks like
![image|690x120](upload://cFH7VbTTPNgGq6VfzlPeCsZ8i2Z.png)
today I saw 
![image|690x90](upload://ujmLISWefZP9JpuFb36lIyWuEc7.png)

I see not much details in 
https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-model-timing-tab

any idea/experience -  what should I look for in the logs? that might be already helpful",1
5148,"waiting-time-tab-appeared-in-model-timing-tab-for-long-running-spikes","Which plan are you on? Waiting time refers to the time where dbt cloud is waiting for an available run slot. If you have too many jobs scheduled, or they are too slow to complete, they may back up and not be able to begin processing immediately when scheduled.",2
5148,"waiting-time-tab-appeared-in-model-timing-tab-for-long-running-spikes","yes that is the time the job `n` had to wait to be executed -  usually that happen when the previous execution  of job `n-1` did not finish in time so the job `n` had to wait  - it's strange that is not reported in the documentation - unless it's there and I missed it",3
5148,"waiting-time-tab-appeared-in-model-timing-tab-for-long-running-spikes","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5150,"unique-key-config-for-snapshots","![image|690x438](upload://yzxbtWF8mZvslkLmQyf9a5qLrfZ.jpeg)
for [the docs about unique_key config for snapshots](https://docs.getdbt.com/reference/resource-configs/unique_key) ,  I think for the snapshot hudi table, the primaryKey should be 'dbt_scd_id', which is used in the 'merge into on' statement generated by 'dbt snapshot' command. And the dbt_scd_id column is related to the primary key of the source table and the update_at referenced column when we reference to timestamp strategy. But now the primaryKey of the snapshot CTAS hudi table, which is created as we run the first 'dbt snapshot' command, is the same with the primary key of the source table.In my experiment, I change 
two parts of the sql generated by the dbt, one is the primaryKey of the snapshot CTAS hudi table using 'dbt_scd_id', the other one is the 'insert [referenced columns]' in the 'merge into insert ' statement  instead of 'insert *'. After these changes, I get the scd2 table, stored in hudi",1
5150,"unique-key-config-for-snapshots","Hi @darcy, is this feedback on the dbt documentation? If so, please open an issue on the developer hub repo: https://github.com/dbt-labs/docs.getdbt.com/issues",2
5154,"error-server-error-database-error-in-rpc-request-from-remote-system","Server error: Database Error in rpc request (from remote system)
001003 (42000): SQL compilation error:
syntax error line 1 at position 23 unexpected '.'.",1
5154,"error-server-error-database-error-in-rpc-request-from-remote-system","copy and paste the compiled version of that in the db and check the actual error
if you have a macro  compile it before to use it so you can check if it's correct
this for ref if you never looked at it
docs.getdbt.com/guides/legacy/debugging-errors",2
5154,"error-server-error-database-error-in-rpc-request-from-remote-system","Server error: Database Error in rpc request (from remote system)
001003 (42000): SQL compilation error:
syntax error line 1 at position 0 unexpected '<'.

<dbt.clients.jinja.MacroGenerator object at 0x7fa1f836ffd0>
limit 500
/* limit added automatically by dbt cloud */",3
5154,"error-server-error-database-error-in-rpc-request-from-remote-system","Did you forget to put brackets at the end of your macro, eg `my_macro` instead of `my_macro()`?

Posting just the error message but not the code that caused it is not very helpful, but that's my best guess. If that's not it, please post the code that you're trying to preview",4
5155,"run-execution-by-cte-in-a-model-ideas-suggestions","hello
you have a model with a couple lot of CTE -  how do you check it looking for slow runners
unfortunately
```
  {{ log(""HERE"", info = true) }}
```
does not work if you paste here and there in the model
they print in the logs at parse time  not at runtime

```
10:42:05  1 of 2 START table model xxx...... [RUN]
10:42:05  HERE
10:42:05  HERE
10:42:05  HERE
```
so not doing what I need 

> apart  splitting them as intermediate models",1
5155,"run-execution-by-cte-in-a-model-ideas-suggestions","interested as well. did you find why it doesnt work ?",2
5155,"run-execution-by-cte-in-a-model-ideas-suggestions","I have been using dbt v1.0 -  I upgraded to v1.2 maybe it will work there 
![image|690x303](upload://iVJWethaJJyG7EI5K9Q3bKUuiox.png)",3
5158,"a-dead-page-is-encountered","I did not received an email, and can not touch the send button.  'goto billing ' does not work either.
![image|690x246](upload://ojgGOMAUn2ZWyEF2mQPEiL5TgWm.png)",1
5158,"a-dead-page-is-encountered","Hi @xiaoqiangnk, I'm sorry that you're having trouble. Please contact the dbt cloud support team at support@getdbt.com - we can't help with account issues here.",2
5158,"a-dead-page-is-encountered","",3
5169,"pattern-for-passing-run-parameters-through-api","We have an ingest process that first writes data to our data lake (hosted on S3) and builds external tables in Snowflake pointing to that. We would like to implement a post hook that invokes a dbt pipeline via the API when a new table is added to a specific folder. The question is how to best pass the name of this table to the pipeline. If invoking the command manually, the command line would look something like this:

dbt run --select +mart/destination --vars 'new_tablename: <new_tablename>'

The API doesn't (as far as I can tell from the docs) allow injecting vars or env_vars as part of the JSON body to the API. Is there an accepted trick to doing that, or is it considered a better practice to inject the entire command line using the ""steps override"" field of the body. To my mind, having to inject the entire command line is a code smell because the calling program should only have to provide information (the parameter) rather than implementation (the command line, which would otherwise be stored only within dbt cloud). However, I see no other way to do it. Am I missing something?

Thanks
Eric Buckley
Architect - Information Management - RGA",1
5169,"pattern-for-passing-run-parameters-through-api","Sorry for the slow reply @ebuckley! I haven't spent much time using the Cloud API, but I was under the impression that you could configure variables inside of it. Maybe I'm wrong!

As you've probably noticed the API's documentation is a bit anemic right now (we're working on it!) but we do have a reasonably robust set of Postman examples: https://docs.getdbt.com/blog/dbt-cloud-api-postman-collection-announcement

If there's nothing in there about controlling variables, then I'd definitely encourage you to open a ticket with the dbt cloud support team requesting this - I agree with you about separation of implementation and information",2
5173,"is-it-possible-to-set-hive-config-variables-in-macro","I'm using dbt core with dbt-hive adapter.
I used to set variables in hive as follows.

```
set hive.server2.enable.doAs=false;
set hive.server2.tez.default.queues=default;
set hive.server2.tez.initialize.default.sessions=false;
set hive.server2.tez.sessions.per.default.queue=1;
```

I would like to apply different configurations using macros.
But I got ParseException. (FAILED: ParseException line 19:0 cannot recognize input near 'set' 'hive.server2.enable.doAs' '=' in create table statement)

```
{% macro tpch_settings(engine='mr') %}
{% if engine == 'mr' %}
set hive.server2.enable.doAs=false;
set hive.server2.tez.default.queues=default;
set hive.server2.tez.initialize.default.sessions=false;
set hive.server2.tez.sessions.per.default.queue=1;
....
{% endif %}
{% endmacro %}
```",1
5173,"is-it-possible-to-set-hive-config-variables-in-macro","You won't be able to configure your database connection from inside of a dbt macro in a model definition - by the time that code is being hit, you already have a database connection and as you've seen, you're inside of a `create table as select` block. 

I'm not familiar with hive but recommend you open an issue on their adapter's repository to describe your use case as it will need to be enabled at the adapter level, not in userspace",2
5173,"is-it-possible-to-set-hive-config-variables-in-macro","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5179,"custom-strategy-macro","Hi.
I'm trying to understand how to create custom snapshot strategy.
I'd like to change the behaviour of snapshot. Column dbt_valid_to would be 9999-12-31 date and not a null date.

To do this, I created a new strategy : ""mystrategy"".

I overwrite some macros like snapshot_staging_table, or build_snapshot_table to change the way code is generated.

But I want this behaviour is only for ""mystrategy"". By overwriting the macros, I changed the behaviour for all strategies. So, the ""check"" strategy has changed too.

How can I only change the behaviour for the ""mystrategy"" strategy, and not all strategies ?

Thanks for your help :slight_smile: 
Regards.",1
5179,"custom-strategy-macro","Have you seen the docs on creating a custom strategy? https://docs.getdbt.com/reference/resource-configs/strategy#advanced-define-and-use-custom-snapshot-strategy

You should only need to make one new macro, not overwrite any existing macros",2
5179,"custom-strategy-macro","I created a new strategy like said in the docs.
But my strategy uses other macros behind the scenes.

When executing, my strategy uses default macros to generate rows in my tables. For example, it uses defaut build_snapshot_table :
```
{% macro default__build_snapshot_table(strategy, sql) %}

    select *,
        {{ strategy.scd_id }} as dbt_scd_id,
        {{ strategy.updated_at }} as dbt_updated_at,
        {{ strategy.updated_at }} as dbt_valid_from,
        nullif({{ strategy.updated_at }}, {{ strategy.updated_at }}) as dbt_valid_to
    from (
        {{ sql }}
    ) sbq
{% endmacro %}
```

But I want this behaviour in my custom strategy :
```

{% macro build_snapshot_table(strategy, sql) %}

    select *,
        {{ strategy.scd_id }} as dbt_scd_id,
        {{ strategy.updated_at }} as dbt_updated_at,
        {{ strategy.updated_at }} as dbt_valid_from,
        to_TIMESTAMP('9999-12-31 23:59:59.999') as dbt_valid_to
    from (
        {{ sql }}
    ) sbq

{% endmacro %}
```

How to do this only for my strategy, and not all strategies ?",3
5179,"custom-strategy-macro","Can you post the code of your strategy? I expect that you'll just have to give your macro a different name, and call _that_ macro in your custom materialization.",4
5184,"usage-of-require-dbt-version-in-dbt-cloud-env","I was reading about 
[require-dbt-version | dbt Developer Hub (getdbt.com)](https://docs.getdbt.com/reference/project-configs/require-dbt-version)
as I am trying v1.2 to upgrade from v1.0 ( we have been using so far in our jobs)
![image|186x183](upload://iil9u9K2a4zQSDMOvshJynGGry6.png)
Should I add this in my  yml?
```
require-dbt-version: ["">=1.2.0"", ""<1.3.0""]
```

as I don't want to use 1.3 yet ... is this the right way?
or it does not matter as I am using dbt cloud?",1
5184,"usage-of-require-dbt-version-in-dbt-cloud-env","You will need to manually select an upgraded minor version (i.e. the 3 in 1.3) in your dbt Cloud environment, so you don't need to set `require-dbt-version`. 

And even if you did, set the required version, it doesn't work the way you described above â€“ `require-dbt-version` is only enforced when dbt is invoked and the dbt Cloud environment configuration UI (as well as dbt Core itself, if you install from the CLI) doesn't take it into account.

In short: It's a way to do a last-minute check that your environment is correctly configured, but isn't used to create the configuration itself.",2
5184,"usage-of-require-dbt-version-in-dbt-cloud-env","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5186,"understanding-how-dbt-handles-tables-for-no-down-time","Hello everyone,

I am running Cloud DBT  (version 1.3) with PG14 and trying to understand how DBT handles table materialization without any down time. The introduction of grant support in the configuration with version 1.2 has gotten ride of any permission issues that happened during the life of a job due to hooks. 
Furthermore, as mentioned in several threads, DBT materializes tables in such a way that the original table is accessible while running the computation and is swapped with the temporary file once it has been written.

From these two I would assume that I should be able to access my tables at all time, even when the hourly job is running. However, in practice whenever I run a job the associated tables become inaccessible.

It seems I have either misunderstood how DBT handles this process or I am unaware of an interaction that I might be causing.  

Thank you for your time!",1
5186,"understanding-how-dbt-handles-tables-for-no-down-time","In debug log you can see the tmp table as they are created .... you could use that info and try to trouble shoot ... in general you are doing something risky  ... look for blue green deployment strategies",2
5186,"understanding-how-dbt-handles-tables-for-no-down-time","Thanks for the feedback, I will look into blue green deployment. Could you elaborate in what regard I am doing something risky ? Are you referring to not using blue green deployment ? Specifically within DBT I am not doing anything out of the ordinary",3
5186,"understanding-how-dbt-handles-tables-for-no-down-time","user can query table that are under refresh - the risky bit is that
with B/G you  do atomic switch and avoid that possibility discourse.getdbt.com/t/performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake/1349",4
5186,"understanding-how-dbt-handles-tables-for-no-down-time","Thank you for the link. It seems to me that the way DBT handled the replacement of tables to be similar in its procedure as blue/green. I understand that there are no tests and validations but the swapping should be the same, no ? If that is the case then either I should see the same performance as green/blue and have no downtime or green/blue will have the current issues with inaccessible tables every hour.",5
5193,"better-performance-of-relationship-testing","`relationships` tests, used for example when testing foreign keys, aim to figure out missing links from the child (referencing) to the parent (referred) table. As of now, this is implemented by left join, which typically triggers `hash anti-join` algorithm

```
with child as (
    select patient_id as from_field
    from ""clinical_observations""
    where patient_id is not null
),

parent as (
    select patient_id as to_field
    from ""demographic_information""
)

select
    from_field
from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null;
```
For the sake of better performance and syntax simplicity, we could make it with simple `except`, to be executed via  `HashSetOp`
 
```
explain
with child as (
    select patient_id as from_field
    from ""clinical_observations""
    where patient_id is not null
),

parent as (
    select patient_id as to_field
    from ""demographic_information""
)

select from_field from child
except
select to_field from parent;
```

NOTE: the second variants answer the question: **what are *distinct* child values that are missing in its parent** - keeping the goal in mind that should be fine as well.",1
5195,"using-vars-more-advanced-samples","looking at [var | dbt Developer Hub (getdbt.com)](https://docs.getdbt.com/reference/dbt-jinja-functions/var) 
I cannot see samples of using it in a more advance way
ex I am using as 
```
dbt build --select +mod1 +mod2    --vars 'id_list: 2050f83f-659d-646d, 4c9ce1fe-2047-e757-3448'
```
I want to add another one like 'another_id_list:a,b' but as 1 params
I think I can use a syntax like 
```
dbt build --select +mod1 +mod2    --vars  {'id_list: 2050f83f-659d-646d, 4c9ce1fe-2047-e757-3448', 'another_id_list:a,b'} 

```
is it possible to use dictionaries as well ? 
could you share some snippets/sample
thanks",1
5195,"using-vars-more-advanced-samples","Agreed, can you open an issue on the documentation site for this? 

github.com/dbt-labs/docs.getdbt.com/issues",2
5195,"using-vars-more-advanced-samples","",3
5197,"bigquery-using-scripting-variables-to-get-max-value-for-partition-exclusion","Hey, 

I've a rather huge source table which is partitioned by a column ""loaded_at"". I have an incremental model which reads from this source table.
To exclude source table partitions to read, I could make use of the _dbt_max_partitions scripting variable -  something like below
```
  {% if is_incremental() %}
    where loaded_at>= coalesce(_dbt_max_partition, '2022-01-01')
  {% else %}
```
But, the problem is, that in my incremental model I do not partition by ""loaded_at"" but by a different column (due to use-case demands). So _dbt_max_partition would not help here, as it would simply return the maximum partition value of the model (which I can't use as filter for the source table). 

In ""native"" BigQuery I would simply use a scripting variable as follows

```
declare max_source_partition timestamp;
set max_source_partition = (select max(loaded_at) as ts from `my_model_table`);
select * from `my_source_table` where loaded_at > max_source_partition
```
How can one implement such a scenario with dbt? Is there a way to create scripting variables as part of my models? Or do I need to add it as a on-start-hook? Or any better strategies to exclude partitions in my source without having the same column as partition field in my model?

Thanks already in advance!!",1
5199,"more-complex-environment-structure","New DBT user here.... Our Snowflake environment is structured such that we have three environments for our development lifecycle: Dev, Integration and Production, but we _also_ have another facet to our environment which is ""consumer"" and ""core"" data. We intend to maintain consumer (i.e. customer) data in a separate database and schema structure to minimize unauthorized access. So effectively we have 6 Snowflake databases: DEV/INT/PROD x CONSUMER/CORE. 

Would it be a better practice in DBT to have 6 targets in the profiles.yml file, or to override any of the database/schema information at the model level? At this point I'm thinking we have consumer and core as subdirectories of our model folder in our project. 

Looking forward to the discussion - thanks!",1
5199,"more-complex-environment-structure","I would think of profiles.yml as Authenticator only and structure models folder so that each folder is a db then override db in dbt_project.yml. 

That beings said, why not have a single db with multiple schemas? You can control permissions the same way at the schema level.",2
5204,"dbt-python-model-dbt-py-best-practices","This discussion will be used for dbt-py model best practices. Contribute your opinions on best practices or ask us about them! We're early on this journey. Some things you shouldn't do:

- use Python for hitting external APIs for EL tasks (caveat: light data enrichment may be okay)
- ...

dbt-py models, like dbt-sql models, are for transformation code -- the equivalent of a `select` statement. You should configure your model as need, `dbt.ref` and `dbt.source` upstream data, write data transformation code, and return a data object to be persisted in the data platform at the end.",1
5204,"dbt-python-model-dbt-py-best-practices","I haven't been able to find any resources for typing Python models, which in general is good practice because linters/mypy can catch a lot of errors and you get autocomplete suggestions in your IDE of choice. Here's a minimal example for Snowflake which does this for you:

```python
from typing import Optional, Union

import pandas as pd

from snowflake.snowpark import DataFrame as SnowflakeDataFrame
from snowflake.snowpark.session import Session
from typing_extensions import Protocol

#: pylint: disable=invalid-name


ConfigValue = Union[str, bool, float, int]


class Config(Protocol):
    """"""Model configuration""""""

    #: pylint: disable=too-few-public-methods

    @staticmethod
    def get(key: str, default: Optional[ConfigValue] = None) -> ConfigValue:
        """"""Get the value of a keky in the config dictionary""""""


class This(Protocol):
    """"""Reference to this model's database table""""""

    database: str
    schema: str
    identifier: str


class Dbt(Protocol):
    """"""DBT interface""""""

    config: Config
    this: This
    is_incremental: bool

    def ref(self) -> SnowflakeDataFrame:
        """"""References to other models""""""

    def source(self) -> SnowflakeDataFrame:
        """"""References to sources""""""

def model(dbt: Dbt, session: Session) -> pd.DataFrame:
    """"""Build the Python model

    Parameters
    ----------
    dbt : Dbt
        DBT object with configuration and references
    session : Session
        Snowpark session

    Returns
    -------
    pd.DataFrame
        pandas DataFrame
    """"""

    #: pylint: disable=unused-argument

    df = dbt.ref(""another_model"").to_pandas()

    return df

```
The return type of the `model` function can also be a Snowflake DataFrame, if you've not used `to_pandas` in your code.",2
5205,"incremental-model-with-no-data-to-merge","Hi all,
My team is using Fivetran and DBT to load/transform the data into Snowflake and we're working on using incremental models. 

What works:
- The initial load of the data works fine
- if there **IS** data to merge into Snowflake works fine

What fails:
- When there **ISN'T** any new data to merge then we get an error in Fivetran similar to below:
```
[0m20:49:18  Database Error in model develop_test_model (models/develop/misc/develop_test_model.sql)
[0m20:49:18    001003 (42000): SQL compilation error:
[0m20:49:18    syntax error line 19 at position 6 unexpected ')'.
```

Looking into the Snowflake query history it eventually errors on a query like this (please ignore line positions, i removed whitespaces):
```
create or replace temporary table WAREHOUSE.DATABASE.develop_test_model__dbt_tmp  as ( );
```

Is there a way to skip the incremental load process if there is no data to merge in?

Any help would be appreciated!",1
5205,"incremental-model-with-no-data-to-merge","Can you post the code of the incremental model you're having trouble with?

Your code needs to be valid both when `is_incremental()` is true or false - it sounds like you have your entire model logic inside an if block. You should return no rows when a model has no changes, but there should still be sql.",2
5205,"incremental-model-with-no-data-to-merge","you were correct. i did have everything in an if block so when it was evaluating what to put into the temporary table, there was nothing to put in there. i added an else statement that just returns 0 rows and that fixed the issue. thank you for the assistance!",3
5205,"incremental-model-with-no-data-to-merge","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5207,"modulenotfounderror-no-module-named-pandas","I created a Python model bellow 


```
*import pandas as pd*
*def model(dbt, session):*
*     old = dbt.ref(""stg_hbam__attorneys"")*
*     new = dbt.ref(""stg_hbam__attorneys"")*
*     key = 'ATTORNEY_ID'*
*     df_merge = pd.merge(old, new, on=key,suffixes=(""_old"", ""_new""))*
*     df_merge = df_merge.fillna(0)*
*     *
*     return df_merge.limit(10)*
```

I have upgraded to dbt v1.3 and get an error   ModuleNotFoundError: No module named 'pandas'.
How can I import Python library in dbt?",1
5207,"modulenotfounderror-no-module-named-pandas","Which  platform are you using? Snowflake?",2
5207,"modulenotfounderror-no-module-named-pandas","Yes, I'm using Snowflake",3
5207,"modulenotfounderror-no-module-named-pandas","Have you defined your dependency as shown in the [configuring packages docs](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/python-models#configuring-packages)?

See also https://docs.getdbt.com/docs/building-a-dbt-project/building-models/python-models#specific-data-platforms which talks about the anaconda specific details for Snowpark",4
5207,"modulenotfounderror-no-module-named-pandas","you will also need to call `.to_pandas()` on the dataframes if using pandas, otherwise they default to Snowpark dataframes",5
5207,"modulenotfounderror-no-module-named-pandas","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
5209,"connecting-to-snowflake-that-uses-sso-from-dbt-cloud-free-account","I have the free account version of Dbt Cloud.  In setting up a new project, in the connectons sheet, when I enter my development authenticaton details, there are only 2 options:  ""Name and Password"" and ""Key-Pair"" authenticaton.  I have a snowflake account that uses SSO (I cannot imagine a company using snowflake that does not use SSO/SCIM).  In other tools, like Dbeaver SQL client, you can specify SSO, and it will authenticate in a browser popup, which authenticates, and passes that back to Dbeaver.  I do not see any opton for that in the free account verson of Dbt Cloud.  

Does anyone know how I can this to work, using the free acount dbt cloud?",1
5209,"connecting-to-snowflake-that-uses-sso-from-dbt-cloud-free-account","Hi @ddumas, 

OAuth connection to Snowflake is only available on the enterprise plans of dbt Cloud: https://docs.getdbt.com/docs/get-started/connect-your-database#snowflake-oauth",2
5209,"connecting-to-snowflake-that-uses-sso-from-dbt-cloud-free-account","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5220,"document-dark-mode-visibility-issue","I prefer dark mode when reading docs on docs.getdbt.com (in fact, I prefer dark mode everywhere). But the font colour in certain sections could really be improved, e.g. https://docs.getdbt.com/docs/build/snapshots:

Dark Mode:
![image|690x440](upload://p7GkWQOSotRzmDceIUBz87yfglv.png)",1
5220,"document-dark-mode-visibility-issue","Hey @ibozhu, good point! Can you open an issue on the documentation repo? That's the best place to get this fixed

https://github.com/dbt-labs/docs.getdbt.com/issues",2
5220,"document-dark-mode-visibility-issue","",3
5229,"split-the-column-value","I have a column value coming in like X.Y, Y,Z 
and want to break this value into 2 variables using macro code in jinja

Main_value= X.Y
Var1= X Var 2=Y ( they are always sepearated by dot)

According to my understanding contains & like is not working in my macro code.

Can you please help me, It is important & urgent at same time",1
5229,"split-the-column-value","There's not enough detail here to completely tell what you are trying to do.  But I've taken a guess at an example based on the info you gave. 
The answer is going to largely depend on what flavor of SQL you are using.  But is sounds like you are trying to split on the dot character.  Here is an example that works in Snowflake.
First I created a table:

```
create temporary table example
as
(
select 'x.y' as my_column
    union all
select 'y.x'
);
```
then I select from it using `split`


```
select 
split(my_column,'.')[0]::text as left_of_dot,
split(my_column,'.')[1]::text as right_of_dot
from
example;
```
returns this:
|LEFT_OF_DOT|RIGHT_OF_DOT|
|---|---|
|x|y|
|y|x|


then you can store the column values in variables as shown here:
https://docs.getdbt.com/reference/dbt-jinja-functions/run_query",2
5229,"split-the-column-value","I agree with @jeffsk's code, but want to check whether you actually need to load the results into the Jinja context. 

If you're just transforming your data for output into a table, you shouldn't load the results into jinja as it will need to be stored in memory and you run the risk of significant performance issues. 

run_query is only suitable for loading small pieces of introspected data to make the query generation process easier, not for storing the result of an entire transformation query.",3
5229,"split-the-column-value","[quote=""jeffsk, post:2, topic:5229""]
```
create temporary table example
as
(
select 'x.y' as my_column
    union all
select 'y.x'
);
```
[/quote]

Hey both, Thanks for sharing the queries, but I am writing this in if/else condition in marco code and it doesn't seem to work there. Any idea why?",4
5229,"split-the-column-value","The create table statement is something I did on my end so I can simulate data that you're explaining you already have. It wasn't intending for you to also create the table because you said you already had data that look like that.",5
5229,"split-the-column-value","Making a macro which took a column and split on the dots would look something like this: 

```sql
{% macro split_on_period(column_name) %}
  split({{ column_name }}, '.')[0]::text as left_of_dot,
  split({{ column_name }}, '.')[1]::text as right_of_dot
{% endmacro %}
```

and then you'd call it as 

```sql
select 
  id, 
  {{ split_on_period('column_containing_period') }},
  some_other_column
from {{ ref('my_table') }}
```

which would render to 

```sql
select 
  id, 
  split(column_containing_period, '.')[0]::text as left_of_dot,
  split(column_containing_period, '.')[1]::text as right_of_dot,
  some_other_column
from database.schema.my_table
```

Check out https://docs.getdbt.com/docs/build/jinja-macros for more info on creating and using macros",6
5234,"how-to-remove-a-comma","I'm a bit lost
In my macros, I would like to get the name of a column and use the name of this column in an other macro.
I tried a lot of things, but I didn't really succeed.
At the moment I'm using a call statement in my macro, but I can't seem to remove the comma after the 'day'...
Does anyone have an idea?

I have already tried ['data'][0] but I encountered an error : 

Compilation Error in model Test (models\Monitoring\Test.sql)
  'None' has no attribute 'data'

  > in macro obtain_date (macros\obtain_date.sql)
  > called by model Test (models\Monitoring\Test.sql)

![Capture|690x285](upload://hg7qWMpGHuNppaJlwdHCRGvXO4.png)",1
5234,"how-to-remove-a-comma","Can you post the actual code inside of backticks? It's hard to read a screenshot 

https://meta.discourse.org/t/post-code-or-preformatted-text/188910",2
5234,"how-to-remove-a-comma","Here is my macro 

```
{% macro obtain_date() %}

{%- call statement('Column_name_date', fetch_result=True) -%}

SELECT column_name
FROM arf-data-mad-dev.SALES.INFORMATION_SCHEMA.COLUMNS
WHERE table_name = 'A_sales_dailySiteFamily' AND (column_name LIKE 'day%' or column_name like 'week%' or column_name like 'month%')

{% endcall %}

{%- set column_name_date1 = load_result('Column_name_date')['data'] -%}

{{return(column_name_date1)}}

{% endmacro %} 
```
Here is my model 

```
SELECT {{obtain_date()}} 
FROM {{sources('SALES', 'A_sales_dailySiteFamily')}}
```
And when I compile I have : 

``` 
SELECT [('day',)]
FROM blablabla 
```

Do you know how to remove the comma after the 'day' ?",3
5234,"how-to-remove-a-comma","I assume that the return type of `load_result` isn't just a simple string, but I'm not sure exactly what it is. I'm on mobile so not able to dig into this deeply, but if your use case is to get specific columns then you can go about it a different way. 

Try using dbt_utils' [get_filtered_columns_in_relation](https://github.com/dbt-labs/dbt-utils#get_filtered_columns_in_relation-source) or the native [adapter.get_columns_in_relation](https://docs.getdbt.com/reference/dbt-jinja-functions/adapter#get_columns_in_relation), then you can filter the names of the columns based on the results of that.",4
5234,"how-to-remove-a-comma","Hi!

I am also getting the similar error.  Is it resolved?

{%- set results = run_query(sql).columns[0].values() -%} -- results into the comma separated values perfectly when there are multiple values to return e.g. ('a','b','c')
but it returns ('a',) when sql fetches only one value.",5
5235,"where-i-can-open-issue-for-the-new-ux","I see an incoherent behaviour / bug in my opinion
in the new UX if you have only 1 job
and you click on it 
it does not update the line 
![image|690x288](upload://8MkJSzY8FuebUSjU6VGwFNmFIER.png)
then if I click rerun ( so now I have 2 entriesd)
and I click on one of 2s jobs
![image|316x189](upload://866HoZoKf7d71Ur6EkVRfNTt6JI.png)
it DOES populate that top left row
```
| v | dbt build ...  |
```
as you can see
so the question which github repo to use to post this?",1
5235,"where-i-can-open-issue-for-the-new-ux","You can message the dbt cloud support team using the chat bubble in-app; there isn't a repo for it since it's proprietary software",2
5235,"where-i-can-open-issue-for-the-new-ux","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5240,"advantages-of-one-monolithic-schema-yml-file-vs-multiple","As we've begun to scale out more and more with dbt my team and I have continued to ask the structure of the schema.yml files within each model folder and how we are using them.  

With tests, documentation and design being driven by these files I've mostly wondered is there any advantage to having the entirety of the model set within one file in that directory as opposed to creating one schema.yml file for each model.  Such that you could have dim_date.yml, dim_customer.yml, and so on with the corresponding information within each file.

I know that dbt supports this methodology but I'm not sure if there are any hidden gotchas on implementing one way or another.  

Any thoughts or insight would be beneficial.",1
5240,"advantages-of-one-monolithic-schema-yml-file-vs-multiple","At my previous job we migrated from a single file per directory to a single file per model. I think one file per directory is currently considered [best practice](https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview).

The big pros are marginally easier auto-generation of the .yml files, a built-in naming convention (use the model name!), and less Ctrl-F to edit super long .yml files.

The cons are that you can't use [YAML anchors](https://support.atlassian.com/bitbucket-cloud/docs/yaml-anchors/) across files, and that you'll probably want to nest all of your yml files into a sub-folder, instead of interleaving them with the model files.

All in all, it doesn't make much of a difference.

It is really easy to write a Python script to do this for you. You can read each big file into memory with `pyyaml` and then dump each model out to its own file. IIRC the annoying part is that it takes some fiddling with a custom Dumper to print out folded scalars (multiline strings) nicely.",2
5240,"advantages-of-one-monolithic-schema-yml-file-vs-multiple","I think if you're writing a decent number of tests (unless they're all singular tests, which I guess could happen), as well as writing basic descriptions, and you're running a lot of models you're pretty naturally (as I guess the subtext of your question suggests!) going to run into a point where it becomes quite inconvenient to find what you're looking for in a massive yml file 100's (1,000s!) of lines long.

Agree with Ted in that having one per directory is a nice midpoint - although as its the only yml file in the directory I guess you can stick with schema.yml as the filename!",3
5242,"compiling-just-one-model-in-a-different-profile","Hi community,

I have 3 different profiles for my dbt core implementation, that are pointing to 3 different AWS account objects (Basically to separate dev, uat and prod environments). When I run a model over dev, with the objects I'm using for developing, everything goes smooth. I want to run just one of my models in uat env, and I'm using:

`dbt run --models uat_table --profiles-dir profiles --target uat`

But the run fails, as at the moment of compiling, there are some source objects for other models that right now just exists on dev environment.

```
dbt.exceptions.CompilationException: Compilation Error in model other_table
(models/other_table.sql)
  Model 'model.other_table' (models/other_table.sql) depends on a source named 'uat.source_table' which was not found
```
How can I run in uat the model that I need, without compiling for the rest?

Thanks in advance!",1
5246,"clarification-on-using-or-not-adapter-in-materialisation-definition","reading [Creating new materializations | dbt Developer Hub (getdbt.com)](https://docs.getdbt.com/guides/legacy/creating-new-materializations)

there is an explicit example on using one for RS as
```
{% materialization my_materialization_name, adapter='redshift' %}
```

but then it's looks not really needed reading a couple of lines below I read:

 `dbt's ability to dynamically pick the correct materialization based on the active database target is called multiple dispatch. ...`

 so it's not really clear if was necessary or not to use it - I am suing RS :) so I am quite interested and I've got some  `Serializable isolation violation on table` even using thread=1 in dbt build  :stuck_out_tongue:",1
5246,"clarification-on-using-or-not-adapter-in-materialisation-definition","code does something for sure [dbt-core/impl.py at 749f87397ec1e0a270b2e09bd8dbeb71862fdb81 Â· dbt-labs/dbt-core (github.com)](https://github.com/dbt-labs/dbt-core/blob/749f87397ec1e0a270b2e09bd8dbeb71862fdb81/plugins/redshift/dbt/adapters/redshift/impl.py)",2
5252,"gdpr-lgpd-brazil-on-dbt-cloud","Hi dbt community, can I use DBT Cloud and also have complience with PII data not leaving the country? DBT Cloud is ok with GDPR on US? And what about in Brazil?",1
5252,"gdpr-lgpd-brazil-on-dbt-cloud","Hi @cesarofuchi, check out [https://www.getdbt.com/security/](https://www.getdbt.com/security/) to see our compliance information, including GDPR (we are compliant).

I'm unfamiliar with LGPD, but it's important to understand that dbt Cloud doesn't store any of your queries' results - see Data Processing and Data Storage on that same page linked above.",2
5254,"server-failure-related-to-partial-parsing","Hey, 

I'm getting a ""server failure"" that does not allow me to do anything on dbt. I see this message on the details:
*14:19:56  Unable to do partial parsing because profile has changed*

I've pulled from main and restarted the IDE and nothing changes.

Any idea of what can I do?

Thanks!",1
5254,"server-failure-related-to-partial-parsing","try to change / create new branch - and delete piece by piece ... leave a minimal prj",2
5254,"server-failure-related-to-partial-parsing","eve deleting `target` folder... in one case it helped me I recall",3
5256,"redshift-configuration-sortkey-auto","Hi there!  I was reading [this page](https://docs.getdbt.com/reference/resource-configs/redshift-configs) on Redshift configs.  It says that...

> `sort` accepts a list of sort keys, for example: `['timestamp', 'userid']`. dbt will build the sort key in the same order the fields are supplied.

I tested it and it doesn't seem that `sort` can accept `auto`.  Setting sort key to auto is [the default recommendation from AWS](https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html).

Can someone confirm that I'm not missing anything?  That I'm correct that we can't set `sort` to auto in `dbt`?",1
5256,"redshift-configuration-sortkey-auto","I would look into the code of [dbt-labs/dbt-redshift: dbt-redshift contains all of the code enabling dbt to work with Amazon Redshift (github.com)](https://github.com/dbt-labs/dbt-redshift)",2
5256,"redshift-configuration-sortkey-auto","My understanding is that `SORTKEY AUTO` only works for long-lived tables, so that their optimizer has data on how the tables are being queried. Except for `materialized=incremental` models, tables are dropped by dbt with every run, which makes `AUTO` ineffective.",3
5256,"redshift-configuration-sortkey-auto","That's a good callout, Ted, thank you!",4
5256,"redshift-configuration-sortkey-auto","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5259,"can-you-test-only-the-seeds-folder","Hi there! 
Can I test dbt seeds folder only?
I manage to test my seed when running dbt test, but it tests everything in my project.... I would like to test only the seeds... 
I've also tried dbt test --select seeds and a bunch of other stuff, and I can't find anything on the internet... Can anyone tell me if it is possible to test it?",1
5259,"can-you-test-only-the-seeds-folder","I've seen this... maybe it will work... give it a try
![1 2022-11-03 19.01.36|690x165](upload://be74uDhKD76jgdcP2k7lgLWDFcr.png)",2
5272,"need-some-help-with-jinja-trying-to-use-a-for-loop-over-a-run-query-result-set-from-a-seed-table","Hi,

In the process of teaching myself dbt. Have run into a bit of a snag trying to iterate over a result set from a seed table. Seed table CSV is pretty basic here, just trying to prove the concept

The example code I'm following for this approach is taken from the dbt documentation on 
using run_query(..) jinja function

|1|New York||
| --- | --- | --- |
|2|Florida||
|3|Oregon||
|4|New Mexico||
|5|Texas||
|6|Colorado||

```
{% set my_seed_query %}
select * from {{ ref('my_seed_table') }} 
{% endset %}
{% set results = run_query('select * from my_seed_query') %}
{% if execute  %}
{% set results_sv_list = results.columns[1].values() %}
{% else %}
{% set results_sv_list = [] %}
{% endif %} 
{% for item in results_sv_list %}
 {{ print(item) }} 
{% endfor  %}
```

I've tried some variation on this

```
with my_seed_query as (
 select * from {{ ref('my_seed_table') }}
)

{% set results = run_query('select * from my_seed_query') %}
{% if execute  %}
{% set results_sv_list = results.columns[1].values() %}
{% else %}
{% set results_sv_list = [] %}
{% endif %}
{% for item in results_sv_list %}
    {{ print(item) }}
{% endfor  %}
```

and also

```
{% set my_seed_query %}
 select * from {{ ref('my_seed_table') }}
{% endset %}
{% set results = run_query(my_seed_query) %}
{% if execute  %}
{% set results_sv_list = results.columns[1].values() %}
{% else %}
{% set results_sv_list = [] %}
{% endif %}
{% for item in results_sv_list %}
    {{ print(item) }}
{% endfor  %}
```

All of the above produce various errors

However, the code below works as expected and returns a result table, so the seed table is being resolved with ref(...) command, at least in the scenario below

```
with my_seed_query as (
 select * from {{ ref('my_seed_table') }}
)

select * from my_seed_query
```",1
5272,"need-some-help-with-jinja-trying-to-use-a-for-loop-over-a-run-query-result-set-from-a-seed-table","hello
check the compiled version of that ninja - you would see ref() DOES explode the actual name - while run_query DOES NOT
plus as beginner join slack `#advice-dbt-for-beginners`  app.slack.com/client/T0VLPD22H/CBSQTAPLG",2
5272,"need-some-help-with-jinja-trying-to-use-a-for-loop-over-a-run-query-result-set-from-a-seed-table","[quote=""jeremy.deats, post:1, topic:5272""]
`{% set results = run_query('select * from my_seed_query') %}`
[/quote]
It's not clear what's going on here, you're trying to select * from a variable, try this
`{% set results = run_query(my_seed_query) %}`",3
5272,"need-some-help-with-jinja-trying-to-use-a-for-loop-over-a-run-query-result-set-from-a-seed-table","[quote=""jeremy.deats, post:1, topic:5272""]
various errors
[/quote]

This isn't really enough to go on, so I'm going to skip over the sample code for now. Especially since it looks like your goal can be achieved via an existing library - the dbt utils package has a macro to get a column's values: [get_column_values](https://github.com/dbt-labs/dbt-utils/#get_column_values-source)

You can either use it directly, or you could have a look at its source code to see how it's implemented",4
5275,"first-time-dbt-core-user-runtimeerror-found-a-cycle","Hello! I am trying out dbt-core for the first time for a POC project. I am trying to do a very simple example, but I keep running into the same error and cannot find a solution from official documention. The same error appears whether I run dbt test on my source, or if I try to dbt run the model.
THis is the error:
```
/dbt/compilation.py"", line 438, in link_graph
    raise RuntimeError(""Found a cycle: {}"".format(cycle))
RuntimeError: Found a cycle: model.wc_demo.wc_demo_model_1
```

Can I have help reviewing my following setup to see what mistake I am making?

I have a profile.yml setup in the default user .dbt folder, and connect to snowflake . (I ran dbt debug and connection looks okay)

Here's the strucuture of my profile.yml:

```
wc_demo:
  outputs:
    dev:
      account: my_account_name
      database: MY_DB
      private_key_passphrase: password
      private_key_path: /path/to/my/keyfile
      role: MY_ROLE
      schema: MY_SCHEMA
      threads: 1
      type: snowflake
      user: MY_USER
      warehouse: MY_WHS
  target: dev
```

My directory structure looks like this:
![image|249x396](upload://w8lMvJM1whl8ICwQvzctRWHmiaU.png)


Here's my files defined:

**dbt_project.yml**
```

name: wc_demo
version: '1.0.0'
config-version: 2
profile: wc_demo
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  
clean-targets: 
  - ""target""
  - ""dbt_packages""

models:
  wc_demo:
    demo:
      materialized: view
```
**packages.yml**
```
packages:

  - package: dbt-labs/metrics
    version: ["">=1.3.0"", ""<1.4.0""]

packages:
  - package: dbt-labs/dbt_utils
    version: 0.9.2
```
**models/sources/sources.yml**
```
version: 2

sources:
  - name: wc_demo
    database: MY_DB
    schema: MY_SCHEMA
    freshness: # default freshness
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
    loaded_at_field: SNAPSHOT_DATE
    description: HERES MY DESCRIPTION

    tables:
      - name: MY_TABLE_NAME
        description: >
          Here's a descripton of my table
        loaded_at_field: SNAPSHOT_DATE
        columns: 
           - name: uid
             description: primary key
             tests:
              - unique
              - not_null
```
**models/demo/demo.yml**
```
version: 2

models:
  - name: wc_demo_model_1
    description: first demo project
    config:
      enabled: true
    columns:
      - name: uid
        description: ""The primary key for this table""
        tests:
          - unique
          - not_null
```
**models/demo/wc_demo_model_1.sql**
```
select uid
from MY_SCHEMA.MY_TABLE_NAME
```
Thank you!",1
5275,"first-time-dbt-core-user-runtimeerror-found-a-cycle","use ref 
```
select uid
from MY_SCHEMA.MY_TABLE_NAME
```
to
```
select uid
FROM {{ source('wc_demo', 'MY_TABLE_NAME')}}
```",2
5275,"first-time-dbt-core-user-runtimeerror-found-a-cycle","Hi @erin.valdes! Thanks for sharing so much context to help with the debugging. 

It's very strange that you're seeing errors about a cycle, when you only have a single model. Normally you'd need multiple models to have a loop between them, *and* you'd need to be using source/ref to define those connections. As @obar1 said you should definitely use `source()` to access your table instead of hardcoding it in! That's how dbt knows what order to build models in and can create the documentation site. 

I'm on mobile right now so can't test it, but I even wonder whether maybe dbt misbehaves if there are no references defined anywhere in the project? Would be a bit weird if it did, but I can't think of another cause offhand. 

Have you gone through our getting started project? It will step you through what's involved in building a project from start to finish, and might be a good jumping off point: https://docs.getdbt.com/docs/get-started/getting-started-dbt-core",3
5281,"failed-job-execution","Hi everyone. I'm encountering some odd behavior and hoping for some assistance. I'm using the dbt cloud service.
I just finished dropping some objects from my Snowflake account, and clearing out a few deprecated files within my dbt models folder.
Within the Develop UI, I'm able to run 'dbt run' from the main branch - all my models are executed and there are no issues whatsoever.
As soon as I navigate back to Deploy > Run History and run the full job, I immediately get this error shown in the screenshot below. It tells me there a syntax error line1 position27 unexpected'<EOF>', however it does not specify a file name.
I tried to change the order that my models were run in, but the same error persists.
 I'm kind of at a loss as to why I can manually run jobs from the Develop UI, but they fail from the Run History UI.
Any insights are greatly appreciated. 

![Screenshot 2022-11-04 175358|690x263](upload://3QFmsxzzB0lEQoFs0LCvstVge8N.png)",1
5281,"failed-job-execution","It's strange that it doesn't tell you which file is impacted! `<EOF>` stands for end of file. My guess is that you have a jinja tag which is not being closed correctly and the parser is getting to the end of the file without finding the close tag. Eg maybe you have `{{ my_macro() }` with only one closing `}` or `{% set something = 123 }` without a `%`. 

If you message the dbt cloud support team using the chat bubble in-app, they might be able to have a look at some logs and see if they can see exactly which file is misbehaving. If nothing else, it'd be worthwhile them knowing that there seems to be a bug where the file name isn't being shown in the logs! 

Note also that dbt will always compile your entire project before running it, even though you've only selected a subset of the project to build.",2
5281,"failed-job-execution","Thanks for much for your reply joellabes. 

I did a little more sleuthing and it turns out that I had simply created a SQL file and forgot to add any code. Looks like the error was due to the empty file.

Thanks again for your help!",3
5285,"spin-up-a-dbt-postgres-environment-with-a-single-click-in-vs-code-for-demos-and-interview-assignments","Being able to spin up a fully functioning dbt environment is very useful for interview assignments and quick demos. I created a template using [dev containers](https://containers.dev) that allows you to spin up dbt + Postgres + Adminer in a single click right from VS Code. Check it out and make sure to star if you find it useful!

https://github.com/panasenco/dbt-devcontainer-demo-template",1
5285,"spin-up-a-dbt-postgres-environment-with-a-single-click-in-vs-code-for-demos-and-interview-assignments","Good job, maybe add a big seed at once? Superstore or something similar",2
5285,"spin-up-a-dbt-postgres-environment-with-a-single-click-in-vs-code-for-demos-and-interview-assignments","Thanks @infl, feel free to contribute a starter data set and/or data model!",3
5285,"spin-up-a-dbt-postgres-environment-with-a-single-click-in-vs-code-for-demos-and-interview-assignments","I just tried this, super easy! Thank you for writing a great readme :tada:",4
5287,"loading-seed-with-super-type-to-redshift","Hi,

I'm trying to load a seed file to Redshift. My seed has a column of the SUPER type (has JSON for value), but the value of that column is not how I expect it to be.
The expected value should be:
`{""parameter"":""82c4aca8-7fca-4f6d-a964-78b116ed9aa9""}`
but when my csv file, no matter how I try to adjust the quotes, I can't get to this result.
When in my csv file the value is:
`""{""""parameter"""":""""82c4aca8-7fca-4f6d-a964-78b116ed9aa9""""}`
I'm getting something like:
`""{\""parameter\"":\""82c4aca8-7fca-4f6d-a964-78b116ed9aa9\""}`
I tried any combination of the quotes that I could think of but still couldn't get the expected result, which means I can't query this column as a JSON.
Any suggestions?",1
5287,"loading-seed-with-super-type-to-redshift","Hi
You need to define a yml for seed telling dbt the columname and colunntypes ... check the docs",2
5290,"dbt-cloud-databricks-unity-catalog","With dbt Cloud, how can I specifiy a ""catalog"" value so the materialised table is written into a desired Databricks Unity Catalog?

I tried the following in a model SQL but it still wrote the table into the default ""hive_metastore"" instead of the desired 'main' catalog:

{{config(
    materialized='table',
    catalog='main',
    schema='dbt_abc,
    file_format='delta',
    location_root='abfss://some_values'
)}}

Update (2022-11-08 21:30 UTC): is it becasue ""dbt-databricks"" adapter does not yet support ""DBT Cloud""? If so, does anyone know when it will be available in ""DBT Cloud""?",1
5290,"dbt-cloud-databricks-unity-catalog","Itâ€™s available now, https://www.databricks.com/blog/2022/11/17/introducing-native-high-performance-integration-dbt-cloud.html",2
5290,"dbt-cloud-databricks-unity-catalog","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5292,"dbt-snapshot-fails","Hi,
I get snapshop datatypes error
`Value has type STRUCT<module STRING, reference_entity_class STRING, reference_table STRING, ...> which cannot be inserted into column created_by, which has type STRUCT<module STRING, reference_entity_class STRING, reference_table STRING, ...> at [16:33]`
the problematic column is a struct and there were new fields/subcolumns added. Does this mean snapshots cannot account for schema changes in nested columns, or am i missing something?
Thank you so much in advance!
Attached on the left is the updated schema of the tmp table to be merged, on the right is the original table
![Screenshot 2022-11-05 at 08.51.56|690x249](upload://u7kCfRxyPCJFuO7tgWTG9X4cfRt.png)",1
5292,"dbt-snapshot-fails","Can you post the config block of your snapshot model? Might be something in there. Also does the model itself run fine locally?",2
5292,"dbt-snapshot-fails","Hi Fraser and thank you for replying!
Attached is the screenshot of the config block. Running it locally also produces the same error.
Thank you for trying to help!
![Screenshot 2022-11-08 at 22.51.55|690x343](upload://jfqWfcVIBCIDmWCUSf7RphWAmGC.png)",3
5292,"dbt-snapshot-fails","I just tried to reproduce this on a simple test model and it failed again with the same error.
Created initial snapshot of a table that contains nested column -> Added one more subcolumn to the nested column of the table -> Tried to ran snapshot and it failed. 
So it does look like snapshots doesn't not capture schema changes in the nested fields",4
5292,"dbt-snapshot-fails","Hey, having the exact same issue. Please how did you resolve your problem?",5
5292,"dbt-snapshot-fails","still not solved unfortunately.  Ended up rebuilding the snapshot. I think its a bug, i opened an issue in dbt's repo",6
5293,"dbt-defaults-to-creates-views-for-each-model-after-version-1-3-update","Hi there!
We have recently updated from `dbt-snowflake` version `1.0.0` to `1.3.0` and noticed that the `*_dbt_tmp` step for each DBT model is now being created from a snowflake view instead of a snowflake temporary table as it was before with `1.0.0`:

1.0.0
```
create or replace temporary table {DATABASE}.{SCHEMA}.{DBT_MODEL_NAME}__dbt_tmp ...
```

1.3.0
```
create or replace view {DATABASE}.{SCHEMA}.{DBT_MODEL_NAME}__dbt_tmp ...
```

I would like to know how I can still maintain the initial behaviour in `1.0.0` (DBT models from temporary tables) while still using `1.3.0`. Is there perhaps a configuration that can be changed to keep this behaviour?

We are using the `incremental` materialization for our DBT models, to transform from one snowflake table to another snowflake table.

Thanks
Matthew Williams
Software Engineer at Ramp Growth",1
5306,"how-to-handle-incremental-changes-in-data-valut-model-bigquery-tables","Hi All,

Looking for some approach on how to handle incremental changes in data vault model using dbt or dbt vault?  I have created data vault using dbt vault package. 
 We have created following  layers in our data model.

source  | schema layer | data vault 

Source is sending json messages and we are flattening those json messages and storing in the schema layer and after that we have created data vault model(hubs, sats, and links) from schema layer.
In case of  incremental changes, source is sending only updated columns (not full record), in this case how to handle incremental changes in data vault model?. 
As you know satellite tables stores history information, every time I should retrieve latest record and apply updates on top o fit.
Is there anyway to handle this easily in dbt? or dbt vault?.

Ex -
source sends ;
initial load - 
----------------
in the source
 custid        | name | age| salary|modified date
        123      | abc      | 24  |50000|2022/8/11 00:00:32

in Schema layer:
----------------------
 custid | name | age| salary|timestamp
 123      | abc      | 24  |50000|2022/8/11 00:00:32

data vault - satellite table:
----------------------------------
 custid | name | age| salary
 123      | abc      | 24  |50000|2022/8/11 00:00:32

Now lets assume source sends incremental changes:

incremental change: [only ID and updated column 'Salary']
source:
 custid    |  salary
        123   |60000

in Schema layer:
----------------------
 custid | name | age| salary|timestamp | changeflag
 123      | abc      | 24  |50000|2022/8/11 00:00:32 | I
 123      | abc      | 24  |60000|2022/9/11 10:20:32 |U      [We have to get latest record from satellite tables and update the record with update columns only] 

data vault - satellite table:
----------------------------------
 custid | name | age| salary |changeflag
 123      | abc      | 24  |50000|2022/8/11 00:00:32 | I
 123      | abc      | 24  |60000|2022/9/11 10:20:32 |U


Thanks for your suggestions!.

Thanks!
Chandra",1
5311,"how-to-join-multiple-bigquery-tables-from-different-gcp-projects","Hello,
While writing dbt models, is it possible to JOIN multiple BigQuery tables from different GCP projects?
Currenlty in profiles.yaml and dbt_project.yaml we enter only one GCP project and dataset name.
Please advise how to use multiple tables from different GCP projects in a dbt model?",1
5311,"how-to-join-multiple-bigquery-tables-from-different-gcp-projects","Hello @dbt101 ,

Please have a look  profile.yml file to find the method.

Eg:
my-profile:
  target: dev
  outputs:
    dev:
      type: bigquery
      **method: oauth**
      project: abc-123
      dataset: my_dataset
      priority: interactive

if you are using OAUTH for running dbt locally, then your gcp profile/user-id should have access bq tables across the gcp projects.
When you use a Service Account, this SA should have access to all bq tables across your GCP projects.

Regards.
Minhaj",2
5311,"how-to-join-multiple-bigquery-tables-from-different-gcp-projects","Thanks Minaj, I tried oauth approach and it works.",3
5311,"how-to-join-multiple-bigquery-tables-from-different-gcp-projects","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5313,"unrecognized-arguments-when-passing-vars-from-command-line","Hi team,

I am having an issue where it keeps throwing me error of unrecognized arguments when  I tried to pass more than one argument from the command line in dbt. I am pretty sure I have followed the syntax/format of the commands by following this guide: https://docs.getdbt.com/docs/build/project-variables. Would be appreciated if someone can give out some pointer here. Thank you.

dbt command:
`dbt run --vars '{""target_table"":""table1"", ""source_table"": ""table2""}'`

Error thrown:  
`dbt: error: unrecognized arguments: ""source_table"": ""table2""}`",1
5313,"unrecognized-arguments-when-passing-vars-from-command-line","Hello @gaojun05 

Can you please share the dbt model which refers these vars?

Regards
minhaj",2
5315,"hy-all-i-getting-error-in-run-time-like-var-is-undefined","Hy all , i getting error in run time like     'Var' is undefined",1
5322,"debugging-run-operation","Fantastic, thanks a lot to all for the contributions, may I ask your thought on the best approach to effectively debug macro and run-operations? Thanks a lot again...",1
5322,"debugging-run-operation","@G14rb i have moved this to a new topic as it wasn't related to the thread you posted it on. 

Can you say more about the problem you're having? It's impossible to give helpful debugging advice with such a broad question. 

Check out https://docs.getdbt.com/guides/legacy/debugging-errors and https://docs.getdbt.com/guides/legacy/getting-help#2-take-a-few-minutes-to-formulate-your-question-well for more advice on debugging in general and what makes a good support request",2
5331,"how-to-remove-comma-from-a-result-set-in-a-macro","Hello!

I have  a question regarding a macro in dbt.

The below statement returns an array with comma separated values. e.g. ('a','b','c'). But when the sql returns only one value, it gives the result as ('a',). If this is the known issue to you, please help on how to remove the comma when the single values are returned.

{%- set results = run_query(sql).columns[0].values() -%}
{% do log(results, info=True) %}

Thanks!",1
5331,"how-to-remove-comma-from-a-result-set-in-a-macro","hi
does {% do log(results| trim(','), info=True) %}
works",2
5333,"azure-sql-setup-in-dbt-project","Hi all,

I am setting up Azure sql project in DBT Core, but I keep on getting the below issue--
 >Database Error
  ('01S00', '[01S00] [Microsoft][ODBC Driver Manager] Invalid connection string attribute (0) (SQLDriverConnect)')

I have installed dbt-sqlserver adapter but when I try to initiate this project it says 'no sample profile found' so I create the profiles.yml file manually with below details.
testdemo:
  target: dev
  outputs:
    dev:
      type: sqlserver
      driver: SQL Server
      server: <server>.windows.net
      port: 1433
      database: RnD
      schema: 
      user: 
      password:
target: dev

In driver value..I have tried like --'ODBC Driver 17 for SQL Server' and 'ODBC Driver 18 for SQL Server', all are installed in my window VM.
any help would really be appreciated.",1
5342,"merge-with-more-complex-condition","Hi all,  we doing migration from MSSQL to BigQuery. 
All our ETL processes convert to DBT. Now I meet a some issue how can I apply MERGE when some of conditions looks like:
[
...
when matched and [col a] <> [col b] 
 then update [col a] = [col b]
....
]
I need to update rows only when matched and only when its not equals

Thanks",1
5346,"dbt-cloud-and-piperider-integration","Hi Guys,

I am looking for piperider integration with DBT Cloud, as of now it is available for DB Core only.
Is there a way we can install the piperider package to out dbt cloud instance and use it for data profiling ?
https://docs.piperider.io/cli/dbt-integration
Please let me know any other alternative as well!

Thanks",1
5346,"dbt-cloud-and-piperider-integration","Hi @sktgupta5,

I am sure that we handled your request via our Discord community, but in case others find this question - 

For this situation we would recommend using a CI process such as  GitHub Actions to achieve this.

Our current documentation on this is here:
https://docs.piperider.io/cli/how-to-guides/github-action

We also have an article that discusses the benefits of using a CI process such as adding a data profile diff to the pull request comments:
https://blog.infuseai.io/how-to-be-more-confident-making-data-model-changes-76a2f65feffa

CI process is one use-case that we are focusing on right now, so there'll be more resources available very soon to help set this up.

If you have any other questions please let me know.

Thanks,

Dave
Tech Advocate @ InfuseAI (PipeRider)",2
5352,"change-in-data-type-for-a-field-not-considered-in-dbt-snapshot","I'm running a snapshot on a table, but on specific fields like follows:
```
{% snapshot prestaging_pipeline_check %}

    {{
        config(
          target_schema='snapshots',
          strategy='check',
          unique_key='col_V',
          check_cols=['col_Y', 'col_Z'],
        )
    }}

    select * from {{ref(""table_X"")}}

{% endsnapshot %}
```
However, the snapshot seems to fail on a completely different field in the table that has changed it's data type. 
```
08:58:52  
08:58:52  Database Error in snapshot prestaging_pipeline_check (snapshots/prestaging_pipeline_snapshot.sql)
08:58:52    Value has type STRING which cannot be inserted into column col_A, which has type ARRAY<STRING> at [16:93]
08:58:52    compiled SQL at target/run/datapipelines/snapshots/prestaging_pipeline_snapshot.sql
08:58:52  
```
Looking at the logs, snapshot does try to access the entire table, but shouldn't it be using the specific columns mentioned in check_cols?
Any suggestions on how to fix this without having to drop the original table? 

Information:
Running on dbtCloud",1
5352,"change-in-data-type-for-a-field-not-considered-in-dbt-snapshot","`col_A` is included in the `select *` statement, which is why it's coming up with the error. 

`check_cols` refers to which columns will be checked to see whether the row has changed and needs a new snapshot created. At least one of the new/changed records in the table is a string instead of an `ARRAY<STRING>`, and so it can't be inserted into the table. 

If you don't want that column to be included in your snapshot, you could exclude it from the select statement, or cast it as an array instead of a single string. 

If you remove it from the select statement, you'll need to drop the column from the snapshot table (or leave it in your select statement and do something like `select null as col_A`)",2
5356,"enquiry-on-incremental-materialisation","Hello,

A few general enquiries on incremental materilisation.

1. If I add new columns in a table that is materialised as ""incremental"". Is the only way to pick up these new columns in the next refresh via --full-refresh? I'd like to confirm that without --full-refresh, and even there are incremental rows that can be picked up, the new columns will not be reflected?

2. As such, if I add new columns into incremental models. Whenever I merge my dev branch into prod, would I need to manually update my prod job to --full-refresh for once then removing the --full-refresh flag after the 1st run is completed. Would dbt ""knows"" new columns are added based on the PR such that the user can avoid the manual change of prod job, please?

Thanks,
Oliver",1
5356,"enquiry-on-incremental-materialisation","I guess it depends on your implementation, but I know that at least on bigquery your build will fail if you're adding new tables to an incremental table without a --full-refresh.

I'm not sure what your use case is, but if that's going to cause a problem maybe [snapshots](https://docs.getdbt.com/docs/build/snapshots) would be a better solution?",2
5356,"enquiry-on-incremental-materialisation","Have a look at the on-schema-change configurations of your incremental model: https://docs.getdbt.com/docs/build/incremental-models#what-if-the-columns-of-my-incremental-model-change

If you use `append_new_columns`, dbt will add the new column and include its values (for new rows). dbt will never go back and do a backfill of old data without you issuing a `--full-refresh` command.",3
5356,"enquiry-on-incremental-materialisation","Thanks @joellabes that is exactly what I am looking for.",4
5356,"enquiry-on-incremental-materialisation","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5361,"dbt-python-model-error-501-received-http2-header-with-status-404","Hello everyone, despite following all the steps on the site, I am getting the error ""501 Received http2 header with status: 404"" or ""None Stream removed"" on my dbt python model. The json serviceaccount I'm working with has all the privileges. (BQ - GStorage) I'm waiting for your help. I try to run it with the command dbt run -s my_python_model. Thanks.
 



def model(dbt,sesssion):

    dbt.config(
        materialized='table'
    )
 

    return session.sql(""select 'hello' as message "")


![Screen Shot 2022-11-17 at 09.55.43|690x248](upload://4bQyrTemUEjTx5MRiu1B6fRDIu.png)",1
5361,"dbt-python-model-error-501-received-http2-header-with-status-404","@yunusemrekarazag Glad to see you're trying out Python models! They're a very new feature, and so debugging them is still a bit tricky (especially on GCP/Dataproc) while we get a handle for all the requisite pieces.

Based on some similar threads in the #dbt-core-python-models channel in the dbt Community Slack, it sounds like this issue comes up if:
- You've configured the `dataproc_region_name` incorrectly, e.g. by putting `us-central-1` instead of `us-central1`
- You need to yeslist the IPs that originate from your dbt process (including dbt Cloud, if you're using it), or you have another issue with your Dataproc network configuration ([GCP docs](https://cloud.google.com/dataproc-serverless/docs/concepts/network))",2
5361,"dbt-python-model-error-501-received-http2-header-with-status-404","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5366,"using-of-vars-in-hooks-query","Hello All

Can we use vars in Hooks query?
Iam trying to run below dbt command and it's fails with below error:
[0m16:47:20.468451 [error] [MainThread]: [33mObject of type date is not JSON serializable[0m

 **dbt command:** dbt snapshot --select customer_snapshot --vars '{execution_date:2022-11-05}'

**DBT Model**
{% snapshot customer_snapshot %}
    {{
        config(
          unique_key='customer_id',
          strategy='check',
          check_cols=['customer_id', 'customer_name'],
          post_hook=[""delete project.dataset.customer_details set status= 'Y' where ingestion_date ='{{var('execution_date')}}'""]
      ) }}
    select * from project.dataset.customer_details
    where ingestion_date = ""{{ var('execution_date') }}"" and status= 'N'
{% endsnapshot %}


Regards
Minhaj Pasha",1
5366,"using-of-vars-in-hooks-query","[quote=""minhajpasha, post:1, topic:5366""]
Object of type date is not JSON serializable
[/quote]

Have you tried quoting the key-value pairs, like this? 
```
dbt snapshot --select customer_snapshot --vars '{""execution_date"":""2022-11-05""}'
```",2
5366,"using-of-vars-in-hooks-query","@joellabes -exactly the same as you mentioned, i tried both and it failed with same error",3
5366,"using-of-vars-in-hooks-query","[quote=""joellabes, post:2, topic:5366""]
```
dbt snapshot --select customer_snapshot --vars '{""execution_date"":""2022-11-05""}'
```
[/quote]

@minhajpasha 
can you try this once
```
dbt snapshot --select customer_snapshot --vars 'execution_date: 2022-11-05'

```",4
5371,"macro-required-arguments","Is there a clean way to require the existence / nonnull-ness of an argument to a macro?",1
5371,"macro-required-arguments","Wonderful question! Right now the answer is no, but I'd love it if you could go and weigh in on this issue: https://github.com/dbt-labs/dbt-core/issues/4999

In that issue, you can also see an example of how I've gone about this in the past:
https://github.com/dbt-labs/dbt_metrics/blob/39ebdaebc72ca80c7ecdbe5df5a0c08adbe5df6f/macros/secondary_calculations/secondary_calculation_period_over_period.sql#L36-L45",2
5378,"snapshotting-source-and-dimension-tables","Hi all,

I have read that snapshotting source data is best practise, but we are planning on snapshotting our dimension tables once we have modelled the data.

Should we do both? Is the source snapshot just used as a fail-safe to roll back to any point in time if anything goes seriously wrong? Or is there any additional advantages of doing this?

Thanks!",1
5378,"snapshotting-source-and-dimension-tables","@ShaunLF, I actually think one is fine, except if you are performing serious upstream changes on your source tables to arrive at your dimension..
If Yes. Then it's logically efficient to stick to your dimensions when they are SCD's.
Yes it can serve for fail over, but I assume your source tables might have landed from a process that can be regenerated.
Its a two way thing depending on what works for your use case.
I won't want to create snapshots and pay for extra storage on sources if I can regenerate them from a data lake layer on cases of failures. That's why dimension snapshoting will be better to track changes over time.",2
5378,"snapshotting-source-and-dimension-tables","Thanks Victor -  appreciate your response! 

We are currently ingesting from a SQL Database that overwrites changes so we wouldn't be able to roll back to a given point in time.",3
5378,"snapshotting-source-and-dimension-tables","[quote=""ShaunLF, post:1, topic:5378""]
additional advantages of doing this?
[/quote]

The use case for snapshotting your source data is described here: https://docs.getdbt.com/docs/build/snapshots#what-are-snapshots

Also check out the best practices detailed further down on the same page: https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices

The reason to avoid snapshotting your final dimension tables is that if you find a bug and need to change your modelling code, you can find yourself with inaccurate results and no way of recalculating the correct data. By contrast, if you snapshot the source data and build on top of that, you can always recalculate everything from first principles if/when you need to. 

I certainly snapshot some final dimensional tables when the additional effort to make the table fully independent of time (or I can't be bothered ðŸ™ˆ), but it's pretty much always in addition to snapshotting the source table instead of a replacement.",4
5378,"snapshotting-source-and-dimension-tables","Thanks for your input Joel - much appreciated! Pretty much what I was thinking and we have now reworked our project to snapshot source data :)",5
5378,"snapshotting-source-and-dimension-tables","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5389,"dbt-model-freezes-on-dbt-run","I'm working on a dbt project with Postgres database, currently there are ~40 models there and they all run fine if I select them with `--models` selector, however when I try to run all models with `dbt run` one of the models (relatively large/complex in the context of the project - a few millions of rows, generates in ~5 mins) gets stuck and does not finishes. Oddly enough I can observe Postgres reporting 3 identical `CREATE TABLE` queries being executed, however no locks are reported by Postgres.",1
5389,"dbt-model-freezes-on-dbt-run","How many threads are you using? (See [understanding threads](https://docs.getdbt.com/docs/get-started/connection-profiles#understanding-threads) for background)

Do you see the same issue if you try doing `dbt run --threads 1`? If you're currently using many threads (say, 8) and it works with a single thread, you could try changing down the number of threads to trade off parallelisation and reliability.",2
5389,"dbt-model-freezes-on-dbt-run","Thanks, seems to solve the issue, I was running DBT with 2 threads, as Postgres instance has 2 vCPUs, but something was obviously deadlocking after all.",3
5389,"dbt-model-freezes-on-dbt-run","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5392,"redshift-connectivity","Hi,
1)Does dbt work with Redshift serverless? 
2)Can dbt cloud be used   with Redshift interface endpoint instead of publicly accessible endpoint? Or does it need a hosted dbt inside customer VPC?",1
5392,"redshift-connectivity","[quote=""dbtraj, post:1, topic:5392""]
Does dbt work with Redshift serverless?
[/quote]

Yes


[quote=""dbtraj, post:1, topic:5392""]
Can dbt cloud be used with Redshift interface endpoint instead of publicly accessible endpoint
[/quote]

Does this mean PrivateLink? [Enterprise versions of dbt Cloud support PrivateLink](https://www.getdbt.com/product/dbt-cloud-enterprise/), but I recommend you [reach out to our sales team](https://www.getdbt.com/contact/) for a demo + discussion if that's something you're interested in as they will be better suited to advise you",2
5399,"how-to-populate-type-in-the-dbt-docs","How do I fill in the column's data type in the generated dbt docs? I referred to this https://docs.getdbt.com/reference/resource-configs/column_types but that's only for seed file and doesn't seem to work for models. I tried using CAST in the table's transformation code but TYPE is still blank when I generate the docs afterwards...",1
5399,"how-to-populate-type-in-the-dbt-docs","dbt gets the type information by querying the model when the docs are generated. Was the model run before the docs were generated?

If so, I wonder if it could be that the adapter doesnâ€™t support it, which warehouse are you on?",2
5399,"how-to-populate-type-in-the-dbt-docs","It is possible to set data types on columns for models, sources etc as well: https://docs.getdbt.com/reference/resource-properties/columns 

However as @jaypeedevlin said, you shouldn't need to - dbt should check your database's information_schema or equivalent. Have a look at your logs when you run `dbt docs generate` - if dbt can't access the information schema with the account it's using, you may need to grant that user permissions, as shown by an error like this (Redshift example): 

>Warning: The database user ""dbt_joel"" has insufficient permissions to query the ""svv_table_info"" table. Please grant SELECT permissions on this table to the ""dbt_joel"" user to fetch extended table details from Redshift.",3
5403,"can-i-create-a-custom-materialized-view-materialisation-for-snowflake","Hi there, I'm looking to create a materialized view materialization for snowflake. What happened to the option to create a custom materialization described in the legacy documentation?
https://docs.getdbt.com/guides/legacy/creating-new-materializations
many thanks,
Christoph.",1
5403,"can-i-create-a-custom-materialized-view-materialisation-for-snowflake","[quote=""leinemann, post:1, topic:5403""]
What happened to the option to create a custom materialization
[/quote]

What do you mean? This behaviour is still possible. 

Before building your own implementation though, check out https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/materialized-views which has an unofficial implementation you could use as-is or as a jumping off point.",2
5403,"can-i-create-a-custom-materialized-view-materialisation-for-snowflake","sorry, I was mislead by the 'legacy' in the path. This is great, thank you!",3
5403,"can-i-create-a-custom-materialized-view-materialisation-for-snowflake","All good! _Legacy_ refers to the fact that these guides aren't up to the standards of new guides we're writing, not that their contents are obsolete. We might have to rethink that label though as it's confused a couple of people!",4
5403,"can-i-create-a-custom-materialized-view-materialisation-for-snowflake","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5407,"how-to-change-execution-commands-with-dbt-jobs","Hi Guys.
Please, I'm trying to create a dbt job that will be performed a lot of times, for example, 3 times.
I need to pass a different parameter for the same model:

dbt run --select fact_xpto --vars '{""SchemaSource"": ""PROD_A""}'
dbt run --select fact_xpto --vars '{""SchemaSource"": ""PROD_B""}'
dbt run --select fact_xpto --vars '{""SchemaSource"": ""PROD_C""}'

How can I schedule a job like that?
Is possible to create a loop based on a table or in dbt_project.yml?

Tks",1
5407,"how-to-change-execution-commands-with-dbt-jobs","What tool are you using for your orchestration? dbt Cloud or a different tool like airflow etc?",2
5407,"how-to-change-execution-commands-with-dbt-jobs","[quote=""danieljnmello, post:1, topic:5407""]
Is possible to create a loop based on a table or in dbt_project.yml?
[/quote]

dbt Core itself does not support looping over multiple invocations - this will have to be something that is triggered by your orchestration tool.",3
5407,"how-to-change-execution-commands-with-dbt-jobs","Hi Joel.
I'm using DBT cloud to control the jobs.",4
5407,"how-to-change-execution-commands-with-dbt-jobs","OK - you could maybe make something work using the API, but out of the box the UI doesn't let you configure anything like that. 

Another consideration might be to change how you're treating your sources. If you have a use case like this, you could union your sources into a single model and then run the downstream analysis only once: [Unioning identically-structured data sources](https://discourse.getdbt.com/t/unioning-identically-structured-data-sources/921)",5
5411,"check-recency-source-for-model-in-dbt-cloud","Hi,

I tried to use [recency from dbt-utils](https://github.com/dbt-labs/dbt-utils#recency-source) to check if data is coming as expected to source tables. For example I want to see if there is data at all in source tables for (-70 minutes from now).
But whatever parameters I use in my source.yml I always get PASS for tests done. 

I am using something like this in my sources.yml:


```
models:
  - name: uat_demand_nosales_flatten
    tests:
      - dbt_utils.recency:
          datepart: minutes
          field: tradeHour
          interval: 70
```",1
5411,"check-recency-source-for-model-in-dbt-cloud","One thing that sticks out to me is that your datepart is `minutes` when it should probably be the singular `minute`. I would expect that to throw an error as opposed to silently passing though. 

I would start by looking at the code in the `target` directory, which will include the rendered versions of your queries to see what was actually run against the database. 

If you run that query you can see why no records are being returned and iterate on the query until you find the cause. 

You talked about testing your sources, but your example code refers to models. Keep in mind that if you are testing sources (the original raw tables that dbt uses in its models as opposed to the ones it creates), then there is native source freshness available: https://docs.getdbt.com/reference/resource-properties/freshness",2
5411,"check-recency-source-for-model-in-dbt-cloud","Thank you Joel for your reply!
By checking recency on model I'm actually checking weather data is properly coming in since this model is first in line,  
But I'll be also using source freshness on source table.

Changed `minute` but didn't observed any difference. 
Checking target directory SQLs generated, that's useful I'll have to do more checks and report back.",3
5411,"check-recency-source-for-model-in-dbt-cloud","Checking generated `dbt_utils_model_name_n_ef0c3b` in `target` folder for my case and from what I observed I see that `current_timestamp` used in recency is using:

 `select current_timestamp::timestamp_ntz` which gives:
2022-12-01T02:22:07.700000-08:00 if you use `timestamp_tz`.

And in our model we are running refresh by hour like:
` select  trunc(dateadd(hour,-1,SYSDATE()),'HOUR')` which currently gives 
`2022-12-01T09:00:00` and if `timestamp_tz` is used thats:
`2022-12-01T09:00:00-08:00`

I was checking difference between SYSDATE and CURRENT_DATE [here](https://stackoverflow.com/a/17925834/2119941).  Although this is for Oracle I believe that same apply for Snowflake

Wouldn't it be logical than that you should use SYSDATE in recency instead of CURRENT_DATE ?",4
5411,"check-recency-source-for-model-in-dbt-cloud","Are you saying that when you change the [recency query defined here](https://github.com/dbt-labs/dbt-utils/blob/0.9.6/macros/generic_tests/recency.sql) to use `sysdate()` instead of `current_date()` that you can cause your test to fail as you expect?

Each warehouse can have a specific implementation of the `dbt.current_timestamp()` macro. It looks like this is how the Oracle adapter has implemented it:
https://github.com/oracle/dbt-oracle/blob/f7fe1fd206286b780ec02238616da6a120b70552/dbt/include/oracle/macros/utils/timestamps.sql#L18-L20

This is based on dbt utils 0.9.6 (the latest shipping version) - which version of dbt utils are you using?",5
5427,"materialization-comparison-of-view-table-incremental-snapshot-in-combination-with-persistent-or-transient-source","I created an article to show differences in materializations for
- view
- table
- incremental
- snapshot
in combination with a persistent (never gets truncated) and transient source/staging (gets truncated before the next load) table.

The article should help to decide what materialization to use if the source is persistent or transient: https://www.buckenhofer.com/2022/11/materialization-examples-of-data-engineering-with-dbt/
code is on github: https://github.com/abuckenhofer/dbt",1
5433,"dbt-docs-throwing-errors-on-hosted-webserver","![Screen Shot 2022-11-28 at 9.30.42 AM|690x351](upload://gnakcnRuKsdidlSigwRxF8uQ4h8.png)

I am not sure why this file cannot be found.
I am using Dagster to run the DBT Docs Generate command, upload those files to S3 so that they can be deployed via CI/CD to an ngnix server.

The documentation will sometimes load after a fresh deploy, but quickly will throw the above error. I can confirm, though, that the debug and docs generate CLI commands produce no error(s), and all the files (including the catalog.json) are present on the ngninx server.

I am not sure how to debug this weird issue. Thanks!

I tried running the docs generate command with the `--no-compile` flag, but it produces the same result.",1
5433,"dbt-docs-throwing-errors-on-hosted-webserver","I haven't deployed the docs site anywhere other than dbt Cloud, but a few guesses:

- I assume that the directory structure is the same on your S3 bucket as when you test locally? (i.e. the catalog file isn't in an unexpected subdirectory)
- Does your S3 bucket have any sort of permissioning/CloudFront signed URL requirements? 
  - Can you access the catalog file by pasting its URL into the address bar, or accessing it with curl/wget etc?
- Have you tried manually generating a manifest, catalog, etc and copying those files onto your S3 bucket and trying to serve them? 
- Are there any other errors in your devtools console, e.g. a 404 showing the path it tried to access?",2
5433,"dbt-docs-throwing-errors-on-hosted-webserver","Hey Joel,

Thanks for responding. Let me say, the S3 bucket is just an intermediary.  I push the files to S3 first, but use Ci/CD to pull those files down from S3 onto a deployed webserver.

* I assume that the directory structure is the same on your S3 bucket as when you test locally? (i.e. the catalog file isnâ€™t in an unexpected subdirectory)
     - Correct. All files live in a single folder without any structure.
* Does your S3 bucket have any sort of permissioning/CloudFront signed URL requirements?
     - N/A.  Files are uploaded to S3 but downloaded to the webserver that serves these files.
  * Can you access the catalog file by pasting its URL into the address bar, or accessing it with curl/wget etc?
    - I can.  If I tack catalog.json on the end of the URL, the JSON loads in the browser.
* Have you tried manually generating a manifest, catalog, etc and copying those files onto your S3 bucket and trying to serve them?
  - Curious what you mean by manually? Currently, I'm using Dagster to issue the CLI command **dbt docs generate**.  
* Are there any other errors in your devtools console, e.g. a 404 showing the path it tried to access?
   - No 404s, just the error in the console which reflects the above error message. (503 - Service Unavailable) 


One thing I'm curious is the suffix on the filename shown in the message.  i.e. cb=1669728929765",3
5433,"dbt-docs-throwing-errors-on-hosted-webserver","OK that is very strange! 

[quote=""dusty, post:3, topic:5433""]
Curious what you mean by manually? Currently, Iâ€™m using Dagster to issue the CLI command **dbt docs generate**.
[/quote]

I meant doing the entire flow without involving any other tools (Dagster, your CI/CD tool), i.e. 
1. Run `dbt docs generate` locally 
2. Copy the files to the ngnix server 
3. Try loading the files

If that works, then something somewhere in the automated pipeline is misbehaving. If it still fails, then I would point the finger at your ngnix server. 

The fact that you're getting a 503 error does imply that it's an oddity of the webserver - I missed that in your original screenshot and assumed it was going to be a 4XX error. If it's a server issue then I am going to be doubly unable to help sorry! 

[quote=""dusty, post:3, topic:5433""]
One thing Iâ€™m curious is the suffix on the filename shown in the message. i.e. cb=1669728929765
[/quote]
I think that is a [cachebuster](https://stackoverflow.com/questions/9692665/cache-busting-via-params) to ensure that changes to the json files are picked up when you refresh the page. That looks like a Unix timestamp in milliseconds, and [maps to 13 hours ago](https://www.epochconverter.com/?q=1669728929765) which is when you posted.",4
5433,"dbt-docs-throwing-errors-on-hosted-webserver","Thanks Joel,

I ran the files locally and copied the files to the pod, and getting the same 503.  

I'll check back with my infra team - this is a tricky one!",5
5433,"dbt-docs-throwing-errors-on-hosted-webserver","Quick Q @joellabes ; 

Does the webserver need the project_dir and profile to be present? I only have the generated target folder on the webserver.  Does the docs need to have a connection to the database, as well?",6
5433,"dbt-docs-throwing-errors-on-hosted-webserver","No the docs are a static site, no database connection or anything other than those handful of html/json files required",7
5433,"dbt-docs-throwing-errors-on-hosted-webserver","@dusty , @joellabes ,

Let me share my experience around dbt docs.

Cloud: GCP

I tried to use GCS bucket, I got the same error as you.

After some googling I found the below explanation with great comparison summary.

 https://link.medium.com/Dm1tKIPguvb

I went with app engine, and it worked perfectly and suits my use case (followed:https://youtu.be/tZ5LZyIqW8g to create static website)

Hope it helps

Regards
Minhaj pasha",8
5433,"dbt-docs-throwing-errors-on-hosted-webserver","I swapped out the NGINX server for the builtin DBT webserver that is ran when the command is issued, yet the same issue plagues me. 

I guess I need to understand what would make a file that is present to suddenly be unavailable.

Thanks",9
5433,"dbt-docs-throwing-errors-on-hosted-webserver","Joel,

Thanks for your time, but the issue has been located, and it was all my infra and proxies fighting.

Thanks again !",10
5433,"dbt-docs-throwing-errors-on-hosted-webserver","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",11
5436,"multiple-case-statements-wildcard-matches","Hello!

I'm trying to create a set something similar to the following
```
{%- set payment_methods = ['bank_transfer', 'credit_card', 'coupon', 'gift_card'] -%}

with payments as (
   select * from {{ ref('stg_payments') }}
),

final as (
   select
       order_id,
       {% for payment_method in payment_methods -%}

       sum(case when payment_method = '{{ payment_method }}' then amount else 0 end)
            as {{ payment_method }}_amount

       {%- if not loop.last -%}
         ,
       {% endif -%}

       {%- endfor %}
   from {{ ref('stg_payments') }}
   group by 1
)

select * from final
```
However, instead of setting the payment methods to be exact matches, I would like to use Wildcard matches. 
e.g. 
```
CASE WHEN payment_method LIKE %Amex% THEN ""Credit Card"" 
        ELSE 
           WHEN payment_method LIKE %Paynow% THEN ""Bank Transfer"" ELSE ""Others"" END
```
My current SQL in the dbt model has many lines of case statements and within each case statements many different variations of wildcard matches.

Appreciate some help on improving upon this and making the model cleaner with lesser lines of code and also making it easier to add on more variations of wildcard matches.

Thanks!",1
5436,"multiple-case-statements-wildcard-matches","Hey @derrickt! If I'm understanding you correctly, you have have a lot of different types of credit card (Amex, Visa, Mastercard, etc), and lots of different types of bank transfers (Paynow etc), and want to flatten them down to just credit card, bank transfer, etc? 

If so, instead of a case statement you could consider a [seed](https://docs.getdbt.com/docs/build/seeds), a csv file that gets turned into a table in your warehouse. 

If you made something like this: 
![A table with two columns: payment type and payment group|446x250](upload://95YhXylxljm0cCvl1Xbwm7an57X.png)

Then you could do a join against the seed:

```sql
with payment_map as (
  select * from {{ ref('payment_map') }} --use the name of your seed file in the ref
),

payments as (
  select * from {{ ref('payments') }}
),

final as (
select 
  payments.*,  
  coalesce(payment_map.payment_group, ""Unknown"") as payment_group
from payments
  left outer join payment_map 
    on payments.payment_method ilike '%' + payment_type + '%'
)
```

For more info on why this works, check out https://stackoverflow.com/questions/1386166/how-to-use-a-like-with-a-join-in-sql 

Also keep in mind that if you have something like `Amex` and `Amex Black` in your seed file, then you will get a fanout because Amex Black will match against both rows. Remember to add a uniqueness test on your finished model!",2
5436,"multiple-case-statements-wildcard-matches","Hey @joellabes 

Thank you for the suggestion!
Appreciate it!

Yeah I think in this case since I have alot of different types of Payment methods and I would want to group them to be under 3 Categories: Credit Card, Bank Transfer and Online Payments.

A Seed file would be the most appropriate solution for this.

Will try it out and see if it would work!",3
5438,"since-upgrading-to-1-3-dbt-cli-mfa-caching-is-no-longer-working","Since upgrading to 1.3 (DBT cli) MFA caching is no longer working - anyone else seeing this?

I have `authenticator: username_password_mfa` set in profiles.yml.",1
5438,"since-upgrading-to-1-3-dbt-cli-mfa-caching-is-no-longer-working","Hey @phil.jones, regressions in dbt Core should be created as issues on the GitHub repo: https://github.com/dbt-labs/dbt-core/issues 

The Core team can help triage your problem from there!",2
5438,"since-upgrading-to-1-3-dbt-cli-mfa-caching-is-no-longer-working","Thanks @joellabes

Will raise an issue, just checking that I wasn't missing out on a new setting or something",3
5441,"dbt-on-pyspark-cost-and-models","We have some data transforms running on AWS EMR with pyspark today, and more tables coming. To evaluate DBT, as we like it keeps the original SQL logic, there are 3 basic questions:
1. Cost. If DBT with pyspark cost more or less vs. pyspark running on EMR.
2.  Is there a way to just specify partial model in DBT? 
    a. For example, I have a table with 30 columns, do I have to write all 30 columns in config file for DBT? Or I can write just (i.e.) 5 columns and rest by using a wild char?
    b. There are many tables in SQL logic with 'select * from table'. I don't care about much on what columns it is. Is it possible for DBT to run without config file for table schema?
3. Since we already have some jobs running in pyspark in AWS EMR, is there a best practice to convert those jobs to DBT with pyspark?",1
5441,"dbt-on-pyspark-cost-and-models","Hi @yzhang, good questions! 
1. I don't know enough about the pricing of pyspark on EMR to be able to answer this question. If you're using EMR then I assume you're embedded into the AWS ecosystem and would be using Redshift, in which case your nodes have a fixed cost per month based on their size. If you do something like Snowflake on AWS, then you're looking at a credit-based system. 
2. 
    - a) You don't have to add *any* columns in your .yml files, unless you want to add documentation, quality tests, etc. Columns that aren't explicitly documented will be inferred in the documentation site by inspecting the information_schema or similar when you run `dbt docs generate`, and will populate an artifact file called `catalog.json`. 
    - b) Yes (see above)
3. We have a guide on [migrating from stored procedures](https://docs.getdbt.com/guides/migration/tools/migrating-from-stored-procedures/1-migrating-from-stored-procedures) which might be partially relevant. You should also consider using the [audit-helper](https://hub.getdbt.com/dbt-labs/audit_helper/latest/) package which will help you generate rollup queries to compare your transformations before and after. For more on this, see https://discourse.getdbt.com/t/how-to-not-lose-your-mind-when-auditing-data/445 and https://discourse.getdbt.com/t/how-to-not-lose-your-mind-when-auditing-data-part-ii/612",2
5444,"dbt-external-tables-can-i-create-a-materialized-table-and-not-external-table","Hi,
the package ""dbt_external_tables"" create in snowflake an ""external"" table and I would like to create a ""real"" table (materialized: table).

can I do it with this package?",1
5444,"dbt-external-tables-can-i-create-a-materialized-table-and-not-external-table","External tables have to be created as the `external` type, but once you have created them you can create an additional model which selects * from the external one and is materialized as a table, at which point the data will actually be persisted inside of Snowflake.",2
5444,"dbt-external-tables-can-i-create-a-materialized-table-and-not-external-table","Hi,
that's what I did.
I thought maybe there was another option as a part of the package.

Thanks",3
5444,"dbt-external-tables-can-i-create-a-materialized-table-and-not-external-table","[quote=""itzik, post:3, topic:5444""]
I thought maybe there was another option as a part of the package.
[/quote]

The package just provides an abstraction over Snowflake's functionality - as far as I can tell from looking at Snowflake's docs, there isn't a way to copy data from an external table directly into a ""real"" table without first staging them.",4
5444,"dbt-external-tables-can-i-create-a-materialized-table-and-not-external-table","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5447,"getting-metadata-at-runtime-in-macro","Hello ppl - 
I have this case
```
model1 
{{ config(materialized=""table"", schema=""clean"") }}

model2
{{ config(materialized=""table"", schema=""stg"") }}
```
then
```
{%- macro round_percentage(col) -%}
    ROUND( {{ col }},3)
{%- endmacro -%}
```
use by  model1 or  2 etc
can I make round_percentage able to recognise in which schema is running and and behave in accordance
```
{%- macro round_percentage(col) -%}
if schema = clean
    ROUND( {{ col }},3)
if schema = stg
    ROUND( {{ col }},6)
{%- endmacro -%}
```",1
5447,"getting-metadata-at-runtime-in-macro","I would like to do without passing extra paramas to macro call ofc",2
5447,"getting-metadata-at-runtime-in-macro","Off the top of my head, `target.schema` will probably do this? Otherwise `{{ model.config.schema }}` (see https://docs.getdbt.com/reference/dbt-jinja-functions/model and https://docs.getdbt.com/reference/dbt-jinja-functions/graph#accessing-models). 

If you use the `model` variable, take careful note of the warnings on the graph documentation - the data inside of it is unreliable during the initial parse (when `execute=false`)",3
5448,"performance-of-each-model-over-time","For all the projects where I use dbt, there's one thing from the operational side of things that's missing. The ability to get the data about how a specific model performed over time.

When the whole job takes more time, you have to go to the logs of that job in order to get some information. How do you obtain that information?",1
5448,"performance-of-each-model-over-time","Great question! The most common approaches I see are:
1. If you use dbt Cloud, you can use the [Model Timing Tab](https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-model-timing-tab) which does exactly this: 
![image|689x361](upload://cgZ3skPg9hzegRMVWiGQtbOHQyR.jpeg)
2. Alternatively, you can also use a package like [dbt_artifacts](https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest/) which creates tables in your warehouse containing the results of each dbt invocation, including time spent building each table.",2
5448,"performance-of-each-model-over-time","I don't have this option, probably because I'm not on Team plan. The question is does it only show Model timing for a specific run or for all the runs?",3
5448,"performance-of-each-model-over-time","[quote=""moseleyi, post:3, topic:5448""]
does it only show Model timing for a specific run or for all the runs
[/quote]

The model timing tab shows the analytics for a single run at a time. The data stored by the artifacts package enables you to compare performance from one run to another",4
5448,"performance-of-each-model-over-time","Any plans to make such important data available in an easy way, without installing extra packages? Operationally it was the first thing I asked when considering dbt.",5
5448,"performance-of-each-model-over-time","If you were on the teams plan you could also use direct access to the metadata api to track performance over time. 

I don't have a specific tool recommendation, but that there are also other observability platforms which will ingest this data and handle that for you if you don't want to build it yourself. 

I don't have anything to share on any future product plans, but will pass the message on! I will note that the fact that users can extend the built in dbt functionality and share their results with the community as packages/plugins is a key benefit.",6
5451,"set-the-project-directory-for-dbt-deps-in-dbt-cloud","hello everyone,

in the job settings, a command can be added to override default dbt run command in order to use the flag --project-dir to indicate the location of dbt_project.yml. however, job settings does not recognize dbt deps as a command so it cannot be overriden to use --project-dir to use the same dbt_project.yml as dbt run. packages.yml is still located in root but dbt deps fails if it does not find dbt_project.yml.

is it possible to override dbt deps in dbt-cloud?",1
5451,"set-the-project-directory-for-dbt-deps-in-dbt-cloud","Instead of using `--project-dir`, have you tried setting the Project Subdirectory in `Account Settings > Project Details?`

Your packages.yml and dbt_project.yml file will still need to be at the same depth, but you can specify a subdirectory of the git repo if you have other things beyond just your dbt project inside of a single git repository.",2
5451,"set-the-project-directory-for-dbt-deps-in-dbt-cloud","That is exactly what I was looking for. Thank you so much and the tip for packages.yml.",3
5451,"set-the-project-directory-for-dbt-deps-in-dbt-cloud","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5454,"macro-for-sanitizing-csv","I have CSV with one column:


```
tradehour
2022-11-21 18:00:00
2022-11-21 19:00:00
2022-11-21 20:00:00
2022-11-21 21:00:00
```
I'll use this to iterate over hours to do backfill with backfill model.

When I run dbt seed I get:
```
12:32:20  Database Error in seed frame-report-backfill-hrs (seeds/frame-report-backfill-hrs.csv)
12:32:20    001003 (42000): SQL compilation error:
12:32:20    syntax error line 1 at position 50 unexpected '-'.
```
Even If I put those records in single or double quotations - I get same error. 
What would be proper way of sanitizing this data so it can be properly loaded as single column table?
How can I run some macro to do that automatically?
I would like to just copy paste those timestamps , run macro and have CSV sanitized.
I don't need full solutions just few pointers.
Thanks!",1
5454,"macro-for-sanitizing-csv","You should be able to specify that these are datetimes in your dbt_project.yml: <https://docs.getdbt.com/faqs/seeds/seed-datatypes>

if not, you could fall back to creating it as a string column and then transform it further in a dbt staging model, but that feels like it should be unnecessary

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1669943038345559?thread_ts=1669898375.447309&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",2
5455,"dbt-using-mixed-profiles","I have a dbt project and profile, let's say its name is `abc`. I've run the models in this project before. I recently changed the profile target schema to a new location for a data refresh. However, when I looked at the compiled codes, I noticed that dbt was using mixed target schema 
- sometimes using the target schema from previous, 
- sometimes using the new target schema. 

I have to change the profile name to a different one, i.e. `abc2` to stop confusing dbt. However, this issue has never happened before. I'm not sure why. It made me wonder if it's related to the upgrade to 1.2.1. Can someone help me? Thank you!",1
5458,"how-to-handle-cdc-changes-updates-dein-data-vault-model","Hi Team,

I am working on DBT , we have requirement where I need to handle batch and CDC(change data capture) changes.

In this case input json messages are different in case of batch and CDC. So I need to write parsing logic differently for batch and CDC based on change type received from source.

Example:
  If change type = â€˜CREATEâ€™

       Call batch dbt models(7 models in my case)
       

Else 
  If change type = â€˜UPDATEâ€™

      Call incremental batch models(7 models in my case)

End if.

How to do we handle this flow in DBT?


Thanks!",1
5463,"dbt-postgres-python","Hi.. Will dbt-python work on Postgres Warehouse?",1
5466,"error-message-python-bridge","Hi Guys!

I got an error message when trying to execute a SQL statement. Do you have any idea what the problem could be?

""could not execute query python bridge has not been initialized""

Thanks in advance!",1
5466,"error-message-python-bridge","Are you using dbt power user in vscode? Is this the issue you're running into? https://github.com/innoverio/vscode-dbt-power-user/discussions/235",2
5467,"customising-macros-in-dbt","Hi, Iam a newbie to DBT here and considering the use of macros to customise the snapshot handling of DBT.
Specifically ,Iam trying to replace the names of the default meta columns and then I end up getting errors when i run the snapshot command on the model for the second time.
The error message i get is :  Snapshot target has (""x"", ""y"", ""z"") but not (""dbt_scd_id"", ""dbt_valid_from"", ""dbt_valid_to"") - is it an unmigrated previous version archive?
Could you help me figure out what the mistake iam doing here ? 
Thanks in advance.
Here are the codes that I use 

--  Person Snapshot 
{% snapshot person_snapshot %}

{{
    config(
      
      
      unique_key='ID',
      strategy='check' ,
        target_schema='myschema',
      check_cols='all',
         )
}}
-- Overriding Macros 
{% macro build_snapshot_table(strategy, sql) %}

    select *,
        {{ strategy.scd_id }} as scd_id,
        {{ strategy.updated_at }} as updated_at,
         {{ strategy.updated_at }} as valid_from,
        
        nullif({{ strategy.updated_at }}, {{ strategy.updated_at }}) as valid_to,
        Case When ({{ strategy.updated_at }}= {{ strategy.updated_at }})
        Then 1 
        Else 0 
        End
        as aktiv
    from (
        {{ sql }}
    ) sbq

{% endmacro %}

-- Overriding Macro build staging tables snapshot 
{% macro snapshot_staging_table(strategy, source_sql, target_relation) %}

    with snapshot_query as (

        {{ source_sql }}

    ),

    snapshotted_data as (

        select *,
            {{ strategy.unique_key }} as dbt_unique_key

        from {{ target_relation }}
        where valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            {{ strategy.unique_key }} as dbt_unique_key,
            {{ strategy.updated_at }} as dbt_updated_at,
             {{ strategy.updated_at }} as dbt_valid_from,
            
            nullif({{ strategy.updated_at }}, {{ strategy.updated_at }}) as dbt_valid_to,
            {{ strategy.scd_id }} as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            {{ strategy.unique_key }} as dbt_unique_key,
            {{ strategy.updated_at }} as dbt_updated_at,
            {{ strategy.updated_at }} as dbt_valid_from,
            
            {{ strategy.updated_at }} as dbt_valid_to
           

        from snapshot_query
    ),

    {%- if strategy.invalidate_hard_deletes %}

    deletes_source_data as (

        select 
            *,
            {{ strategy.unique_key }} as dbt_unique_key
        from snapshot_query
    ),
    {% endif %}

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.scd_id = source_data.dbt_unique_key
        where snapshotted_data.scd_id is null
           or (
                snapshotted_data.scd_id is not null
            and (
                {{ strategy.row_changed }}
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.scd_id = source_data.dbt_unique_key
        where (
            {{ strategy.row_changed }}
        )
    )

    {%- if strategy.invalidate_hard_deletes -%}
    ,

    deletes as (
    
        select
            'delete' as dbt_change_type,
            source_data.*,
            {{ snapshot_get_time() }} as dbt_valid_from,
            {{ snapshot_get_time() }} as dbt_updated_at,
            {{ snapshot_get_time() }} as dbt_valid_to,
            snapshotted_data.dbt_scd_id
    
        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.scd_id = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )
    {%- endif %}

    select * from insertions
    union all
    select * from updates
    {%- if strategy.invalidate_hard_deletes %}
    union all
    select * from deletes
    {%- endif %}

{% endmacro %}



select ID,name,CityID
from {{ ref('vw_person') }}


{% endsnapshot %}",1
5467,"customising-macros-in-dbt","A more sustainable way to do this which will not cause problems through upgrades, etc. is to just let dbt Snapshot do its thing... then if you want to change the column names, put a view after the snapshot which re-names the columns.  You can hide the original model from non-developer people in a stg schema and only expose the model with the desired column names to your users.",2
5472,"is-it-possible-to-add-a-description-to-singular-tests","Per topic, is it possible to add descriptions to Singular tests and expose these descriptions to dbt docs?",1
5472,"is-it-possible-to-add-a-description-to-singular-tests","Currently, this is not possible.  See more info here:
https://github.com/dbt-labs/dbt-core/issues/2578",2
5472,"is-it-possible-to-add-a-description-to-singular-tests","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5475,"how-to-pass-variable-and-string-to-macro","Hi,

I would like to use macro returning values depending on parameter like this.

- macros/fn_get_bar.sql
```
{% macro fn_get_bar(foo) %} 
{% if foo== 1 %}
    {{return ('aaa')}}
{% else %}
...
{% endif %}
{%- endmacro %}
```

In this case, how should I call macro in my models to get ""aaa"" ?

Thanks,

Cinyoung",1
5475,"how-to-pass-variable-and-string-to-macro","Hello @hurcy ,

Call it as below as below:
{{fn_get_bar(1)}}

Example in query:

select
  id as payment_id,
  {{ fn_get_bar(1) }} as payment_description,
  ...
from raw_dataset.payments

Regards
Minhaj",2
5478,"trying-to-install-dbt-artifacts","I am getting an error when trying to install dbt artifacts. I have tried to run a model after following these steps https://github.com/brooklyn-data/dbt_artifacts#quickstart and ran into an error (Unable to do partial parsing because config vars, config profile, or config target have changed). To try and investigate this i wanted to run a test or two. https://github.com/brooklyn-data/dbt_artifacts#contributing I tried running pip install pipx
and got this error:
dbt: error: invalid choice: 'pip' (choose from 'docs', 'source', 'init', 'clean', 'debug', 'deps', 'list', 'ls', 'build', 'snapshot', 'run', 'compile', 'parse', 'test', 'seed', 'run-operation'). How should I fix this?",1
5478,"trying-to-install-dbt-artifacts","Hello @emilyrmiles ,

Hope you have completed first two instructions: These two steps is way to install dbt packages
1. Add this package to your `packages.yml`:

```
packages:
  - package: brooklyn-data/dbt_artifacts
    version: 2.2.1
```

2. Run `dbt deps` to install the package



Regards
Minhaj",2
5480,"using-dbt-core-on-heroku-app-with-proxy-to-connect-to-a-private-database","Hi dbt community!

I've connected to private databases using dbt-core before by whitelisting a static IP that the app uses in AWS.

I'm trying this on a **Heroku** app with the [Proximo addon](https://devcenter.heroku.com/articles/proximo#forwarding-traffic-through-proximo) that forwards all the apps traffic through a static IP that I'm whitelisting on the database server's firewall. 

When I run 'dbt debug' in this environment, I get ""All checks passed"". But when I try to run one of my models, I get a Database Error: ""timeout expired"". I've never seen a situation where debug succeeds and run fails to connect! I've also successfully connected to the database in this environment in a python app that uses psycopg2. Please share any insight that you may have. Thank you! Detailed logs below:

dbt run logs:
```
19:17:22.536295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd8ce4c8-6736-4d26-a10f-d2820dffd438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc29836d0>]}
19:17:22.544922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd8ce4c8-6736-4d26-a10f-d2820dffd438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc2982fb0>]}
19:17:22.546241 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 193 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
19:17:22.546740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd8ce4c8-6736-4d26-a10f-d2820dffd438', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc314e470>]}
19:17:22.549925 [info ] [MainThread]: 
19:17:22.550592 [debug] [MainThread]: Acquiring new redshift connection ""master""
19:17:22.555779 [debug] [ThreadPool]: Acquiring new redshift connection ""list_dev""
19:17:22.581653 [debug] [ThreadPool]: Using redshift connection ""list_dev""
19:17:22.583627 [debug] [ThreadPool]: On list_dev: /* {""app"": ""dbt"", ""dbt_version"": ""1.1.0"", ""profile_name"": ""ip_test"", ""target_name"": ""prod"", ""connection_name"": ""list_dev""} */

    select distinct nspname from pg_namespace
  
19:17:22.584550 [debug] [ThreadPool]: Opening a new connection, currently in state init
19:17:22.585092 [debug] [ThreadPool]: Redshift adapter: Connecting to Redshift using 'database' credentials
19:17:32.606154 [debug] [ThreadPool]: Postgres adapter: Got an error when attempting to open a postgres connection: 'connection to server at ""[removed for privacy]"" ([removed for privacy]), port 5439 failed: timeout expired
'
19:17:32.606839 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {""app"": ""dbt"", ""dbt_version"": ""1.1.0"", ""profile_name"": ""ip_test"", ""target_name"": ""prod"", ""connection_name"": ""list_dev""} */

    select distinct nspname from pg_namespace
  
19:17:32.607185 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
19:17:32.608147 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
19:17:32.608396 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
19:17:32.609018 [debug] [ThreadPool]: On list_dev: No close available on handle
19:17:32.611206 [debug] [MainThread]: Connection 'master' was properly closed.
19:17:32.611439 [debug] [MainThread]: Connection 'list_dev' was properly closed.
19:17:32.611669 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at ""[removed for privacy]"" ([removed for privacy]), port 5439 failed: timeout expired
  
19:17:32.612484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc2983a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc4b2e710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fc41c5390>]}
19:17:32.612953 [debug] [MainThread]: Flushing usage events
```

dbt debug logs:

```
Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]

Required dependencies:
19:26:32.050026 [debug] [MainThread]: Executing ""git --help""
19:26:32.060361 [debug] [MainThread]: STDOUT: ""b""usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""""
19:26:32.061313 [debug] [MainThread]: STDERR: ""b""ERROR: ld.so: object 'libdl.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n""""
 - git [OK found]

Connection:
  host: [removed for privacy]
  port: 5439
  user: awsuser
  database: dev
  schema: public
  search_path: None
  keepalives_idle: 240
  sslmode: None
  method: database
  cluster_id: None
  iam_profile: None
  iam_duration_seconds: 900
19:26:32.066420 [debug] [MainThread]: Acquiring new redshift connection ""debug""
19:26:32.067448 [debug] [MainThread]: Using redshift connection ""debug""
19:26:32.067980 [debug] [MainThread]: On debug: select 1 as id
19:26:32.073439 [debug] [MainThread]: Opening a new connection, currently in state init
19:26:32.074219 [debug] [MainThread]: Redshift adapter: Connecting to Redshift using 'database' credentials
19:26:32.359175 [debug] [MainThread]: SQL status: SELECT in 0.28 seconds
19:26:32.362744 [debug] [MainThread]: On debug: Close
  Connection test: [OK connection ok]

All checks passed!
19:26:32.377432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae8e270fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae8e271330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae8e2738e0>]}
19:26:32.378168 [debug] [MainThread]: Flushing usage events
19:26:32.676501 [debug] [MainThread]: Connection 'debug' was properly closed.
```",1
5483,"referring-current-model","Hello All,

I have a table orders and iam using snapshot to build the data change history.I am using  dbt utils macro to filter column names to generate surrogate key.

Iam generating surrogate key **orders_surrogate_key** will all columns expect unique column(orders_id)
The model is as below.

**{% snapshot **orders** %}**
    {{
        config(
          unique_key='orders_id',
          strategy='check',
          check_cols=['**orders_surrogate_key**'],
          partition_by={
                ""field"": ""file_created_date"",
                ""data_type"": ""date"",
                ""granularity"": ""day""
                 },
      ) }}

orders_surrogate_key_cols=dbt_utils.get_filtered_columns_in_relation(from=ref('**orders**'), except=[""order_id""])

select
    *,
    {{ generate_key(orders_surrogate_key_cols) }} 
    as orders_surrogate_key
    from {{ ref('stg_orders') }}

**{% endsnapshot %}**

When i try to run the model i gives me below error:
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py"", line 89, in compile_manifest
    self.graph = compiler.compile(self.manifest)
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/compilation.py"", line 480, in compile
    self.link_graph(linker, manifest, add_test_edges)
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/compilation.py"", line **418, in link_graph**
**    raise RuntimeError(""Found a cycle: {}"".format(cycle))**
**RuntimeError: Found a cycle: snapshot.dbt_project.orders**
**07:42:51  Encountered an error:**
**Found a cycle: snapshot.dbt_project.orders**


I tried to use {{this }}
orders_surrogate_key_cols=dbt_utils.get_filtered_columns_in_relation(from={{this}}, except=[""order_id""])
is giving  different error:

Please let to know how to refer the current models to be passed in dbt.utils.macros for eg:get_filtered_columns_in_relation

Thanks in advance
Minhaj Pasha",1
5483,"referring-current-model","Create a view model that comes before the snapshot that contains the orders_surrogate_key",2
5486,"implementing-scd2-best-practices","Hi All,

Looking for Best Practices in DBT - Snowflake to implement SCD2. Trying to understand if Snapshot is the final layer of the data in warehouse or do we have any other approach where we store the SCD2 data and expose it downstream applications.

Thanks
Santi",1
5486,"implementing-scd2-best-practices","[quote=""SSChoudhury, post:1, topic:5486""]
Looking for Best Practices in DBT - Snowflake to implement SCD2. Trying to understand if Snapshot is the final layer of the data in warehouse or do we have any other approach where we store the SCD2 data and expose it downstream applications.
[/quote]

This is going to widely vary based on what the business users want to see.  The number of scenarios are limitless.  But I think the best answer is always start with dbt Snapshot and then model from there.

Here are some generalizations....
**Snapshotting Dimensions**:
Start with dbt Snapshot.  Then work with the end users on their requirements.  Do they want to see  the dimension data as of the time of the transaction, or do they always want to see today's version of the dimension?  Then I build a model that solves their requirement.  Usually the business users do not query the dimension table directly.  We join it to a fact table for them and present the BI tool with one wide table and all of the columns.  (The BI Users will not need to know how to use the valid_from / valid_to dates).

**Snapshotting Facts**:
Again start with dbt Snapshot.  We usually have a model after the snapshot to only display the active record so the Business folks don't need to worry about filtering in the BI tool.  Occasionally users will require a ""snapshot trend view"", example, count active rows by day displayed on a bar chart in the BI tool.  In this case we need to create a view to populate the entire data set (active rows) for each calendar date.",2
5490,"pipelines-failing-with-incremental-models-due-to-dbt-temp-tables-left-behind","Hi there,

We are having an issue with most of our pipelines using incremental models. They fail every morning due to dbt internal temp tables already existing(__dbt_tmp). Has any one faced this issue and managed to fix it?",1
5490,"pipelines-failing-with-incremental-models-due-to-dbt-temp-tables-left-behind","hi, database? version dbt, version connector? error code?",2
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","Hey everyone, 

can cloud dbt jobs be pinned to a commit? I know packages.yml can but it seems that cloud dbt always takes HEAD. It would be extra special if it can so using a main can be used for commits without those commits being deployed to production.",1
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","I believe specifying custom branch will take a tag or commit too. Or at the least thatâ€™s the first thing Iâ€™d try",2
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","Nice, I will try it out. That makes sense if I think about it - checking out a commit is not really any different than checking out a custom branch. Tag maybe a stretch too far but

I will reply once I confirm. This will be my second great find this week that I thought was something I could not do. Woot woot!",3
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","Close, but no cigar. Looks like it cloned and checked out the commit in the beginning but then tries to hard reset to the commit from origin and fails. Maybe I did something wrong? The only thing I did was to replace the custom branch name with the 7 char commit which seems to work fine but fails in the last few steps.

Output in dbt cloud console:
Cloning into '/tmp/jobs/103062679/target'...
Successfully cloned repository.
Listing tags in repo...
Checking out to aee4192...
Note: switching to 'aee4192'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at aee4192 feat: updated sqlfluff byte limit, clean-targets for dbt_project, added package.json to support git hooks, added prettier to devcontainers, and added profiles.yml to cloud-dbt with last updated profiles.yml from dbt_project_dan
Checking out branch 'aee4192'Failed to checkout to specified revision.
git reset --hard origin/aee4192
fatal: ambiguous argument 'origin/aee4192': unknown revision or path not in the working tree.
Use '--' to separate paths from revisions, like this:
'git <command> [<revision>...] -- [<file>...]'",4
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","So, seems I can't get back to checking out branches on that environment. I have tested pretty thoroughly with updating and attempting to build with a branch name but the commit id seems to stick. I updated to a new commit id and it finally updated but no luck with an actual branch name. Weird! I blame user error - but I know not what :).",5
5497,"cloud-dbt-using-git-and-pinning-to-a-commit-or-tag","I will take it no other comments for 8 days from the community means that the community does not know if cloud dbt can pull anything but head and I consider this question closed.

I would request cloud dbt team add this to their feature list. dbt labs must understand the importance of pinning to a commit revision or tag(tags are better) if that functionality exists in packages.yml. This is an important capability for deployments through cloud dbt and forces SDLC branching paradigms to stay off of main/production branch for staging/shippable increment builds.",6
5504,"completing-the-project-wizard","I am a complete dbt newbie. I'm guessing that the process for creating a new project is as follows. Please correct or add anything I got wrong or missed:

1. Create a Snowflake account, login, create a sample database, create a table, and fill it with sample data. Note the account credentials for later use.
2. Note the special Snowflake connection parameter, for later use.
2. Create a dbt account
3. In the project creation wizard, put the connection parameter you noted in step 2 into the Account textbox.
4. put the name of the database you created in Snowflake in the database textbox.
5. put the word 'Snowflake' in the warehouse textbox.
5. put ACCOUNTADMIN in the role textbox
6. use the Snowflake credentials you created in step 1 in the Development Credentials Username and Password textboxes.
7. Click the 'Test Connection' button.",1
5504,"completing-the-project-wizard","Hello @pudnik26354 , welcome to the dbt community!

Overall, see this link first.
https://docs.getdbt.com/docs/get-started/getting-started/getting-set-up/setting-up-snowflake

Specific notes on your steps:
[quote=""pudnik26354, post:1, topic:5504""]
2. Note the special Snowflake connection parameter, for later use.
[/quote]
It should be in the form of:  `my-account-id.my-region.my-cloud-provider` for example `ab138543.east-us-2.azure` or `zo234532.east-us-2.aws`

[quote=""pudnik26354, post:1, topic:5504""]
put the word â€˜Snowflakeâ€™ in the warehouse textbox.
[/quote]

the warehouse is not ""Snowflake"", it is the name of the [Virtual Warehouse](https://docs.snowflake.com/en/user-guide/warehouses.html) within snowflake.

[quote=""pudnik26354, post:1, topic:5504""]
put ACCOUNTADMIN in the role textbox
[/quote]

You can, but I would use a different role.  See [here](https://www.getdbt.com/blog/how-we-configure-snowflake/) and [here](https://discourse.getdbt.com/t/setting-up-snowflake-the-exact-grant-statements-we-run/439)",2
5504,"completing-the-project-wizard","Ok, but there are TWO 'warehouse' textboxes on the new project form. The first one appears under the pick-the-vendor area, and the second one appears in the snowflake account area.

In snowflake, if one clicks on the area in the lower lefthand corner to the right of the snowflake icon, one can get all the info needed to properly format the 'Account' parameter for the dbt new project form. That I get.

But what does one put in the first 'warehouse' textbox?

Thanx for your help,

John",3
5504,"completing-the-project-wizard","The first Warehouse text box should be greyed out since you have already selected ""Snowflake"" in step 1.

Let's try with some screenshots:
![image|690x138](upload://G2mvO2MrI7LeJqURt1Wg6ccZGl.png)

Then...
![image|690x342](upload://ukErh16WmHs90PTdW755Gg2ZuSj.png)",4
5504,"completing-the-project-wizard","...I think it worked! so now I just need a sample dbt script/model that will run a simple query on the default sample Snowflake database....",5
5504,"completing-the-project-wizard","The little issue is for the ones using snowflake trail ... in the new ui, there no partner connect item in the menu as reported in the image in the guide... but it works using a more manual entry as above",6
5504,"completing-the-project-wizard","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
5506,"how-to-clean-orphaned-schema-snowflake-specific","Hi,

   I'm new to dbt and there's a requirement to drop a snowflake schema if it is already orphaned in dbt. Do you have any suggestions on how to do it?",1
5506,"how-to-clean-orphaned-schema-snowflake-specific","Have a look at https://discourse.getdbt.com/t/clean-your-warehouse-of-old-and-deprecated-models/1547 for some examples!",2
5506,"how-to-clean-orphaned-schema-snowflake-specific","thanks for that joellabes. do you know if dtb has custom_schema instead of custom_models?",3
5506,"how-to-clean-orphaned-schema-snowflake-specific","Do you mean <https://docs.getdbt.com/docs/build/custom-schemas>?",4
5523,"reference-model-name-variable-from-config-portion-of-model-file-in-macro-from-ref-clause","Hi! I am wondering if it is possible to reference the model name from the model file config portion in the macro. The source table name is different on the model level, so it make sense to specify it in the config, and write macro using the variable name, so it can be used on all models. However, the format for the model should be as follows: {{ ref('model_name') }}, so when I use the variable it is not adding the quotes, and doesn't compile correctly.
Config:
{{
    config(
        materialized='incremental',
        incremental_strategy='delete+insert',
        unique_key='unique_key',
        tags=""incremental"",
        source_date_column=""balance_date"",
        target_date_column=""balance_date"",
        source_date_table=""asset_accounts_transactions""
    )
}}

Macro(one of the options that I tried and that is not working):

{% macro incremental_condition (report_hour=0, date_offset_hour=0) %}

{%- set source_date_column = config.require('source_date_column') -%}
{%- set target_date_column = config.require('target_date_column') -%}
{%- set source_date_table = ""{{ ref('"" + config.require('source_date_table') + ""') }} ""-%}

 WHERE

 {% if is_incremental() %}

    DATEADD(HOUR, {{report_hour}}, DATE(DATEADD(HOUR, {{date_offset_hour}}, {{source_date_column}} ))) > (
        SELECT MAX( {{target_date_column}} ) - {{ var('incremental_lookback_days') }} FROM {{ this }}

        )
{% else %}

    {{source_date_column}} >= (
        SELECT MIN( {{source_date_column}} ) FROM {{source_date_table}}

{% endif %}

{%- endmacro -%}


Is there a specific syntax for {{source_date_table}} ? So it adds the full path for the model schema.db_name.model_name ?",1
5528,"use-dbt-metrics-in-dbt-core","Is dbt_metrics package available for dot-core?",1
5528,"use-dbt-metrics-in-dbt-core","[quote=""raymond_liu, post:1, topic:5528, full:true""]
Is dbt_metrics package available for dot-core?
[/quote]
assuming ""dot-core"" is a typo and you meant ""dbt-core"", then the answer is YES!

https://hub.getdbt.com/dbt-labs/metrics/latest/

Currently, if you are running dbt version `>=1.3.0-b1, <1.4.0` then you can install this package:

```
  - package: dbt-labs/metrics
    version: 1.3.2
```",2
5528,"use-dbt-metrics-in-dbt-core","Thank you so much for confirmation!",3
5528,"use-dbt-metrics-in-dbt-core","Sure thing.  Please mark the answer as solved if it helped!",4
5528,"use-dbt-metrics-in-dbt-core","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
5537,"re-open-thread-on-insert-only-in-incremental-load","With a unique key in the .yml file, the merge statement in an incremental model looks like this:	
	
    merge into `bigquery_table_tgt` as DBT_INTERNAL_DEST
        using (
          select * from `bigquery_table_tgt__dbt_tmp`
        ) as DBT_INTERNAL_SOURCE
        on 
                    DBT_INTERNAL_SOURCE.key = DBT_INTERNAL_DEST.key
    
    when matched then update set
        `key` = DBT_INTERNAL_SOURCE.`key` --etc 
    

    when not matched then insert
        (columns etc)
    values
        (values etc)
		
A user asked how to achieve insert only in an incremental model and the proposed solution was to remove the unique key in the .yml file. 
Without a unique key in the .yml file, the merge statement looks like this:

merge into `bigquery_table_tgt` as DBT_INTERNAL_DEST
        using (
          select * from `bigquery_table_tgt__dbt_tmp`
        ) as DBT_INTERNAL_SOURCE
        on FALSE 
		
		#no join on source/target ->> you will have to check source against target yourself to avoid duplicates

    when not matched then insert
        (columns etc)
    values
        (values etc)	


The solution I really would like to see is this		

With a unique key in the .yml file: 
#new feature in dbt:
merge_option: insert_only  

    merge into `bigquery_table_tgt` as DBT_INTERNAL_DEST
        using (
          select * from `bigquery_table_tgt__dbt_tmp`
        ) as DBT_INTERNAL_SOURCE
        on 
                    DBT_INTERNAL_SOURCE.key = DBT_INTERNAL_DEST.key
    
    # remove the following statement when merge_option = insert_only
	#     when matched then update set `key` = DBT_INTERNAL_SOURCE.`key` --etc 
    #but of course keep this:

    when not matched then insert
        (columns etc)
    values
        (values etc) 
Will this kind of feature ever show up in dbt?",1
5537,"re-open-thread-on-insert-only-in-incremental-load","Thank you for your question! I ran into a similar issue (I want to create a historized table based on Full-Load (or Delta Load) coming regularly to an ""inbound/staging"" table).

They only solution I found (so far) was to use the ""incremental_predicates"" https://docs.getdbt.com/docs/build/incremental-models#about-incremental_predicates

In the documentation, notice this sentence that is the key for this (hacky) solution: ""dbt does not check the syntax of the SQL statements.""
When DBT see this incremental_predicates, it adds a AND statement in the ON (merge_condition) and put your additional statement without any further check. This is very useful for our problem because we can then hackily add:
DBT_INTERNAL_SOURCE.key = DBT_INTERNAL_DEST.key) OR (FALSE

DBT then compile it to this:
on (
DBT_INTERNAL_SOURCE.key = DBT_INTERNAL_DEST.key) OR (FALSE
) and (FALSE)

Which is what we were trying to implement.

Long term I obviously would love to remove this hack and have this insert_only statement too.",2
5538,"is-pyspark-supported-in-dbt","I am a bit confused with DBT support with PySpark.
I have read one article where DBT supported PySpark  with DataBricks, but I wanted to know whether DBT supports PySpark with Snowflake. I know, we can write Python code with Snowpark, and PySpark  and Python are pretty similar. But I couldn't find a very concrete answer on whether PySpark is supported in DBT with Snowflake.",1
5541,"how-do-you-delete-a-project-in-dbt-cloud","I have a project in DBT Cloud that was created by mistake.  I am at a loss to figure out how to remove it from the UI.  Can someone help?",1
5541,"how-do-you-delete-a-project-in-dbt-cloud","If you have permissions to manage projects in your account, you can click the gear icon in the top right and select Account Settings: 
![image|689x341](upload://pQnUWEBDVKBf0qJqzeiNKd90t1m.png)

From there, you can click a Project's name to bring up this slideover:
![image|644x500](upload://h9whfpMxAt3Qmnnh0j91u0cIkyG.png)

And then click Edit at the bottom right to bring up the delete button 
![image|669x500](upload://wNNHg9CD9SPqPyeCL3Z2CGrQo6U.png)",2
5550,"customizing-dbt-test-output-name","Is there a way to customize the auto-generated dbt test name? By default I see it's test_name, model name, properties.... Can I customize the name myself?

![image|690x51](upload://AtEYE6OKfHCCJawuCRdOIJoVHkl.png)",1
5550,"customizing-dbt-test-output-name","Curious... customize how given those examples  ?",2
5550,"customizing-dbt-test-output-name","Let's say for the first one, I just wanted to call it ""is_unique"" (for example, if I knew it would be the absolutely ONLY place I was calling this test)

(This is for a case where the generated name is extremely long since there are a lot of properties past in. I'd like to truncate the name to make the output easier to parse)",3
5550,"customizing-dbt-test-output-name","You can do this by using the `name` property on the test: <https://docs.getdbt.com/reference/resource-properties/tests#define-a-custom-name-for-one-test>",4
5550,"customizing-dbt-test-output-name","Does that imply that I can use the list_unexpected_properties test, renaming it to contract_unexpected_properties in this particular test? That doesn't seem to work here. Am I misunderstanding something?

![image|360x272](upload://watY4FUEOmUpqx1DnzfWcTIh7nN.png)

I get a compilation error that the 'test_contract_unexpected_properties' is undefined",5
5550,"customizing-dbt-test-output-name","This is the example code from the docs:
```yaml
version: 2

models:
  - name: orders
    columns:
      - name: status
        tests:
          - accepted_values:
              name: unexpected_order_status_today
              values: ['placed', 'shipped', 'completed', 'returned']
              config:
                where: ""order_date = current_date""
```

In your case, you would do something like: 
```
models: 
  - name: contract_schema
    columns: 
      - name: key
        tests:
          - list_unexpected_properties:
              name: contract_unexpected_properties
              value_set: [caseId, billingMethod, ...]
```

In the code you provided, you're trying to attach two tests to the `key` column, a `contract_unexpected_properties` test with no arguments (like the built-in `unique` or `not_null` tests) and a `list_unexpected_properties` test which has a `value_set` argument.",6
5550,"customizing-dbt-test-output-name","That is beautiful and exactly what I'm looking for. Thank you!

For my own understanding, I don't see the exact sample code you pasted in the docs you linked. Can you help me find that? It would have made things much clearer for me initially.",7
5550,"customizing-dbt-test-output-name","It's towards the bottom of the page (screenshot below); if you can't see it, check which version of the docs you're using (in my screenshot I have v1.3 selected). If you are looking at an old version then it might not show up; I don't remember when this functionality was added! 

![image|690x449](upload://2fK6a9YmYQprlYNS2oPQ1wf6G1S.jpeg)",8
5550,"customizing-dbt-test-output-name","Changing the version did it for me. Good to know that it was on v1.0 by default and I didn't easily see how to change it. Hopefully, this makes things much clearer going forward.

Thanks again for your help!",9
5550,"customizing-dbt-test-output-name","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",10
5557,"use-of-bigquery-multi-statement-transaction-in-dbt","Dear all,
I am trying to integrate DBT in a workflow where I have a set of table in big query being continuously updated by a 3rd party tool (insert or update only).
I know that the provider is supposed to guarantee the consistency of the table with respect to some statements, ie. the set of table is always updated with the transaction feature.
Know, I am wondering if I can regularly extract a snapshot of all the tables throughout the use of multi statement transactions in bigquery:
https://cloud.google.com/bigquery/docs/reference/standard-sql/transactions
To me it seems that for a specific set of model (in my case the snapshot models), I could potentially configure  pre/post hook in dbt project like in here: https://docs.getdbt.com/reference/resource-configs/pre-hook-post-hook to use that feature for my usecase.
In my understanding it should be possible to have the pre-hook (ie begin transaction) running before the first model of the directory is executed, and the post hook (end) is ran after the last model of the directory.

My questions are now:
1:  Is my understanding correct, a pre-post hook can be ran only once before and after a set of models ?
2: See the following comment from dbt hook documentation:

> To achieve this, you can use one of the following syntaxes. (Note: You should NOT use this syntax if using a database where dbt does not use transactions by default, including Snowflake, BigQuery, and Spark/Databricks.)

```
BEGIN

  BEGIN TRANSACTION;
  INSERT INTO mydataset.NewArrivals
    VALUES ('top load washer', 100, 'warehouse #1');
  -- Trigger an error.
  SELECT 1/0;
  COMMIT TRANSACTION;

EXCEPTION WHEN ERROR THEN
  -- Roll back the transaction inside the exception handler.
  SELECT @@error.message;
  ROLLBACK TRANSACTION;
END;
```


Why one couldn't use this syntax with bigquery as bigquery is supposed to support it ?

Thank you in advance for your help

Best regards",1
5557,"use-of-bigquery-multi-statement-transaction-in-dbt","Trying to up that topic for visibility (I'll delete the message if this is an issue)",2
5560,"exposure-support-for-docs-block-and-markdown-files","Hello,

I have created an exposure and in the description field I have added doc JINJA macro to reference markdown file with more information. However, after dbt docs generate; dbt docs serve; the web page section which should display markdown contents says 'no documentation'.
General question, if description field in exposure supports markdown files and docs blocks as in dbt models, dbt sources?

![image|668x182](upload://hjSTk1vBPsrpEPIRNRohRAoIAYo.png)

![image|424x93](upload://mc00g8Y2mXZtcCUq7fXqjExHVnn.png)

![image|293x499](upload://6WdQtH09AxQVuQvFBWkr8zGWdX5.png)",1
5560,"exposure-support-for-docs-block-and-markdown-files","We have upgraded to dbt 1.3.1 and now exposures display markdown docs. I guess question is resolved.",2
5562,"how-to-extract-artifacts-manifest-catalog-run-results-and-sources-files","Hello Everyone,

I'm trying to ingest dbt metadata into datahub for which I need the artifact files (manifest, catalog, run-results and sources).

Please help me where to find or how to create if they do not exist by default.

Thanks",1
5562,"how-to-extract-artifacts-manifest-catalog-run-results-and-sources-files","These artifacts are generated after dbt is invoked (different files come from different commands) - see [dbt Artifacts](https://docs.getdbt.com/reference/artifacts/dbt-artifacts)

They will be found in your `target` directory (or wherever you have defined [target-path](https://docs.getdbt.com/reference/project-configs/target-path) to be)",2
5577,"notimplementederror-when-running-dbt-version-check-on-installing-dbt-bigquery","I am trying to install dbt bigquery in my Windows system by running the following pip commands -

```
pip install dbt-bigquery
```

Installation has finished successfully but when I run the dbt --version command here is the error I am getting

> Traceback (most recent call last): File """", line 198, in *run_module_as_main File """", line 88, in *run_code File ""C:\Users\1354750\Documents\code\env\Scripts\dbt.exe_*main**.py"", line 4, in File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\dbt\main.py"", line 2, in from dbt.logger import log_cache_events, log_manager File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\dbt\logger.py"", line 16, in from dbt.dataclass_schema import dbtClassMixin File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\dbt\dataclass_schema.py"", line 15, in from mashumaro import DataClassDictMixin File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro_*init**.py"", line 4, in from mashumaro.serializer.json import DataClassJSONMixin File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\serializer\json.py"", line 28, in class DataClassJSONMixin(DataClassDictMixin): File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\serializer\base\dict.py"", line 16, in **init_subclass** builder.add_from_dict() File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\serializer\base\metaprogramming.py"", line 270, in add_from_dict pre_deserialize = self.get_declared_hook(**PRE_DESERIALIZE**) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\serializer\base\metaprogramming.py"", line 255, in get_declared_hook if not is_dataclass_dict_mixin(cls): ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\meta\helpers.py"", line 247, in is_dataclass_dict_mixin return type_name(t) == DataClassDictMixinPath ^^^^^^^^^^^^ File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\meta\helpers.py"", line 93, in type_name elif is_generic(t) and not is_type_origin: ^^^^^^^^^^^^^ File ""C:\Users\1354750\Documents\code\env\Lib\site-packages\mashumaro\meta\helpers.py"", line 161, in is_generic raise NotImplementedError NotImplementedError

Prior to the dbt command, I am running this in a Python virtual environment in the 'code' folder using the following commands.

```
python -m venv env
.\env\Scripts\activate
```

Can someone help me what the error is pointing at?",1
5577,"notimplementederror-when-running-dbt-version-check-on-installing-dbt-bigquery","I had an incompatible Python version installed in the system (3.11.x) which was causing the error. Reference - [What version of Python can I use?][1]. The issue is now resolved.


  [1]: https://docs.getdbt.com/faqs/core/install-python-compatibility",2
5577,"notimplementederror-when-running-dbt-version-check-on-installing-dbt-bigquery","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5579,"schema-yml-reading-a-view-as-a-source","For the yml file, are sources **view** type that can be used as a source?  Our source systems have logical views, and we don't want to duplicate the complex logic in dbt schema.yml to avoid sync issues. It seems that yml doesn't have a views type option.

Models are materializing as a view, so it compiles a nested view reading a view or CTA reading a view.  Hence, it produces an unnecessary nested view redundancy. Is there another better practice?

Thanks,
Laura B
 
Sample schema.yml
```yaml
sources:
  - name: table label
    schema: schema name
    tables:
      - name: database table name
```",1
5579,"schema-yml-reading-a-view-as-a-source","Welcome @lbert!

[quote=""lbert, post:1, topic:5579""]
Our source systems have logical views
[/quote]
Does this mean you are building your dbt project inside of the same production database where the data is created, as opposed to using an ETL tool to move the data into a separate analytics warehouse? 

[quote=""lbert, post:1, topic:5579""]
we donâ€™t want to duplicate the complex logic in dbt schema.yml to avoid sync issues
[/quote]
Can you give an example of what you mean here? There won't be any complex transformation logic in a yaml file, and there is no yaml in a view definition so I don't understand what logic you're worried about duplicating. 

[quote=""lbert, post:1, topic:5579""]
Models are materializing as a view, so it compiles a nested view reading a view or CTA reading a view. Hence, it produces an unnecessary nested view redundancy. Is there another better practice?
[/quote]
In the dbt paradigm, it's extremely common to have a stack of views reading from one another, as this enables you to break up your modelling logic into individual steps. Taken to an extreme this results in performance issues, so we [recommend](https://github.com/dbt-labs/dbt-project-evaluator#chained-view-dependencies) limiting the number of views chained together without a table along the way, but otherwise this isn't an antipattern or something you need to worry too much about.",2
5579,"schema-yml-reading-a-view-as-a-source","Thank you for your response. There is a table_type option. The data provider owns the source view, and it contains business logic. DBT project can read a view vs creating a model that uses view and then creates a view of view.

scheme.yml
sources:
  - name: bi
    schema: bi
    tables:
      - name: fact_email_funnel
        identifier: fact_email_funnel
        table_type: table
        description: email campaign engagement metrics
      - name: fact_clickstream
        identifier: fact_clickstream
        **table_type: view**
        description: web clickstream events",3
5579,"schema-yml-reading-a-view-as-a-source","[quote=""lbert, post:3, topic:5579""]
There is a table_type option
[/quote]

Can you link me to the dbt documentation where you found this setting? I don't see it in the [source properties documentation](https://docs.getdbt.com/reference/source-properties) and have never heard of it. 

[quote=""lbert, post:3, topic:5579""]
DBT project can read a view vs creating a model that uses view and then creates a view of view.
[/quote]
Regardless, you can query sources that are views if you want to. Our modelling best practice strongly recommends using a staging layer to ensure that you only directly access sources once (and then build other models on top of the staging layer), but you can query them directly if you want to remove a layer of abstraction. 

If you choose to go down that path, keep in mind that dbt projects pretty much always have views stacked on top of each other. It's not clear to me why it's important to you that the source not be a view on top of a view, when your downstream models are very very likely to be views on top of views anyway.",4
5579,"schema-yml-reading-a-view-as-a-source","I understand that one of DBT mainframe work is stacked views. In the past, we have seen performance issue with nested views. Sometimes phyical tables are the best options. 

The great benefit of views is having business logic can be change quickly with litle impact and quick turnaround for the stackholders and little backfill efforts.",5
5584,"incremental-load-first-run-check-table-exists-contains-rows","I understand on the first run of an incremental model all source rows will be added and a table created if it doesn't exist:

> Incremental models are built as tables in your data warehouse. The first time a model is run, the table is built by transforming *all* rows of source data. On subsequent runs, dbt transforms *only* the rows in your source data that you tell dbt to filter for, inserting them into the target table which is the table that has already been built.

In my case, the source model contains multiple rows per ID:

|ID|Region|Timestamp|
|---|---|---|
|1|X|01/01/2023|
|1|Z|02/01/2023|
|1|A|03/01/2023|

Within my incremental model I'd like to include a single row per ID that's associated with the latest timestamp.  In the example provided above only the following row would appear in the incremental model:
|ID|Region|Timestamp|
|---|---|---|
|1|A|03/01/2023|

Each time I run the incremental model I'd like to:
* Check if the SQL table generated by the incremental model already exists on the database
* For robustness, if the SQL table exists is not empty/contains rows

If the table doesn't exists or the table does not contain any rows the following script would run:

```
SELECT a.*
FROM  {{ref('orders')}}  a
INNER JOIN (SELECT id, MAX(timestamp) AS MAX_timestamp 			
            FROM  {{ref('orders')}} 
			GROUP BY id) AS max_a ON a.id = max_a.id
```

I believe the initial checks above would be common design pattern and was wondering if there are any standard design patterns or set of functions to use?

I found the following thread helpful but would like to ask if others have any alternative suggestions/ best practices?

https://discourse.getdbt.com/t/writing-packages-when-a-source-table-may-or-may-not-exist/1487

Any pointers would be much appreciated ðŸ‘",1
5584,"incremental-load-first-run-check-table-exists-contains-rows","dbt incremental models are the same as any other model type, in that you don't need to (and shouldn't!) spend any time thinking about whether the table already exists, and minimal time thinking about whether the table is empty or not. 

This manifests as a query that transforms data in the same way whether or not the table exists/contains rows, with the only difference being _how much data is processed_. 

Have you looked into the [`is_incremental()`](https://docs.getdbt.com/docs/build/incremental-models#understanding-the-is_incremental-macro) macro? Basically what you'd do is 

```sql
with orders as (
  select * from {{ ref('orders') }}
),

windowed as (
  select   
    id, 
    region, 
    timestamp, 
    row_number() over (partition by id order by timestamp desc) as rownum
  from orders

  {% if is_incremental() %}
    -- if the table doesn't exist, then every order row will be processed. 
    -- If it does, then only rows that have changed since the last invocation need to be processed.
    where exists (
      select null   
      from {{ this }} as this
      where this.id = orders.id
      and orders.timestamp > this.timestamp
    )
  {% endif %}
),

final as (
  select 
    id,
    region,
    timestamp
  from orders
  where rownum = 1
)

select * from final
```

The article you linked refers to _source tables_ existing or not, i.e. the raw untransformed tables created by your ETL tool.",2
5584,"incremental-load-first-run-check-table-exists-contains-rows","Many thanks for this.
My initial SQL query to populate the incremental model wasn't in a particularly dbt friendly format.

Your example has been a great help and I can use that as a template in the future. ðŸ‘",3
5584,"incremental-load-first-run-check-table-exists-contains-rows","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5592,"source-from-staging-output-to-dev","Is there anyway to build a model (`dbt run`) and have `ref` source from one schema for model relationships, but have the run output to a different schema?  i.e. avoid having to build upstream models by just running against a staging schema that already has all the models built, but output the new model (and/or downstream models) to a dev schema.",1
5592,"source-from-staging-output-to-dev","@Proton look into deferral - this should be exactly what you're after! https://docs.getdbt.com/reference/node-selection/defer

For an example of this in the real world (in a CI context instead of a development context, but the behaviours are the same), check out [How we sped up our CI runs by 10x using Slim CI](https://discourse.getdbt.com/t/how-we-sped-up-our-ci-runs-by-10x-using-slim-ci/2603)",2
5592,"source-from-staging-output-to-dev","Yes, this is exactly what I was after.  Thanks!",3
5592,"source-from-staging-output-to-dev","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
5602,"filter-out-debug-logs","Is there a way to just log only info, warning, and error logs into dbt.logs without logging debug logs?",1
5613,"apply-multiple-macros-as-hooks-on-a-model","Hello,

I'm currently trying to develop a model to which i want to deploy some quality checks in terms of data quality. The thing is, as it stands i can't address on post_hook more than one macro. How can i work around this to reference multiple macros in the same model?

**Example:**
post_hook=""{{ Macro1(Arg1, Arg2,Arg3) }},{{ Macro2(Arg4, Arg5,Arg6,Arg7) }}""

Thank you.",1
5613,"apply-multiple-macros-as-hooks-on-a-model","Hey @jpgsa11! To call multiple macros, you should pass each one in as its own element of a list: 

```sql
--some_model.sql
{{ config(
  post_hook=[
    ""{{ Macro1(Arg1, Arg2, Arg3) }}"",
    ""{{ Macro2(Arg4, Arg5, Arg6, Arg7) }}""
  ]
)}}
```

For more details check out the docs on hooks: https://docs.getdbt.com/reference/resource-configs/pre-hook-post-hook",2
5613,"apply-multiple-macros-as-hooks-on-a-model","[quote=""jpgsa11, post:1, topic:5613""]
deploy some quality checks in terms of data quality
[/quote]

It's worth noting that the default way to do this is to use dbt tests, not post-hooks. If you haven't looked into tests much, keep in mind that you can create your own tests, which are just SQL queries that return failing rows: https://docs.getdbt.com/guides/best-practices/writing-custom-generic-tests",3
5613,"apply-multiple-macros-as-hooks-on-a-model","True and thank you for the advice, but i want to store logs of test execitions and the results combined in one table alone, instead of the default way to generate an automatic table with the results. So the focus is to centralize the results and logs of executions. Only workaround i found was to create a macro and do this myself. If you have any suggestions regarding this, please",4
5613,"apply-multiple-macros-as-hooks-on-a-model","Depending on the level of granularity you need from executions and results, using the [dbt_artifacts](https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest/) package might work for you. it will create a table with a row for each test invocation and its outcome (but doesn't include all of the failing rows for example.)",5
5613,"apply-multiple-macros-as-hooks-on-a-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","Hi,  We are using DBT 1.2.0 with Redshift.
Based on https://docs.getdbt.com/reference/resource-properties/tests
My understanding is that the ""accepted_values"" should support boolean type, as quoted

```
accepted_values
This test validates that all of the values in a column are present in a supplied list of values. If any values other than those provided in the list are present, then the test will fail.

The accepted_values test supports an optional quote parameter which, by default, will single-quote the list of accepted values in the test query. To test non-strings (like integers or boolean values) explicitly set the quote config to false.
```

But neither of the following YAML file formats works for us

```
        tests:
          - accepted_values:
              values: [true, false]
              quote: false
```
Or
```
        tests:
          - accepted_values:
              values: ['true', 'false']
              quote: false
```

And I got the following errors in the log:

```
*[2022-12-13, 20:17:34 UTC] {{subprocess.py:89}} INFO - [0m20:17:34 [33mDatabase Error in test accepted_values_dp_dim_clients_is_g_cfg_pm_checkin_enabled__False__true__false (models/marts/base/dim/dp_dim_clients.yml)[0m*
*[2022-12-13, 20:17:34 UTC] {{subprocess.py:89}} INFO - [0m20:17:34 operator does not exist: character varying <> boolean*
*[2022-12-13, 20:17:34 UTC] {{subprocess.py:89}} INFO - [0m20:17:34 HINT: No operator matches the given name and argument type(s). You may need to add explicit type casts.*
*[2022-12-13, 20:17:34 UTC] {{subprocess.py:89}} INFO - [0m20:17:34 compiled SQL at target/run/data_platform_models/models/marts/base/dim/dp_dim_clients.yml/accepted_values_dp_dim_clients_8aca7694846b065184b5b74c9dfc691c.sql*
```

1. Is the boolean type supported?
2. If so, what is the syntax?

Thanks",1
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","@java8964 can you post the compiled code at the location described in your error message? i.e. `target/run/data_platform_models/models/marts/base/dim/dp_dim_clients.yml/accepted_values_dp_dim_clients_8aca7694846b065184b5b74c9dfc691c.sql`",2
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","@joellabes 
Thanks for your reply. We are running this on AWS, so I need to contact a team member to access the sql file.
Meantime, can you tell me which format is the right one for the boolean ""accepted_values""? ['true', 'false'] or [true, false], or either? I cannot find any example in the document.

I can change to the ""supposed correct one"" and re-generate the compiled sql file.

Thanks",3
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","If you enclose the `values` in quotes, then they will be treated as strings by the YAML parser. Otherwise they will be inserted into the query as booleans (which will likely still work, it just means that they will be `True` and `False` instead of `true` and `false`. 

For example, this SQL and YAML: 

```sql
--my_model.sql
select true as col1, 1 as id

union all 

select false, 2
```
```yaml
version: 2 
models: 
  - name: my_model
    columns: 
      - name: col1
        tests:
          - accepted_values:
              values: ['true', 'false']
              quote: false

```

Resolves to this query: 

```sql
with all_values as (

    select
        col1 as value_field,
        count(*) as n_records

    from analytics.dbt_jlabes.my_model
    group by col1

)

select *
from all_values
where value_field not in (
    true,false
)
```

So the answer is that you _should_ enclose them in single quotes, but you _can_ do either. I suspect that the column you are testing could be a `character varying` type which is why you're getting errors - the bool is being passed through correctly but the other column is the wrong type.",4
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","Yes, it is our mistake that one column in the DB is ""varchar"", instead of ""boolean"".

Thanks",5
5622,"do-i-need-to-set-quote-false-for-an-accepted-values-test-on-booleans","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5632,"macro-to-load-json-from-file","The fromjson macro expects the input to be a json string. 

Are there any macros to load json from a file? 
If there are none, can you help me with pointers on how to write custom macros that do file io? 

Thanks.",1
5632,"macro-to-load-json-from-file","@venkatm dbt is designed to work with data that's already in your database, and doesn't have the ability to do file IO outside of what your warehouse exposes. What sort of JSON files are you trying to work with?",2
5632,"macro-to-load-json-from-file","
Thanks for getting back to me. I was trying to see if I could dynamically construct the SQL query using attributes read from a json file at runtime, without having to modify the dbt model file.",3
5632,"macro-to-load-json-from-file","Depending on the number of attributes you need to manage, you might be able to use vars and the fact that you can pass those into a dbt invocation: https://docs.getdbt.com/docs/build/project-variables#defining-variables-on-the-command-line

Can you post an example of the sort of query you want to generate, and which parts of it would vary based on the attributes? It's hard to be much help in a hypothetical scenario",4
5636,"compilation-error-tag-name-expected","Hello, using dbt on postgres db, I am seeing the below error;

```
06:50:11  Running with dbt=1.3.1
06:50:12  Unable to do partial parsing because a project config has changed
06:50:14  Encountered an error:
Compilation Error in model vw_connections_call (folder_name/Views/vw_connections_call.sql)
  tag name expected
    line 60
      WHEN c.direction::text = 'Inbound'::text AND ""substring""(c.extension_route::text, 1, 3) ~~ '%{%'::text THEN ""substring""(c.extension_route::text, 1, 5)
```
Any suggestions for how I can quote the curly brace and percent signs here to prevent dbt from seemingly thinking something python/jinja related is being invoked with that character?",1
5636,"compilation-error-tag-name-expected","In case someone else finds this, I worked around the error by changing the syntax of the quoted SQL. Referenced the position function instead of `~~ '%{%'`. dbt doesn't seem to want to parse a curly brace followed by the similarity operator (%) as SQL.",2
5636,"compilation-error-tag-name-expected","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5640,"running-a-tag-with-seeds-and-sources","I'm trying to create a test tag that would run the following:

1. Two source tables from two different folders
2. All seeds in a specific folder
3. 5 transformation models

I used the following syntax:

For source tables:

```
sources:
  - name: source_name
    
    tables:
      - name: hist_table
        tags: [""my_tag""]
```

for seeds in `dbt_project.yml`:
```
seeds:
  project_name:
    folder_name:
      +tags: my_tag
```
(also tried `+tags: ""my_tag""`

And for models in the yml file:
```
models:
  - name: my_model_1
     config:
       tags:  ""my_tag""
```

The seeds or sources do not show up in the execution list when I use `dbt run --select tag:my_tag`. Anything I'm missing?",1
5640,"running-a-tag-with-seeds-and-sources","[quote=""moseleyi, post:1, topic:5640""]
The seeds or sources do not show up in the execution list when I use `dbt run --select tag:my_tag`.
[/quote]

Seeds are not included in `dbt run`. You can either run `dbt seed --select tag:my_tag` first, or you could run `dbt build --select tag:my_tag` which will do everything (including execute tests; see https://docs.getdbt.com/reference/commands/build)

Sources themselves don't need to be run because they already exist, do you mean that you want to build the models that those sources depend on? If so then you should use the [`+` operator](https://docs.getdbt.com/reference/node-selection/graph-operators): `dbt run --select tag:my_model+`, but note that this will also build the downstream models from `my_model_1` etc.",2
5640,"running-a-tag-with-seeds-and-sources","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5643,"unable-to-connect-to-specified-database-snowflake","Trying to setup dbt core to a snowflake db, fails everytime, if i use the same creds in dbt cloud all ok. Doing this on an M1 Mac

error is:

  Connection test: [ERROR]

1 check failed:
dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  250001: Could not connect to Snowflake backend after 0 attempt(s).Aborting

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile",1
5643,"unable-to-connect-to-specified-database-snowflake","I think the issue is the `account` parameter in your profile. (I always get this wrong with snowflake...). dbt expects just the part of the url *before* snowflakecomputing.com, like `abc1234` or `abc1234.us-east-1.aws`

See the docs: [Snowflake setup | dbt Developer Hub (getdbt.com)](https://docs.getdbt.com/reference/warehouse-setups/snowflake-setup#account)",2
5643,"unable-to-connect-to-specified-database-snowflake","In the account parameter there will be""."" instead of ""."" use ""-"", as while copying the account ID it gets copied with ""."". I got the error resolved by doing this.",3
5672,"snapshotting-transformed-fields","Hello! I want to snapshot a field that results from several layers of data transformations and I'd appreciate some help thinking through whether it makes sense to build this from the source table. It would be analogous to a situation where:

1. I have a field called `customer_prefers_widget`
2. That field is a combination of fields called `customer_has_bought_widget` and `customer_has_reviewed_widget` from two different upstream tables
3. Each of those fields is a combination of multiple fields from multiple upstream tables

Using the best practice of snapshotting the source tables, that means that I would have to re-transform all the data that results in `customer_prefers_widget`. This is both time-consuming and creates a vulnerability if the team in future decides to change logic behind `customer_prefers_widget`. 

I'm wondering if others have encountered this situation and how they've dealt with it?",1
5743,"scheduling-dbt-pipelines-using-two-mandatory-tags","I'm trying to find a solution for scheduling and tagging my models. I have two tags e.g. environment and schedule, where environment can take values like staging/production, while schedule can take values like daily/weekly/monthly. I want to only tag the final model of each data pipeline I have, and when I run my dbt command I want the entire pipeline (inlcuding upstream models) to run. However, I only want to run pipelines that are tagged with both an environment and schedule tag. Ultimately I'm looking for a solution for this task in DBT, however, the solution I've trying to get working is bumping into this problem below.

So the command I've being trying to use is `dbt run --select +tag:staging,+tag:daily` and this does work most of the time. However, if I have say pipeline A, and one of my upstream models (model 1) is a final model of another pipeline. Model 1 is tagged with staging and daily, while the final model of pipeline A (model 2) is tagged with production and weekly. To recap, model 1 is an upstream model of model 2.

If I run the following command for a completely separate pipeline B (not sharing any upstream models with pipeline A) `dbt run --select +tag:production,+tag:daily `. I wouldn't expect any of my models from pipeline A to run (since model 1 and 2 would need to have the tags production and daily, and or be part of the upstream models of a model with those tags, and both of these cases are not true), however, model 1 ends up being run because of the way the intersection operator works. E.g. `+tag:production` will pick up model 2 and all it's upstream models therefore picking up model 1, while `+tag:daily` will pick up model 1 because it has the daily tag. And because each intersection group picks up model 1, model 1 will run even though it shouldn't have. To recap, although  `dbt run --select +tag:production,+tag:daily ` works, and runs what I need, it also unintentionally runs a model it shouldn't have.

Basically because of the current order of operations for DBT syntax selection, selection methods, then graph operators, then set operators I get this problem. If say the order was instead selection methods, then set operators, then graph operators this method would work. Is there anyway around this so that I could make this work? If not, does anyone have any ideas for how I could get this to work?

Thanks!",1
5743,"scheduling-dbt-pipelines-using-two-mandatory-tags","Hello - Please use --exclude flag to skip the models which are not required in your pipeline

--exclude tag:model1/2 [add tag as : model1/2] to model",2
5743,"scheduling-dbt-pipelines-using-two-mandatory-tags","Thanks for the reply. Although I'm looking for a more general solution approach, as I won't always know what model 1 and 2 are if there are lots of pipelines being scheduled.",3
5743,"scheduling-dbt-pipelines-using-two-mandatory-tags","I think your diagnosis of the problem (the order of evaluation) is correct, so I think you'd be better served opening an [issue on the dbt Core repo](https://github.com/dbt-labs/dbt-core/issues) to see whether there is any appetite to make this more customisable. 

I haven't looked into it, but you *might* also be able to get somewhere with YAML selectors if you haven't already? They have a bit more control: https://docs.getdbt.com/reference/node-selection/yaml-selectors",4
5746,"version-error-for-package-dbt-labs-dbt-utils","Hi,

When running dbt deps, dbt-labs/dbt_utils would successfully install in our cloud job, the last time this successfully ran was on Thursday.

23:08:22  Installing dbt-labs/dbt_utils
23:08:22    Installed from version 0.8.6

When trying to run this again yesterday with no code changes, I received the below error on the dbt deps step: 

Version error for package dbt-labs/dbt_utils: Could not find a satisfactory version from options: ['>=0.9.0', '<0.10.0', '>=0.8.0', '<0.9.0']

![image|690x166](upload://jep2Rf4V6YdRd9gQ3sAbUkbHw76.png)

Locally I have tried to add the below to my packages.yml file (one at a time) but still receive the same error when I run dbt deps:

  - package: dbt-labs/dbt_utils
    version: 0.7.1

  - package: dbt-labs/dbt_utils
    version: 0.8.6

Version error for package dbt-labs/dbt_utils: Could not find a satisfactory version from options: ['=0.8.6', '>=0.9.0', '<0.10.0']

  - package: dbt-labs/dbt_utils
    version: 1.0.0

  - git: ""https://github.com/dbt-labs/dbt-utils.git""
    revision: 0.9.2

Some assistance here would be greatly appreciated, please let me know if further information is required.

Kind regards,
Kayne",1
5746,"version-error-for-package-dbt-labs-dbt-utils","Hi Kayne, 

Can you post the full contents of your `packages.yml` file inside of a code block? 

It looks like you have multiple packages installed, each of which is trying to install a different version of dbt utils. By seeing which packages you have installed, we can work out what combination is causing the issue.",2
5746,"version-error-for-package-dbt-labs-dbt-utils","Hi joellabes

Thank you, please see below:

```
packages:
  - package: Datavault-UK/dbtvault
    version: ['>=0.8.3']
  - package: Snowflake-Labs/dbt_constraints
    version: ['>=0.4.0']
  - package: calogica/dbt_expectations
    version: ['>=0.5.0', '<0.6.0']
```",3
5746,"version-error-for-package-dbt-labs-dbt-utils","Because your first two packages don't have upper bound limits, you will get the latest releases for all versions of the package, even those that may contain breaking changes. 

In your case, it looks like dbtvault had a release a couple of days ago which raised the version of dbt utils required: https://github.com/Datavault-UK/dbtvault/releases/tag/v0.9.1

You could solve this by setting an upper bound for your dbt vault installation, or by installing the latest version of dbt expectations which no longer depends on dbt utils at all.",4
5746,"version-error-for-package-dbt-labs-dbt-utils","[quote=""joellabes, post:4, topic:5746""]
dbtvault
[/quote]

Thanks very much, all sorted now. Thanks for the prompt response.

Regards,
Kayne",5
5746,"version-error-for-package-dbt-labs-dbt-utils","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
5758,"exists-function-is-failing","when I remove the condition with EXISTS function, it works but now Its failing.
![Screenshot 2022-12-20 at 15.27.16|690x311](upload://1k0WLwuvrxp7ePZmCfOALod5Iag.png)",1
5761,"how-to-run-dbt-models-in-a-particular-sequence","I have two models: ""transform"" and ""product"".

When executing `dbt run --profiles-dir .` my product model is running first and transform is running later.

How do I ensure that transform is executed first and then product",1
5761,"how-to-run-dbt-models-in-a-particular-sequence","Hello @chronos2.03 

Can you please provide the model templates.

Regards
Minhaj",2
5787,"big-query-materialized-view","Hello


I followed the instructions given in below:
https://github.com/dbt-labs/dbt-labs-experimental-features/blob/main/materialized-views/README.md

I have copy/pasted [installation via files COPY]
1 adapters.sql
2 materalized_view.sql from bigquery folder [as my dwh is BQ]

* Below is my model(customers.sql)
{{ 
    config(materialized='materialized_view',
    auto_refresh = true,
    )
}}

select * from stg_customers;

* when iam running : dbt run  --select customers iam getting below error.

**18:42:20  Compilation Error in macro materialization_materialized_view_bigquery (macros/materialized-views/big_query/materalized_view.sql)**
**18:42:20    'dbt_labs_materialized_views' is undefined**

Can you please help me on this


Regards
Minhaj Pasha",1
5791,"does-dbt-core-has-any-rest-apis","So, we're using DBT locally (DBT Core) for transformations with snowflake as the data warehouse and will be integrating it with our own created UI as well. 

But to integrate with our UI, we'll need some REST API which is exposed publicly for DBT Core.

So, just wanted to check if DBT Core has anything which will help us to integrate it with our UI and how can we do that.",1
5793,"how-to-handle-the-table-name-with-when-sourcing-it","select * from {{ source('a2p', '/SIE/AD_**') }})

Database Error in rpc request (from remote system) 001003 (42000): SQL compilation error: syntax error unexpected '/'.",1
5796,"can-we-use-dbt-as-a-code-coverage-tool-and-what-kind-of-code-coverage-report-formats-does-dbt-generate","Hi Team,

We are trying to generate Code Coverage for SQL code using DBT tool. We need to understand the set up and configuration for dbt tool. So regarding DBT, I have the below queries.

1. Can we use DBT as a Code Coverage Tool ?

If Yes,
2. What kind of Code Coverage Report Formats does DBT generates (Ex: HTML, XML or CSV).

3. Is there any sample repository to generate Code Coverage for SQL using DBT tool.


Thanks,
Murali Sulam",1
5796,"can-we-use-dbt-as-a-code-coverage-tool-and-what-kind-of-code-coverage-report-formats-does-dbt-generate","Hi Team,

Any update on the above query?

Regards,
Murali Sulam",2
5796,"can-we-use-dbt-as-a-code-coverage-tool-and-what-kind-of-code-coverage-report-formats-does-dbt-generate","Hi Team,

It's been almost a month I have raised this query, but still now I didn't get any Update. Could you please let me know the update on the above mentioned query.


Thanks,
Murali Sulam",3
5796,"can-we-use-dbt-as-a-code-coverage-tool-and-what-kind-of-code-coverage-report-formats-does-dbt-generate","Hey @smk.yadav22 

Have you checked this dbt package https://github.com/slidoapp/dbt-coverage?

It reports how well your models are documented and how many models have associated tests.",4
5797,"running-multiple-sql-statements-from-a-macro-in-a-pre-hook","I want to run multiple sql statements before my main model (due to some idiosyncracies of teradata data warehouse that I'm using). The sql statements I want to run in the pre_hook depend on whether the model is being run incrementally or as a full refresh, so I have written a macro which needs to have access to the result of the `is_incremental` function.  In my examples there are just two statements but the actual number of statements will also depend on this context.

I am aware of several github issues ([2370](https://github.com/dbt-labs/dbt-core/issues/2370), [4023](https://github.com/dbt-labs/dbt-core/issues/4023), 3985, 3986, 5413 (I can only hyperlink the first two as i'm a new user) ) where they mention config information such as is_incremental or custom schema get rendered at parse time instead of run time, and the solution seems to be to nest curlies to make sure it gets re-rendered, e.g.:

```
{{ config(
    materialized='incremental',
    pre_hook=""{{ conditional_pre_hook_macro() }}""
) }}
```
However this solution seems to only work for singular string value for the pre_hook argument of config, whereas I need to pass an array of strings as the pre_hook argument. I can't figure out how to get the macro to render properly with the correct result from `is_incremental` being rendered at run time, whilst having the pre_hook argument render as an array, not a string.

Here an example of the macro I am using:
```
{% macro conditional_pre_hook_macro() %}
{% if is_incremental() %}
create multiset volatile table temp as (
    select
    now() as ts,
    'ran incrementally' as status
)
with data and stats
primary index (ts)
on commit preserve rows
;
insert into temp
select
now(),
'foo            '
;
{% else %}
create multiset volatile table temp as (
    select
    now() as ts,
    'fully refreshed  ' as status
)
with data and stats
primary index (ts)
on commit preserve rows
;
insert into temp
select
now(),
'bar            '
;
{% endif %}
{% endmacro %}
```

 And here is an example of the model. I'm using the `split` method to ensure that the result is an array of strings instead of just one string, but it seems to have no effect:
```
{{ config(
    materialized='incremental',
    pre_hook=""{{ conditional_pre_hook_macro() }}"".split(';')
) }}
select 1 as foo
```
Running this gives the following error, because it is trying to run two statements in one go (this is why the pre_hook needs to be an array, not just a single string):
```
[Teradata Database] [Error 3722] Only a COMMIT WORK or null statement is legal after a DDL Statement.
```
I have tried making my macro return an array of strings, but then the pre_hook just gets rendered as a string with an array inside of it, e.g. ```'[""select foo"", ""select bar""]'```, and I can't seem to coerce this back to an actual array.

Is there a trick to this or is what i'm trying to do fundamtentally at odds with how dbt is desgined to work?

Many thanks,

Sam",1
5798,"package-duplication-bug-when-running-dbt-deps","Iâ€™m having an issue when running `dbt deps` to install the packages we have set up in our packages.yml file. I end up seeing some packages and macros within those packages installed multiple times under a file name following the format `[macro name/package name] 2`. For example, I have a `dbt_utils` folder that contains everything I expect it to, and then a `dbt_utils 2` folder that is empty. This causes an error when I try and do `dbt run` along the lines of :

`no dbt_project.yml found at expected path .../dbt_packages/dbt_utils 2/dbt_project.yml`

dbt_utils is the only package being duplicated, but macros within almost all other packages are being duplicated too. 
I am using dbt version 1.3.0 and bigquery 1.3.0. Any thoughts on what might be happening? Thanks!

packages.yml:
> packages:
> 
>   - package: dbt-labs/dbt_utils
>     version: 0.9.0
> 
>   - package: dbt-labs/codegen
>     version: 0.9.0 # dependent on dbt_utils
> 
>   - package: dbt-labs/audit_helper
>     version: 0.6.0 # dependent on dbt_utils
> 
>   - package: dbt-labs/dbt_project_evaluator
>     version: 0.2.1 # dependent on dbt_utils
> 
> 
>   - package: calogica/dbt_expectations
>     version: 0.8.0 # dependent on dbt_date
> 
>   - package: calogica/dbt_date
>     version: 0.7.0 # no package dependencies",1
5811,"dbtmock-end-to-end-pipeline-testing","Hello dbt community,

End to end testing pipelines is super important for data quality.

I've been working on a tiny tool to make mocking and testing easier. yes, yet another tool, I tried a few other tools provided by the community but after hitting a few dead ends, I decided to give a try to my own implementation:

https://github.com/dav009/dbtmock",1
5818,"incremental-merge-strategy-with-no-update-case","Hello,  i wonder if there is a way to load a satellite on BigQuery with the incremental Merge strategy.  In my point of view i dont need a 
when matched then update set...   part
if my merge condition is source.hashkey = target.hashkey and source.hash_diff = target.hash_diff
i only need to insert new records. In case of update ( hash_diff is different ) its a new Sat version as insert.
Is there a way to config the Sat Definition only use the    when not matched then insert
Thank you very much",1
5830,"why-are-dbt-models-called-models","The mathematical definition of a **model** is a process that generates data.

Was this the thinking followed by dbt by using this term?",1
5830,"why-are-dbt-models-called-models","[quote=""marcel, post:1, topic:5830""]
The mathematical definition of a **model** is a process that generates data.
[/quote]

I couldnâ€™t get Google to give me anything like ""a process that generates data"" as a definition for `mathematical model`, `model mathematics` etc. 

Models were named before I started using dbt, but I imagine it came from the result of _data modeling_: 
> Data modeling is the process of analyzing and defining all the different data your business collects and produces, as well as the relationships between those bits of data

(from https://powerbi.microsoft.com/en-us/what-is-data-modeling)",2
5835,"same-exposure-model-in-dag","Hi There,

I just started to explore the exposures feature in DBT, and I see that the final model I want to treat as **exposure appear twice in the DAG**: both as **model** and as **exposure**. 

![Screenshot 2022-12-26 at 13.05.41|690x477, 75%](upload://n6umYdsQfzHhq9nSKw4ckgilxls.png)

My brief understanding (looking at the docs:) ) is that if you define it as exposure, it should not be anymore a model. Here is the YAML file, in the same folder as the exposure. What it was the previous mfhd_clients model (now turned into exposure) it is not defined in any other YAML. 

Does anyone know how this is happening? Or any intersting resource to understand this deeper, thanks:)",1
5835,"same-exposure-model-in-dag","**EDIT:** *Good morning, I just solved this Issue, so it can be closed.*

The solution was that in the dbt_project.yml I was declaring all the models in Mart folders to materialise as views (including the folder where I had my exposures). The solution is then to disable the model materialisation for the .sql files in the folder of my exposures.

![Screenshot 2022-12-30 at 09.50.03|690x365, 50%](upload://u26bMlQy6gDosg64JwtBkJqxHpi.png)",2
5835,"same-exposure-model-in-dag","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
5844,"serving-multiple-projects-using-dbt-serve","Hello. I am using dbt-core on premises and I have multiple dbt projects which I would like to serve on the web-server for better documentation and visualization for my data analysts team.

I have multiple databases with I will apply a transformation process on them using DBT.
I also have an Airflow running on my cluster which I am also willing to use to schedule DBT models.

I thought of some options:
1. Build a big project including multiple databases and models with a great documentation.

2. Try and generate multiple projects(?) which didn't really work

This is my very first project and I tried to code some models to run it and then I ran into this problem.

Have anyone experienced something similar? What would you suggest?",1
5851,"dbt-installation-on-macbook-failure","Am getting the following error when running the simple command dbt --version upon performing the steps mentioned in the link - [Install with HomeBrew][1]

> -bash: dbt: command not found

I am doing this on Macbook Pro running MacOS Monterey with Intel chip. Can someone tell me where I am going wrong?

  [1]: https://docs.getdbt.com/docs/get-started/homebrew-install",1
5851,"dbt-installation-on-macbook-failure","You need to put your dbt binary path in your $PATH.

To find where dbt is installed, run this:
```which dbt```
And then add to your path:
```export PATH=/path/to/dbt-binary:PATH```

<sub>Note: `@magnus.avitsland` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1672659714962299?thread_ts=1672276285.499789&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",2
5856,"one-model-file-multiple-tables-metrics-active-aggregation","Hi,

I had a look at dbt Metrics recently and even though it's a great move forward, I find myself still not being able to fulfill my requirements.

When it comes to timeframe metrics (or as I call it active aggregation) that cound unique ids or sums values for unique ids in a specific timeframe, I want to be able to to query it from outside of dbt (BI Tool).

Right now I use a dynamic SQL in Looker to build the SQL on the fly depending on whether somebody chooses day/wek/month and so on. Those tables are cached but the upside is that I have one code = 5 tables. Of course ideally I would like to move the code outside of Looker to dbt but I wouldn't want to keep the same code in 5 files with one dimension changed.

Has anybody tried to solve such problem?",1
5861,"main-branch-not-read-only","Hi! I am trying to implement the slim ci job in dbt and want to create feature branches off the main branch in dbt. Right now our main branch is not appearing to be read-only and so will not allow us to create branches off of main. Is there a way we can fix this? And more importantly, how did our main branch get changed to be not read-only so this doesn't happen again. Our develop branch is the only branch that is appearing as read-only at the moment.",1
5861,"main-branch-not-read-only","@abi.minton did you configure a custom branch? https://docs.getdbt.com/faqs/environments/custom-branch-settings

If so, that could explain why `develop` is readonly and `main` isn't",3
5865,"macros-obtain-column-list-for-non-relation-output","Hello, I'm trying to build a macro and am stuck on a specific problem.

Is there a dbt macro that allows me to get the columns list from a query set result (e.g. CTE)? I am looking for something similar to [this function](https://github.com/dbt-labs/dbt-utils#get_filtered_columns_in_relation-source) without the requirement for the input to be a relation. The reason being that I am trying to one-shot a macro and do not want to persist the intermediate steps.

For example, I want to be able to do this: 
```
WITH cte 1 AS (
SELECT * FROM table
)
,cte 2 AS (
SELECT ... 
FROM some_table LEFT JOIN cte1 ON
{ for col in cte1}
some_table.{{ col }} = cte1.{{ col }}
{ endfor }
```",1
5869,"when-running-the-dim-customers-sql-it-would-show-the-created-columns-but-no-values-within","![image|690x388](upload://htcykFyQqQrF8bSDwN6ojzX6kgI.png)",1
5878,"dbt-cloud-target-error-but-not-in-dbt-core","Hi, i'm new in dbt .
I want to run different models with different targets at dbt cloud. I saw the command :  
"" dbt run --select model.sql --target dev "" 
This command line works in dbt core, but when I tried to do it at dbt cloud  , it doesn't run my model in the target called dev, it takes the default one.

I have tried doing dbt debug --target dev to see the connection but also got errors in dbt cloud (but no at dbt core): 

""No matching handler found for rpc method None (which=debug)""

I want to work at dbt cloud because it easier for me to set jobs, so how can I fix it? 

Thanks!",1
5878,"dbt-cloud-target-error-but-not-in-dbt-core","Hello,
Modify the value of target in the dbt cloud environments... when you create a new env the target is set to a value equal to default... you can change it to dev  
To handle multiple targets in dbt models you need to use jinja macros",2
5885,"images-in-assets-directory-not-displayed-in-dbt-cloud-documentation","project yml contains
```
 asset-paths: [""assets""]
```
jpg or png files located in assets directory and referred to by a readme.md containing (very simplified version):

```
{% docs macro_display_image %}
{% raw %}
##display image
![image](/assets/image.jpg ""Image"")
{% endraw %}
{% enddocs %}
```
the yml file referring to the doc is displayed fine locally after
```
dbt docs generate
dbt docs serve
```
The problem appears in another environment on dbt cloud, where the image fails to display,
whereas all images located on web displayed correctly. I would appreciate if someone can suggest how to to use images saved in the directory to display on cloud. Thanks",1
5885,"images-in-assets-directory-not-displayed-in-dbt-cloud-documentation","@alezar did you find any solution yet?  I am seeing the same issue.  dbt docs generate / serve work fine locally, but not on server.  Images are in assets directory and dbt_project.yml has asset-paths defined.",2
5898,"incremental-model-compute-costs-are-the-same-or-more-than-creating-a-table","hello - trying to figure out why this model has the same or more compute costs when merging 0 rows as opposed to a full-refresh that builds it from scratch. seems pretty basic, not sure what i'm missing. 

let's say a full-refresh processes 5GB
if i run it as it is below after that full-refresh, it will merge 0 rows and process 10GB
if i run it w/ `unique_key='id'` removed after that full-refresh, it will merge 0 rows and process 5GB

it is true that the time for the runs goes down for the incremental runs.
""_last_updated"" is a timestamp.

```
{{
    config(
        materialized='incremental',
        unique_key='id'
    )
}}

select 
    * 
from {{ref('my_table_name_here')}}
{% if is_incremental() %}
where _last_updated > (select max(_last_updated) from {{ this }})
{% endif %}
```",1
5898,"incremental-model-compute-costs-are-the-same-or-more-than-creating-a-table","Hey @testing, I assume you're using BigQuery since you're talking about data scanned? 

I'm not super familiar with how BQ bills, but I wonder whether it's having to scan the whole table checking the _last_updated value for each column. Does BQ have indexes you can add or anything? 

One way to work around this might be to run a separate query to check the value. Something like this: 


```
{% if is_incremental() %}
{% set max_last_updated_input = dbt_utils.get_single_value(""select max(_last_updated) from {{ ref('the_table') }}"" %}
{% set max_last_updated_this = dbt_utils.get_single_value(""select max(_last_updated) from {{ this }}"" %}

{% if max_last_updated_input != max_last_updated_this %}
where _last_updated > '{{ max_last_updated_this }}'
{% endif %}
{% endif %}
```

(Written on mobile, not tested and not indented ðŸ˜¬)",2
5898,"incremental-model-compute-costs-are-the-same-or-more-than-creating-a-table","In BigQuery `incremental` model with the `merge` strategy scans the source table with specified filters and the whole target table to look for changes using `uniquer_id.`  If your target table grows, that means that you scan more data each time to check for any changes to any row.

If you have a large partitioned target table, you can use `incremental` model with `insert_overwrite` strategy. In this case, DBT will not scan the full target table but rather drop particular partitions and insert new data in place.

https://docs.getdbt.com/reference/resource-configs/bigquery-configs#the-insert_overwrite-strategy",3
5902,"exclude-the-mutliple-folders-in-dbt-run","In the dbt run command, how to exclude the multiple folders?",1
5906,"how-do-you-run-multiple-models-manually-using-dbt-command-which-are-scheduled-for-the-same-date-and-time","Can anyone  answer this question?",1
5908,"none-has-no-attribute-table","
I'm trying to create a macro to do some checks before performing a calculation with the field. but when checking the return of the qury I can't, it gives an error.
How can I do this?
Follows my macro in a generic way


```
{% macro fn_verifica_id(id, ds_campo_pesquisar) %}

        {% set query %}
            select
                1 as resultado
        {% endset %}

        {% set resultado = run_query(query) %}

        {% if resultado == 1 %}
            ({{id}} + 1000 * (-1))
        {% else %}
            ({{id}})
        {% endif %}

{% endmacro %}
```",1
5908,"none-has-no-attribute-table","Hi, I've come across this exact error before. Basically, dbt will first parse the macro prior to execution. In this parsing stage, you get the error `'None' has no attribute 'table'`. 

To avoid this, you can wrap your macro in an if block to check whether dbt is in parsing or execution mode, and then write some valid but useless SQL for the parsing stage.

I've tested this solution and it works for me. So what you can do is

```
{% macro fn_verifica_id(id, ds_campo_pesquisar) %}

{% if execute %}

        {% set query %}
            select
                1 as resultado
        {% endset %}

        {% set resultado = run_query(query) %}

        {% if resultado == 1 %}
            ({{id}} + 1000 * (-1))
        {% else %}
            ({{id}})
        {% endif %}

{% endif %}

select 1 -- dummy SQL for parsing stage

{% endmacro %}
```

Hope it works!",2
5908,"none-has-no-attribute-table","Supporting documentation: <https://docs.getdbt.com/reference/dbt-jinja-functions/execute>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1680098066455859?thread_ts=1680089616.089559&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",3
5910,"how-can-i-add-model-configuration-using-a-macro-on-this-in-dbt-project-yml","I want to add node_color to all of my dbt models based on my filename prefix to make it easier to navigate through my dbt documentation :

fact_ => red
base__ => black.

To do so i have a macro that works well :

```
{% macro get_model_color(model) %}

    {% set default_color = 'blue' %}
    {% set ns = namespace(model_color=default_color) %}

    
    {% set dict_patterns = {""base__[a-z0-9_]+"" : ""black"", ""ref_[a-z0-9_]+"" : ""yellow"", ""fact_[a-z0-9_]+"" : ""red""} %}

    {% set re = modules.re %}

    {% for pattern, color in dict_patterns.items() %}

        {% set is_match = re.match(pattern, model.identifier, re.IGNORECASE) %}
        {% if is_match %}
            {% set ns.model_color = color %}
        {% endif %}
        
    {% endfor %}

    {{ return({'node_color': ns.model_color}) }}
{% endmacro %}
```

And i call it in my model .sql :

```
{{config(
    materialized = 'table',
    tags=['daily'],
    docs=get_model_color(this),
)}}
```

This works well but force me to add this line of code in all my models (and in all the new ones).

Is there a way i can define it in my dbt_project.yml to make it available to all my models automatically?

I have tried many things like the config jinja function or this kind of code in dbt_project.yml

```
    +docs:
        node_color: ""{{ get_model_color(this) }}""
```

returning `Could not render {{ get_model_color(this) }}: 'get_model_color' is undefined`

But nothing seems to work

Any idea? Thanks",1
5912,"commit-the-transaction-only-if-the-sql-statement-used-returns-a-non-empty-result-set","Background: I have a few models that are materialized as 'Table'. This table is populated by wipe (truncate) and load. Now I want to protect the table's existing data if the query used to populate it returns an empty result set. How can I make sure an empty result set is not replacing my existing data in the table? My table lies in Databricks UC, and I am using dbt to model the output table.**In a nutshell, only commit the transaction only if the SQL statement used returns a non-empty result set.**",1
5915,"connecting-dbt-to-snowflake-when-snowflake-is-only-accessible-via-azure-private-link","Hi all, our client wants to setup azure private link to connect to snowflake.  If I understand the documentation correctly, this will eliminate access to Snowflake via the public internet -  all access would have to route through their VPN or a virtual machine in their Azure Vnet.

I presume this means that the current DBT connection to Snowflake would fail.  What do we need to do setup a connection between DBT and Snowflake once they migrate Snowflake access exclusively to Azure Private link?",1
5926,"preserve-query-results-at-a-point-in-time","Mostly, we want models to refresh every time we run, but I am in a situation where I would like to make a model that is only run once, to preserve the query result for a particular point in time. Could also hypothetically be an expensive query that we want to run extremely rarely. Is there any way to ""protect"" the resulting materialized table from writes by DBT? I assume making it explicitly read-only will cause problems when DBT is run.
I guess I can run the other models selectively, but I'd worry about human error corrrupting the table .",1
5926,"preserve-query-results-at-a-point-in-time","Make it incremental.

Then add

```
{% if is_incremental() %}

  -- this filter will only be applied on an incremental run
  where 1=0
```

Add this configuration:

```
models:
  [<resource-path>](resource-path):
    +full_refresh: false
```

Or at the model level:

```
{{ config(
    full_refresh = false
) }}
```

This will make it so nothing ever happens on the table.",2
5926,"preserve-query-results-at-a-point-in-time","An incremental model which doesn't add any extra rows after its initial invocation is a good option! 

[quote=""bonnevie, post:1, topic:5926""]
to preserve the query result for a particular point in time
[/quote]

For this use case, snapshots _could_ also be an option: https://docs.getdbt.com/docs/build/snapshots",3
5926,"preserve-query-results-at-a-point-in-time","Thanks for the advice @jeffsk and @joellabes, a bit hacky, but sounds like it will do the trick!
I tried turning it into a macro, but it gets a bit wonky if you have to take into account that the WHERE is optional;  maybe there's a better way:

```
{% macro ensure_only_run_once(
        append = false
    ) %}

{% if is_incremental() %}
{% if append %}
    AND 1 = 0
{% else %}
    WHERE
        1 = 0
    {% endif %}
{% endif %}
{% endmacro %}
```",4
5926,"preserve-query-results-at-a-point-in-time","@joellabes tried looking a bit at snapshots, and I see how they might be a good solution. Could you give an example of doing a ""time travel""/fixed-point-in-time query with a snapshot? Is it as simple as only selecting rows in the correct valid range?
And are you at risk of a full refresh with snapshots, or are they always continuously updated?",5
5926,"preserve-query-results-at-a-point-in-time","[quote=""bonnevie, post:5, topic:5926""]
Could you give an example of doing a â€œtime travelâ€/fixed-point-in-time query with a snapshot?
[/quote]
Do you mean how do you query a snapshot table to get a point-in-time result? If so then, 

[quote=""bonnevie, post:5, topic:5926""]
Is it as simple as only selecting rows in the correct valid range?
[/quote]
yes! 

You would do something like 
```sql
select * 
from {{ ref('the_snapshot') }} 
where getdate() between dbt_valid_from and dbt_valid_to
```
, keeping in mind that the current version of a row will have `dbt_valid_to` of null. It's common to coalesce that to a value in the distant future for ease of querying, normally something like `2099-12-31`. 

Check out https://discourse.getdbt.com/t/building-models-on-top-of-snapshots/517 for our best practices when working with snapshots. 

[quote=""bonnevie, post:5, topic:5926""]
And are you at risk of a full refresh with snapshots, or are they always continuously updated?
[/quote]
Snapshots are updated each time you run the `dbt snapshot` command, and will never be dropped by dbt, even with a `full-refresh` command.  It's a good idea to put your snapshots into a different schema to prevent someone from accidentally dropping them, as their data is by definition non-recoverable (without backups), in contrast to most dbt models where as long as you have the source tables you can get back to the starting point.",6
5926,"preserve-query-results-at-a-point-in-time","Great info, that really helped me get a better understanding of snapshots - and I think they might actually be a great solution for our problem. And great link too, had been looking for something like that in the docs. Thanks :smiley:",7
5926,"preserve-query-results-at-a-point-in-time","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",8
5937,"scd-type-1-scd-type-2-cdc-integration-in-dbt","Can we have CDC in DBT ?",1
5937,"scd-type-1-scd-type-2-cdc-integration-in-dbt","Hi, welcome to the forum.

I think this covers what you need for type 2:
https://docs.getdbt.com/docs/build/snapshots

Type 1 is a regular table.",2
5938,"using-python-in-web-interface","I have built an ETL tool and I was doing a POC on DBT. Some of my logics are in python , Can I use the web version to emulate those python logics as it is not in SQL ?",1
5938,"using-python-in-web-interface","Hi @purwarapar, which data platform are you using? Right now, dbt models can be built in Python using Snowflake, Databricks and BigQuery. 

The dbt Cloud web interface doesn't expose any additional ways to build Python models compared to dbt Core at the CLI",2
5939,"replace-value-in-column-based-upon-subquery","I have a few dbt models where I need to replace a string value of one column with the result from another query.  i.e. a renamed object, the old source still has the old name, but we want to replace the name outputted in the model with the current name.  The subquery requires a recursive CTE.  This is on Snowflake.  Some of the models are only views, so weâ€™re not persisting data in a table (thus post hooks to update values after the model is built are not always an option either).  Anyone have any ideas on the best way to do that?  Can you use cursors to iterate over records in a dbt model?",1
5947,"production-credentials","Hi!
Using dbt Cloud with Team license.
A project should have different development and production database credentials.
But a developer can go to the production environment and see production database credentials. 
My question is: is there a way to avoid that, and protect/hide the production credentials?",1
5947,"production-credentials","[quote=""fernando.amaral, post:1, topic:5947""]
see production database credentials
[/quote]

What credentials do you mean? It's not possible to view the password for a database after initial setup.",2
5955,"invalid-incremental-strategy-provided-none","Hello,
I'm a newbie to dbt pipeline development, I've created a pipeline but when I run ""dbt run"" I'm getting the below errors - any ideas what might be wrong please?

Unable to do partial parsing because env vars used in profiles.yml have changed
10:06:23  Found 1 model, 1 test, 0 snapshots, 0 analyses, 301 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
10:06:23
10:06:24  Concurrency: 1 threads (target='current')
10:06:24
10:06:24  1 of 1 START sql incremental model dbt.coa_daily_1_0 ........................... [RUN]
10:06:25  1 of 1 ERROR creating sql incremental model dbt.coa_daily_1_0 .................. [ERROR in 0.06s]
10:06:25
10:06:25  Finished running 1 incremental model in 0 hours 0 minutes and 1.67 seconds (1.67s).
10:06:25
10:06:25  Completed with 1 error and 0 warnings:
10:06:25
10:06:25  Compilation Error in model coa_daily_1_0 (models/raw/coa_daily_1_0.sql)
10:06:25    Invalid incremental strategy provided: None
10:06:25        Expected one of: 'append', 'delete+insert', 'merge'
10:06:25
10:06:25    > in macro validate_get_incremental_strategy (macros/materializations/incremental.sql)
10:06:25    > called by macro materialization_incremental_trino (macros/materializations/incremental.sql)
10:06:25    > called by model coa_daily_1_0 (models/raw/coa_daily_1_0.sql)
10:06:25
10:06:25  Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1",1
5955,"invalid-incremental-strategy-provided-none","Do you have any config block in your `coa_daily_1_0` model? If so can you share it? 

It looks like you have not got an incremental strategy set, however the [documentation](https://docs.getdbt.com/reference/warehouse-setups/trino-setup#incremental-models) says that the default is `append` in that case, so I wonder if you've set something strangely.",2
5964,"dbt-cloud-commands-in-script","
Hi,

is there a possibility in dbt cloud to put some dbt commands into a script file (or macro?) and then execute it ?

e.g.

-- dbt run-operation macro_123
-- dbt run-operation macro_123 --vars 'unit_test_name: test_no_1' 
-- dbt run-operation macro_123 --vars 'unit_test_name: test_no_2'

I want to gather all tests, and then run it always at once.",1
5964,"dbt-cloud-commands-in-script","What code do you hope you can put inside of macro_123? It is possible to set vars via the command line, but you can't have a macro that calls other dbt command like run or test. 

If you want to run dbt tests, why would you try to invoke an operation instead of just running dbt test directly?",2
5982,"using-dbt-on-aws-emr","Hi, I am a data engineer who is relatively new to dbt, but our data science and analytics team has been using dbt for couple months. And the DE team recently is required to do an audit on the cost for the dbt pipelines.

Our architecture is quite standard: 

dbt + spark via thrift server running on AWS EMR, data stored on S3 as parquet files registered in hive catalog.

So far, the main saving opportunity we found was to utilize spot instance rather than running all instances On-demand. But the problem is that from our observation, dbt pipeline rarely use the task instance but most of the work happened on core nodes (where the data node lives). But we don't want to run core nodes with spot lifecycle as the loss of a core node may introduce data loss. And the run time of the pipeline seems to be in negative correlation with the number of core nodes.

Are there any suggestion on how to isolate the root cause of this behavior? Was it simply because the dbt pipeline we are running is mainly talking to HDFS? So the heavy lifting all happens on core nodes? Or thrift server itself doesn't utilize task nodes (this might be a stupid question, cause I don't fully understand how thrift server works, it sounds like a simple JDBC/ODBC, so it shouldn't be different from a normal spark application right?)

Very much appreciate!",1
5996,"creating-a-custom-check-strategy","Hi all,

I'm trying to create a custom snapshot strategy which changes the way in which hard deletes are invalidated.

I have created a custom `check` strategy by creating a new macro based off the original found [here](https://github.com/dbt-labs/dbt-core/blob/main/core/dbt/include/global_project/macros/materializations/snapshots/strategies.sql#L131).

However when I reference my new check strategy in the snapshot config block, I'm receiving an error which relates to another macro (a macro that I have not changed): `snapshot_check_all_get_existing_columns`

The following error is received:
![image|690x108](upload://o2pMVOuqCejnqsvF5WxcTPU28kL.png)

I have tried to troubleshoot by using the default `check` strategy definition/logic but I still receive the same error message.

The macro causing the error definitely takes in 3 args as shown [here](https://github.com/dbt-labs/dbt-core/blob/main/core/dbt/include/global_project/macros/materializations/snapshots/strategies.sql#L93), so really not sure why the error message received states differently.

Any help in understanding the cause of this issue would be much appreciated.

Thanks all!",1
6001,"dbt-and-aws-secrets-manager","Hi,
Does DBT core know how to work with aws secrets manager?

Thanks",1
6001,"dbt-and-aws-secrets-manager","Hi @itzik,

the simple answer is no. DBT doesn't work with all cloud services. Its main purpose is to connect to supported databases.

But don't worry.  You could pass many variables from your environment (https://docs.getdbt.com/docs/build/environment-variables)

It means that if you're running DBT CLI on Airflow, you could retrieve the secret and pass it to the environment.",2
6055,"splitting-up-a-model-query-that-is-too-big-for-snowflake","We have 3300+ schemas in our database (one per customer). We have a model that loops through each schema and selects a specific table and then UNION ALL to combine them into one table in our analytics database. The query has now gotten too big for snowflake to run. 

Snowflake's suggestion was to break it up into separate queries with 100 union statements each into separate transient tables, then union all the transient tables at the end (while the current query in snowflake has all 3300+ union statements in one query).

Any suggestions on how to do that?

Our current model looks like this:

```
{{ config(materialized='table') }}

{%- set db = ""my_db"" -%}
{# custom macro that get's a list of all 3300+ schemas #}
{%- set schemas = run_query(nex_utils.find_schemas_sql(db=db, tbl='my_table')) -%}

{% if execute %}
{% for row in schemas.rows %}
SELECT
      column_1
      , column_2
      , column_3
      ....
FROM {{ db }}.{{ row.get('TABLE_SCHEMA') }}.my_table
{% if not loop.last %} UNION ALL {% endif %}
{% endfor -%}
{% endif %}",1
6055,"splitting-up-a-model-query-that-is-too-big-for-snowflake","Alternative suggestion: Does the code run as-is if you don't run all 3300 schemas at once? Perhaps you could chunk the 3300 into groups, and have one model for group 1, another model for group 2, etc. with as many groups as required to bring down the processing power so it can be handled by snowflake. I think that would be logically easier to read/reason about, rather than messing with a certain amount of union all statements (and wondering whether you got them all).",2
6055,"splitting-up-a-model-query-that-is-too-big-for-snowflake","Thanks for the suggestion @patkearns10. Another complicating factor is that the number of schemas will continue to change, so it seems like it would be a bit tedious to manage explicit models for groups of schemas.

What I've done for now is added a macro to replace the `find_schemas_sql` macro we currently use to build our list of schemas to union. The new macro uses the `find_schemas_sql` and loops through all the schemas 100 at a time and unions them into transient tables, then returns a list of transient tables that were created. The model then goes through that list similar to how it does now and unions the transient tables together. 

If there is a way to create more of a dynamic set of models that could be run in parallel that would include the groups of schemas to be unioned, and then one model that unions them all at the end, that would be awesome. Right now my macro that breaks it up is not in parallel. Each union of 100 tables takes about 10 minutes, so the whole process takes about 2 hours.",3
6064,"error-when-using-sequence-object-does-not-exist-or-not-authorized","I found this [article](https://discourse.getdbt.com/t/splitting-arrays-into-rows-in-redshift-with-dbt-macros/19) referencing to this [gist](https://gist.github.com/clrcrl/58062fbbe2af91cceab0e9187a510101#file-unnest_delimited_list-sql) and tried using that macro. But I'm always getting an error on line 15 where the sequence is used. I created that sequence manually in Snowflake and made sure the permissions were set correctly. Then I also tried creating the sequence from within dbt. Creating the Sequence this way also worked without error, but still I'm getting an error when trying to use that sequence.

I'm new to dbt so possible it is something simple I'm forgetting, but the last hours of research have been without success.

In case I should share some more details let me know.
Thanks for the support.

Philipp",1
6064,"error-when-using-sequence-object-does-not-exist-or-not-authorized","Hey Philipp, that article is for Redshift, which didn't (at the time of writing) have a native way to unnest json. I think that's changed, but still a moot point since you're using Snowflake.

On Snowflake, you should be able to use a combination of `flatten()` and the `lateral` keyword. There's an example here: <https://community.snowflake.com/s/question/0D50Z00006uSiEUSA0/how-to-implement-unnest-function-in-snowflake>

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1673836218726859?thread_ts=1673531386.786679&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",2
6064,"error-when-using-sequence-object-does-not-exist-or-not-authorized","Thanks for the feedback.

I got it working doing the following:

```
select my_table.*, flat_table.value as flat_value
from my_table
left join
    lateral flatten(strtok_to_array(my_table.netesd_value, ',')) as flat_table
```",3
6064,"error-when-using-sequence-object-does-not-exist-or-not-authorized","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6066,"dbt-cloud-run-in-deployment-only-one-model-of-job","Hi all,

I am trying to run one model in production and outside of the Job (set of models) but can't figure out how it is possible.

I have two environments on dbt Cloud:
- development with the target name 'dev' and the bigquery dataset 'analytics_dev'
- deployment with target name 'default' and bigquery dataset 'analytics

I have modified a model in the development environment and it does return the expected result by running 'dbt run --select model_name' on the command line. The code modification was merged to the git repo.

I would now like to deploy it in production.
This model is part of a Job that runs a set of models.

The problem is that I don't want to run all the models in the Job. I only want to run the modified model.
Is there any way through the command line (or whatever) to run only one model from the models in the job in production?

I tried 'dbt run --select model_name --target default' but the target name is ignored since it is launched in the development environment.

However I know that this command would work with dbt CLI if profiles.yml is configured properly. I therefore imagine that it is somewhat also possible in dbt Cloud

Best,",1
6066,"dbt-cloud-run-in-deployment-only-one-model-of-job","Hey Yoni! 

Deployment runs can only run through the scheduler, not the command line inside of the IDE. 

What I normally do is have a Job called something like `Adhoc Runs` and edit its command when I need to do this sort of one-off refresh. 

[quote=""yoni, post:1, topic:6066""]
I only want to run the modified model.
[/quote]
The other thing you could do is set up a job which takes advantage of deferral and the [`state:modified` selector](https://docs.getdbt.com/docs/deploy/about-state). Our docs mostly refer to this in a [CI context](https://docs.getdbt.com/docs/deploy/cloud-ci-job#configuring-a-dbt-cloud-ci-job), but you can use it in the same way for an adhoc production job which compares itself to your daily production run.",2
6066,"dbt-cloud-run-in-deployment-only-one-model-of-job","Defining an adhoc job is exactly what I was planning to do but it seemed a bit time-consuming each time.
I will study the second solution
Thank you for your help Joel!",3
6073,"source-from-one-database-target-to-different-database","I am trying to use a big query source from a different project and schema and want to target to a different project schema.

I am getting Access denied error.  Please advise on what could be wrong. I am authenticated with gcloud cli.",1
6073,"source-from-one-database-target-to-different-database","Hi @RajV 

Do your service/personal account have required permissions in both projects? 

You need Data Editor permission to create tables in the GCP BIgQuery project.",2
6078,"dbt-orchestration-airflow-vs-dbt-jobs","hello!

I'm interested in running our entire model on a 2 hour cadence.

I just started researching DBT Cloud and see you can schedule jobs within the UI. Is there a certain use case for when to use AIrflow vs just scheduling dbt cloud jobs? I only see information online about integrating airflow with dbt, however if I wanted to get up and running quickly it appears from the documentation that scheduled jobs is available within the UI. 
Dbt cloud jobs appears to be very easy to set up and run, and I'm confused on why I only see information on Airflow integration with dbt and not much about setting up scheduled jobs.

Primarily:
1. Can I run my entire dbt run through a scheduled job, instead of using airflow completely, and save some complexity?
2. Does this allow for running 'dbt run' through different BigQuery projects (not just schemas)?

What am I missing?",1
6078,"dbt-orchestration-airflow-vs-dbt-jobs","Hey @Zenb! 

Selfishly, I suspect that because dbt Cloud's scheduler is pretty simple to set up, there's not a lot of demand for lots of people building explainers on how to use it :sweat_smile: We should do a better job of making that information available though - what were you you searching online that brought up mostly Airflow?

A lot of companies do use Airflow alongside dbt Cloud, but unless you have other tools in your data transformation process (e.g. an ML process that you need to trigger in a different system after a dbt run completes) it's definitely not mandatory and you'll be well served by using dbt Cloud's built in scheduler to avoid the extra complexity. 


To answer your questions:
[quote=""Zenb, post:1, topic:6078""]
1. Can I run my entire dbt run through a scheduled job, instead of using airflow completely, and save some complexity?
[/quote]
Short answer: yes! Long answer: probably, unless you have a particularly complex setup as described above. 

[quote=""Zenb, post:1, topic:6078""]
2. Does this allow for running â€˜dbt runâ€™ through different BigQuery projects (not just schemas)?
[/quote]
I don't normally use BQ myself so I'm not 100% sure, but it's [supported in dbt Core](https://discourse.getdbt.com/t/can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project/769) so I don't see why not!",2
6078,"dbt-orchestration-airflow-vs-dbt-jobs","Thanks Joel! Sounds like I might be able to use DBT Cloud for my scheduled runs. Do you know of any resources for deploying dbt runs in different environments (not different schemas)? I currently have a dev, staging and prod dataset in bigquery (which is like it's own database) and would want different versions of our git repo to be in different environments.",3
6078,"dbt-orchestration-airflow-vs-dbt-jobs","Hi @Zenb 

As far as I know, there are two main options with Airflow.
1. Trigger DBT Cloud Jobs from Airflow (https://airflow.apache.org/docs/apache-airflow-providers-dbt-cloud/stable/operators.html)
2. Parse the DBT project and create DAG by yourself.

The second option relies on DBT CLI and requires more coding, but you can fully manage any part of DBT and save on using the open-source version of DBT rather than paying for dbt Cloud.  On the other hand, dbt Cloud provides many features and saves developing time.


You could setup different projects for BIgQuery using `database` configuration in your `dbt_project.yml` file 
https://docs.getdbt.com/reference/resource-configs/bigquery-configs#use-project-and-dataset-in-configurations",4
6078,"dbt-orchestration-airflow-vs-dbt-jobs","With the disclaimer that this isn't BQ-flavoured advice, here's the broad strokes of how it would work: 

## Dev
Your development environment gets its own schema which can also be on a separate dataset (for example each developer at dbt Labs has a schema named something like `dbt_jlabes` in a Snowflake database called `analytics_dev`).

## Staging (CI)
You can [set up CI jobs](https://docs.getdbt.com/docs/deploy/cloud-ci-job) so that when a PR is opened in GitHub/Azure DevOps/GitLab, dbt Cloud will build the changed models in a separate temporary schema. I don't know offhand if you can configure it to build in a different dataset sorry! 

## Prod 
The main environment that you've probably already configured",5
6078,"dbt-orchestration-airflow-vs-dbt-jobs","Thanks all!

We have separate projects (essentially servers) for our dev, staging, prod etc. So having different schemas probably won't work for our usecase. That said, It sounds like there is a path forward for us to be able to develop within one 'master' repo, and then push different versions of that repo out to the different servers.",6
6078,"dbt-orchestration-airflow-vs-dbt-jobs","[quote=""Zenb, post:6, topic:6078""]
So having different schemas probably wonâ€™t work for our usecase
[/quote]

You should definitely have individual schemas for each dbt user on your dev server, to avoid two people accidentally clobbering each other's tables. For example I would have a copy of the `customers` model in my own dev environment at `analytics_dev.dbt_jlabes.customers`, and you would have `analytics_dev.dbt_zenb.customers`, which means that I can mess around with the customers table without impacting you. Once my changes have gone through the code review process and been merged into the `main` branch, you'd get the new version of the table on your schema too. 

[quote=""Zenb, post:6, topic:6078""]
develop within one â€˜masterâ€™ repo, and then push different versions of that repo out to the different servers
[/quote]
Yes this is spot on - this is how you can be confident that your dev/staging/prod environments all look the same, because they're built from the same code.",7
6080,"how-to-use-tags-as-macro-parameters","Hello ! 

Do you know how to import tags in a macro ?
Here is an example of what I would like to do:
```
{% macro my_macro(tag) %}
           {% if var('tag') == 'hourly' %}
              Do stuff
          {% elif var('tag') == 'daily' %}
              Do other stuff
          {% endif %}
{% endmacro %}
```
Thank you for your help
Cheers",1
6080,"how-to-use-tags-as-macro-parameters","The code you've written is looking for a variable defined in your `dbt_project.yml` called `tag`. To access the tags on a model, you can use the `{{ model }}` object: https://docs.getdbt.com/reference/dbt-jinja-functions/model

Keep in mind that it could be an array, so you'd have to [do an `in` check](https://jinja.palletsprojects.com/en/3.1.x/templates/#other-operators), something like this: 

```sql
--models/some_model.sql
select *
from {{ ref('an_upstream_model') }}
{{ filter_based_on_tag(model.tags) }}
```

```sql
--macros/filter_based_on_tag.sql
{% macro filter_based_on_tag(tags) %}
where 
  {% if execute %} -- https://docs.getdbt.com/reference/dbt-jinja-functions/execute
    {% if 'hourly' in tags %}
      some_thing > dateadd('hour', -1, getdate())
    {% elif 'daily' in tags %}
      some_thing > dateadd('day', -1, getdate())
    {% else %}
      true
    {% endif %}
  {% endif %}
{% endmacro %}
```",2
6081,"implementing-script-which-worked-in-notebook-causing-a-typeerror-for-several-operations-in-dbt-py-model","I have managed to get a python model working in my DAG with very simple operations (manually make a DataFrame in-code and return it, materializing it as a table).

Having proved the concept, I wanted to migrate a functioning jupyter notebook script into the model, replacing where my test script was. When I did, I ran into this error for multiple different operations:

`TypeError: Cannot convert a Column object into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' if you're building DataFrame filter expressions. For example, use df.filter((col1 > 1) & (col2 > 2)) instead of df.filter(col1 > 1 and col2 > 2).`

As far as I can tell, I am not comparing different types in an incompatible way whenever this error appears. For example, the first line at which it happens looks like this:

`data_dbt['is_completed'] = np.where(data_dbt['status'] == 'completed', 1, 0)`

Column `status` has contains strings and column `is_completed` doesn't exist before this line.

Has someone come across this error before?",1
6081,"implementing-script-which-worked-in-notebook-causing-a-typeerror-for-several-operations-in-dbt-py-model","We managed to solve it, so here's the answer if anyone else has the same issue:

the solution is that `dbt.ref()`
didnâ€™t seem to return a DataFrame.

What worked for us was using to_pandas() to convert the data to a DataFrame that the rest of the code could recognise.

`### Getting the data`
`data_dbt = dbt.ref(""model"")`
`    `
` # Convert to DF`
` data = ` `data_dbt.to` `_pandas()` 

`# DF operation now works`
`# data['is_completed'] = np.where(data['status'] == 'completed', 1, 0)`

Here is an explanation from  @jerco ([dbt slack source](https://getdbt.slack.com/archives/C03QUA7DWCW/p1673875702398339?thread_ts=1673610738.092019&cid=C03QUA7DWCW)):

> To clarify, `dbt.ref()` does return a dataframe, but it will be the dataframe specific to your data warehouse â€” so a Snowpark dataframe on Snowpark, PySpark dataframe on Databricks or GCP (Dataproc). You're right that you can convert from those native ""distributed"" dataframes to a Pandas dataframe, and then write transformations using the familiar Pandas API. Performance is a consideration as your data volume scales.There's more about this in the docs: https://docs.getdbt.com/docs/build/python-models#dataframe-api-and-syntax",2
6081,"implementing-script-which-worked-in-notebook-causing-a-typeerror-for-several-operations-in-dbt-py-model","Great writeup, thank you for coming and closing the loop @Terroface!",3
6081,"implementing-script-which-worked-in-notebook-causing-a-typeerror-for-several-operations-in-dbt-py-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6083,"dbt-with-unity-catalog-failing-to-perform-incremental-materialization","HI all,

We are using DataBricks with Unity Catalog and we observe a peculiar issue with the Incremental Materailization. This Materialization is not working as expected, it behaves as table Materialization rather than Append, we tried all the options mentioned as per the documentation https://docs.getdbt.com/reference/resource-configs/spark-configs. can you please check let us know if this a know issue or a config gap I am doing ?

**DBT Versions:**
Core: 1.3.2
databricks: 1.3.2
Spark: 1.3.0

**Profile:**
```
dbt_databricks:
  outputs:
    dev:
      catalog: 'hive_metastore'
      host: adb-99999999999999.azuredatabricks.net
      http_path: sql/protocolv1/o/99999999999999/9999-999999
      schema: 'jaffle_shop'
      threads: 1
      token: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
      type: databricks
  target: dev
```
**Model Config:**
```
1. {{ config(
    materialized='incremental',
    incremental_strategy='append',
) }}

2. {{ config(
    materialized='incremental',
    file_format='delta',
    unique_key='copa_unique_key',
    incremental_strategy='merge'
) }}
```",1
6091,"order-of-execution-macros-vs-pre-hook-for-an-incremental-model","I need some help or guidance in here.

I have an incremental model which creates a temp table as part of a pre_hook statement.
Later it should execute a macro which call a stored procedure. The create temp table is an input parameter of the sp.

My problem is the macro executes before the pre_hook gets executed.

Is there anyway to achieve this?

I will attach the model and the dbt logs.

DBT MODEL: dx_store_plck.sql
```
{{
    config(
        materialized='incremental',
        unique_key='DX_ID',
        pre_hook=[
                """"""
                    CREATE OR REPLACE TEMP TABLE DXH_STORE_PLCK_IN
                    AS
                    SELECT DISTINCT
                        s.dx_id AS msa_id,
                        s.btg_store_nm AS store_name,
                        s.btg_city AS city,
                        s.btg_state AS state,
                        s.btg_address AS street_addr,
                        s.btg_zip5 AS zipcode,
                        'US' AS COUNTRY_CODE,
                        s.latitude AS LATITUDE,
                        s.longitude AS LONGITUDE,
                        COALESCE(p.last_updt_dt, TO_DATE('2023-01-01')) AS last_updt_dt
                    FROM {{ ref('dx_store_attr') }} s
                {%- if is_incremental() %}
                        FULL JOIN {{ this }} p USING (dx_id)
                    WHERE
                        (
                            p.placekey IS NULL
                        OR
                            DATEDIFF(day, p.last_updt_dt::DATE, CURRENT_TIMESTAMP()::DATE) > 90
                        )
                    ORDER BY last_updt_dt ASC, s.dx_id ASC
                    LIMIT 10000
                {%- endif -%}
                ;
                """"""
            ],
        tags=[""dx"", ""refresh:daily""]
    )
}}

--load dx_store_plck

{{
    append_placekeys(
        tbl_in='DXH_STORE_PLCK_IN',
        tbl_out='DXH_STORE_PLCK_TEMP'
    )
}}

SELECT
    t.msa_id AS dx_id,
    t.placekey AS placekey,
    t.error AS error,
    CURRENT_TIMESTAMP() AS last_updt_dt
FROM
    {{ database }}.{{ schema }}.DXH_STORE_PLCK_TEMP AS t
```
DBT LOG:
```
============================== 2023-01-13 21:46:41.153585 | 02dee2e8-4e8a-4811-b95c-53983cd91787 ==============================
[0m21:46:41.153646 [info ] [MainThread]: Running with dbt=1.3.0
0m21:46:45.658109 [info ] [MainThread]: Concurrency: 8 threads (target='dev')
[0m21:46:45.659835 [info ] [MainThread]: 
[0m21:46:45.704811 [debug] [Thread-1  ]: Began running node model.sdna_us_project.dx_store_plck
[0m21:46:45.708568 [info ] [Thread-1  ]: 1 of 1 START sql incremental model HOUSEHOLD_DEV.dx_store_plck ................. [RUN]
[0m21:46:45.712115 [debug] [Thread-1  ]: Acquiring new snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:46:45.714225 [debug] [Thread-1  ]: Began compiling node model.sdna_us_project.dx_store_plck
[0m21:46:45.717461 [debug] [Thread-1  ]: Compiling model.sdna_us_project.dx_store_plck
[0m21:46:45.727261 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""

[0m21:46:45.730138 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:46:46.411899 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.68 seconds
[0m21:46:46.413582 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:46:46.602455 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m21:46:46.604585 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
CALL DP_SDNA_US.HOUSEHOLD_DEV.APPEND_PLACEKEYS(
          'DXH_STORE_PLCK_IN',
          (
            SELECT object_construct(MAPPING.*)
            FROM (
              SELECT
                'msa_id' AS PRIMARY_KEY,
                'store_name' AS LOCATION_NAME,
                'city' AS CITY,
                'state' AS REGION,
                'street_addr' AS STREET_ADDRESS,
                'zipcode' AS POSTAL_CODE,
                'LATITUDE' AS LATITUDE,
                'LONGITUDE' AS LONGITUDE,
                'false' AS strict_address_match,
                'false' AS strict_name_match
            ) AS MAPPING
          ),
          'DXH_STORE_PLCK_TEMP', 'temp', 'get_placekeys_v', 1000
        );
[0m21:54:05.939981 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 439.33 seconds
[0m21:54:05.996189 [debug] [Thread-1  ]: Writing injected SQL for node ""model.sdna_us_project.dx_store_plck""
[0m21:54:06.007982 [debug] [Thread-1  ]: finished collecting timing info
[0m21:54:06.009677 [debug] [Thread-1  ]: Began executing node model.sdna_us_project.dx_store_plck
[0m21:54:06.028810 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:06.171949 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.14 seconds
[0m21:54:06.174469 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:06.343966 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.17 seconds
[0m21:54:06.419534 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:06.636179 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.21 seconds
[0m21:54:06.637514 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:06.836104 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m21:54:06.844633 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
CREATE OR REPLACE TEMP TABLE DXH_STORE_PLCK_IN
                    AS
                    SELECT DISTINCT
                        s.dx_id AS msa_id,
                        s.btg_store_nm AS store_name,
                        s.btg_city AS city,
                        s.btg_state AS state,
                        s.btg_address AS street_addr,
                        s.btg_zip5 AS zipcode,
                        'US' AS COUNTRY_CODE,
                        s.latitude AS LATITUDE,
                        s.longitude AS LONGITUDE,
                        COALESCE(p.last_updt_dt, TO_DATE('2023-01-01')) AS last_updt_dt
                    FROM DP_SDNA_US.HOUSEHOLD_DEV.dx_store_attr s
                        FULL JOIN DP_SDNA_US.HOUSEHOLD_DEV.dx_store_plck p USING (dx_id)
                    WHERE
                        (
                            p.placekey IS NULL
                        OR
                            DATEDIFF(day, p.last_updt_dt::DATE, CURRENT_TIMESTAMP()::DATE) > 90
                        )
                    ORDER BY last_updt_dt ASC, s.dx_id ASC
                    LIMIT 10000;
[0m21:54:09.451083 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.6 seconds
[0m21:54:09.463680 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:09.633206 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.17 seconds
[0m21:54:09.635563 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
create or replace  view DP_SDNA_US.HOUSEHOLD_DEV.dx_store_plck__dbt_tmp
     as (
    --load dx_store_plck
SELECT
    t.msa_id AS dx_id,
    t.placekey AS placekey,
    t.error AS error,
    CURRENT_TIMESTAMP() AS last_updt_dt
FROM
    DP_SDNA_US.HOUSEHOLD_DEV.DXH_STORE_PLCK_TEMP AS t
  );
[0m21:54:09.980829 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.34 seconds
[0m21:54:09.994182 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:10.200047 [debug] [Thread-1  ]: SQL status: SUCCESS 4 in 0.2 seconds
[0m21:54:10.211034 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:10.399277 [debug] [Thread-1  ]: SQL status: SUCCESS 4 in 0.18 seconds
[0m21:54:10.409359 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:10.570893 [debug] [Thread-1  ]: SQL status: SUCCESS 4 in 0.16 seconds
[0m21:54:10.616046 [debug] [Thread-1  ]: Writing runtime sql for node ""model.sdna_us_project.dx_store_plck""
[0m21:54:10.632582 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
BEGIN
[0m21:54:10.845295 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.21 seconds
[0m21:54:10.847299 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
[0m21:54:10.848481 [debug] [Thread-1  ]: On model.sdna_us_project.dx_store_plck: /* {""app"": ""dbt"", ""dbt_version"": ""1.3.0"", ""profile_name"": ""sdna_us"", ""target_name"": ""dev"", ""node_id"": ""model.sdna_us_project.dx_store_plck""} */
alter session set query_tag ='';
[0m21:54:11.027177 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.18 seconds
[0m21:54:11.030989 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""
*/
merge into DP_SDNA_US.HOUSEHOLD_DEV.dx_store_plck as DBT_INTERNAL_DEST
        using DP_SDNA_US.HOUSEHOLD_DEV.dx_store_plck__dbt_tmp as DBT_INTERNAL_SOURCE
        on 
                DBT_INTERNAL_SOURCE.DX_ID = DBT_INTERNAL_DEST.DX_ID
            

    
    when matched then update set
        ""DX_ID"" = DBT_INTERNAL_SOURCE.""DX_ID"",""PLACEKEY"" = DBT_INTERNAL_SOURCE.""PLACEKEY"",""ERROR"" = DBT_INTERNAL_SOURCE.""ERROR"",""LAST_UPDT_DT"" = DBT_INTERNAL_SOURCE.""LAST_UPDT_DT""
    

    when not matched then insert
        (""DX_ID"", ""PLACEKEY"", ""ERROR"", ""LAST_UPDT_DT"")
    values
        (""DX_ID"", ""PLACEKEY"", ""ERROR"", ""LAST_UPDT_DT"")

;
[0m21:54:11.918211 [debug] [Thread-1  ]: SQL status: SUCCESS 20000 in 0.88 seconds
[0m21:54:11.922083 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck"" */
COMMIT
[0m21:54:12.400002 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.47 seconds
[0m21:54:12.454299 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""*/
drop view if exists DP_SDNA_US.HOUSEHOLD_DEV.dx_store_plck__dbt_tmp cascade
[0m21:54:12.647964 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m21:54:12.675216 [debug] [Thread-1  ]: Using snowflake connection ""model.sdna_us_project.dx_store_plck""*/
use warehouse DP_SDNA_US_M
[0m21:54:12.822067 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.14 seconds
[0m21:54:12.827425 [debug] [Thread-1  ]: finished collecting timing info
[0m21:54:12.829087 [debug] [Thread-1  ]: On model.sdna_us_project.dx_store_plck: Close
[0m21:54:13.114290 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02dee2e8-4e8a-4811-b95c-53983cd91787', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xffff7808ed00>]}
[0m21:54:13.118092 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model HOUSEHOLD_DEV.dx_store_plck ............ [[32mSUCCESS 20000[0m in 447.40s]
[0m21:54:13.127935 [debug] [Thread-1  ]: Finished running node model.sdna_us_project.dx_store_plck
[0m21:54:13.145933 [debug] [MainThread]: Acquiring new snowflake connection ""master""
[0m21:54:13.149558 [info ] [MainThread]: 
[0m21:54:13.151689 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 7 minutes and 30.39 seconds (450.39s).
[0m21:54:13.153361 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:54:13.343818 [debug] [MainThread]: Flushing usage events
```",1
6091,"order-of-execution-macros-vs-pre-hook-for-an-incremental-model","Hi,

Multiple macros can be called using a list. Please refer the post,
https://discourse.getdbt.com/t/apply-multiple-macros-as-hooks-on-a-model/5613/2

following that approach,
pre_hook=[
                """"""
                    CREATE OR REPLACE TEMP TABLE DXH_STORE_PLCK_IN
                    AS
                    ......
                """""", ""{{append_placekeys('DXH_STORE_PLCK_IN', 'DXH_STORE_PLCK_TEMP' )}}""
            ]
wont work?!",2
6091,"order-of-execution-macros-vs-pre-hook-for-an-incremental-model","Hi @bk123 ,

Your suggestion was correct. That was the final approach which worked out (even though the several dbt anti-patterns included in the model). 

Thanks for your help.

Best regards,
raul",3
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","Let's say I have a model that `ref()`s models of deprecated tables which will not be modified.

I have other things that depend on this model, and I'd therefore like to be able to `ref()` this model in those models for the dbt docs.

I also want to be able to version control the SQL.

However, I don't ever want this model to run, because it'll always return the same results and is therefore wasteful.

I've looked at analyses, seeds, and snapshots, but it seems like none fit. I've looked at the `enabled` config, but that prevents anything from referencing the model.

I know I can simply use `exclude` in a `dbt run` command, but that requires us to always remember to exclude this one model, and we're running dbt from within Fivetran, so passing custom CLI flags isn't even an option for us.

I've also looked at custom selectors, where we could default to excluding this model. However, unless I'm misunderstanding, those don't seem very useful, given:

> If I run a command that defines its own selection criteria (via --select, --exclude, or --selector), dbt will ignore the default selector and use the flag criteria instead. It will not try to combine the two.

`exclude` gives me exactly what I want in terms of functionality - I just need a way to apply it permanently to that model.

If this just isn't possible, any suggestions that'll accomplish my below objectives would be most appreciated!:
1. Keep it in the repo for documentation (using `ref()`s)
2. Version control the SQL


Thank you!",1
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","Hi @ptrn 

Have you considered referring to this table as a source table?

Could you leave the disabled model, add the table to your source list, and refer to it as `source(table_name)`?",2
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","I was also going to suggest creating a source, but the version control requirement makes it fiddly (you could have a disabled node as well as a source, but it's a bit weird). Perhaps if you wanted to create it as a source, you could move the code to an analysis file?

Alternatively, you could configure it as an incremental model, and put `where 1 = 0` inside of your `is_incremental()` block which would mean it would never change anything.

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CCKAGDE0M/p1673834938821399?thread_ts=1673730764.066919&cid=CCKAGDE0M). It might not have transferred perfectly.</sub>",4
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","Thanks for the suggestions!

> Have you considered referring to this table as a source table?
> Could you leave the disabled model, add the table to your source list, and refer to it as `source(table_name)`?

I was considering doing this, but then I don't think I'd have any data lineage for what's upstream from the table, since it'd be a ""source"". Unless I'm misunderstanding that?

>Alternatively, you could configure it as an incremental model, and put `where 1 = 0` inside of your `is_incremental()` block which would mean it would never change anything.

Hmm yeah. This could do it! Probably the best approach now.

I appreciate the help!",5
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","The proposed solution has one caveat which you should consider, if you place the condition in `is_incremental` block, it would not work on full-refresh.

Maybe you should also add `full-refresh: false` in `config` block

https://docs.getdbt.com/reference/resource-configs/full_refresh",6
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","Hey thanks. That's a great callout; I wouldn't have thought of that!",7
6098,"how-to-ref-a-table-i-want-to-keep-static-skip-exclude-not-run","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",8
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","The error is: Database Error in rpc request (from remote system.sql) 001003 (42000): SQL compilation error: syntax error line 1 at position 0 unexpected 'limit'.

Database is Snowflake. I'm not able to provide compiled code as I cannot get the code to compile due to this error

Code from the training video is:
```
{% macro template_example() %}

    {% set query %}
        select true as bool
    {% endset %}

{% if execute %}
    {% set results = run_query(query).columns[0].values()[0] %}
    {{ log(""SQL results "" ~ results, info=True) }}

    select {{ results }} as is_real
    from a_real_table

{% endif %}

{% end macro %}
```
Any help is greatly appreciated.

Thanks.",1
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","I believe I have answered my own question. This error occurs when I try to compile macros, which I now see is not necessary.  When I run them with dbt run-operation I have no trouble.  I'm new to this platform and still learning...",2
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","Good solve @jjage5729! One other solution for the future - if you need to preview the behaviour of a macro, you can open a new Scratchpad and call it from there:

![image|589x500](upload://s1b94riRXmCZ2Tv7rtC2eyTFuJi.png)",3
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","Excellent. Thank you for the help and I love your tool. Thi is a really cool development environment.",4
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","Thank you for noting this @jjage5729!! I'm going to make a note for our team to add a note to the course about this to clarify for future learners. :tada:",5
6102,"error-working-through-union-tables-by-prefix-macro-example-in-the-jinja-macros-packages-training-class","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
6111,"dynamic-models-specific-in-config-tables","Hi all!

Any help you can give on the below would be gratefully received.

We have the following challenge:

- We have many customers for a platform we're building that ingests data using DBT from various sources
- The bulk of the logic in the DBT flow that process this incoming data is the same for all customers
- For some models however, a different approach needs to be configured per customer. Eg. Both customers use a table called ""Accounts"" to hold customer data, but customer A uses a standard field ""Industry"" to hold the customer segment, whilst customer B uses a custom field ""Segment_C"".
- We break the logic for how to capture ""customer_segment"" into two separate models (one for customer A and one for customer B) and then ask DBT to use the correct model when it gets to that part of the flow.
- The ""which is the correct model"" config lives in a table in our data warehouse. We want to have DBT read the configuration for which model to correctly include for customers A and B whilst retaining the rest of the flow identical for both customers

Challenge:
-- DBT won't let us dynamically insert ""--depends on comments"" in models, which breaks our approach

Question:
- Is there a way to add --depends on comments for models in some kind of pre-hook to get around this?
- Is there a better way to deal with dynamic model requirements?

Many thanks!",1
6111,"dynamic-models-specific-in-config-tables","Interesting question! Is the (very cut down) DAG that you're working with something like this? 
![image|690x182](upload://dGiSoz8athhF9IqRzTMxpdW4NIU.png)

If not, can you let me know what it actually looks like? You can use https://mermaid.live to generate a diagram, this is the template I used: 
```
graph LR
    acc1[source_accounts_1] --> ta1(transformed_accounts_1)
    acc2[source_accounts_2] --> ta2(transformed_accounts_2)
    ta1 --> seg(customer_segment)
    ta2 --> seg
```

It's not possible to embed arbitrary `depends_on`s earlier in the flow, as the DAG is generated before any of those things are rendered. 

One idea I just had (pretty sure it'll technically work, but might not be practical for you) would be to put both customer A and B's `depends_on` into each other's files, i.e. 

```sql
--depends_on: {{ ref('transformed_accounts_1') }}
--depends_on: {{ ref('transformed_accounts_2') }}

select * 
from {{ determine_correct_table() }}
where ... 
```

As I've been writing this, I'm realising my example DAG above must be completely wrong, but I'm not sure what it should look like. Hopefully this is a jumping off point for you though; if you can give some more real-world context I can try to be more helpful",2
6116,"how-do-i-get-help-from-people-who-are-using-slack","I have a question but I would also like people in the #advice-dbt-help Slack channel to see it; what should I do?",1
6116,"how-do-i-get-help-from-people-who-are-using-slack","Great question! When you make a post in the forum, it will automatically post to <#CBSQTAPLG|advice-dbt-help>. From there, anyone can reply in Slack and those replies will be shown on your original forum post.

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1673932189665149?thread_ts=1673932116.935819&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6116,"how-do-i-get-help-from-people-who-are-using-slack","I silenced <#CBSQTAPLG|advice-dbt-help> a long time ago because it was too noisy.  Should each discourse category have their own slack channels, where people are not permitted to post in slack?

<sub>Note: `@Sterling Paramore` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674002302593059?thread_ts=1673932116.935819&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",3
6116,"how-do-i-get-help-from-people-who-are-using-slack","@gnilrets good questions! Here's how we're thinking about this: 

- We (intentionally) only have the one overarching Help category on Discourse, to avoid people getting bogged down in ""is this a deployment question or a tooling question or a getting started question?""
- Discourse topics can be tagged, so people who have a particular interest in answering questions about #orchestration-and-deployment or #jinja or anything else can watch those tags 
![image|690x357](upload://7iOb6G09M5JTDO6xPZ3kMctTcOt.png)
- The main goal here is to minimise the behavioural change needed for folks who are already active in `#advice-dbt-help` - if this works, then Discourse will wind up as noisy as Slack is currently (maybe a bit less since there should be fewer duplicate questions), so I don't know that a read/comment-only feed in Slack will solve your problem in the medium term. However watching specific tags that interest you (above) might work?",4
6116,"how-do-i-get-help-from-people-who-are-using-slack","I had a question about documenting singular tests and I was searching slack for the right channel to post to.  Then I figured Iâ€™d post it on discourse.  When I started writing the title to my post, I got the answer from another post: <https://discourse.getdbt.com/t/is-it-possible-to-add-a-description-to-singular-tests/5472>

Very encouraging!

<sub>Note: `@Sterling Paramore` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674262895921009?thread_ts=1673932116.935819&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",6
6116,"how-do-i-get-help-from-people-who-are-using-slack","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","Am getting the following error when running dbt parse against set of models as shown below. There dbt_project.yml file is same as what is generated during the dbt set-up. I am not able to understand how to debug this error

![error-1|690x111](upload://us7tIDHlV7tifwzMi8WmUlLQutK.png)

Prior to getting that error, I encountered the following one which is indicating the version number to be integer error. I have put the value as 2 and then the above error cropped up.

> *The yml property file at models\...\dbt_project.yml is invalid because its 'version:' tag must be an integer (e.g. version: 2). 1.0.0 is not an integer. Please consult the documentation for more information on yml property file syntax:*
> 
> *  https://docs.getdbt.com/reference/configs-and-properties*


It's failing for this particular set of models. The same project file is working for others. Any pointers on where to look for error?",1
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","HI @vkarthik21, a couple of comments: 
- it's very uncommon to call a yaml file in the `models` directory `dbt_project.yml`. There should only be a single `dbt_project.yml` file at the root of your project. It's not *illegal* to call your model config files that (they can be whatever you want), but it's very confusing. [Here's how we normally name our files](https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview#guide-structure-overview)
- With that said, it sounds like you have a yaml file that looks something like this: 

```yaml
version: 2 
sources: 
  - name: something
    ... #rest of a source definition here
models: 
#nothing at all here
```

That is, you have a `models` key which doesn't contain a list - it's probably empty but I guess it could also contain a dictionary or some other structure. Can you post the offending file?",2
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","Thank you so much Joe for your reply. Actually we have dbt_project.yml across each sub-directory of datasets. 

Am attaching the offending dbt_project.yml file for reference.  In this example below, first error I get is - version number should be integer. If I change that to 2, that's when I get error saying models should be a list. Please let me know if you need additional details. Not sure why it's happening.



```
name: 'dev_project'
version: '1.0.0'
config-version: 2

# This setting configures which ""profile"" dbt uses for this project.
profile: 'default'

# These configurations specify where dbt should look for different types of files.
# The `source-paths` config, for example, states that models in this project can be
# found in the ""models/"" directory. You probably won't need to change these!
source-paths: [""models""]
analysis-paths: [""analysis""]
test-paths: [""tests""]
data-paths: [""data""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  # directory which will store compiled SQL files
clean-targets:         # directories to be removed by `dbt clean`
    - ""target""
    - ""dbt_modules""


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/ directory
# as tables. These settings can be overridden in the individual model files
# using the `{{ config(...) }}` macro.
models:
  +persist_docs:
    relation: true
    columns: true
```",3
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","[quote=""vkarthik21, post:3, topic:6117""]
Actually we have dbt_project.yml across each sub-directory of datasets
[/quote]

This isn't how dbt works - it expects to find a single `dbt_project.yml` file in the root of a project, and it expects that all yaml files it finds in the `models/` directory to be config/property files which look like [this](https://docs.getdbt.com/reference/model-properties). This is why it's asking you to set `version: 2` - that is the correct value for a yaml file inside of the models directory.

What are you trying to do that means you want to put multiple `dbt_project.yml` files throughout your directory? Did you know that you can configure behaviour at the directory level from the root of the project? 

For example: 

```yaml
#dbt_project.yml

name: 'dev_project'
version: '1.0.0'
config-version: 2

# This setting configures which ""profile"" dbt uses for this project.
profile: 'default'

#...

models:
  subdirectory_1:
    +persist_docs:
      relation: true
      columns: true
  subdirectory_2:
    +persist_docs:
      relation: false
      columns: false

```",4
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","Thank you. What you are saying makes perfect sense. I hate to say what I am about to say but 'it was working before' with the same set-up. We have a shell script task in Azure DevOps pipeline that does the parse against each dataset with dbt parse command. A month prior everything was working fine and then stopped working. I did go back to that change set but don't see any differences in the .yml files. It's just that few more models (datasets) have got added to the project but essentially set-up is the same. 

The actual error that we are seeing in the when that DBT Parsing step is run is as shown below. To debug the issue when I am doing it from local system, I ran into those two errors as per my original post. So not really sure what is happening. 

Am unable to reproduce the error at DevOps pipeline server and am stuck at this place.


> 2022-12-22T05:44:26.6568883Z Traceback (most recent call last):
> 2022-12-22T05:44:26.6571316Z   File ""/home/vsts/.local/lib/python3.10/site-packages/logbook/handlers.py"", line 216, in handle
> 2022-12-22T05:44:26.6572019Z     self.emit(record)
> 2022-12-22T05:44:26.6572722Z   File ""/home/vsts/.local/lib/python3.10/site-packages/dbt/logger.py"", line 472, in emit
> 2022-12-22T05:44:26.6575523Z     assert len(self._msg_buffer) < self._bufmax, \
> 2022-12-22T05:44:26.6576312Z AssertionError: too many messages received before initilization!
> 2022-12-22T05:44:26.6577214Z Logged from file /home/vsts/.local/lib/python3.10/site-packages/logbook/concurrency.py, line 141



Thing is, the whole architecture is set-up in a such a manner that, it loops through each individual directory , parses it as first step of ensuring there are inconsistencies in the model.

In terms of how it's getting used, all the models for given dataset are first copied over to the cloud. We then have a Airflow composer job (given dataset) running a kubernetes pod, which in turn runs the dbt commands for a given job (against the given dataset).",5
6117,"schema-file-invalid-as-models-is-not-a-list-error-performing-dbt-parse","[quote=""vkarthik21, post:5, topic:6117""]
it loops through each individual directory , parses it as first step of ensuring there are inconsistencies in the model.
[/quote]

So it sounds to me like you actually have dozens of tiny little dbt projects - a dbt project is defined as any directory that contains a `dbt_project.yml` file at its root. If you're iterating through each directory one at a time, then I can see why you would be able to do this. 

[quote=""vkarthik21, post:5, topic:6117""]
A month prior everything was working fine and then stopped working. I did go back to that change set but donâ€™t see any differences in the .yml files. Itâ€™s just that few more models (datasets) have got added to the project but essentially set-up is the same.
[/quote]
Does this mean you have also not added any extra `.yml` files? 

Because you have blurred out the original screenshot, I can't see what paths your files are located at. I suspect it's something like this though: 

```shell
(dbt-prod) joel@Joel-Labes joel-sandbox % dbt parse
...
Parsing Error
    The schema file at joel-sandbox/subdirectory/dbt_project.yml is invalid because the value of 'models' is not a list
```

This would happen when you're invoking dbt from the root instead of one of your subdirectories, which means it's hitting a second `dbt_project.yml` file which has a different shape",6
6122,"post-hook-with-incremental-tables","I'm creating an incremental model and I want to run a post hook in order to save the new records to a file in my cloud provider. The key concept is that I only want to save those records which were appended to the table in the current execution.

Is there a way to refer to those records which have been queried in that increment, or any other workaround for this?

Usage example:

```
{{-
    config(
        materialized = 'incremental',
        incremental_strategy = 'merge',
        unique_key = 'id',
        post_hook = """"""
EXPORT DATA
    OPTIONS (
        uri = 'gs://bucket/path/{{ run_started_at.strftime('%Y-%m-%d_%H-%M-%S') }}/*.csv',
        format = 'CSV',
        overwrite = true,
        header = true,
        field_delimiter = ';')
AS (
    SELECT * FROM {{ this }}
);
""""""
    )
-}}

SELECT *
FROM {{ source('my_source', 'my_table') }}
WHERE
    {%- if is_incremental() %}
    date_hit > (SELECT MAX(date_hit) from {{ this }})
    {%- else %}
    date_hit >= ""1970-01-01""
    {% endif %}
```

The `SELECT * FROM {{ this }}` clause in my post hook will save the whole table, so that would be what I need to change",1
6122,"post-hook-with-incremental-tables","Hi @luiscristobal.lopez 

You could add the current date/timestamp inserting new records, and filter them out.",2
6122,"post-hook-with-incremental-tables","Can you elaborate on that @wefo ? I have tried using timestamps, but post hooks can't access context at compilation/execution time, so even if I save the max timestamp used for the increment in a set, it's not accessible when the post hook runs.",3
6122,"post-hook-with-incremental-tables","[quote=""wefo, post:2, topic:6122""]
You could add the current date/timestamp inserting new records, and filter them out.
[/quote]

I think this means that you would do something like this: (the two changed lines are indicated)

```
{{-
    config(
        materialized = 'incremental',
        incremental_strategy = 'merge',
        unique_key = 'id',
        post_hook = """"""
EXPORT DATA
    OPTIONS (
        uri = 'gs://bucket/path/{{ run_started_at.strftime('%Y-%m-%d_%H-%M-%S') }}/*.csv',
        format = 'CSV',
        overwrite = true,
        header = true,
        field_delimiter = ';')
AS (
    /* this line is changed */
    SELECT * FROM {{ this }} where loaded_at = {{ run_started_at }}

);
""""""
    )
-}}

SELECT *,
  {{ run_started_at }} as loaded_at /* this line is changed*/
FROM {{ source('my_source', 'my_table') }}
WHERE
    {%- if is_incremental() %}
    date_hit > (SELECT MAX(date_hit) from {{ this }})
    {%- else %}
    date_hit >= ""1970-01-01""
    {% endif %}
```",4
6122,"post-hook-with-incremental-tables","thanks @joellabes.  that was my point.",5
6122,"post-hook-with-incremental-tables","I partitioned my table by this `run_started_at` field and the suggestion fit perfectly to my use case.

Thank you very much both for the help",6
6122,"post-hook-with-incremental-tables","The only thing to keep in mind with this approach is that it assumes your post hook will run successfully every time. If the model builds successfully but the export step fails for some reason, then those records will never be exported without manual intervention because subsequent runs will obviously have a different `run_started_at`. 

It's unlikely to be a major issue but something worth keeping in mind",7
6122,"post-hook-with-incremental-tables","Sure! It's not a problem since the export select statement should be simple enough to ensure it doesn't fail. If eventually it happens, I will receive an error message and export the rows manually. Thanks for the disclaimer",8
6122,"post-hook-with-incremental-tables","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
6127,"dbt-sqlserver-python-3-11-old-pyodbc-version","Trying to install the `dbt-sqlserver` package via pip (Python 3.11) results in an error related to `pyodbc`.

The error is: ""Microsoft Visual C++ 14.0 or greater is required.""

Microsoft Visual C++ is installed on the local machine, and in the right version.

Installing pyodbc on its own goes just fine. However, the version of `pyodbc` that `dbt-sqlserver`is trying to install is 4.0.32 whereas the most recent version currently available is 4.0.35.

I asked pyodbc maintainers for help and they said that version 4.0.32 didn't have wheels for Python 3.11: https://github.com/mkleehammer/pyodbc/issues/1010#issuecomment-1385655688

I am just starting my Python adventure and I don't really understand what is going on here. Pyodbc maintainers are suggesting reaching out to dbt-sqlserver authors and clarify with them why they are including an older version of pyodbc library in their product.

This is the exact error message I am getting:
```
      running build
      running build_ext
      building 'pyodbc' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  Rolling back uninstall of pyodbc
  Moving to c:\program files\python311\lib\site-packages\pyodbc-4.0.35.dist-info\
   from C:\Program Files\Python311\Lib\site-packages\~yodbc-4.0.35.dist-info
  Moving to c:\program files\python311\lib\site-packages\pyodbc.cp311-win_amd64.pyd
```

It is a Windows 11 PC with Python 3.11.0

Any hints welcome.",1
6127,"dbt-sqlserver-python-3-11-old-pyodbc-version","Hey @xpil, dbt Core only supports Python 3.11 from v1.4, which is [currently in release candidate status](https://docs.getdbt.com/guides/migration/versions/upgrading-to-v1.4?version=1.4).

Once it goes GA, the dbt-sqlserver package can also add 3.11 support. I imagine that part of that process will include reviewing the pyodbc version it depends on. 

In the meantime, are you able to try installing dbt on Python 3.10? I don't use Windows so can't give hyper-specific guidance, but I have skimmed this article and think that it does a good job of showing how to get multiple python environments set up. When you create a python virtual environment for 3.10 you can install dbt into that. (It's a good idea to put dbt into a venv regardless, to protect it from other weird things happening as you install other Python libraries and vice versa.)

PS: thanks for the great writeup of your issue! Really helpful :pray:",2
6127,"dbt-sqlserver-python-3-11-old-pyodbc-version","I've also run into a similar issue while trying to create an image, although using Ubuntu 20.04 and tried Python v 3.7, 3.8, 3.9 & 3.10",3
6127,"dbt-sqlserver-python-3-11-old-pyodbc-version","For info,  turned out to be a driver issue for me.
The following line in my Dockrfile solved the issue:

> RUN apt-get install unixodbc-dev -y",4
6128,"question-on-aws-athena-and-s3-use-dynamic-external-location-in-models-file","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

I am working on a demo project on DBT with AWS Athena, I can build dbt models and working with Athena well. There are two problems:
1. The output location in AWS s3 requires a dynamic location. In our case, we need the storage path with date, i.e. s3://mybucket/dbt/2023/01/17/demo.
2. The generated parquet file is all good except its file name. I got something like: ""20230112_023258_00015_ddqb9_6b8d55cf-02bd-4f7b-80f4-f6dc991829b3"". I would like to customize its name like: ""mydemo001.parquet"".

Note: I tried but cannot find much helpful info from internet. I also tried to define various 'vars' but seem not working in this case. 

Here are the sample code:
>
>  {{ config(materialized='table', file_format='parquet',  **external_location**='s3://mybucket/dbt/demo') }}
>  
 > with source_data as (
 >   select distinct emplid from {{ source('demo_athena', 'demo_table') }}
  >)
  >select * from source_data",1
6128,"question-on-aws-athena-and-s3-use-dynamic-external-location-in-models-file","Hi @yzhang, I asked a couple of people who use the Athena adapter and got these pieces of feedback: 

> Dynamic output locations are definitely possible--my project uses them for nearly every model. I use a macro to build the external location string and then call that macro from my model config.

>Athena does not provide any mechanism for naming the files it creates 

If you use a macro to build the external location string, remember to [not nest your curlies](https://docs.getdbt.com/docs/building-a-dbt-project/dont-nest-your-curlies)",2
6131,"dbt-support-for-schema-versioned-database-objects","Hello - dbt newbie here asking if there is a feature in the platform to help us manage multiple (simultaneous) versions of database objects where schemas change over time.

Initially (v1.0 of data product) we have ""modelX"" with 6 columns.  Let's call it modelX_1.0.

Later, the team adds two more columns, and we want to serialize it into modelX_2.0.

We would want a user in the database or client accessing from BI layer or in code to automatically get the latest version if they try to access the name ""modelX"" directly.  But if they access a specific version (e.g., ""select * from modelX_1.0"" it should go to that appropriate version.

Is there a way to have dbt facilitate this behavior (custom schemas?), or am I looking for a supplemental tool or methodology?  Any help greatly appreciated.",1
6131,"dbt-support-for-schema-versioned-database-objects","I love this question and line of thinking! I've been noodling on a similar thing in the back of my head but haven't written anything down yet. 

Today, dbt doesn't have a model versioning system, but you can use [exposures](https://docs.getdbt.com/docs/build/exposures) to document which models power which dashboards so you know where you'll need to change things.

In the future, I think the dbt Semantic Layer might make this possible. It enables you to use full dbt code (including macros like `ref`) inside of your supported BI tool, notebook, etc. What that would mean is that you could override the built-in `ref` macro, or build a different macro, like this:

```sql
--pseudocode...
{% macro version_aware_ref(model_name, version=None) %}
  {% if version is None %}
    {% do return(ref(model_name)) %}
  {% else %}
    {% do return(ref(model_name_v ~ version)) %}
  {% endif %}
{% endmacro %}
```

You could already do :point_up:  today, but it wouldn't be accessible from your BI layer, only from inside the dbt project. The Semantic Layer is what would unlock the full potential. 

What do you think?",2
6146,"how-to-copy-data-from-s3-to-redshift-using-dbt-core","I'm trying to copy data from S3 to Redshift using  dbt core. But simple copy command is not working.
Can somebody please suggest, how to copy data from S3 to Redshift using DBT core.

This is the sample code that I was using:
COPY <table_name>
FROM 's3://<bucket_name>/<path_to_file>'
CREDENTIALS 'aws_access_key_id=<access_key>;aws_secret_access_key=<secret_key>'
REGION '<region>'
FORMAT AS <file_format>;",1
6146,"how-to-copy-data-from-s3-to-redshift-using-dbt-core","What error messages do you see? Have you tried running that same command in a query runner (DataGrip etc)? Commands issued through dbt are the same as if they're done through any other sql interface, so I think the problems you're running into will be with the format of your copy command. I'm not super familiar with the syntax though unfortunately",2
6146,"how-to-copy-data-from-s3-to-redshift-using-dbt-core","Hi, Thanks for you reply.
This command is working in SQL workbench etc. 
Is there any other way to copy data from S3 to Redshift using dbt core. I'm using dbt core in VS code.",3
6146,"how-to-copy-data-from-s3-to-redshift-using-dbt-core","What errors are you seeing? Have you had a look at the logs file to see what command is being sent to the server?",4
6148,"is-user-tracking-possible-in-dbt-docs","## The problem I'm having

We are using the cloud-hosted version of dbt docs and I've been trying to figure out if there's a way to see the user engagement there (mainly, page views) but I can't find anything in the documentation. I'm not sure if this is a feature that doesn't exist yet or one that simply isn't documented.

## The context of why I'm trying to do this

I'm interested in tracking the user engagement (just the **page views** would be more than enough) in dbt docs. That could provide some insight as to what models need more attention in terms of documentation.

## What I've already tried 

I've also searched through the community discussions and through the GitHub repo without much progress. I know there is an [optional user tracking](https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats) that dbt collect on their side. I also found a stale and closed [GitHub issue](https://github.com/dbt-labs/dbt-docs/issues/63) basically requesting the same thing that I'm trying to do. It doesn't seem like there were any updates, unfortunately.

Any advice would be greatly appreciated!",1
6148,"is-user-tracking-possible-in-dbt-docs","Hey @golemshinska.kosara, there isn't existing functionality for this I'm sorry! I have reopened the issue you found - I don't know that it will immediately lead to anything but it's the best place to weigh in with your use case.",2
6148,"is-user-tracking-possible-in-dbt-docs","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6150,"development-with-full-autocomplete","Like many others, I'm developing in VS Code, mainly python, and (like many others) I'm used to having autocomplete when coding.

I'm using postgres as the backend, and there is no autocompletion of tables and columns as far as I can tell. Can that really be true? I guess you need a connection to the backend, but that should be possible with the information available to DBT. I can see that there are VSCode extensions for standard postgres that allow for autocomplete, but they on the other hand do not work well with Jinja. 

Any advice on a good IDE setup?",1
6150,"development-with-full-autocomplete","Are you using the dbt-power-user extension?

<sub>Note: `@Josh Devlin (he)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674077801551379?thread_ts=1674049104.803179&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6150,"development-with-full-autocomplete","Hi, I was using dbt-formatter (which is basically a light version?), but got power-user working yesterday after some hassle. But the problem is the same no? From what I can tell (both from docs/issues and personal experience), power user does not allow for column completion.",3
6189,"possible-to-use-defer-to-testing-time-in-cloud-ide","I'm building a big new dbt project and developing in the cloud IDE. The models I'm developing depend on intermediate tables that take hours to run. 

Is it possible to use `defer` or another feature of the dbt CLI to source data from the production version of tables from the cloud IDE but build the target models in my personal dev environment?",1
6189,"possible-to-use-defer-to-testing-time-in-cloud-ide","You can definitley use `defer` within the IDE, you'll just need to have the relevant, up-to-date artifact to defer to.  [Here's an example of a github action](https://github.com/dpguthrie/snowflake-dbt-demo-project/blob/main/.github/workflows/run_job_on_merge.yml) that's triggered when code is merged to your main branch.  It's doing several things:

- Running a dbt Cloud job.  The job I have configured will run any modified models and anything downstream.  You can also do something really simple here like `dbt compile --exclude fqn:*`, which will still generate the `manifest.json` file but not actually run any queries.
- Retrieve the `manifest.json` file from the completed run
- Save the `manifest.json` file into a directory in your project.  Once there, you'll now be able to defer to that artifact within the IDE.

Another option is using the [upstream_prod](https://hub.getdbt.com/lewisdavies/upstream_prod/latest/) package.  This package overrides the built-in `ref` function and allows you to select from production objects when appropriate.  No github action or deferral necessary here!

Hope that helps!",2
6204,"post-hook-when-a-model-fails","## The problem I'm having
I have a pre hook and post hook in a model. The pre hook inserts audit information into an admin table. The post hook is intended to update that table when the model succeeds/fails. The post hook works fine when the model succeeds. But if the model fails, the post hook does not get executed.
update_status.sql macro looks like below!

```
{% macro update_status(modelName) %}
    {% if execute %}
    {{log(""Calling update_status_success()"", info=True)}}
        {{ update_status_success(modelName) }}
    {% else %}
    {{log(""Calling update_status_failure()"",info=True)}}
        {{ update_status_failure(modelName) }}
    {% endif %}
{% endmacro %}

```
Is it  possible to call the post hook based on whether a model succeeds or fails in the runtime?

Thanks in advance!",1
6204,"post-hook-when-a-model-fails","Which database are you using?",2
6204,"post-hook-when-a-model-fails","Hi,

We are using Bigquery!",3
6204,"post-hook-when-a-model-fails","[quote=""bk123, post:1, topic:6204""]
ate_status.sql macro looks like b
[/quote]

I cannot provide a complete working example, but I think the best approach would be to use DRY principle and parse run results with an `on-run-end` macro once than using a post-hook after each model.

Check dbt Docs https://docs.getdbt.com/reference/dbt-jinja-functions/on-run-end-context#results
Also, this post could be insightful https://medium.com/@oravidov/dbt-observability-101-how-to-monitor-dbt-run-and-test-results-f7e5f270d6b6",4
6204,"post-hook-when-a-model-fails","[quote=""bk123, post:1, topic:6204""]
The pre hook inserts audit information into an admin table
[/quote]

If you're trying to track the results of your dbt invocations, you could also look at the [dbt_artifacts](https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest/) package, which will deal with a lot of this for you",5
6204,"post-hook-when-a-model-fails","Thanks for the reply! This package is very useful!",6
6204,"post-hook-when-a-model-fails","Thanks for the reply! I tried implementing this with some additions, like, adding compile start and end time, execute start and end time. Now I am exploring more about run_results and other artifacts. One thing I would like to explore is tracking the model runs as and when they run. ""on-run-end"" uploads the results only after the run completes. To track the execution as it progresses, I have implemented pre-hooks and make an entry into a different table. 

If the model succeeds, I updated that table's entry with end time. Otherwise, the end time will be null. Using this, we can track if some model is running for longer times.

Is there a better way to accomplish this?",7
6204,"post-hook-when-a-model-fails","I think you mix too many things. You should separate the logic if you want a scalable and easily maintainable solution.
DBT should run the models because of its primary purpose. For monitoring, you should implement something that doesn't depend on the models themselves.
Package `dbt_artifacts` collects dbt run results and stores them for future analysis.  You don't need to analyze DBT on the run for optimizations. If you want to control the duration of the models, you could add a `timeout` parameter to the `profiles.yml` file to fail long-running models after a specific number of seconds.

However, if you want to track DBT  progress, it depends on how you are running DBT. Either you check CLI output, where you can see the model status and how many models are left, or you could run each DBT model as a separate task, for example, in Airflow, and check the status and duration of each task.
Checking CLI output is a manual task, but you could separate monitoring and DBT processes with Airflow

I am not very familiar with DBT Cloud, but maybe @joellabes could explain if DBT Cloud has some kind of solution.",8
6204,"post-hook-when-a-model-fails","I agree that you should use the artifacts package to understand your models' behaviour as opposed to tracking their behaviour in real time. `post hooks` only run for successful models, whereas `on-run-end` hooks always run. Additionally, all this secondary logging can drastically slow down your project, making your timing endeavours somewhat counterproductive! 

In dbt Cloud you can review model build times using https://docs.getdbt.com/docs/dbt-cloud/using-dbt-cloud/cloud-model-timing-tab",9
6204,"post-hook-when-a-model-fails","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",10
6239,"dbt-slim-ci-with-gitlab-ci-runner","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I'm attempting to implement a Slim CI configuration on our deployment of dbt Core, and I'm running into an error where the image I'm using isn't recognizing the `manifest.json` file in the root directory of my dbt project, even after I've copied it from my source.

## What I've already tried 
I've been able to successfully copy a `manifest.json` file from an S3 bucket during our CI run, but when I attempt to run dbt with state selectors,  I'm receiving the following error:

```
Runtime Error
  Got a state selector method, but no comparison manifest
```

## Some example code or error messages
This is the setup of my CI run. I have a current `manifest.json` file stored in the `dbt-manifests` bucket.
I've tested this out locally by just removing a column from two different models, so there should be a modified state to check.
```
$ aws s3 ls
2023-01-19 14:26:48 dbt-manifests
$ aws s3 cp s3://dbt-manifests/manifest.json ./
download: s3://dbt-manifests/manifest.json to ./manifest.json   
$ ls
Dockerfile
Makefile
README.md
analysis
data
dbt_packages
dbt_project.yml
deployment.yml
logs
macros
manifest.json
models
packages.yml
poetry.lock
profiles.yml
pyproject.toml
requirements.txt
snapshots
tests
$ dbt run --target staging --select state:modified+ state ./
15:53:38  Running with dbt=1.3.1
15:53:38  Partial parse save file not found. Starting full parse.
15:53:50  Found 502 models, 211 tests, 11 snapshots, 0 analyses, 922 macros, 9 operations, 7 seed files, 218 sources, 15 exposures, 0 metrics
15:53:51  Encountered an error:
Runtime Error
  Got a state selector method, but no comparison manifest
```",1
6239,"dbt-slim-ci-with-gitlab-ci-runner","Nevermind - I screwed up the syntax.
I forgot to add the `--` flag to my state selector.
I changed it to `--state:modified+`  and it works.
We can close this out!

<sub>Note: `@Joey Berkowitz` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674145749593389?thread_ts=1674143703.968319&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6239,"dbt-slim-ci-with-gitlab-ci-runner","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6250,"check-tags-in-a-macro","Hello

How can I, inside a macro, check which tags were used in the execution of the pipeline?

Thank you!",1
6250,"check-tags-in-a-macro","I think you should have what you need in the invocation_args_dict: <https://docs.getdbt.com/reference/dbt-jinja-functions/flags#invocation_args_dict>

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674180654259609?thread_ts=1674167640.972379&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6250,"check-tags-in-a-macro","Thank you for your answer
But I didn't found any flag on the documentation that will bring information about TAGS",3
6253,"incremental-models-not-visible-to-snowflake-roles-after-build","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

i have an issue with my snowflake roles permissions related to my incremental models...when I run `dbt build -s <incremental_model>` , snowflake roles with select permissions on the table suddenly lose visibility to the table (though I know table is there because when I switch to accountadmin I can see it). And when I run `dbt build -s <incremental_model> --full-refresh` , there is no problem - snowflake roles retain permissions. Below are the grants on the role that loses visibility:

```
grant usage on database db_analytics_dev to role wd_test;
grant usage on all schemas in database db_analytics_dev to role wd_test;
grant usage on future schemas in database db_analytics_dev to role wd_test;
grant create schema on database db_analytics_dev to role wd_test;
grant create table on all schemas in database db_analytics_dev to role wd_test;
grant create view on all schemas in database db_analytics_dev to role wd_test;
grant select on all tables in database db_analytics_dev to role wd_test;
grant select on all views in database db_analytics_dev to role wd_test;
grant select on future tables in database db_analytics_dev to role wd_test;
grant select on future views in database db_analytics_dev to role wd_test;
```

This is important because our client wants to view tables in snowflake.

I've found a work around by adding a grant statement to the dbt_project file (granting select to all tables in the schema).  But this feels like a work around...",1
6253,"incremental-models-not-visible-to-snowflake-roles-after-build","Are you attaching the permissions using post-hooks (old-behaviour) or the new grant configs? https://docs.getdbt.com/reference/resource-configs/grants

In either case, I don't think it should be removing permissions from the table though; have you had a look at the full logs from your run to see whether any `revoke` commands are being run?",2
6253,"incremental-models-not-visible-to-snowflake-roles-after-build","HI Joel, thanks for your response!  Yes! there is a revoke being run.  And it turns out it's related to the `grants` I'd added to my dbt_project.yml
![image|441x76](upload://fMQUl7rH1dFwi4NX0yxPpgAP0hs.png)
If the role is not explicitly listed here, then the permission is revoked.  Oddly that revoke does not run on a full-refresh dbt run.  Only when you perform the dbt build.  Thanks for your help!",3
6253,"incremental-models-not-visible-to-snowflake-roles-after-build","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","## The problem I'm having
Libraries listed in packages.yml are failing in cloud dbt. The libraries are located in a github private repository.

## The context of why I'm trying to do this
Retrieving model code that has been packaged into a module.

## What I've already tried 
For testing, the github username and access token are directly in the URL. dbt deps works locally but when the code runs in cloud dbt it fails.

## Some example code or error messages
This is the error message with ellipses replacing some of the pathing: 

Error checking out spec='None' for repo https://xxxxxxxxxxxxxxxxxxxxxx@github.com/....feeds/....bingads.git
remote: Repository not found.
fatal: repository 'https://github.com/...-feeds/...bingads.git/' not found",1
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","Hey @dandoyle, I understand that you can't show the actual tokens in the public message, but it makes it tricky to debug! Could you reach out to the dbt Cloud support team in-app? They will be able to have a closer look at logs etc and be able to be more helpful",2
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","Hi @dandoyle, I'm facing the exact issue with importing private packages from Github in dbt Cloud. I was wondering whether you have managed to find a solution or workout for the issue yet :pray:",3
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","This might be the cause of your issue: https://docs.getdbt.com/docs/dbt-versions/release-notes/dec-2022/private-packages-clone-git-token",4
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","[quote=""joellabes, post:4, topic:6254, full:true""]
This might be the cause of your issue: [Private packages must be cloned using access tokens provided by environment variables | dbt Developer Hub](https://docs.getdbt.com/docs/dbt-versions/release-notes/dec-2022/private-packages-clone-git-token)
[/quote]

We did that too and it failed. The debug message makes it very difficult to know if we configured it incorrectly if indeed Cloud dbt is looking for variable substitution. Well, I guess I will give it go again after a week with a fresh mind. Maybe I will not repeat a syntactical error if that is what was tripping things up.

As a heads up, we contacted support. They told us to go back to the community. I am back checking on this thread after so long because support just determined they could not support us - after more than 10 days. I am a little frustrated and disappointed. Thank you for your help.",7
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","[quote=""enikov, post:4, topic:6254, full:true""]
Hi @dandoyle, Iâ€™m facing the exact issue with importing private packages from Github in dbt Cloud. I was wondering whether you have managed to find a solution or workout for the issue yet :pray:
[/quote]

Nope. Support said to come back to the community because it likely had to do with our structure or syntax.. I attached the response so you can see for yourself. 

I am still working the problem and hopeful I can find the solution and will reply with anything I find.

Attachment:
![image|647x411](upload://8HdeI8RGfgAlVkQFO1pNd5SFtak.png)",8
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","I figured it out. The documentation is not necessarily clear about this but 2 things:

1. A personal token does not require the username.
2. Your access token env var MUST be named DBT_ENV_SECRET_GIT_CREDENTIAL. I had mine originally named DBT_ENV_SECRET_GITHUB_ACCESS_TOKEN. 

After I did these 2 things, the logic is building. 

From the documentation Joel linked, it implies the environment variable used in the URL can be an environment variable. Bad documentation? A bug?

![image|690x52](upload://sTeE7wf6CuMA2LAYMozn2gc6Tkp.png)

I recommend using an application access token but a personal token suffices. I also recommend not using 'never' expiration for a personal token.",9
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","After some testing, I determined that whatever was failing is now fixed in Cloud dbt. So the rules are not a thing and everything is working as documented.

Here are the tests I ran after the above success which used the access token env var from the page Joel linked as DBT_ENV_SECRET_GIT_CREDENTIAL:
```
- git: ""https://{{ env_var('DBT_ENV_SECRET_GIT_CREDENTIAL') }}@github.com/feeds/feed-site-googleanalytics.git""
```
1) passed: substituted in DBT_ENV_SECRET_GITHUB_ACCESS_TOKEN for DBT_ENV_SECRET_GIT_CREDENTIAL:
```
- git: ""https://{{ env_var('DBT_ENV_SECRET_GITHUB_ACCESS_TOKEN') }}@github.com/feeds/feed-site-googleanalytics.git""
```
2) passed: added back {{ env_var('DBT_GITHUB_USERNAME') }} to the form that failed for the original post to the community:
```
- git: ""https://{{ env_var('DBT_GITHUB_USERNAME') }}:{{ env_var('DBT_ENV_SECRET_GITHUB_ACCESS_TOKEN') }}@github.com/feeds/feed-site-googleanalytics.git""
```
Make sure your URL for github uses all forward slashes.

One thing I am not sure of, which may not be a thing, I could not reset the ""environment"" between tests and I don't know how Cloud dbt functions behind the scene so all the success after the first success may have had state that allowed the other forms to pass.

Good luck!",10
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","Thank you very much @dandoyle! This is extremely helpful! :slight_smile: :pray:",11
6254,"dbt-deps-fails-to-retrieve-module-from-github-when-ran-in-cloud-dbt","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",12
6256,"find-the-list-of-files-using-table-references","I have a table referenced in the source.yml file. I like to find the list of files in the project that uses the table. How can I find the dependency. Thanks in advance.",1
6256,"find-the-list-of-files-using-table-references","If you have [generated your documentation site](https://docs.getdbt.com/docs/collaborate/documentation), you can use the lineage graph viewer. Right click on the green source node and click `Refocus on node. Here's a cut-down version from our project:
![Screen Shot 2023-01-20 at 4.01.47 PM|690x376](upload://of7lJd49HgFGKgvJcfXkZ01UzM6.jpeg)
![image|690x373](upload://qsikktEMy2gxY3B3whF76h1t0sM.png)

Otherwise, you can use [`dbt ls`](https://docs.getdbt.com/reference/commands/list) on the command line: `dbt ls --select source:intercom.admins+`",2
6256,"find-the-list-of-files-using-table-references","Thank you joel. I was able see all the dependencies through lineage map. But i'm not able to run the command. It gives me an error.

![image|690x133](upload://kGY77ZEGQdEqo8ivcIwP2GjmELt.png)",3
6256,"find-the-list-of-files-using-table-references","`dbt ls` isn't supported by the rpc server that powers dbt cloud unfortunately, so that only works on local installations of dbt",4
6256,"find-the-list-of-files-using-table-references","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
6261,"how-can-i-disable-debug-logs","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

After upgrading from 1.3.0 to 1.3.2, I'm getting debug logs all of a sudden and I don't know why.

```
$ DBT_DEBUG=false dbt build --select models/staging/yobify/+
INFO:configured_file:

============================== 2023-01-20 08:57:13.991556 | 7f36c610-b7a7-4282-ab93-80cfd435a872 ==============================
08:57:13.991591 [info ] [MainThread]: Running with dbt=1.3.2
08:57:13  Running with dbt=1.3.2
INFO:configured_std_out:08:57:13  Running with dbt=1.3.2
DEBUG:configured_file:08:57:13.992240 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/workspaces/elt/transform', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'select': ['models/staging/yobify/+'], 'which': 'build', 'rpc_method': 'build'}
DEBUG:configured_file:08:57:13.992439 [debug] [MainThread]: Tracking: do not track
DEBUG:configured_file:08:57:14.241083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
DEBUG:configured_file:08:57:14.241355 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
INFO:configured_file:08:57:14.405057 [info ] [MainThread]: Found 148 models, 304 tests, 0 snapshots, 0 analyses, 670 macros, 0 operations, 2 seed files, 68 sources, 0 exposures, 3 metrics
08:57:14  Found 148 models, 304 tests, 0 snapshots, 0 analyses, 670 macros, 0 operations, 2 seed files, 68 sources, 0 exposures, 3 metrics
INFO:configured_std_out:08:57:14  Found 148 models, 304 tests, 0 snapshots, 0 analyses, 670 macros, 0 operations, 2 seed files, 68 sources, 0 exposures, 3 metrics
INFO:configured_file:08:57:14.450782 [info ] [MainThread]: 
08:57:14  
INFO:configured_std_out:08:57:14  
```

## The context of why I'm trying to do this

## What I've already tried 

I've tried running with `DBT_DEBUG=false` as  above but there is no change.

I have searched and searched but have not found any ENV var set or anything that could explain this.

Any suggestions on what to try next??",1
6261,"how-can-i-disable-debug-logs","I nailed it down to something weird in my vs code devcontaner. I rebuilt it and the problem went away.",2
6261,"how-can-i-disable-debug-logs","Thanks for closing the loop!",3
6261,"how-can-i-disable-debug-logs","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6263,"what-are-your-top-tips-on-testing-cost-efficiently-in-production-on-big-query","Hey,

We've got loads of tests on all our models, but running all the tests every time we do a production run has started creating costs. We're in BigQuery. We were considering changing some of our staging models from tables to views, but this would incur even more cost if we're testing several columns in the view.

Perhaps we have too many tests, but they are very useful when developing to check we've not messed something up. (We dev on limited datasets for costs).

Are there nice solutions for setting tests to only run in certain environments? (I know we could tag all the tests so that they are excluded in production - is that the best way?)

Any other tips on how others have covered this?

Thanks",1
6272,"how-to-coonect-to-db2-database","Hi All,
i am trying to use dbt to connect to IBM db2 database. but can not find any useful information online. wondering if anyone has done this before, thanks in advance.

Thanks,
Co",1
6272,"how-to-coonect-to-db2-database","<https://docs.getdbt.com/reference/warehouse-setups/ibmdb2-setup>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674231904460469?thread_ts=1674231372.847809&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6272,"how-to-coonect-to-db2-database","Thanks so much. 
Thanks,
Co",3
6272,"how-to-coonect-to-db2-database","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6273,"custom-materialization-run-ddl-on-snowflake-error","Hi there,

I wanted to run DDL statements on snowflake and so wanted to use the custom materialization.
https://docs.getdbt.com/guides/advanced/creating-new-materializations

I tried creating a macro (persistent_table) similar to this one below

[dbt_hacks/persistent_table_materialization.sql at master Â· venkatra/dbt_hacks Â· GitHub](https://github.com/venkatra/dbt_hacks/blob/master/plugins/snowflake/dbt/include/snowflake/macros/materializations/persistent_table_materialization.sql)

and run the below model
```
{{ config(materialized=â€™persistent_tableâ€™ ,retain_previous_version_flg=false ,migrate_data_over_flg=true )}}
CREATE OR REPLACE TABLE â€œ{{ database }}â€.â€{{ schema }}â€.â€CONTACTâ€ (
FIRST_NAME VARCHAR(100),
LAST_NAME VARCHAR(100),
EMAIL VARCHAR(100),
STREETADDRESS VARCHAR(100),
CITY VARCHAR(100)
);
```
I get an error , can you please let me know what i am missing? Thank you in advance!!

No materialization 'persistent_table' was found for adapter snowflake! (searched types 'default' and 'snowflake')


Thank you!!

Sara",1
6273,"custom-materialization-run-ddl-on-snowflake-error","I was able to get this to work, it was just missing more macro files. The error was due to the macro files were not having .sql extension (my bad). I was able to create DDL defined in the model in snowflake.",2
6273,"custom-materialization-run-ddl-on-snowflake-error","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6276,"export-dbt-docs","Is there is a way to export DBT docs from the cloud website and share it across the teams?",1
6276,"export-dbt-docs","You can't ""export"" the files from dbt Cloud, but it's easy to rebuild them with `dbt docs generate`.

The docs site is a static SPA, so it's pretty easy to host anywhere -- you just copy the artifacts from `dbt docs generate` into S3 or another hosting provider. Official docs are [here](https://docs.getdbt.com/docs/collaborate/documentation#deploying-the-documentation-site).

I've used GitHub pages for this, since all you have to do is copy the artifacts into a subdirectory in your project, and GitHub will automatically host them. See [this package](https://github.com/pvcy/dbt-privacy) for an example. I have `make` command to generate the docs and move them from the `/integration-tests/target` directory to the `/docs` directory. Then you can configure GitHub Pages in your repo settings to serve the `/docs` directory. Here's the config:

![image|632x500](upload://nzMzKK50shvbixPX8CU1y1eODym.png)

If you need auth, you can use Netlify: [Publishing dbt Docs to Netlify - Show and Tell - dbt Community Forum (getdbt.com)](https://discourse.getdbt.com/t/publishing-dbt-docs-to-netlify/121)",2
6276,"export-dbt-docs","[quote=""rjames, post:1, topic:6276""]
share it across the teams
[/quote]

You can also create read-only viewer accounts for your colleagues to access",3
6276,"export-dbt-docs","But there's a limited number of read-only users, right ?  So if you run out of them eventually?

<sub>Note: `@Richard Rivera` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674431174819409?thread_ts=1674241438.669869&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",4
6276,"export-dbt-docs","It's possible to buy more if you need extras",5
6295,"deploying-just-one-dbt-model-using-gitlab-ci-cd-to-snowflake","

So, I have a dbt project, which contains a number of models in the models folder, divided into mart, intermediate, and staging subfolders. I use GitLab CI/CD to deploy these models to Snowflake.

Now, I'm currently testing these models, and would like to deploy them one by one.

Is it possible to somehow set up GitLab CI/CD (or maybe the dbt project) to just run/deploy one model at a time?",1
6295,"deploying-just-one-dbt-model-using-gitlab-ci-cd-to-snowflake","GitLab CI/CD is not a DBT orchestrator. I wonder what you want to achieve by running each model as a separate stage.

For CI/CD pipeline, you need a testing environment in Snowflake.  Then you run the entire CI/CD pipeline in that environment. After that, you check the output of CI/CD.

In addition, you could use the `dbt_artifacts` package or save DBT `run_results.json` file as an artifact to check and get more information about each model in the testing environment.",2
6299,"set-the-file-name-of-dbt-log-file-as-invocation-id-log","Hi All,

I am trying to set the log file's name as {{invocation_id}}.log. I came across the variable name {{invocation_id}} and ""log-path"" config. But log-path sets the directory name. Is it possible to set the log file name as {{invocation_id}}.log?! It would be good to collect the logs for a particular day under a directory, like given below.

-dbt_logs -> date in YYYYMMDD format -> {{invocation_id}}.log

**Example:**
- dbt_logs -> 20230123 -> 0baeae28-0b43-4b58-8de2-66acdd302efe.log",1
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","## The problem I'm having

I want to use different databases in my model creation. I want my model to be created in my dev database but use the data from production environnement. I can produce this behaviour (i'm on bigquery) but i fail at automatizing it.
To do that, i try to leverage the {{this}} and {{selected_resources}} jinja macro, but it fail because one is empty at compilation and the other at execution.",1
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Hi @ludovic.gayet 

What do you want to achieve building the model in the dev environment using the production model?

If you want to test your incremental model, you can build it from scratch in dev and rerun the same model, testing its incremental behavior.
 
It could be handy to limit dev data (https://docs.getdbt.com/guides/legacy/best-practices#limit-the-data-processed-when-in-development)

On the other hand, if you need to create up-stream models in dev, you could avoid it using the `defer` flag (https://docs.getdbt.com/reference/node-selection/defer). Missing up-stream models will be referred from `prod` env.",2
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Hum i don't think this is what i want, i will try to re-explain. @wefo 

I want all my selected models to run on some database and all the other models referenced in my selected model to run on another database. For example 

Model A : 
Select * FROM {{ref('B')}}

dbt run --select model_A

Would give : 
CREATE TABLE database1.table_A
AS
SELECT * FROM database2.table_B

And database1 and database2 are 2 different GCP project",3
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","In this case, you should specify a database in the configuration. You should point out that `modelA` should be created in project1 and `modelB` in project2. DBT will handle everything else.

https://discourse.getdbt.com/t/can-we-use-multiple-bigquery-datasets-or-projects-in-the-same-dbt-project/769",4
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","I think it's a bit more complicated than that. @wefo 

I want some logic for which database to choose based on the fact that a model is selected in the ""dbt run"".
So if i do : 
dbt run modelA => modelA is created in project1.
CREATE TABLE database1.table_A
AS
SELECT * FROM database2.table_B

dbt run modelB => modelB is created in project1.
CREATE TABLE database1.table_B
AS
SELECT * FROM database2.table_A

As we can see the ""database"" in configuration is not fixed, it depends on if the model is part of the {{selected_models}}",5
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","I'm really confused now :smiley: 

Based on your initial post, you want to run some models in the development environment but avoid recreating required models and instead use production data.

In this case, use the `defer` flag (https://docs.getdbt.com/reference/node-selection/defer) and the `state` flag pointing to the production `manifest.json`.

Creating `model_A` dbt will check if `model_B` exists in the development environment and refer to it, if not then dbt will take the `model_B` from production.

P.S. `model_A` and `model_B` cannot refer to each other because you will get a loop in your DAG and the dbt run will fail.",6
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Lol i can see that because it's still not what i want. I will give the code that doesn't work properly, maybe it will work clarify:
this is my model configuration: 
{{config(
    materialized = 'table',
    schema='mart',
    database=get_database(this)
)}}

and this is my macro : 
{% macro get_mart_database(model) %}

    {{print(this)}}
    {{print(selected_resources)}}
    {%- if this.identifier in selected_resources -%}
  
        {%- do log(this.identifier ~ "" is included based on the current selection"", info=true) -%}
        databaseA
    {%- else -%}

        {%- do log(this.identifier ~ "" is not included based on the current selection"", info=true) -%}
        databaseB
    {%- endif -%}
    
{% endmacro %}

This code doesn't work because one is empty at compilation and the other at execution.
Any idea how i can make it work? @wefo",7
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Ok, now I see what kind of technical problem you are facing. Could you also provide more context why you are doing it and what result you want to achieve?",8
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Nice! @wefo 

Why i want to do it?
Some team members need to create dataset, to do so we allow them to contribute to our dbt project.
They perform all of their test in dev and once their happy with their model they open a Pull Request to our repo. They have *read_only* right on my production environment and *create* right on my development environment.
So they build model in development but the data consistency in dev is not always perfect. And as they build complex model, for an easier feedback loop we allow them to build dev table based on production data. 
That's why i need a way to have my model created in dev but all the {{ref}} inside my model to refer to my production env. I thought the way of doing it was the macro i wrote but as you can see it's not working.",9
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","I don't want to write an entire article about dev and prod, but all my post and hits were guiding towards the solution. Let's go step by step because I have implemented it.

Here is an example of lineage 
[![](upload://qtfCqjHlGSmoWOlZRBFJKpyCKjV.png)](https://mermaid.live/edit#pako:eNpVT8sKwjAQ_JUy51jStKk1B8HHUS96k4AEk2qhaUpNQS39d-PjoHOand2ZZQacnDYQOHeqvUSbnWyiABvE-riIJpP5ly9_-AoE1nRWVTo4h5dFwl-MNRIiUG1K1ddeQjZjOFW9d_t7c4LwXW8I-lYrb9aVCj8tRKnqa1Bb1Ryc-5shBtwgsiKPecqmxYxmKaMsIbhDJDSJKecFyyjPizxh6UjweCfQmNMPwoKmLGMERlfeddtP2Xfn8Ql6pErk)

1. You should have a compiled `manifest.json` file. You could place it in an accessible place for your developers.
2. You give BigQuery DataEditor and Bigquery JobUser permissions for your developers in a development project and BigQuery DataViewer permission in a production project.
3. The developers have to set up their `profiles.yml` file configured. `project` key should refer to a development project, `dataset` key to the developer dataset e.g., `dev_alice` (https://docs.getdbt.com/docs/collaborate/environments#how-do-i-maintain-different-environments-with-dbt)
4. Developer writes or modifies a model and runs the following command:
`dbt run -s model_B --target dev --defer --state folder_path_to_production_manifest`
>Read this article carefully and what the difference gives flag `defer` and `state` (https://docs.getdbt.com/reference/node-selection/defer)

Simply put, it will create `model_B` in the development project under dataset `dev_alice` using data from `model_A` in the production project.

Furthermore if developer writes another command:
`dbt run -s model_C --target dev --defer --state folder_path_to_production_manifest`
Given that `model_B` already exists in the `dev_alice` dataset, it will refer to this modified model in dev env.

This workflow allows us to go further through the pipeline and implement required code changes without modifying production data and testing if the changes work with other models.

6. When the development part is over, the developer pushes code and creates a pull request.

### Summary

The main point is that in most cases, you should handle the environment using the `target` flag and define the target project and dataset in the `project.yml` file.

Regarding production data, you should use flags `defer` and `state` to dynamically change referred models to production models in case they do not exist in the development dataset.

NOTES:
- you should constantly update compiled `manifest.json` file for the developers to point to the newest version.
- each developer defines their development dataset in the `project.yml` file. It is possible to write a more advanced materialization macro if there is a need to change the dataset based on a Jira task, for example (https://docs.getdbt.com/docs/build/custom-schemas#why-does-dbt-concatenate-the-custom-schema-to-the-target-schema)
- One more useful article (https://getindata.com/blog/up-running-data-pipeline-BigQuery-dbt/)",10
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Hi @wefo ,

Thanks for the time you took to give a well structured answer!
I have been working with `--defer` and `--state` previously and the reason why i think this is **not** a solution in my case is because i don't want this behaviour : 
`Given that model_B already exists in the dev_alice dataset, it will refer to this modified model in dev env.`

Let's say i want to build `model_C` in dev. I want that whatever if `model_A` and `model_B` exists in my current environment (dev), i will still be using production data. (because we don't know if `model_A` have been build with production data or dev data or if the data are up to date in dev).
And if i want to build `model_B` then  `model_A` will be pointing to prod env. So there have to be a macro deciding which env to take based on the fact that i'm part of the {{selected_resources}} no?

To renforce the fact that the `--defer`flag is not my solution. This mention in the `--defer` documentation is not ok with what i want to produce =>

> When the `--defer` flag is provided, dbt will resolve `ref` calls differently depending on two criteria:
> 1. Is the referenced node included in the model selection criteria of the current run?
> 2. Does the reference node exist as a database object in the current environment?
> If the answer to both is **no**â€”a node is not included *and* it does not exist as a database object in the > > current environmentâ€”references to it will use the other namespace instead, provided by the state manifest.

Do you see why i think the `--defer` flag is not a solution in my particular case?",11
6300,"using-macro-this-and-selected-resources-fail-because-one-is-empty-at-compilation-and-the-other-at-execution","Yes, I get your situation, but it's very strange anyway.

Have you thought about misalignment? If I add a column to `model_B` and test it in dev, after that, I want to adjust `model_C`, including this column. I have to take into account that the column exists, and I cannot take production data. I should use `model_B,` which I already created. This workflow ensures that there will be no discrepancies between related models when I push all changes to production.

On the other hand,  if you work on entirely different tasks and want to make independent adjustments to `model_B` and `model_C`. And as you mention, in this case `model_B` should take `model_A` from prod and `model_C` from `model_B` in prod.

I have solved this using a custom schema name, and my developers work not on a dedicated dataset (which we could set in the `profiles.yml` file), but they pass an additional argument (Jira task ID) to `dbt run` command. Then we have a defined custom schema generator (https://docs.getdbt.com/docs/build/custom-schemas#how-does-dbt-generate-a-models-schema-name) which takes that argument, e.g.

`dbt run --select model_B --target dev --vars 'task: JIRA_ID'`
this compiles to
`dev_project.JIRA_ID.model_B`

Obviously, you should remember not to create `model_A` in your dev so `defer` could do the job, otherwise it will not work.

What I like about it is that we could check not only the code during pull request, but also check how the data looks in dev env. Furthermore, the developer could work on different tasks simultaneously without messing with different code changes in the same dataset. After the pull request passes all tests, we delete the associated dataset.",12
6303,"incremental-merge-on-postgres","
## The problem I'm having
I have a table core.table with historical data that is to be updated from a table stage.table.
I want to merge the new data into the historical table. So it should update existing records or add new ones.
Postgres only seems to support delete+insert and append. Since I also want to update existing records, I need to use delete+insert. (Also, I do not understand why merge is not supported, since it is an available command for postgres??)
However, the problem is that is seems to simply delete ALL records in the historical table, instead of just the those that are to be updated...

I am not doing a --full-refresh every time...

## The context of why I'm trying to do this
Postgres 15
dbt 1.3.1

## Some example code or error messages
``` 
{{
    config(
        materialized='incremental',
        unique_key=""'GUID'"", 
        on_schema_change='fail'
    )
}}

SELECT * FROM {{ source('stage', 'table') }}
```",1
6303,"incremental-merge-on-postgres","To your minor question: `merge` command is new to Postgres as of [v15.0](https://www.postgresql.org/docs/release/15.0/) (October 2022). I don't see an enhancement request to add `merge` incremental strategy to Postgres in the [open dbt enhancement requests](https://github.com/dbt-labs/dbt-core/issues), but that would be an excellent submission.

To your major question: do you see the ""delete all records"" behavior each time you run your model?

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674480587509899?thread_ts=1674480053.735299&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6303,"incremental-merge-on-postgres","Hi a_slack_user!
thank you for your fast answer. Yes, I tested it by staging different subsets of data and always only this subset ends up in the core table. So it basically behaves like a regular table materialization.",3
6308,"best-practice-for-dev-environment-in-a-project-writing-to-multiple-databases","I'm following [How do I maintain different environments with dbt?](https://docs.getdbt.com/docs/collaborate/environments#how-do-i-maintain-different-environments-with-dbt)

In our Snowflake instance, we use dbt to write to multiple databases.

Let's say I have:
ANALYTICS_DB (production)
- marketing
- finance

ANALYTICS_DEV_DB (dbt development)
- marketing
- finance

PARTNER_FACING_DB (production)
- customers

What is the best way to set up per-engineer schemas to accommodate for writing to more than one database? I have modified generate_schema_name() to give me per-engineer schemas in development, such as john__marketing and bruce__marketing, so that each individual can develop in an isolated environment. This works just fine for the default database specified in profiles.yml

But how do I apply this to multiple databases? Some of the code in our project will write to PARTNER_FACING_DB. If I let my generate_schema_name() macro do it's thing there, then it'll try to create bruce__customers in the customer-facing DB, which I don't want. Is it best to:
1. do that (and just hide those schemas from everyone except the engineers using privileges)
2. have a PARTNER_FACING_DEV_DB (if so, how would I do this? with something like `database: ""{{ 'PARTNER_FACING_DB' if target.name == 'prod' else 'PARTNER_FACING_DEV_DB' }}""` in dbt_project.yml ?)
3. configure something in generate_database_name() (if so, I'm not sure what)
4. do something like `database: ""{{ 'PARTNER_FACING_DB' if target.name == 'prod' else 'ANALYTICS_DEV_DB' }}""` in dbt_project.yml + prepend the schema name with `PARTNER_FACING_DB__`  using generate_schema_name()
5. Other

I apologize if this is a stupid question or is answered elsewhere! I searched around but didn't come up with anything.",1
6316,"how-to-configure-environment-variables-for-dbtoperator","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
Can't run dbt model in airflow due to missing environment variables.


## The context of why I'm trying to do this
Setup of environment variables in airfow dag using dbtoperator(https://github.com/gocardless/airflow-dbt)

## What I've already tried 
1) using env in the default _args

## Some example code or error messages
```
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow_dbt.operators.dbt_operator import DbtRunOperator

default_args = {
'dir': '/opt/airflow/dbt',
'profiles_dir': '/opt/airflow/dbt',
'owner': 'airflow',
}

with DAG(dag_id='example', default_args=default_args, schedule_interval=None) as dag:


    dbt_run= DbtRunOperator(
    task_id='dbt_run',
    select='/opt/airflow/dbt/models/core/example.sql'
    )
    
    dbt_run

```",1
6316,"how-to-configure-environment-variables-for-dbtoperator","Hey @III 

Based on airflow-dbt documentation, operators accept multiple arguments, including `target`.

Have you tried this:

```
    dbt_run= DbtRunOperator(
        task_id='dbt_run',
        select='/opt/airflow/dbt/models/core/example.sql',
        target=[YOUR_TARGET_NAME]
    )
```",2
6316,"how-to-configure-environment-variables-for-dbtoperator","Thanks, this can now be marked as resolved.",3
6316,"how-to-configure-environment-variables-for-dbtoperator","Hey, I would not recommend to Use Operators serviced by gocardless as this repo is not managed upto date. Most of the latest dbt cli flags are not supported 

Regards
Minhaj",4
6316,"how-to-configure-environment-variables-for-dbtoperator","[quote=""minhajpasha, post:4, topic:6316""]
Hey, I would not recommend to Use Operators serviced by gocardless as this repo is not managed upto date. Most of the latest dbt cli flags are not supported
[/quote]

Thanks Minhaj, can you please recommend an alternative for dbt_core?",5
6316,"how-to-configure-environment-variables-for-dbtoperator","Hey, 
Easy option: Run the dbt commands from Bash Operator to start with.
-If you are on airflow composer then would recommend running dbt over kubernatespodoperator
-not yet ready for production workloads, but you may try this option too.
https://www.linkedin.com/posts/marclamberti_dataengineering-dataengineer-airflow-activity-7023650783468355585-vCPZ?utm_source=share&utm_medium=member_android",6
6316,"how-to-configure-environment-variables-for-dbtoperator","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
6321,"leveraging-invocation-args-dict-for-a-query-comment","## The problem I'm having
As per [this recommendation](https://docs.getdbt.com/reference/project-configs/query-comment#advanced-use-a-macro-to-generate-a-comment), I am building a macro to use as a custom `query_comment`. I can't seem to leverage the invocation_args_dict in this macro the way I do in other macros. The [compilation context](https://docs.getdbt.com/reference/project-configs/query-comment#compilation-context) does not include invocation_args_dict in its list of accessible variables.

My questions are:

1. Is there a way to include args in the query_comment? Not just command line args, but all args, the way `invocation_args_dict` captures them.
2. Why do query_comment's not allow you to leverage invocation_args_dict at this time?

## The context of why I'm trying to do this
We want better visibility in our Redshift query logs - when we view query text right now, we don't know what job a given model is a part of, which makes QAing a longer process than it should be.

## What I've already tried 
I've tried using `node.invocation_args` instead, but this doesn't supply me the args I need // any args for most invocations.

## Some example code or error messages
This is the macro that I have built that is failing due to invocation_args_dict.
```
{% macro query_comment(node) %}
    {%- set comment_dict = {} -%}
    {%- do comment_dict.update(
        app='dbt',
        dbt_version=dbt_version,
        profile_name=target.get('profile_name'),
        target_name=target.get('target_name'),
        args=invocation_args_dict
    ) -%}
    {%- if node is not none -%}
      {%- do comment_dict.update(
        file=node.original_file_path,
        node_id=node.unique_id,
        package_name=node.package_name,
        relation={
            ""database"": node.database,
            ""schema"": node.schema,
            ""identifier"": node.identifier
        }
      ) -%}
    {% else %}
      {%- do comment_dict.update(node_id='internal') -%}
    {%- endif -%}
    {% do return(tojson(comment_dict)) %}
{% endmacro %}
```",1
6324,"how-to-run-jobs-in-the-background-with-on-ci-cd","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I am new to Dbt and working with our data team. We have built our models and we are thinking about how to deploy them to our dev and production warehouse which is running on Snowflake.

We are currently using corn jobs with GitHub action to run the command and update our models.
The issue we are having is when we run a simple `dbt build` command it is taking around 11 minutes to complete.

I am looking for optimal ways of doing this. I don't want the command to hang on GitHub action while the queries are being executed on Snowflake. We are being billed for this on Github.

##  What I am thinking about as a solution

Is there a way to run the command `dbt build` and not wait for the query to complete on Snowflake to finish the job? Then on the data warehouse use a sort of hook that will send a message let's say in a slack channel if there was an error on the queries we are running.

Did someone face a similar problem in his job and what approach did they take to solve it?

I am trying to avoid using debt cloud to run the query because it does not offer the flexibility and customization we want.",1
6324,"how-to-run-jobs-in-the-background-with-on-ci-cd","Couldn't you self host the runner? <https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners>

<sub>Note: `@Vince Faller` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674586778578789?thread_ts=1674582891.582389&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6330,"rename-an-existing-table","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I have an existing model materialized as a table on Big Query. The table name is `daily-events_DATE`

I would like to rename this table to `daily_events_DATE` without losing any of the data in the table. (Replace the hyphen with underscore)

Is this possible?

## The context of why I'm trying to do this
Big Query's UI doesn't display date-sharded tables correctly if there's a `-` (hyphen) in the table name and only supports `_` (underscore)

## What I've already tried 
Google searching but I'm quite new to DBT and didn't find any straightforward answers to the question. https://github.com/dbt-labs/dbt-core/issues/68 seems to suggest renaming will drop the table and all old data will be lost.",1
6330,"rename-an-existing-table","Generally the process you'll need is:

1. Rename the model in dbt
2. Manually rename the table in BQ

<sub>Note: `@Josh Devlin (he)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674615961695599?thread_ts=1674615855.620139&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6330,"rename-an-existing-table","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6333,"using-variables-in-dbt-cloud","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

Hey,
I'm working on orchestration of our dbt-cloud models using airflow. I'd like to make an incremental models updated for specific dates. The only way I've found to make it work is to provide a date as a variable. That works well for local dbt setup, but I don't see a way to make it work in dbt cloud. I don't see how I can supply a variable for a dbt run in the cloud.
The only way I'm currently see that may work is to create new model for each run and hardcode variable in `execute_steps` parameter. But that is an overkill and I don't like that solution.
Do you know another workaround?
Thanks",1
6336,"dbt-is-incorrectly-altering-my-table-structure","dbt is changing the definition of my database tables while this is not required. I can see it is trying to execute an ALTER statement to change the type of a column from VARCHAR(255) to VARCHAR(500), although the maximum length of the source column is 98 characters so this is not even required.

I can see why dbt is doing this, because it creates a temporary view based on the model definition and uses this view to insert the data into a temporary table. This table is created with a `SELECT * INTO` statement, so it leaves the table definition to Azure SQL Server. Then it compares the table definition of this new temporary table to the existing table in the data warehouse (manually created with a DDL) and detects a difference, which dbt thinks it needs to resolve.

I would like to avoid this all together and tell dbt to leave the structure of existing tables alone. The tables have been created with a custom DLL after careful consideration and I know for sure that all source data will fit into these tables. My question is how do I tell dbt to not change the table definitions but just move the data from A to B?

NB: All model materializations are set to ""incremental"", so dbt is not dropping and recreating the entire table every time a model is run.

I'm running dbt version 1.3.2 and connection to Azure SQL Server",1
6342,"running-macros-at-on-run-end","I'd like to access the full name of a failed test at the target database on on-run-end. I figured I'd use the 'model' part of a result object to do this, but it's a string like this:

`'model': ""{{ get_where_subquery(ref('rawinvoice_dim_analytics')) }}""``

and when I try to for example log it, it doesn't resolve to its target name since macros aren't resolved at runtime (?). Is there a way to resolve it to its full target name?

I want to do this because I'd like to add Snowflake constraints even if the test failed (wouldn't use the rely keyword). The reason I want to add them is so our customer can use them to create an ER diagram with a third party tool",1
6346,"testing-source-data-with-user-defined-tests-macros","Hi Everyone,

## The problem I'm having
I'm trying to develop some tests/macros that can be specified in the project models .yml file. But dbt either returns a warning that there's nothing to do or that the selection criteria does not match any nodes.

## The context of why I'm trying to do this
Our source data is the culprit of most of our data issues. Identifying issues there is ideal. Although, our stage tables are just unions of the various source tables; testing their will suffice. But for my knowledge I'd like to know how to test on sources. 

## What I've already tried
I started with the basic built-in tests like `unique` on the models, then applied them to the sources. Running `dbt test` does not work, but that might be expected behavior if the sources are not in the DAG (ie called by `source()`)? Some of the sources are in the DAG while others are not. 

`dbt test --select source:<source>` results in the same issue, and actually `source:*` throws a zsh error. 

So then I tried adding the sources as a model and attempted to test them as such. Same issue as above.

Obviously this does not get to the heart of the issue, that I'm trying to develop my own functions, but using a built-in test is a good starting point. One question related to the real issue: do all dbt tests consider having any rows in the final query a failure? Is there a way to specify a failure using a jinja variable, maybe a boolean?

## Some example code or error messages
Testing as a source:
```
sources:
  - name: nm_trace
    description: Trace tables in snowflake
    tables:
      - name: bmsi_customers_raw
        columns:
          - name: customerid
            tests:
              - not_null
```
```
dbt test -s source:bmsi_customers_raw
```
```
The selection criterion 'source:bmsi_customers_raw' does not match any nodes
                                                                             
[WARNING]: Nothing to do. Try checking your model configs and model specific
```

Running it as a model:
```
models:
  - name: bmsi_customers_raw
    columns:
      - name: customerid
        tests:
          - not_null
```
```
dbt test -s bmsi_customers_raw
```
```
[WARNING]: Did not find matching node for patch with name 'bmsi_customers_ra
w' in the 'models' section of file 'models/bt_backend_data.yml'
[WARNING]: Nothing to do. Try checking your model configs and model specific
ation args
```",1
6350,"where-to-save-models-that-are-used-one-time-efforts-ad-hoc-analysis","## The problem I'm having

Looking for suggestions on where to push and save dbt models used for one-time, ad hoc analysis.

## The context of why I'm trying to do this

Using dbt Cloud and Snowflake.

I had to perform a backfill on a database table. To validate, I created a model that executes against a backup of the table before the backfill. I created a second model that executes against the database table after the backfill.

When done, I have two tables that I was able to join and look for errors in the backfill. I used my dbt development schema for my user for the validation.

Now, I have these two dbt models, and related changes to the relevant `.yml` files in a feature branch. 

These models will not run in production. It was a one-time effort.

I don't want to lose the work. Yet, I don't want to put them in the `models` directory with production models. 

## What I've already tried 

I thought about putting them in the `analysis` folder.  There are changes to the `.yml` files in my branch. It'd be ok if I lose the content in the `.yml` files. I could replace the entries in the `.yml` files if needed. There is the issue of having non-production models and artifacts in a repo where the production models are saved.

I also also looked at committing to a repo that is used for ad hoc work. There is not file history in the repo. I'd probably copy-n-paste the models in `.sql` files in the repo used for ad hoc work, and commit them.

I also thought about leaving the models and related changes in a branch in the production repo, unmerged. Let it sit there in the repo until it's obsolete or until it reaches the stale date in a README file in the branch.

## Screen Capture  'analysis' Directory
![image|468x500](upload://7xkBfPwcI6hflLB1pcBJzIAXapS.png)",1
6350,"where-to-save-models-that-are-used-one-time-efforts-ad-hoc-analysis","Check out analyses!

<https://docs.getdbt.com/docs/build/analyses|https://docs.getdbt.com/docs/build/analyses>

<sub>Note: `@dave` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1674707964791859?thread_ts=1674695926.643589&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6357,"configuration-paths-exist-in-your-dbt-project-yml-file-which-do-not-apply-to-any-resources-there-are-1-unused-configuration-paths-models-dbt-project-example","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
The problem I'm having is after the profiles.yml All checks passed! (the connection successful)
when i run dbt get this message
Configuration paths exist in your dbt_project.yml file which do not apply to any resources. There are 1 unused configuration paths:- models.dbt_project.example

## The context of why I'm trying to do this
adapter dbt-synapse
dbt version: 1.4.1
python version: 3.9.6
python path: c:\dbt-project\python.exe
os info: Windows-10-10.0.19042-SP0
Using profiles.yml file at C:\Users\user1\.dbt\profiles.yml
Using dbt_project.yml file at C:\DBT-Project\Scripts\dbt_project.yml

also here is the profiles.yml code
""
dbt_project:
  target: dev
  outputs:
    dev:
      type: synapse  #synapse  #type: Azuresynapse
      driver: 'ODBC Driver 17 for SQL Server' # (The ODBC Driver installed on your system)
      server: ######## # (Dedicated SQL endpoint of your workspace here)
      database: ##########
      port: 1433
      schema: dev_dbt_dbo
      user: ##########
      password: #######
""


## What I've already tried 

## Some example code or error messages

Models\Example
```
my_first_dbt_model.sql
""{{ config(materialized='table') }}

with source_data as (

    select 1 as id
    union all
    select null as id

)

dbt_project.yml
""
 Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: 'dbt_project'
version: '1.0.0'
config-version: 2

# This setting configures which ""profile"" dbt uses for this project.
profile: 'dbt_project'

# These configurations specify where dbt should look for different types of files.
# The `model-paths` config, for example, states that models in this project can be
# found in the ""models/"" directory. You probably won't need to change these!
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  # directory which will store compiled SQL files
clean-targets:         # directories to be removed by `dbt clean`
  - ""target""
  - ""dbt_packages""


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/ directory
# as tables. These settings can be overridden in the individual model files
# using the `{{ config(...) }}` macro.
models:
  dbt_project:
    # Config indicated by + and applies to all files under models/example/
    example:
      +materialized: view
""
my_second_dbt_model.sql
""select *
from {{ ref('my_first_dbt_model') }}
where id = 1""

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
```",1
6357,"configuration-paths-exist-in-your-dbt-project-yml-file-which-do-not-apply-to-any-resources-there-are-1-unused-configuration-paths-models-dbt-project-example","Looks like the issue is in this area

--this is for dbt-synapse
--I used the default one.. there a different setup for dbt-synapse?

--#and underscores. A good package name should reflect your organization's
--#name or the intended use of these models
name: 'dbt_project'
version: '1.0.0'
config-version: 2

--#This setting configures which ""profile"" dbt uses for this project.
profile: 'dbt_project'

--#These configurations specify where dbt should look for different types of files.
--#The `model-paths` config, for example, states that models in this project can be
--#found in the ""models/"" directory. You probably won't need to change these!
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  # directory which will store compiled SQL files
clean-targets:         # directories to be removed by `dbt clean`
  - ""target""
  - ""dbt_packages""


--# Configuring models
--#Full documentation: https://docs.getdbt.com/docs/configuring-models

--# In this example config, we tell dbt to build all models in the example/ directory
--#as tables. These settings can be overridden in the individual model files
--# using the `{{ config(...) }}` macro.
models:
  dbt_project:
 --# Config indicated by + and applies to all files under models/example/
    example:
      +materialized: view",2
6363,"database-error-250001-could-not-connect-to-snowflake-backend-after-0-attempt-s","Hi,

My dbt setup used to work perfectly, but suddenly I started to get this error after `dbt debug`. I uninstalled dbt several times, installed it in different venvs, change the `profiles.yml` file in several ways (although the one I'm attaching here is the supposedly correct format, and the one that was working for me and my team), looked up every similar issue in Google... I really have no clue how to fix it. Please help!

### Error message (changing personal info):
```
(dbt-snowflake) name.surname@WX577G44TP dbt % dbt debug               
19:43:25  Running with dbt=1.3.2
dbt version: 1.3.2
python version: 3.9.16
python path: /Users/name.surname/.virtualenv/dbt-snowflake/bin/python
os info: macOS-13.0-arm64-arm-64bit
Using profiles.yml file at /Users/name.surname/.dbt/profiles.yml
Using dbt_project.yml file at /Users/name.surname/projects/saltdata-build-tool/dbt/dbt_project.yml

Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]

Required dependencies:
 - git [OK found]

Connection:
  account: rt11111.eu-west-1
  user: NAME.SURNAME@COMPANY.CO
  database: NAMES
  schema: MARTS
  warehouse: DATA_ANALYSTS
  role: NAMES
  client_session_keep_alive: False
  Connection test: [ERROR]

1 check failed:
dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  250001: Could not connect to Snowflake backend after 0 attempt(s).Aborting
```
### profiles.yml file:
```
saltdata_sf:
  target: loc
  outputs:
    loc:
      type: snowflake
      threads: 4
      account: rt11111.eu-west-1
      authenticator: 'externalbrowser'
      user: NAME.SURNAME@COMPANY.CO
      role: NAMES
      database: NAMES
      schema: MARTS
      warehouse: DATA_ANALYSTS
      query_tag: dbt
```",1
6365,"error-with-generate-surrogate-key","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I am stuck on an error. I want to generate surrogate key but keep getting the error below.

```
select {{ dbt_utils.generate_surrogate_key(
      ['date',
      'user_id',
      'type']
  ) }} as contribution_id,
....
```

What is wrong with my query?

## What I've already tried 
I originally was using `dbt_utils.surrogate_key()` but noticed that it is the older version. 

I also added this var in `dbt_project.yaml` : 
`  surrogate_key_treat_nulls_as_empty_strings: true #turn on legacy behaviour` 

But nothing worked.

## Some example code or error messages
```
Compilation Error in model dau (models/dau.sql)
  macro 'dbt_macro__default__surrogate_key' takes not more than 1 argument(s)
```",1
6371,"preserving-double-curly-braces-after-compile","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I would like to partially compile dbt model.
so, I tried to set variable values like this.

mymodel.sql
```
select * 
from {{ ref('mymodel') }}
where dt between '{{ start_date }}' and '{{ end_date }}'  
```


## What I've already tried 

```
dbt compile --vars '{""start_date"": ""2022-12-01"", ""end_date"": ""{{end_date}}"" }'
```

expected compiled result
```
select * 
from dw.mymodel
where dt between  '2022-12-01' and '{{ end_date }}'  
```

but I got

```
select * 
from dw.mymodel
where dt between '2022-12-01' and ''
```",1
6371,"preserving-double-curly-braces-after-compile","I found solution.

```
dbt compile --vars '{""start_date"": ""2022-12-01"", ""end_date"": ""{% raw %}{{end_date}}{% endraw %}"" }' 
```",2
6378,"dbt-project-level-documentation-not-loading-when-running-dbt-project-in-a-docker-container","Hi everyone :wave:

I'm running a DBT project inside a docker container. Everything works fine. But the project-level documentation is not able to load.

![image|690x232](upload://rGmteclRb5RBICuYVnfnzJ2jDZ8.png)

Here's my Dockerfile to build the image:
```
FROM python:3.9

RUN apt-get update \
    && apt-get install -y --no-install-recommends

WORKDIR app
COPY . .

RUN pip install --upgrade pip
RUN pip install -r requirements.txt --no-cache-dir
RUN dbt deps

EXPOSE 8080
ENTRYPOINT dbt build --profiles-dir profiles && \
           dbt docs generate --profiles-dir profiles && \
           dbt docs serve --profiles-dir profiles

```

And here's the docker-compose file:
```
version: ""3.7""

services:
  db:
    image: postgres:13
    restart: on-failure
    container_name: dvdrental-postgres
    environment:
      POSTGRES_USER: ""postgres""
      POSTGRES_PASSWORD: ""postgres""
      POSTGRES_DB: ""tempdb""
    volumes:
      - ./data/dvdrental:/docker-entrypoint-initdb.d
    ports:
      - ""5432:5432""
    networks:
      - dvdrental-network

  dbt-workload:
    container_name: dbt-dvdrental
    restart: on-failure
    build: .
    image: dbt-dvdrental-image
    ports:
      - ""8080:8080""
    depends_on:
      - db
    networks:
      - dvdrental-network

networks:
  dvdrental-network:
    driver: bridge
```
All the dbt tests and transformations are applied to the data successfully and docs are generated as well. Only the `project overview` is loading. All other docs related to models is loading correctly.

Is there anything I'm missing?",1
6378,"dbt-project-level-documentation-not-loading-when-running-dbt-project-in-a-docker-container","Not sure why, but I replaced `ENTRYPOINT` with `CMD` in the Dockerfile and now the Documentation is loaded successfully.

![image|690x270](upload://qH0DZZh5Mp5qX8DoRiHJJ9jTiUh.png)",2
6382,"how-to-run-dbt-using-visual-code","Hi Everyone,
plan to use visual code to develop dbt package. but having hard time install dbt. i did install the python and add python path to the environment. but when in the visual code terminal, tried to install dbt, it keep saying 
ERROR: Could not find a version that satisfies the requirement dbt==0.19.0 (from versions: none)
ERROR: No matching distribution found for dbt==0.19.0


anyone can help  me on this? thanks so much. 
![image|690x231](upload://cKta2pgkPKQiVgLdIaLLlub4vqI.png)
Thanks,
co",1
6384,"how-to-use-incremental-models-to-detect-regressions-in-historic-metrics","## Background
Imagine that you have expected outputs for a historic metric (total revenue) as described below:
| Year | Total Revenue |
|---|---|
| 2019 | 1 million |
| 2020 | 1.5 million |
| 2021 | 2 million |

How can you test that these historical metrics **do not change** in dbt?

This is an alternative solution to the snapshot-based one proposed [here](https://discourse.getdbt.com/t/build-snapshot-based-tests-to-detect-regressions-in-historic-data/1478). 

## Step 1
Let's say that you have a `fct_orders` table, which has one row for each order:
| order_id | order_date | amount |
|---|---|---|
| 1 | 2019-01-05 | 10 |
| 2 | 2019-02-06 | 50 |
| 3 | 2020-02-07 | 8 |
| ... | ... | ... |

First, you should create a model that sums the amount for each year excluding the current one (for simplicity, we're assuming you have no costs). Let's call this `fct_revenue_summary`:
```
select
        year(order_date) as year,
        sum(amount) as total_revenue
from {{ ref('fct_orders') }}
where year <> year(current_timestamp())
group by 1
```

## Step 2
Next, create an incremental model on top of `fct_revenue_summary` which captures the historical view of revenue outputs. Let's call this `fct_revenue_summary_history`:
```
{{
    config(
        materialized='incremental',
        unique_key=['year', 'total_revenue']
    )
}}

select
        year,
        total_revenue
from {{ ref('fct_revenue_summary') }}

{% if is_incremental() %}
where true
{% endif %}
```

## Step 3
Finally, create a test on `fct_revenue_summary_history` to check that each year has a single source of truth for `total_revenue`:
```
version: 2

models:
  - name: fct_revenue_summary_history
    columns:
      - name: year
        tests:
          - unique
```

When you run a `dbt build`, you will get an error if you ever output a _new_ `total_revenue` value for a historic year that differs from the original. For example, if your original `fct_revenue_summary_history` looks like this:
| year | total_revenue |
|---|---|
| 2019 | 1 million |
| 2020 | 1.5 million |
| 2021 | 2 million |

But then, you introduce a breaking change such that dbt now calculates the `total_revenue` for 2019 as 0.8 million, `fct_revenue_summary_history` will now look like this:
| year | total_revenue |
|---|---|
| 2019 | 1 million |
| 2019 | 0.8 million |
| 2020 | 1.5 million |
| 2021 | 2 million |

And the uniqueness test on the year column will fail. 

This allows you to detect regressions in historic metrics!",1
6384,"how-to-use-incremental-models-to-detect-regressions-in-historic-metrics","This is really interesting! 
Thanks for sharing",2
6390,"airflow-on-docker-how-to-update-git-version-from-2-20-to-atleast-2-25-when-creating-a-dbt-package","I  set up an airflow environment (on Docker platform) an year back. I am not in a position to upgrade to latest airflow version.

Customer is trying to create a dbt package (essentially a git repo) and link to it within dbt.

Error message: Please update your git version to pull a dbt package from a sub directory: your version is 2.20.1, >= 2.25.0 needed.

How to upgrade git version?

I can share my Dockerfile",1
6390,"airflow-on-docker-how-to-update-git-version-from-2-20-to-atleast-2-25-when-creating-a-dbt-package","following is my Docker file

FROM apache/airflow:2.2.4-python3.9 RUN pip install ""apache-airflow-providers-microsoft-mssql"" ""apache-airflow-providers-snowflake"" ""authlib"" ""Flask-OpenID""

USER root RUN apt-get update
&& apt-get install -y git libpq-dev python-dev python3-pip
&& apt-get remove python-cffi

RUN pip install --upgrade cffi RUN pip install cryptography~=3.4

USER airflow RUN pip install airflow-dbt-python RUN pip install dbt-sqlserver==1.0.0 RUN pip install dbt-snowflake==1.0.0 RUN dbt --version",2
6394,"run-query-before-models-and-use-output-within-models","### What do i want to achive?

Query existing tables to find dates (=partition) filters to be used as date (=partition) filters in dbt models. For filtering i could do joins in each and every dbt model but it turns out to execute slowly. Rather i want to find certain dates first and then substitute those dates as variables in dbt models. 

### How would i like to achive this?

I would like to 
1. run a query before any model exectuion. The query yields a table with a single row like: `date_1: 2020-01-01, date_2: 2020-03-1, date_3: 2020-09-01, date_4= 2020-12-01`. Store this output into variables (best dictionary) and then
2. select those values in a dbt model (e.g. in `where date_column = '{{ fetched_dates['date_1'] }}'` such that after compilation we get hardcoded values in the sql `where date_column = '2020-01-01'`

### What i tried:

I used `on_run_start` in `dbt_project.yml` to execute a macro. The macro uses `run_query` to execute a sql and store its single row output with multiple columns in a dictionary via `set fetched_dates = sql_result.rows[0]`. This seems to work. But when i reference this variable later in a dbt model as in `where date_column = '{{ fetched_dates['date_1'] }}'` it complains with `fetched_dates is undefined. This can happen when calling a macro that does not exist. Check for typos and /or install package dependencies with ""dbt deps""`.

Any advice on how to proceed? Do i have to execute my date fetching query outside from dbt and then pass values to dbt as project variables? Would have much rather relied on logic within dbt itself.",1
6396,"how-to-get-the-count-of-records-updated-by-the-dml-statement","## The problem I'm having
I am writing update SQL in DBT. But , I want to know how many records this UPDATE affected. Like SQL%ROWCOUNT was the solution in Oracle.

## The context of why I'm trying to do this


## What I've already tried 
Nothing yet",1
6397,"how-to-implement-autonomous-transaction","## The problem I'm having
I want to commit only the part of code which is called. Like there are 4 update statements made one after another as shown in code. But when I write commit; I want only 3rd one to commit and not 1st and 2nd.
Similar thing is achieved in Oracle using autonomous transaction.

## The context of why I'm trying to do this

## What I've already tried 

## Some example code or error messages
```
<update sql>
<update sql>
<update sql>
<update sql>

```",1
6398,"how-to-compare-variable-with-null-value","## The problem I'm having
How to compare NULL value passed in macro?

## The context of why I'm trying to do this

## What I've already tried 
Check the code below

## Some example code or error messages
```
{% macro m(p1, p2, p3) %}
{% if p3 == '' %}
    v3 = 100
{% else %}
   v3 = p3
{% endif %}
{% endmacro %}
```",1
6400,"best-way-to-used-shared-profiles-yml","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I'm brand new to testing out DBT and just working through configuration. I've read a lot of the docs and have found ways around what I'm trying to do, but would like to figure out the easiest way to do this.

We use Domino Data Labs at work and I would like people to use standard environment variables and profiles to connect to the right database and schema. 

However, when issuing the dbt run command, it doesn't like that I've put profiles.yml in the root of my project, it's looking for it in a user directory. I saw I can pass the --config-dir to the run command, but I'm trying to make it slightly more seamless for people using the software. Is it possible to set the default location somewhere so dbt run can run without the ""dbt run --profiles-dir /opt/dbt/"" option?

Similarly, can I default to the dev vs prod profile based on which branch of the github account the user is working in? So if they are running code out of the develop branch, it would know this and target the correct dev credentials?",1
6412,"how-to-raise-exception","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
In Oracle, we used to use RAISE_APPLICATION_ERROR to raise exceptions and terminate the execution of the program immediately. What could be the way here?

## The context of why I'm trying to do this
I have written a macro which has 100 lines of code; In middle of code; based on some condition, I want to raise the error and terminate the code immediately.

## What I've already tried 
I have done logging of the error  and used IF/ELSE to process. But this will not fail the program or raise the error. I checked https://docs.getdbt.com/reference/dbt-jinja-functions/exceptions ; But this show ""raise_compiler_error"" and not the exception which might occur during execution. 

## Some example code or error messages
```
```",1
6412,"how-to-raise-exception","Hi Ashish!

I would recommend consulting the docs for this sort of question.  I searched 'exception' and the first result was this: <https://docs.getdbt.com/reference/dbt-jinja-functions/exceptions>

<sub>Note: `@Josh Devlin (he)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675117111727229?thread_ts=1675074500.184939&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6412,"how-to-raise-exception","Can you provide the code that you're running which isn't raising an exception? As Josh noted, the exceptions object that you found should behave the way you're hoping. 

Or have you not tried it because it calls itself a _compiler_ error instead of a runtime error? dbt will compile your code (including running any introspective queries) before running it, which means that a compiler error is correct. 

If you provide more details about this

[quote=""ashish.agrawal.inbox, post:1, topic:6412""]
In middle of code; based on some condition, I want to raise the error and terminate the code immediately.
[/quote]

then it might be possible to help further",3
6412,"how-to-raise-exception","A post was split to a new topic: [Does dbt have exception handling for DML/DDL code?](/t/does-dbt-have-exception-handling-for-dml-ddl-code/6867)",4
6412,"how-to-raise-exception","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
6413,"how-to-pass-a-hardcoded-string-to-surrogate-key-function","I'm trying to generate a surrogate key based on the business key of the table, and the name of the source system it is coming from (this is so I can eventually merge data from two different source systems, that use similar conventions for generating ids, without unintentional collisions).  At the moment, I basically have something like this

```

with customers as
(
select  *,
		'My Sales System' as source_system
from    {{ source('datalake', 'customers')}}
)

select 	{{ dbt_utils.generate_surrogate_key(['customer_id', 'source_system']) }},
		*
from 	customers
```

For simplicity sake, I'd like to skip the CTE and be able to reference the name of the source system directly in the surrogate key function, eg;

```
select 	{{ dbt_utils.generate_surrogate_key(['customer_id', ''My Sales System'']) }},
		*
from    {{ source('datalake', 'customers')}}
```

But all my attempts to escape the quote marks seems to fail.  How do I pass a hardcode string directly to a function like this (as opposed to a column output)?",1
6413,"how-to-pass-a-hardcoded-string-to-surrogate-key-function","generate surrogate key is a macro which concat columns and add hash function on top of it
if you want to add string, you have 2 ways i could think of 
1. you have already shown above
2. create your own macro 

<sub>Note: `@mdayunus` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675077177087889?thread_ts=1675075297.757119&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6413,"how-to-pass-a-hardcoded-string-to-surrogate-key-function","You can use double quotes and single quotes to achieve this: 

```
select 	{{ dbt_utils.generate_surrogate_key(['customer_id', ""'My Sales System'""]) }},
		*
from    {{ source('datalake', 'customers')}}
```",3
6417,"install-dbt-issue","i am using python 3.11.0
when running 
pip install dbt==0.18.0 --trusted-host pypi.org --trusted-host files.pythonhosted.org pip setuptools
to install dbt, getting below error. saying ERROR: No matching distribution found for cffi<1.15,>=1.9.
questions, is python version too high for the dbt? anyone successfully install the dbt using this version? if so, what would be fix below error, if not, any suggest python version and install steps? thanks so much for the help.
![image|690x352](upload://x4mXJp7rimcMOL9ZmpOfgOmnTt3.png)
![image|690x96](upload://u0WZDU68CAOUQUJDhqWpHbxjfQt.png)
Thanks,
Co",1
6417,"install-dbt-issue","Do you need to use such an old version of dbt?
Per <https://docs.getdbt.com/docs/dbt-versions/core>:
""All dbt Core versions released prior to 1.0 and their version-specific documentation have been deprecated.""

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675089398338579?thread_ts=1675088484.042329&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6417,"install-dbt-issue","Thanks for pointing it out. the reason i put 0.18.0 because i was using it while ago. so what would be the mature dbt version? can you suggest me one?  just now try to remove version number and use default version, still get error. can you share me one version number that works?with command pip install dbt==?.?.?.
also, saw one youtube video, it is saying not, pip install dbt command will no longer be used, we will need to use below command instead, if that true? Thanks so much for the help and your valuable time. 
![image|690x312](upload://brSJk3K7djxdTBVHYK3XcTqY0lj.png)


Thanks,
cO",3
6417,"install-dbt-issue","As of 1.0, you do not install dbt-core directly: <https://docs.getdbt.com/docs/get-started/pip-install>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675092749209909?thread_ts=1675088484.042329&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",4
6417,"install-dbt-issue","Thanks for the quick reply, when i try pip install dbt==1.0.0 --trusted-host pypi.org --trusted-host files.pythonhosted.org pip setuptools.
i got below error, so any version before 1.0.0, don't need to install dbt-core, any version after that, will need to install dbt-core directly?  
![image|690x350](upload://dJMzXm8mnlkmYl2lgMmeNMbMcyK.png)
, 
also i  tried to install install dbt-core, it shows successful,but when try to run dbt run , it shows error :The term 'dbt' is not recognized as the name of a cmdlet,  i run  , pip install dbt-core, pip install dbt-snowflake ,  pip install snowflake  all successfully. any idea?
![image|690x105](upload://uZhlFLzbVlivSXEurjebIwRfbuZ.png)

Thanks,
Co",5
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","Hi everyone,

I'm getting the error 'Schema has to be provided to write_pandas when a database is provided' when attempting convert a dataframe to pandas in a python script. I'm using snowflake.

I've seen the following error related to using a custom schema name: https://github.com/dbt-labs/dbt-snowflake/issues/393 however when testing without a custom schema name the error persists.

Anyone else experienced this?

Thanks",1
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","Hey @darren.hickey, are you able to post the code you're trying to run?",2
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","Hey Joel,

Yeah the offending piece of code is the following: final_df = tickets_df.to_pandas()

Thanks",3
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","The good news is that issue you linked above has been resolved and will come out in dbt-snowflake 1.4.1. 

We've only seen this with custom schema names configured - is it possible that when you changed to testing without a custom schema, a config file didn't save or something?",4
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","+1 on this-one! Any timeline for 1.4.1 ?",5
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","I am having the same issue.
Running with dbt=1.4.1",6
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","dbt-snowflake v1.4.1 came out earlier today, have you checked that you have the patch for dbt-snowflake installed, not just dbt-core 1.4.1?",7
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","I just tested dbt-snowflake 1.4.1 and can confirm the issue still persist @joellabes . The changelog of 1.4.1 also does not address this issue, so I am not surprised.",8
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","Is there any other alternative to ""to_pandas()""",9
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","Hi Joel, we have now tried on 1.4.1 and having fully removed the custom schema name and we are still getting the error.",10
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","OK - we don't really have enough to reproduce this, can you post the _full code file_ (not just the final line of code) for the model you're trying to run as well as the logs from when you try doing a dbt run?",11
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","@troyel yes my mistake, it didn't go out in 1.4.1, but confirmed with the PM that it is actively being worked on! I don't have a version to share sorry",12
6420,"error-when-attempting-to-convert-a-dataframe-to-pandas","We're experiencing the same issue for one of our Python dbt models that uses a custom schema. Wanted to hop on this thread to be notified of when this would be resolved! (Should also mention that we are on `dbt-snowflake==1.4.1`)",13
6421,"when-try-to-connect-to-ibm-db2-database-got-issue","Hi All,
i installed dbt-core, installed pip install dbt-ibmdb2. but when try to run dbt, it shows below error:17:27:52  Encountered an error while reading profiles:  Believed i have set the dbt_project and profiles right, i have attached these fileds snapshot below as well, anyone have similar issue? please help. thanks so much
  ERROR: Runtime Error
  Credentials in profile ""db2-db"", target ""dev"" invalid: 'user' is a required propertyDefined profiles:.   

![image|690x200](upload://ciNz7hMxoaObvaqjPGU5ITNmdxX.png)


![image|566x445](upload://vOjPaO9jeekttN8sCHTZzjWJo4d.png)
![image|690x329](upload://voDWDcsvJJkeQUw3Zh1HojhMqWi.png)

Thanks,
Co",1
6421,"when-try-to-connect-to-ibm-db2-database-got-issue","Hi @xzheng, 

The error message says that `user is a required property`. I looked at the [example profiles.yml file](https://docs.getdbt.com/reference/warehouse-setups/ibmdb2-setup#connecting-to-ibm-db2-with-dbt-ibmdb2) and it references `username`, but the [sample_profiles.yml file in the repo](https://github.com/aurany/dbt-ibmdb2/blob/634aa3ea6b394669b073e2111a53efb1a1a8fa4d/dbt/include/ibmdb2/sample_profiles.yml) uses `user`, so I think the docs are incorrect here. 

Can you try replacing `username` with `user` and see what happens?",2
6424,"how-to-reference-postgresql-index-names","## The problem I'm having

For PostgreSQL configs, dbt creates indexes with names that are [a hash of its properties and the current timestamp](https://docs.getdbt.com/reference/resource-configs/postgres-configs#indexes). What's a good way for me to reference these generated index names? Is there a way to customize or set the name of the index?

## The context of why I'm trying to do this

I'd like to add a post hook to [CLUSTER](https://www.postgresql.org/docs/current/sql-cluster.html) tables. `CLUSTER` takes the name of an index to order the table rows by. But, there isn't an obvious way to figure out the name of the indexes created by dbt-postgres.

## What I've already tried

I briefly [tried to find the implementation](https://github.com/dbt-labs/dbt-core/blob/main/plugins/postgres/dbt/adapters/postgres/impl.py) for the index name in the code. But, couldn't see where it was.

## Some example code or error messages

*None*",1
6424,"how-to-reference-postgresql-index-names","Not a direct answer to your question, but I think this is where the index name is calculated: <https://github.com/dbt-labs/dbt-core/blob/main/plugins/postgres/dbt/adapters/postgres/impl.py#L39>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675106980530719?thread_ts=1675104112.577999&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6424,"how-to-reference-postgresql-index-names","Thanks, that helps. Looks like it's going to be impossible to generate a matching index name for `CLUSTER` later on since it gets the datetime on every call.",3
6424,"how-to-reference-postgresql-index-names","Perhaps you could do something clever by querying `<http://pg_catalog.pg|pg_catalog.pg>_indexes`  system catalog view? <https://www.postgresql.org/docs/current/view-pg-indexes.html>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675201883879729?thread_ts=1675104112.577999&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",4
6440,"create-custom-schema-that-is-just-table-names","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I have a pre-set database schema in teradata that cannot be changed, but I have write access to said schema. However, I'd like to group all my said objects together under a custom ""prefix"" for the table names without manually maintaining this in file names. Is there a way to do this using custom schema's or any other aliasing features that can be broad to a set of models and not individually set in each SQL file?

I tried placing a custom schema in my project.yml, but i'm just getting access errors that my account doens't have ""create database"" access. So I assume that isn't working as I expected.",1
6452,"syntax-error-in-models-on-lines-that-dont-exist","<!-- https://docs.getdbt.com/guides/legacy/getting-help 

## The problem I'm having
I'm getting syntax errors for my model files on lines that don't exist.
for model #1, last line of code is 93. message: syntax error at LINE 106
for model #2, last line of code is 58. message: syntax error at LINE 69

I'm really lost as to what caused those errors.

Please advise!

## Some example code or error messages
```
syntax error at or near "";""
21:18:24    LINE 106: ;
21:18:24              ^
21:18:24    compiled Code at target/run/ph/models/model1.sql

21:18:24    syntax error at or near "";""
21:18:24    LINE 69: ;
21:18:24             ^
21:18:24    compiled Code at target/run/ph/models/model2.sql
```",1
6453,"pull-column-comments-from-snowflake-to-model-column-descriptions","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
Rather than type all column descriptions in dbt, is there a way to generate those from Snowflake directly ? Information_Schema.Columns.Comments field has comments (definitions) for each column.

## The context of why I'm trying to do this
Generate Base Models Column Descriptions from Snowflake Information Schema

## What I've already tried 

## Some example code or error messages
```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
6453,"pull-column-comments-from-snowflake-to-model-column-descriptions","[quote=""mahermo, post:1, topic:6453""]
Rather than type all column descriptions in db
[/quote]

I did similar of this in dbt + snowflake (fivetran as my source of data to my DWH). I wrote the steps in https://discourse.getdbt.com/t/how-to-generate-tables-and-its-columns-schema-yaml-reading-from-snowflake-information-schema-columns/7135. I hope this helps. Thanks",2
6453,"pull-column-comments-from-snowflake-to-model-column-descriptions","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6463,"dbt-with-aws-athena-and-python-models","From https://docs.getdbt.com/docs/build/python-models#specific-data-platforms ,  seems only the following 3 platforms supports Python models. 
* Databricks, Snowflakes, BigQuery.

I am thinking to have DBT get source tables from Athena, doing some transform, then store the result in S3. However, I cannot find a way if I can use dbt-python models on this? Or it must involve one of above 3 platforms.

Any examples would be super helpful.

Thanks
yanan",1
6463,"dbt-with-aws-athena-and-python-models","Hi Yanan, 

At the moment, those three platforms are the only ones that support dbt Python models natively. 

However, you might be able to use [the fal-dbt adapter](https://blog.fal.ai/announcing-dbt-fal-adapter/) to run your Python workloads.",2
6463,"dbt-with-aws-athena-and-python-models","Thanks Joel for the help. I did a quick try on dbt-fal, by following the link:
1. *The profile seems file, with fal in the picture:*
```
        target: dev
        outputs:
          dev:
            type: fal
            db_profile: dev_athena
          dev_athena:
            type: athena
```
2. *I have a very simple sql and py file, the py ref to the sql:*
```
      import pandas
      def model(dbt, fal):
          df = dbt.ref(""test_query"")
          return df
```
3. *dbt run got the success on sql, but not the py:*
```
       $ dbt run
        19:14:51  Running with dbt=1.3.2
        19:14:51  Found 2 models, 0 tests, 0 snapshots, 0 analyses, 282 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics
        19:14:51
        19:14:55  Concurrency: 1 threads (target='dev')
        19:14:55
        19:14:55  1 of 2 START sql table model dbt.test_query .................................... [RUN]
        19:15:14  1 of 2 OK created sql table model dbt.test_query ............................... [**OK -1 in 18.75s**]
        19:15:14  2 of 2 START python view model dbt.demo_athena_python .......................... [RUN]
        19:15:14  2 of 2 ERROR creating python view model dbt.demo_athena_python ................. [**ERROR in 0.03s**]
        19:15:14
        19:15:14  Finished running 1 table model, 1 view model in 0 hours 0 minutes and 23.07 seconds (23.07s).
        19:15:14
        19:15:14  Completed with 1 error and 0 warnings:
        19:15:14
        19:15:14  Runtime Error in model demo_athena_python (models\demo_athena_python.py)
        19:15:14    **Materialization ""materialization_view_athena"" only supports languages ['sql']; got ""python""**
        19:15:14
        19:15:14  Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
```
feels like it still may not work on my scenario 'athena' case. Or something I missed?",3
6463,"dbt-with-aws-athena-and-python-models","Hey there, this is Meder from fal. We indeed do not support Athena yet (thanks for pointers @joellabes). I just created a ticket for it, so we should get it working pretty soon. In the meantime, @yzhang, if you have time, you can create an issue in our github repository: https://github.com/fal-ai/fal",4
6463,"dbt-with-aws-athena-and-python-models","Hi again! So we just released dbt-fal 1.3.13 that enables support for dbt-athena-community. To install, run `pip install dbt-fal==1.3.13`. @yzhang, your Python models should compute as long as you set `materialized` config to `table`:
```
import pandas
def model(dbt, fal):
  dbt.config(materialized=""table"")
  df = dbt.ref(""test_query"")
  return df
```
Let me know if it works!",5
6463,"dbt-with-aws-athena-and-python-models","Thank you! I just confirmed, with *dbt.config(materialized=""table""*  , it went through my code. Again, thank you very much for the help.",6
6463,"dbt-with-aws-athena-and-python-models","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
6467,"how-to-only-build-models-that-passed-tests-in-the-previous-build","Hi everyone,

I'm trying a new approach for data quality in our warehouse where we would build models on an audit schema and then build again on a prod schema, excluding models that failed the tests during the audit build.

For the 2nd build I tried with the command
`dbt build --exclude 1+result:fail` as mentioned in the doc but I'm getting the following error:
```
Internal Error
  No comparison run_results
```

which I don't really understand since I actually have a run_results.json file stored in my `/target` folder.


`dbt build --exclude 1+result:fail --defer --state ./target` seems to work but I don't really know why and I'm not sure it does the same thing, I still have to wrap my head around the defer method.

Does someone have an idea?

Thanks :slight_smile:",1
6467,"how-to-only-build-models-that-passed-tests-in-the-previous-build","Without providing the `--defer` flag,  dbt doesn't try to defer to an old version. Likewise, without the `--state ` flag, it doesn't know where to look for the comparison manifest. 


[quote=""Thmsrey, post:1, topic:6467""]
`dbt build --exclude 1+result:fail --defer --state ./target`
[/quote]

Is the correct command to run to successfully use the result method in your selector",2
6489,"is-it-possible-that-models-using-the-dbt-metrics-package-metrics-calculate-macro-can-become-incremental","## The problem Iâ€™m having

I have a table where we report metrics that is generated through the dbt_metrics package and contains more than 100 million rows in a day granularity, so in many occasions the execution fails.

## The context of why Iâ€™m trying to do this

Running the job with the model.

## What Iâ€™ve already tried

I am trying to find out if there is any option of changing the model to incremental but dbt_metrics, unless I'm mistaken, does not contain any parameter to do it. I would like to know if anyone has encountered this problem before and see possible options to solve it.

## Some example code or error messages

Query (268807711) cancelled by WLM abort action of Query Monitoring Rule ""LongQueryDefaultAbort"".",1
6503,"help-with-tags-versus-just-tags","Hi there! I'm sorry for the super basic question but I can't seem to find a clear answer.  After some reading, I've come to learn that tags are inherited. So, if I set a tag on a source, my models will inherit that tag.  Makes good sense!  What I can't seem to get clarity on is where to use +tags versus just tags when configuring tags.

I thought it might be related to graph operators, but the leading plus selects upstream but the tags are being inherited downstream.

I would appreciate any help you could offer clarifying +tags versus tags and where to use one or the other.",1
6503,"help-with-tags-versus-just-tags","Hey @robin.sarac, 

I assume you're talking about your `dbt_project.yml` file? The `+` prefix makes it possible to differentiate between resource paths and their configs:

https://docs.getdbt.com/reference/resource-configs/plus-prefix

If you were talking about node selection, then you're correct that `dbt run -s +tag:my_tag` would run nodes tagged `my_tag` and their upstream parents.",2
6503,"help-with-tags-versus-just-tags","Gosh, thank you, @joellabes !  I really appreciate the thorough answer.  I feel much better proceeding...I'm a details person. :slight_smile:  Thank you once again!",3
6503,"help-with-tags-versus-just-tags","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6522,"error-while-reading-partial-parse-msgpack-file","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->
Hi,

I'm using dbt on an airflow setup to to create some tables (using models). I have a combination of airflow operators and python code behind that will generate around 10 airflow tasks, each is a bashoperator that execute a dbt run command and each command is about a specific model and each command has it own variables. All the tasks are running concurrently but they sometimes fail randomly with an error that seems to indicates that the partial_parse.msgpack file is unavailable.


The models i have,  uses the same variables (key) but they can have different values from a model to another. 

I'm wondering if this kind of dynamic workflow is a good fit for dbt and what conditions can cause the issue. 
I must mention that i cannot reproduce the issue in local, it only occurs in production ( airflow instance deployed with google cloud platform (GCP)



```
[2023-01-26T08:20:27.267+0000] {subprocess.py:85} INFO - Output:
[2023-01-26T08:20:35.405+0000] {subprocess.py:92} INFO - [0m08:20:35  Running with dbt=1.3.1
[2023-01-26T08:20:36.852+0000] {subprocess.py:92} INFO - [0m08:20:36  Unable to do partial parsing because config vars, config profile, or config target have changed
[2023-01-26T08:21:15.901+0000] {subprocess.py:92} INFO - [0m08:21:15  Encountered an error:
[2023-01-26T08:21:15.902+0000] {subprocess.py:92} INFO - [Errno 5] Input/output error: 'target/partial_parse.msgpack'
[2023-01-26T08:21:15.903+0000] {subprocess.py:92} INFO - [0m08:21:15  Traceback (most recent call last):
[2023-01-26T08:21:15.904+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/main.py"", line 135, in main
[2023-01-26T08:21:15.904+0000] {subprocess.py:92} INFO -     results, succeeded = handle_and_check(args)
[2023-01-26T08:21:15.905+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/main.py"", line 198, in handle_and_check
[2023-01-26T08:21:15.906+0000] {subprocess.py:92} INFO -     task, res = run_from_args(parsed)
[2023-01-26T08:21:15.906+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/main.py"", line 245, in run_from_args
[2023-01-26T08:21:15.907+0000] {subprocess.py:92} INFO -     results = task.run()
[2023-01-26T08:21:15.907+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/task/runnable.py"", line 453, in run
[2023-01-26T08:21:15.908+0000] {subprocess.py:92} INFO -     self._runtime_initialize()
[2023-01-26T08:21:15.908+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/task/runnable.py"", line 161, in _runtime_initialize
[2023-01-26T08:21:15.909+0000] {subprocess.py:92} INFO -     super()._runtime_initialize()
[2023-01-26T08:21:15.909+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/task/runnable.py"", line 94, in _runtime_initialize
[2023-01-26T08:21:15.910+0000] {subprocess.py:92} INFO -     self.load_manifest()
[2023-01-26T08:21:15.910+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/task/runnable.py"", line 81, in load_manifest
[2023-01-26T08:21:15.911+0000] {subprocess.py:92} INFO -     self.manifest = ManifestLoader.get_full_manifest(self.config)
[2023-01-26T08:21:15.917+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/parser/manifest.py"", line 221, in get_full_manifest
[2023-01-26T08:21:15.918+0000] {subprocess.py:92} INFO -     manifest = loader.load()
[2023-01-26T08:21:15.919+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/parser/manifest.py"", line 410, in load
[2023-01-26T08:21:15.919+0000] {subprocess.py:92} INFO -     self.write_manifest_for_partial_parse()
[2023-01-26T08:21:15.920+0000] {subprocess.py:92} INFO -   File ""/opt/python3.8/lib/python3.8/site-packages/dbt/parser/manifest.py"", line 559, in write_manifest_for_partial_parse
[2023-01-26T08:21:15.920+0000] {subprocess.py:92} INFO -     with open(path, ""wb"") as fp:
[2023-01-26T08:21:15.921+0000] {subprocess.py:92} INFO - OSError: [Errno 5] Input/output error: 'target/partial_parse.msgpack'
[2023-01-26T08:21:15.921+0000] {subprocess.py:92} INFO -
[2023-01-26T08:21:16.570+0000] {subprocess.py:96} INFO - Command exited with return code 2
[2023-01-26T08:21:16.725+0000] {taskinstance.py:1904} ERROR - Task failed with exception
```",1
6533,"after-install-dbt-visual-code-can-not-recognize-the-dbt-cmd","Hi all,
i just install the dbt in my local through visual code. but when try to run dbt, it is showing ""The term 'dbt' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name"" error. i have install dbt-core , anyone know why ?
![image|451x499](upload://tNRPrXnjiJoKfzLhunTDx0gNuQt.png)

Thanks,
Co",1
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","I have snapshots that are built a separate Snowflake database, which is defined by an environment variable.

```sql
{% snapshot snap_qb_etahub__assemblies %}
   {{
        config(
          tags=['snowflake-only'],
          target_database=env_var('DBT_DB_SNAPSHOTS', 'UNDEFINED__DBT_DB_SNAPSHOTS'),
          target_schema='qb_etahub',
          alias='assemblies',
          transient=False,
          unique_key='record_id_nbr',
          strategy='timestamp',
          updated_at='_sdc_batched_at',
          invalidate_hard_deletes=True,
        )
    }}

    SELECT
      *,
      CURRENT_TIMESTAMP()::TIMESTAMP AS dbt_snapshot_at
    FROM
      {{ source('qb_etahub', 'assemblies') }}
{% endsnapshot %}
```

I recently added a new snapshot, pushed a PR, then decided that I needed to change the `unique_key`.  After adding a new commit, I started getting errors.  That's when I realized that dbt was not building this new snapshot in a separate CI-only schema.

This seems really bad because I might make snapshot-breaking changes during CI, which would end up affecting production snapshot data.  Am I doing something wrong here?  Are there configuration changes I can make that would tell SlimCI to build my snapshots in a separate schema?",1
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Yikes! I'm pretty sure that this is <s>*correct*</s> *expected* behaviour out of the box, even if it's not *friendly* behaviour. 

Some options: 
1. Check out https://discourse.getdbt.com/t/using-dynamic-schemas-for-snapshots/1070, especially the first option where you override the schema generation macro
2. Could you pass the schema name as an env var too?
3. Might be worth opening an issue about this! I think it's old behaviour, that probably hasn't been reviewed in a more CI-heavy world",2
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","I'd argue that it's _incorrect_ for a CI process to alter production, even if it's snapshots themselves are behaving as _expected_ according to the design.

1 - I saw that, but I also saw [this comment](https://discourse.getdbt.com/t/using-dynamic-schemas-for-snapshots/1070/5), which indicated that SlimCI treats _all_ snapshots as modified during _every_ CI job, which would mean my entire dag would need to be built every run CI run.  This would also snapshot every table in production on every CI run, which could be fraught with side effects.
2 - I thought about that, but it would start out empty and thus all CI jobs using snapshot models would fail, unless I manually maintained/synced the CI schema.  :thinking: ..... I suppose I could set up a dbt operation to zero-copy-clone my prod snapshots and have that run as part of my prod build job.  All CI jobs would use the same schema (to avoid the dynamic problem), so in principle there could be conflicts if multiple CI jobs were running simultaneously; but I don't expect that would be an issue for me.  I might try this approach..
3 - Honestly, snapshots feel like they were left behind in the run up to 1.0.  I still get annoyed when I have to write them and re-learn that many config.yml options are ignored and I have to put them in a jinja config block.  It almost feels like they should just be normal models that use a dbt-supported snapshot macro with some guidance around when it's appropriate to use them (e.g., save them in a permanent location that can be archived and not in a transient tables).",3
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","[quote=""gnilrets, post:3, topic:6536""]
Iâ€™d argue that itâ€™s *incorrect* for a CI process to alter production, even if itâ€™s snapshots themselves are behaving as *expected* according to the design.
[/quote]

Yes that's much better - edited for clarity :pray: 
[quote=""gnilrets, post:3, topic:6536""]
I might try this approachâ€¦
[/quote]

Would love to hear how this goes! 
[quote=""gnilrets, post:3, topic:6536""]
I still get annoyed when I have to write them and re-learn that many config.yml options are ignored and I have to put them in a jinja config block
[/quote]

I feel you ðŸ˜¬ gonna write something up about this when I get back to my computer properly",4
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Oh no!  @joellabes , #2 doesn't really work.  My CI job is now running the entire dag.  I guess it sees the difference in the snapshot database as a difference with the production run.

Not sure what to do at this point without abandoning SlimCI (BIG driver for using dbt-cloud) or putting my snapshots at risk (can't do this for long).",5
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Hmmm OK I'm going to ask around and see if I can find any other suggestions for you. To double check, this would all work fine (as far as you know) if you could define the schema in a yaml file outside of the config block, right? 

In honesty, I never put my snapshots through CI at my last job - I would just test them manually by making a copy of the table (something like `create table snapshots.my_snapshot_backup as (select * from snapshots.my_snapshot)`) and then try running the snapshot normally safe in the knowledge that I could drop it if something went wrong. 

That was an objectively bad strategy :grimacing:",6
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Nevermind, this strategy does mostly work!  I realized the problem here.  I was defining the snapshot database in the jinja config block.  After moving it to `dbt_project.yml`, it works as expected.  According to the [state comparison caveats](https://docs.getdbt.com/reference/node-selection/state-comparison-caveats#false-positives), dbt ignores environment changes if they're defined in the `dbt_project.yml`.  Fortunately, snapshots honor this yaml variable.  

I develop snapshots the same way you do.  The problem I was trying to solve is that if I submitted a commit that had a change to a snapshot, then SlimCI would run that snapshot (because we use `dbt build`) in my production snapshot location.  This is the (dangerous) default behavior.  ..... I suppose you could also exclude snapshots from the SlimCI job, but you'd have to explicitly do so.",7
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Hurrah! So let me repeat this back and check I've got it (I've turned this into a Wiki post, so you can edit it directly if I've got anything wrong, then I'll mark it as the solution and use it as the bones of a GH issue)

- By default, dbt snapshots build into a hardcoded schema. [Making the schema dynamic](https://discourse.getdbt.com/t/using-dynamic-schemas-for-snapshots/1070) means that every snapshot is considered `state:modified` on every CI run because [in-file environmentally-aware configs trigger false positives](https://docs.getdbt.com/reference/node-selection/state-comparison-caveats#false-positives) and `target_schema` can't be defined in `dbt_project.yml`. 
- You can make a zero-copy clone of the snapshot to a different database at the start of a CI run. `target_database` _can_ be defined using `env_var` jinja in the `snapshots` section of `dbt_project.yml` which means other unchanged snapshots don't get picked up as modified unnecessarily. 
- Cloning to a different database as described in the second bullet and then setting the snapshot database environment variable in your SlimCI job enables snapshots to be Slim CI'd. If you can't do that, you'd need to exclude them from the CI job definition. That is tricky in `dbt  build` context because the `dbt build --resource-type` filter seems to only takes a single resource type anyway, *and* it's not able to be used for exclusion)",8
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","@gnilrets I just started writing this up as a GH issue, and then I found a couple of related things ([this issue comment](https://github.com/dbt-labs/dbt-core/issues/4000#issuecomment-1249728725) and [this issue](https://github.com/dbt-labs/dbt-core/issues/6745) specifically) which made me realise I had misunderstood a couple of fundamental things in each of my bullets.

[quote=""joellabes, post:8, topic:6536""]
`target_schema` canâ€™t be defined in `dbt_project.yml`.
[/quote]

This isn't true - both `target_schema` and `target_database` can be defined in`dbt_project.yml` along with most snapshot behaviour. It just doesn't work in `whatever.yml` property files (you said this above and I just blanked on it).

---

[quote=""joellabes, post:8, topic:6536""]
`target_database` *can* be defined using `env_var` jinja in the `snapshots` section of `dbt_project.yml`
[/quote]

:point_up: true, but maybe unnecessary, because this:

[quote=""gnilrets, post:3, topic:6536""]
I also saw [this comment](https://discourse.getdbt.com/t/using-dynamic-schemas-for-snapshots/1070/5), which indicated that SlimCI treats *all* snapshots as modified during *every* CI job
[/quote]

is resolved by this:

[quote=""gnilrets, post:7, topic:6536""]
I was defining the snapshot database in the jinja config block. After moving it to `dbt_project.yml`, it works as expected. According to the [state comparison caveats ](https://docs.getdbt.com/reference/node-selection/state-comparison-caveats#false-positives), dbt ignores environment changes if theyâ€™re defined in the `dbt_project.yml`.
[/quote]

---

And if you're not using an env var to define the location of the model, shouldnâ€™t standard deferral prevent the failures described here:

[quote=""gnilrets, post:3, topic:6536""]
it would start out empty and thus all CI jobs using snapshot models would fail
[/quote]

? I haven't tested how deferral works with snapshots, but I'd be mad to discover it doesn't work the same as any other `ref`. 

---

So I _think_ the main thing to do here is to go and +1 https://github.com/dbt-labs/dbt-core/issues/4000, and I'll open a different issue saying that dbt's default schema logic for snapshots is suboptimal, but purely because it creates a bad default experience, not because it makes CI impossible as first believed. 

Does that sound right to you? I thought I was so clever getting a communal wiki post up :grimacing:",9
6536,"snapshots-are-not-being-built-in-a-ci-schema-during-slimci-production-affected-during-ci-run","Issue #4000 is a good one to solve, but I'm not clear on whether having all of my config in a .yml file would have helped.

The root issue is that when I set up SlimCI in dbt-cloud, it was not creating CI-specific schemas when a snapshot model was changed and instead ran changed snapshot code in the production schemas.

I put all of my snapshots in a separate database (defined by an environment variable), and I specify the schemas in config blocks (only because I've previously discovered that some options are not honored in a snapshot .yml file).  If my schema was defined in a .yml file, would that allow SlimCI to create CI-specific snapshots in the same way that it works for regular model schemas?  If so, then solving #4000 would address this, but it's not clear from the issues whether that is the case.",10
6538,"exclude-multiple-tags","I have a project in which we're trying to deprecate a few directories. To do this, we've gone through and tagged models in one directory with ""tagname_a"". In another directory we tagged models with ""tagname_b"".

If I run `dbt run --exclude tag:tagname_a`, I can see that those models do not run.

If I run `dbt run --exclude tag:tagname_b`, I can see that those models do not run.

However, if I run `dbt run --exclude tag:tagname_a,tag:tagname_b`, all of the models run and nothing gets excluded.

What is the proper syntax to run dbt and exclude multiple tags?

We're on DBT version 1.3. Any help would be appreciated. Thank you.",1
6538,"exclude-multiple-tags","the comma in the node selector syntax means you select the intersection of those two tags, i.e. models that are tagged with tagname_a and with tagname_b. if you want to exclude tagname_a and tagname_b individually then leave a whitespace in the dbt command:
dbt run --exclude tag:tagname_a tag:tagname_b

<sub>Note: `@Paul P (Target Reply)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675276957789559?thread_ts=1675276801.470949&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6538,"exclude-multiple-tags","Thank you for the reply, that's exactly what I needed.",3
6538,"exclude-multiple-tags","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","I'm trying to create a model in an existing schema. However, on run, dbt runs ""CREATE SCHEMA {} IF NOT EXISTTS"". The user doesn't have privileges to create a schema in the target database and i don't want to enable so. As a pre-hook the user assumes a role which can operate on the target schema but it appears as if dbt tries to create it before it gets to the pre-hook
I'm using version 0.19.1, I saw a PR from 2020 suggesting the issue should have been fixed but I still have it",1
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","How many schemas are you trying to create? If you create the schema separately in advance, then your dbt user won't need to try and make its own. 

I think that pre-hooks run inside their own transaction by default, which would be why assuming the role in advance doesn't work. 

As a side note, dbt 0.19 is about 2 years old now - you should consider moving to at least v1.0 if you can and ideally 1.4 (the latest version).",2
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","I'm not trying to create a schema myself. The schema already exists, but dbt runs 'CREATE  SCHEMA IF NOT EXISTS' that's the problem! It's this problem I'm having:
https://github.com/dbt-labs/dbt-core/issues/2186",3
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","Have you checked whether this unwanted behaviour still happens in version 1.x of dbt? Versions 0.x of dbt Core are no longer supported",4
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","I am having the same problem. DBT (dbt=1.3.0) trying to create a schema which already exist. The user doesnâ€™t have privileges to create a schema in the target database and i donâ€™t want to enable so.

```
airflow@airflow-854558f444-r9rdn:/opt/airflow/dags/dbt/projects/sdna_us$ dbt run --select last_updt_dt --profiles-dir .
20:47:46  Running with dbt=1.3.0
20:47:47  Found 335 models, 20 tests, 0 snapshots, 0 analyses, 764 macros, 0 operations, 6 seed files, 6 sources, 0 exposures, 0 metrics
20:47:47  
20:47:50  Encountered an error:
Database Error
  003041 (42710): SQL compilation error:
  Schema 'SDNA_US_SDNA_UI_DEV' already exists, but current role has no privileges on it. If this is unexpected and you cannot resolve this problem, contact your system administrator. ACCOUNTADMIN role may be required to manage the privileges on the object.
```

dbt_project.yml files has the following section:

```
    sdna_ui:
      +tag: sdna_ui
      +transient: false
      +schema: SDNA_US_SDNA_UI_{{ env_var('AIRFLOW_ENVIRONMENT')|upper }}
      intermediate:
        materialized: view
      transformations:
        materialized: table
        +transient: false
```",5
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","Thanks for confirming that it's still happening in new versions of dbt Core @rmenendezm! I asked around internally, and here's what I learned: 

- dbt only considers creating schemas for models that are actually selected to run ([relevant lines of code](https://github.com/dbt-labs/dbt-core/blob/6b5e38ee2855e8739414c7f5172b80a2090c3a0c/core/dbt/task/run.py#L431-L432))
- dbt first determines what schemas exist, and will run `create schema ... if not exists` **only if it doesn't think the schema exists.** 

With this in mind, our hypothesis is: 
- the schema does exist (already confirmed)
- The database user that is using dbt doesn't have permission to see the schema _at all_, even that it exists
- One or more of the selected models in your run are intended to land in the `SDNA_US_SDNA_UI_DEV` schema
- dbt can't see that the schema already exists (it's not allowed to!) so it tries to create it, and hits an error. 

I would suggest that you check what permissions the database user has, to ensure it can at least see the schemas. 

From there, if you have a different situation then it would be worth [opening a GitHub issue](https://github.com/dbt-labs/dbt-core/issues) with the details",7
6540,"dbt-tries-to-create-schema-which-already-exists-as-user-with-no-create-permissions","I found the role set by default in the `profiles.yml`, which was the one used by dbt to initial query the schema, did not have the appropriate privilege, so it could not see the schema. And that's why it try to create an already existing schema.",8
6545,"how-to-exclude-seeds-from-running-in-dbt-build","
I have a few csv files that I've loaded as seeds. Because they are relatively static mappings I do not need to load these files each time I run my models. However, I'd prefer not to delete them from the seed folder either because once every couple months I will probably need to update them. I attempted to run `dbt build --exclude seeds`, but the logs still showed the seeds running. Wondering if there is a simple/elegant way to achieve this or if I'm missing something obvious (fairly new to dbt)",1
6545,"how-to-exclude-seeds-from-running-in-dbt-build","`dbt build  --exclude {path to seed files, e.g seeds/ }` can do the trick

<sub>Note: `@Chris Rhinehart` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675287421152499?thread_ts=1675286837.551999&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6545,"how-to-exclude-seeds-from-running-in-dbt-build","I don't think Chris' suggestion works, as I think it assumes it's starting from inside the `models` directory for path traversal. 

In general, it's not necessary to try to avoid building seeds etc. The good thing about dbt is that you describe the way you want your project to look, and it makes that happen. Keeping track of whether a seed has changed or not is the sort of mental overhead that dbt wants to save you from, especially for tiny csv files which don't have much compute overhead to reload. 

With that said, if you want to avoid rebuilding unchanged nodes unnecessarily, you could look into using the [state:modified selector](https://docs.getdbt.com/reference/node-selection/methods#the-state-method) instead",3
6558,"source-table-configuration-based-on-prefix","I have a schema with tables that have the same structure for different accounts that I am dynamically unioning using Jinja.

I would like to make the source configuration dynamic so if we add new accounts in the future, the source config (freshness, tests) will automatically run on these tables.

Example:

myschema.advertising_data_account1
myschema.advertising_data_account2
myschema.advertising_data_account3

What I would like to be able to do is something like this in sources.yml

tables:    
      - name: advertising_data
        prefix: advertising_data
        freshness: 
           warn_after: {count: 12, period: hour}
           error_after: {count: 24, period: hour}

I found this article which is close, but I'd rather not split into separate schemas unless that is the only way-
https://discourse.getdbt.com/t/unioning-identically-structured-data-sources/921

Is this possible?",1
6558,"source-table-configuration-based-on-prefix","You can probably just skip the step of creating sources for all of these tables. You lose a little bit of observability, but gain a lot of productivity.

dbt_utils has a macro called `get_relations_by_pattern` to easily build new Relations for matching tables in your database: [dbt-labs/dbt-utils: get_relations_by_pattern. (github.com)](https://github.com/dbt-labs/dbt-utils#get_relations_by_pattern-source). The name is a little misleading --  you don't use this to get relations that are already defined in the graph, you use it to construct new Relation objects out of tables in your database.

You could use it like this:

```sql
-- models/staging/myschema/stg__advertising_data.sql
{% set source_tables = dbt_utils.get_relations_by_pattern(
    'myschema', 'advertising_data_account%'
) %}
{{ dbt_utils.union_relations(relations=source_tables) }}
```

Then in lieu of your source freshness test, you can create a custom data test that checks for the same thing. The `union_relations` macro adds a column called `_dbt_source_relation` that contains the name of the source table, which is useful here:

```sql
-- tests/test_advertising_data_freshness
select _dbt_source_relation, max(updated_at)
from {{ ref('stg__advertising_data') }}
group by 1
having max(updated_at) < sysdate() - interval '12 hours'
```",2
6558,"source-table-configuration-based-on-prefix","Thanks - i'm using dbt_utils.get_relations_by_pattern already, but didn't think about making the custom test.

The custom test should work for my needs.",3
6566,"using-variables-inside-the-dbt-utils-date-spine-macro","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

I'm trying to create a date spine that where the first and last timestamps are taken from another model.

For example, if I have a table that shows the times that someone logs onto their computer, I want to create a date spine that will take the earliest logon time and the latest logon time. I can use `dbt_utils.date_spine` if I enter set dates for the beginning and end of the spine. I have tried a few different methods of setting the start and end timestamps using variables, such as `select min(logon_time) from activity_table` but they don't work.

Does `dbt_utils.date_spine` have to take set dates as arguments or is there a way to use variables?",1
6566,"using-variables-inside-the-dbt-utils-date-spine-macro","For example, this works:
`    {{ dbt_utils.date_spine(
    datepart=""second"",
    start_date=""2022-01-01"",
    end_date=""2022-01-01"",
   )
}}`

This does not:

`    {{ dbt_utils.date_spine(
    datepart=""second"",
    start_date=select min(logon_time) from activity_table,
    end_date=select max(logon_time) from activity_table,
   )
}}`",2
6571,"sourcing-from-a-table-in-different-gcp-project-based-on-environment","Hi All,

I am new to Dbt and I am trying to source from a table 'xyz' but it would belong to 'proj-d' in development, 'proj-i' in integration and 'proj-p' in prod.
The table name would remain the same.
 Is there a way I can parameterise the project name based on env in the sources.yml.file ? 

In the Dbt documentation, I did not see the mention of project name in the sources.yml file

Any pointers/advice ? 

Thanks in advance.",1
6571,"sourcing-from-a-table-in-different-gcp-project-based-on-environment","Hey, check the last section in dbt docs

https://docs.getdbt.com/reference/dbt-jinja-functions/target#use-targetname-to-change-your-source-database.",2
6571,"sourcing-from-a-table-in-different-gcp-project-based-on-environment","Great thanks !  this looks promising, 
I will try doing it this way",3
6571,"sourcing-from-a-table-in-different-gcp-project-based-on-environment","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6574,"dbt-found-two-models-with-the-name-error-when-the-two-models-are-in-different-projects","Hello!

In my organization,  we are using a  *single repo/multiple projects* approach. In some cases, the sources/stating tables have to be duplicated across projects. 

That is usually not a problem. We do have occurences of duplicated model names across projects and it works just fine - {{ ref }} seemed to be project-specific.

However, for one specific project, {{ ref }} seems to scan for models across multiple projects. 

I tried comparing the the dbt_project.yml of the projects for which there is an error vs ones where duplicated model names work fine - but couldn't find any differences.

I know there's a two-argument variant for {{ ref }}, and that schemas can be specified in the model config - but none of that is being used in the projects where things work fine.

Any idea what could be different between the projects, that would explain the different behaviors of {{ ref }}?

Thanks!",1
6574,"dbt-found-two-models-with-the-name-error-when-the-two-models-are-in-different-projects","Is the misbehaving project importing one of the other projects as a package? have a look in your `packages.yml` file",2
6574,"dbt-found-two-models-with-the-name-error-when-the-two-models-are-in-different-projects","Hi Joel,

Thanks a lot for your help.

My `packages.yml`for that project is:

```
packages:
    - package: dbt-labs/dbt_utils
      version: 0.8.5
```",3
6574,"dbt-found-two-models-with-the-name-error-when-the-two-models-are-in-different-projects","Any chance youâ€™re invoking dbt from a root folder that has multiple dbt projects inside of it, and two of those projects contain the same model name? 

When you run into this sort of error, dbt will show the names of the offending models: 
```
  dbt found two models with the name ""my_first_dbt_model"".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for ref(""my_first_dbt_model"").
  
  To fix this, change the name of one of these resources:
  - model.joel_sandbox.my_first_dbt_model (models/example/my_first_dbt_model.sql)
  - model.joel_second_sandbox.my_first_dbt_model (models/my_first_dbt_model.sql)
```

In this case, the projects are `joel_sandbox` and `joel_second_sandbox`. If you do a search for `joel_second_sandbox` inside of the `joel_sandbox` project I imagine you should be able to find the cause? 

You could also try running `dbt clean` to get rid of any package imports that were in `packages.yml` earlier but aren't anymore.",4
6603,"setting-meta-for-all-tests-on-a-model","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I'm not able to define `meta` config on all tests inside specific model.
Tests have FQN that don't include model name, so I'm not able to select only specific models.
I can set meta on package level, but not on model level


## The context of why I'm trying to do this

## What I've already tried 

Setting `meta` on model directly

## Some example code or error messages
```
tests:
  project:
    package1:
      package2:
        +store_failures: true
        +severity: warn
        +meta:
             test: 1000
```

This works, but only on package level.

tests have FQN like this:

```
project.package1.package2.not_null...
```

How can I set `meta` for tests on a model level ?",1
6605,"dbt-fal-or-python-with-spark-emr","I am trying to run Spark over JDBC using dbt-python. It seems it work only with Databricks and GCP Dataproc. We are doing a POC to see if dbt can be integrated with open source Spark using Python Models.

We are trying to do a POC for one of our clients. We have tried running DBT against locally setup Spark. We are able to run SQL models using thrift server but not Python based models (as it is only possible to run queries using Spark Thrift Server). It is asking for Databricks cluster or Dataproc cluster.

We would like to understand the roadmap of dbt-python or dbt-fal with respect to support to open source Spark, EMR, Spark on Kubernetes, etc.",1
6613,"dbt-views-send-to-algolia-indexes","Hi All,

I am sending some data to redshift, which is then sourced to dbt to create views/transform the data. I want to send the updated records (based on date) to algolia index

I am trying to do this since our entire transformation layer is handled by dbt now

I am trying to understand if I can use macros for this?

I am new to this community, so any pointers will help a lot.

Thank you.",1
6614,"dbt-core-integration-tests-not-passing","
## The problem I'm having
I want to submit a small contribution, but when I set up dbt core for development and run the integration test on the unchanged repo, it does not fully pass. 

Now I am unsure if this is an issue on my side or not...

## The context of why I'm trying to do this
Ubuntu 20.04 LTS WSL2 on Windows 10
Python 3.9.5
dbt:
  - installed: 1.5.0-a1
  - latest:    1.4.1 

## Some example code or error messages
![image|690x116](upload://ncoL0Dm99lT9dihM1wfRgT2NHrK.png)",1
6617,"your-ide-session-has-timed-out","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
dbt IDE is unusable. I keep getting:
IDE Requires Restart
Your IDE session has timed out. Refresh to get back up and running! It should only take a moment.
## The context of why I'm trying to do this
Anything. I'm trying to fix a problem, change code.
## What I've already tried 
Restarted Edge browser
Cleared Edge cache
Restarting laptop
## Some example code or error messages
```
IDE Requires Restart
Your IDE session has timed out. Refresh to get back up and running! It should only take a moment.
```",1
6621,"wildcard-in-dbt-project-yml","## The problem I'm having
We are having a lot of models, tests and macros. The default structure became hard to navigate, that's why we would like to change the dbt_project.yml definitions so we could keep the related models, tests, seeds and macros in same directory for example

```
root
-> a
--> model
---> a.sql
--> test
---> a_test.sql
```
That would be possible, if dbt_project.yml accepted wildcards we could do sth like

```
models:[""root/**/model""]
tests:[""root/**/test""]
```

## What I've already tried 
I have already tried *, **, .*

Is there anyway we could generate the dbt_project.yml before execution? I could write an appropriate script for that.",1
6623,"supported-postgres-version-for-dbt","Hi, our project's database is using Postgres version 14 and I see the current Postgres supported version for dbt 1.4 goes up 11. I would like to know if it would be a problem for us to still use dbt 1.4 or do we need to wait for a future release of dbt to use? Also I would like to know what will the supported Postgres versions be in the future.

Thank you in advance.",1
6623,"supported-postgres-version-for-dbt","What documentation do you see that indicates dbt supports only up through Postgres 11? I'm using dbt with Postgres 13 and it works fine.

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675693334802999?thread_ts=1675673051.660979&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6623,"supported-postgres-version-for-dbt","I have read it on the page listing the prerequisites: https://docs.getdbt.com/docs/dbt-cloud/on-premises/prerequisites#google-cloud-storage

""Supported versions include 9.6, 10, and 11. The database should be configured with at least 50GB of storage.""

The thing is that it mentions this for dbt Cloud and not dbt Core. But since dbt Cloud is built upon dbt Core I figured it will be the same.",3
6623,"supported-postgres-version-for-dbt","Those are the pre-requisites for hosting the dbt Cloud application on-premise (which IIRC is no longer supported anyway).

I don't believe there is any known restriction on postgres versions with the dbt postgres adapter.",4
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","## The problem I'm having
We populate our raw database using Fivetran. Since dbt automatically hardcodes the columns in a view definition, our staging views break every time a new column is added to the underlying table, with, for example:
`View definition for 'ANALYTICS_DB.STG_SCHEMA.STG_VIEW' declared 16 column(s), but view query produces 19 column(s).`

## The context of why I'm trying to do this
I'm trying to figure out a way to not have to re-run all staging views every time Fivetran finishes syncing to prevent this.

## What I've already tried 
I've been looking at dbt-coves and dbt-osmosis. I think I can get things to work ok with a decent amount of modification and combination, but a) it seems like way more work than should be required for something like this, b) the performance is terrible, etc.

## Some example code or error messages
Model ANALYTICS_DB.STG_SCHEMA.STG_VIEW is defined as
```
SELECT
	*

FROM
	FIVETRAN_DB.SCHEMA.TABLE
```

But gets compiled to

```
CREATE OR REPLACE VIEW ANALYTICS_DB.STG_SCHEMA.STG_VIEW AS
(
	SELECT
		COL1
	,	COL2
	,	COL3

	FROM
		FIVETRAN_DB.SCHEMA.TABLE
);
```",1
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","What adapter are you using?  I ask because when I look in `target/run` for both my test (duckdb) project I don't see it enumerate the columns like your example shows.

![image|690x378](upload://sGfqhbFxRj6pzzMiEWyHIUwh0uP.png)",2
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","Thanks @jaypeedevlin for the quick response!

I'm using Snowflake. I just assumed the behavior would be consistent across adapters.",3
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","I had a play with Snowflake, and this is not actually dbt doing this, but instead how Snowflake stores the resultant created view.  I did the same thing as above and the code in /target/run includes the select * but I pointed it to a test table, which I then added a column to and got the error you're seeing.

What dbt ran:

```sql
  create or replace  view db.schema.test
  
   as (
    

select
  *
from db.scratch.jd_table
  );
```

What snowflake stores as the definition of the view:

```sql
create or replace view DB.SCHEMA.TEST(
	FOO,
	BAR
) as (
    

select
  *
from db.scratch.jd_table
  );
```

So when the underlying table adds a column the view fails.

You can automate adding the columns in using [dbt_utils.star()](https://github.com/dbt-labs/dbt-utils#star-source) which will mean your views don't fail when columns are added, and also will update without you having to touch the code on your next dbt run.

**All of that said**, my subjective opinion is that staging models are for renaming, casting types, etc.  Essentially creating an endorsed view of your source data.  With that framing, I prefer to manually enumerate the columns (using codegen to help initially).  Otherwise, you may as well not have staging models at all and just select directly from the source.",4
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","That was awesome of you to dive in there. I should've looked at the compiled code myself, instead of just assuming that it was dbt doing this rather than Snowflake.

You make a great point at the end, and I agree.

Thanks again!",5
6634,"how-to-prevent-views-from-constantly-breaking-due-to-underlying-table-changes","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
6640,"dbt-docs-generate-exclude-not-working-and-doesnt-exclude-the-models-based-on-tags","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
running the docs generate command for commercial environment like ""#dbt docs generate --target <env> --exclude tag:<dev-only>"" is still trying to access the excluded models tag with 'dev-only' 

## The context of why I'm trying to do this
So, we have separate environments like dev and prod and want to run the generate command for a single environment only 

## What I've already tried 
just tried the --exclude option 

## Some example code or error messages
```
Building catalog
 Encountered an error while generating catalog: Database Error
  002003 (02000): SQL compilation error:
  Database '<DB_NAME_1>' does not exist or not authorized.
  Encountered an error while generating catalog: Database Error
  002003 (02000): SQL compilation error:
  Database '<DB_NAME_2>' does not exist or not authorized.
/home/github/_work/_temp/.sh: line 3:  2745 Killed                  dbt docs generate --target prod --exclude tag:frh-only

```",1
6640,"dbt-docs-generate-exclude-not-working-and-doesnt-exclude-the-models-based-on-tags","Hi @shantanu.panda, at present dbt docs generate only captures the entire project; `--exclude` isn't taken into account. 

I would encourage you to weigh in on this discussion thread: https://github.com/dbt-labs/dbt-core/discussions/5096",2
6640,"dbt-docs-generate-exclude-not-working-and-doesnt-exclude-the-models-based-on-tags","I believe `docs generate` **does** take account of `--select` /`--exclude` , but **only for compilation!** It doesn't respect selection criteria for actual **catalog generation** .

That's this issue (in addition to the one Joel linked above):
- https://github.com/dbt-labs/dbt-core/issues/6014",3
6642,"semantic-layer-enablement-in-production","I'm trying to enable the dbt semantic layer in my prod environment in a project but it keeps telling me that I can't do that because my metrics package needs to be higher than 0.3.2.

The thing is that I have the 1.4 version of the package and when running jobs it is stated like that in the log of the dbt deps execution so I am not understanding well what I'm missing.

Any help on this configuration is welcome :slight_smile:",1
6642,"semantic-layer-enablement-in-production","In addition to updating the `packages.yml` file, you also need to have [completed a production run](https://docs.getdbt.com/docs/use-dbt-semantic-layer/quickstart-semantic-layer#run-your-production-job), is it possible that your most recent successful run still used v0.3.2 of the metrics package?",2
6645,"pass-a-string-into-a-model","


## The problem I'm having
I want to create a model in a schema related to a customer id -e.g. FOO1_BAR_LND where FOO1 is the customer ID. I also want to pass this customer ID into a column in the view

I have at the moment
`{%set customer='FOO1' %}`
`{{ config(schema=customer+'_BAR_LND') }}`

and this works ok to create the schema  FOO1_BAR_LND and creates the view in there

But when I want to put 'FOO1' into the model itself
` , {{customer}} as FOO_CUSTOMER_BKCC `

the compiler puts 
`, FOO1 as FOO_CUSTOMER_BKCC`
rather than 
`'FOO1' as FOO_CUSTOMER_BKCC`


What am I missing here! I am new to this so trying to get my head around how the yaml works.",1
6645,"pass-a-string-into-a-model","Quote it in the model SQL:
`, '{{customer}}' as FOO_CUSTOMER_BKCC`

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1675778985999939?thread_ts=1675774109.165099&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6645,"pass-a-string-into-a-model","ah, you mean outside the curly brackets  {{}} so 
`,'{{customer}}' as FOO_CUSTOMER_BKCC`
gives 
`'FOO1' as FOO_CUSTOMER_BKCC`

that works :). thanks",5
6645,"pass-a-string-into-a-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",6
6650,"previewing-the-code-generated-inside-of-a-macro","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I am trying to include an insert statement inside a macro. But I am facing error so please guide me through this.

## Some example code or error messages
```
{% macro centralize_test_failures(results) %}

  {%- set test_results = [] -%}
  {%- for result in results -%}
    {%- if result.node.resource_type == 'test' and result.status != 'skipped' and (
          result.node.config.get('store_failures') or flags.STORE_FAILURES
      )
    -%}
      {%- do test_results.append(result) -%}
    {%- endif -%}
  {%- endfor -%}
  
  {%- set central_tbl -%} {{ target.schema }}.test_failure_central {%- endset -%}
  {%- set monitoring_tbl -%} {{ target.schema }}.monitoring_table {%- endset -%}
  
  {{ log(""Storing test failures in "" + central_tbl, info = true) if execute }}

  create or replace table {{ central_tbl }} as (
  
  {% for result in test_results %}

    select distinct

        '{{result.node.unique_id}}'                     as unique_id,
        '{{ result.node.name }}'                        as test_performed,
        '{{result.node.resource_type}}'                 as resource_type,
        '{{ result.status }}'                           as status,
        '{{ target.schema }}'                           as schema,
        'test_failure_central'                          as target_table,
         object_construct_keep_null(*)                  as failed_records,
        '{{result.execution_time}}'                     as execution_time_sec,
         current_timestamp                              as date_time
      
    from {{ result.node.relation_name }}
    
    {{ ""union all"" if not loop.last }}
  
  {% endfor %}

  )

    insert into {{monitoring_tbl}}
    as 
    select * from {{ central_tbl }})

{% endmacro %}
```

Encountered an error:
Database Error
  001003 (42000): SQL compilation error:
  syntax error line 115 at position 3 unexpected 'insert'.",1
6650,"previewing-the-code-generated-inside-of-a-macro","Have you looked at the compiled code in your `target` directory?",2
6650,"previewing-the-code-generated-inside-of-a-macro","Hi, 
The code was not compiled so that's why I was not sure what is the issue. But now I have found the issue and resolved the error after putting a semi colon before insert.",3
6650,"previewing-the-code-generated-inside-of-a-macro","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6659,"source-freshness-results-are-coerced-as-string-on-dbt-spark-livy-adapter","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
my environment:
```
dbt: v1.3.1
adapter: dbt-spark-livy v1.3.1 (https://github.com/cloudera/dbt-spark-livy)
```

the results from running `source freshness` being coerced as `str` instead of `timestamp`
```
$ dbt --profiles-dir $PWD/profiles/ source freshness
07:16:37  Database Error in source checkout_v2 (models/bnpl/sources.yml)
07:16:37    Expected a timestamp value when querying field 'ingestion_time' of table bnpl.checkout_v2 but received value of type 'str' instead
```
noticed that all values in `ingestion_time` are timestamp (as sparksql schema)

I have tried to debug the result
```
(Pdb) result.print_table()
| max_loaded_at        | snapshotted_at       |
| -------------------- | -------------------- |
| 2023-02-06T17:03:30Z | 2023-02-08T07:01:45Z |
```
and investigate dbt core, looks like all returned result have been coerced to string datatype in agate table.
is that the bug from adapter or dbt core? if it was adapter impl, which module I should looking to fix it?
appreciate your help",1
6659,"source-freshness-results-are-coerced-as-string-on-dbt-spark-livy-adapter","By default, I would assume it's a bug in the adapter, so I'd encourage you to open an issue on its GitHub page!",2
6666,"deploy-macro-functions-to-custom-schemas","How to deploy macro to schemas based on folder name? I tried like below. But it errors out.
```
models:
  portal_data_migration:
    # Config indicated by + and applies to all files under models/example/    
    staging:
      +schema: _STAGING
    cleansing:
      +schema: _CLEANSING
```",1
6666,"deploy-macro-functions-to-custom-schemas","What do you mean by ""deploy macro to schemas""? Macros are used to generate SQL which is used inside of models.",2
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","Hi,

I have created a model that I only want to run one time per month (lets say Model Month), but the other models need to run twice a day (lets say Model Day). I want to exclude the first model from the dbt run without using every time the --exclude function. 

This is because we don't want other colleagues to add the --exclude command to every dbt run. So we prefer to have a solution where we can run the 'dbt run' but without Model Month. Ones a month we would like to run the Model Month without changing any settings. 

I tried to set the 'enabled' to false in the config and that works fine. But when I want to run Model Month, it says 'The selection criterion 'Model Month' does not match any nodes'. This is because it is set to enabled false. 

Is there an option to have by default Model Month disabled, but when I want it to run, I don't need to change the settings from enabled false to true?",1
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","Can you explain more about why you want to skip this most of the time? Your dbt project shouldn't depend on nodes running an exact number of times â€“ or on specific days â€“ for it to be correct; it should be built in an [idempotent](https://docs.getdbt.com/terms/idempotent) manner. 

Also keep in mind that when a node is disabled, it also can't be `ref`'d by downstream models, so it would always have to be the final model in your DAG. 

[quote=""AmbervanA, post:1, topic:6675""]
This is because we donâ€™t want other colleagues to add the --exclude command to every dbt run. So we prefer to have a solution where we can run the â€˜dbt runâ€™ but without Model Month. Ones a month we would like to run the Model Month without changing any settings.
[/quote]
Your colleagues should be working in their own development environments, so this should only impact your production jobs which you should be able to set up once and then leave alone for the most part. 

With all that said, you could use a YAML selector and set the [`default` property](https://docs.getdbt.com/reference/node-selection/yaml-selectors#default). This means that when you do a standard `dbt run` or `dbt build` without any other selection commands, it will exclude the monthly model. You would then have a separate job configured which is `dbt run --select monthly_only_model`. 

```yaml
selectors:
  - name: everything_except_the_monthly
    description: ""All nodes except the monthly one""
    default: true
    definition:
      union:
        - method: fqn
          value: ""*""
        - exclude:
            - method: fqn
              value: monthly_only_model
```",2
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","Hi Joel,

Thanks for you reply!

We would like to skip this most of the time, because the data only needs to be refreshed ones per month. We don't get new data in the intermediate period. In the model is a lot of complex sql code, so for the performance we would like to skip this file in the normal dbt run.

Thanks for the suggestion, I will try it asap.",3
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","I am not sure if the selectors can fix the problem, because you still have to call the command dbt run --selector everything_except_the_monthly. I want to call the command dbt run and by default it excludes the monthly_model instead of excluding the the model in the command. Is that possible?",4
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","[quote=""AmbervanA, post:3, topic:6675""]
We donâ€™t get new data in the intermediate period. In the model is a lot of complex sql code, so for the performance we would like to skip this file in the normal dbt run.
[/quote]

Have you considered an [incremental model](https://docs.getdbt.com/docs/build/incremental-models)? This is a perfect use case - you can put a check in for new data, something like 

```sql
with upstream as (
  select * from {{ ref('upstream_complex_table') }}
)

select 
  -- complex transformations here... 
from upstream
{% if is_incremental() %}
  where upstream.loaded_at > (select max(this.loaded_at) from {{ this }} as this)
{% endif %}
```

[quote=""AmbervanA, post:4, topic:6675""]
I want to call the command dbt run and by default it excludes the monthly_model instead of excluding the the model in the command. Is that possible?
[/quote]
It is possible, but you need to set the `default` property described above. When you do you'll see this output: 
```
$ dbt run
Running with dbt=1.4.1
[...]
Using default selector everything_except_the_monthly
```",5
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","Would tags work?
Tag everything with 'monthly', then tag everything excepting the 'model month' model with 'daily'. then in the daily run, call tag:daily, and on the monthly run call tag:monthly",6
6675,"exclude-model-from-dbt-run-by-default-but-still-want-to-run-the-model-separately","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
6676,"understanding-the-components-of-the-dbt-semantic-layer","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/understanding-the-components-of-the-dbt-semantic-layer",1
6677,"how-we-shaved-90-minutes-off-our-longest-running-model","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/how-we-shaved-90-minutes-off-model",1
6678,"power-up-your-data-quality-with-grouped-checks","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/grouping-data-tests",1
6679,"generating-surrogate-keys-across-warehouses","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/sql-surrogate-keys",1
6680,"how-we-calculate-time-on-task-the-business-hours-between-two-dates","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/measuring-business-hours-sql-time-on-task",1
6681,"how-to-upgrade-dbt-versions-mostly-without-fear","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/upgrade-dbt-without-fear",1
6682,"strategies-for-change-data-capture-in-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/change-data-capture",1
6683,"the-exact-github-pull-request-template-we-use-at-dbt-labs","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/analytics-pull-request-template",1
6684,"surrogate-keys-in-dbt-integers-or-hashes","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/managing-surrogate-keys",1
6684,"surrogate-keys-in-dbt-integers-or-hashes","Great article, I do have a question:

BI tools like (Azure) Analysis Services & Power BI perform best when join columns have a small datatype (like int) and memory usage is reduced as much as possible. Importing a pure star schema with hashed values as PK/FKâ€™s in to these tools wouldnâ€™t be the best solution especially not with wide fact tables with many dimension FKâ€™s.

Since last year databricks / delta lake supports auto increment [generated columns](https://www.databricks.com/blog/2022/08/08/identity-columns-to-generate-surrogate-keys-are-now-available-in-a-lakehouse-near-you.html). 


Is it possible to include this column in the model of a dimension, I did some attempts but werenâ€™t able to succeed. I wonder if there is a way to hook in to the create table statement of the dimension and add a line like: 

pk_customer_id BIGINT GENERATED ALWAYS AS IDENTITY

Looking forward to your reply.",2
6684,"surrogate-keys-in-dbt-integers-or-hashes","",3
6685,"autoscaling-ci-the-intelligent-slim-ci","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/intelligent-slim-ci",1
6685,"autoscaling-ci-the-intelligent-slim-ci","Is there a dbt version minimum for this solution?",2
6685,"autoscaling-ci-the-intelligent-slim-ci","",3
6686,"leverage-accounting-principles-when-modeling-financial-data","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/financial-modeling-accounting-principles",1
6687,"migrating-from-stored-procedures-to-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/migrating-from-stored-procs",1
6688,"dateadd-sql-function-across-data-warehouses","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/sql-dateadd",1
6689,"the-case-against-git-cherry-pick-recommended-branching-strategy-for-multi-environment-dbt-projects","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/the-case-against-git-cherry-picking",1
6690,"enforcing-rules-at-scale-with-pre-commit-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/enforcing-rules-pre-commit-dbt",1
6691,"the-spiritual-alignment-of-dbt-airflow","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment",1
6692,"the-exact-dbt-commands-we-run-in-production","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-production-commands",1
6693,"dbt-machine-learning-what-makes-a-great-baton-pass","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/maching-learning-dbt-baton-pass",1
6694,"towards-an-error-free-union-all","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/sql-union-all",1
6695,"making-dbt-cloud-api-calls-using-dbt-cloud-cli","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/making-dbt-cloud-api-calls-using-dbt-cloud-cli",1
6696,"demystifying-event-streams-transforming-events-into-tables-with-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/demystifying-event-streams",1
6697,"updating-our-permissioning-guidelines-grants-as-configs-in-dbt-core-v1-2","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/configuring-grants",1
6698,"an-introduction-to-unit-testing-your-dbt-packages","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/unit-testing-dbt-packages",1
6699,"date-trunc-sql-function-why-we-love-it","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/date-trunc-sql",1
6700,"your-essential-dbt-project-checklist","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/essential-dbt-project-checklist",1
6701,"how-to-move-data-from-spreadsheets-into-your-data-warehouse","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/moving-spreadsheet-data",1
6702,"founding-an-analytics-engineering-team","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/founding-an-analytics-engineering-team-smartsheet",1
6703,"analysts-make-the-best-analytics-engineers","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/analysts-make-the-best-aes",1
6704,"a-star-generator-is-born","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/star-sql-love-letter",1
6704,"a-star-generator-is-born","Hello, Is there a guaranteed order for returned columns by the Star macro and if yes, is it the ordinal position of the columns? Thanks.",2
6704,"a-star-generator-is-born","",3
6705,"tackling-the-complexity-of-joining-snapshots","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/joining-snapshot-complexity",1
6706,"coalesce-sql-function-why-we-love-it","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/coalesce-sql-love-letter",1
6707,"stakeholder-friendly-model-names-model-naming-conventions-that-give-context","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/stakeholder-friendly-model-names",1
6707,"stakeholder-friendly-model-names-model-naming-conventions-that-give-context","With the introduction of dbt metrics,
I suppose that suggesting a [`__daily`](https://docs.getdbt.com/blog/stakeholder-friendly-model-names#:~:text=can%20add%20clarity.-,__daily,-or%20__pivoted%20will) suffix might conflict with the suggestion in the [best practices/how we structure](https://docs.getdbt.com/guides/best-practices/how-we-structure/4-marts#:~:text=%E2%9C%85%C2%A0Name%20by%20entity.%20Use%20plain%20English%20to%20name%20the%20file%20based%20on%20the%20concept%20that%20forms%20the%20grain%20of%20the%20mart%20customers%2C%20orders.%20Note%20that%20for%20pure%20marts%2C%20there%20should%20not%20be%20a%20time%20dimension%20(orders_per_day)%20here%2C%20that%20is%20typically%20best%20captured%20via%20metrics.) that time granularities are best captured with metrics.",2
6707,"stakeholder-friendly-model-names-model-naming-conventions-that-give-context","",3
6709,"making-the-leap-from-accountant-to-analytics-engineer","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/sam-foundry-experience",1
6711,"optimizing-dbt-models-with-redshift-configurations","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/redshift-configurations-dbt-model-optimizations",1
6712,"how-to-build-a-mature-dbt-project-from-scratch","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/how-to-build-a-mature-dbt-project-from-scratch",1
6713,"welcome-to-the-dbt-developer-blog","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/welcome",1
6714,"narrative-modeling-how-structure-can-tell-a-story","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/narrative-modeling",1
6715,"introducing-the-dbt-cloud-api-postman-collection-a-tool-to-help-you-scale-your-account-management","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-cloud-api-postman-collection-announcement",1
6716,"on-the-importance-of-naming-model-naming-conventions-part-1","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/on-the-importance-of-naming",1
6717,"how-we-cut-our-tests-by-80-while-increasing-data-quality-the-power-of-aggregating-test-failures-in-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/aggregating-test-failures-with-dbt",1
6718,"slim-ci-cd-with-bitbucket-pipelines","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/slim-ci-cd-with-bitbucket-pipelines",1
6719,"so-you-want-to-build-a-dbt-package","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/so-you-want-to-build-a-package",1
6720,"whats-a-primary-key-and-why-do-we-test-them","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/primary-key-testing",1
6722,"the-jafflegaggle-story-data-modeling-for-a-customer-360-view","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/customer-360-view-identity-resolution",1
6723,"how-we-structure-our-dbt-projects","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/how-we-structure-our-dbt-projects",1
6724,"how-to-create-near-real-time-models-with-just-dbt-sql","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/how-to-create-near-real-time-models-with-just-dbt-sql",1
6725,"how-we-remove-partial-duplicates-complex-deduplication-to-refine-your-models-grain","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/how-we-remove-partial-duplicates",1
6728,"the-missing-role-of-design-in-analytics","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/design-for-analytics-towards-analytical-applications",1
6729,"how-i-study-open-source-community-growth-with-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/open-source-community-growth-analysis",1
6730,"from-the-slack-archives-when-backend-devs-spark-joy-for-data-folks","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/when-backend-devs-spark-joy",1
6735,"accessing-state-from-jinja-context","Hello,

We're trying to develop a QA test that compares a table in the QA environment with the corresponding table in the Prod environment. The goal here is to check, for example, if some rows are missing from the QA version when we know they should be there (because they are in Prod and we know Prod is correct).

So the question is:  when we run `dbt build -s my_table --target qa` , is it possible to reference the version of `my_table` that lives in Prod?  I've tried passing the state from the prod manifest (e.g., `dbt build -s my_table --target qa --defer --state /path/to/prod/manifest`), but upon inspection of the `graph` context var, all values of the `my_table` node point to the QA database/schema, not the prod one.

Am I missing something here? Is it bad practice to do this comparison between QA and Prod in this way?

Thanks!",1
6735,"accessing-state-from-jinja-context","As of 1.4.0, there is a new `--favor-state` flag which I think will do what you want. <s>It doesn't look like it got documented, but check out the issue for it: https://github.com/dbt-labs/dbt-core/issues/5016</s> 

Documented here: <https://docs.getdbt.com/reference/node-selection/defer#favor-state>",2
6739,"dbt-cloud-multiple-execution-projects","I'm using dbt cloud, trying out the single developer version. My DW is BQ. 

I wish to add a deployment environment which would be a different project and dataset to that of my development environment.

I can work around this using a macro set the target name etc. However I want the execution context (billing and slots usage) to be against the target project. 

In dbt CLI this seems possible by setting this in the profiles.yml

https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup#execution-project

Is there any way to do this with dbt Cloud? Why isn't the profiles.yml available?

I see demo videos of people adding an environment and setting the deployment credentials. However, I only get the option to set a dataset:

![image|662x500](upload://aP9JA8tyRWHxK91EzMKwASXGZld.png)",1
6744,"how-can-i-run-metrics-and-store-results-on-data-platform-using-dbt-core","How can I run metrics and store results on data platform using dbt core?",1
6744,"how-can-i-run-metrics-and-store-results-on-data-platform-using-dbt-core","You can materialize your dbt_metrics query as a model, like any other macro. Keep in mind that it will not be dynamic though, so you will effectively be making a single slice of an OLAP cube",2
6745,"extract-sql-function-why-we-love-it","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/extract-sql-love-letter",1
6746,"python-model-on-dbt-cloud","Hi,

I am trying the deploy my python model and I am getting error ""Schema has to be provided to write_pandas when a database is provided"". It works fine in my local. Is there any additional set up I am missing. I have also used Snowpark to build a ML model.

I have configured the dbt_project.yml file with DB and schema.

      python:
        +database: 
        +schema: 

Thanks Appreciate your help!!",1
6751,"snapshots-and-incremental-models-implementation-in-dbt-with-bigquery","Hi,

I'm looking for information about implementing snapshots and incremental model in dbt, using Bigquery.
I've read the documentation of [snapshots](https://docs.getdbt.com/docs/build/snapshots) and [incremental](https://docs.getdbt.com/docs/build/incremental-models)  but there are no solution codes for **bigquery** users, just for *snowflake*.


If you're familiar with the combination of DBT and Bigquery, or you could direct me to related information - I would love to hear.

Thanks, Appreciate your help :)",1
6751,"snapshots-and-incremental-models-implementation-in-dbt-with-bigquery","Hello @avigailg 

Please have a look below notes to start with.

https://medium.com/@minhajpasha/detail-explanation-of-incremental-model-in-dbt-afba491c8817

Regards
Minhaj",2
6751,"snapshots-and-incremental-models-implementation-in-dbt-with-bigquery","thanks for the information!",3
6752,"dbt-variable-defined-in-dbt-project-yml-cant-be-used-in-same-dbt-project-yml-file","
Hello 
@joellabes

Iam Unable to used the variable defined in dbt_project.yml  in the same file.

I have defined the variable in dbt_project.yml file as below

vars:
    gcp_project_id: 'gcp-dev-sandbox'
    local_time_zone: 'Europe/London'
    dag_id: ""customers_load""

I want to use the the variable dag_id to create target folder as below

target-path: ""target/{{ var('dag_id') }}"" 


When i run it as below it fail's

--> dbt run --select tag:customers_load

  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/utils.py"", line 196, in _deep_map_render
    ret = func(value, keypath)
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/renderer.py"", line 143, in render_entry
    result = super().render_entry(value, keypath)
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/renderer.py"", line 33, in render_entry
    return self.render_value(value, keypath)
  File ""/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/renderer.py"", line 45, in render_value
    raise CompilationException(msg) from exc
dbt.exceptions.CompilationException: Compilation Error
  Could not render target/{{ var('dag_id') }}: Required var 'dag_id' not found in config:
  Vars supplied to <Configuration> = {}
09:46:53  Encountered an error:
Compilation Error
  Could not render target/{{ var('dag_id') }}: Required var 'dag_id' not found in config:
  Vars supplied to <Configuration> = {}

Alternatively 
i need to run it to pass the variable  in dbt run , this make life complex as we need to pass var for each run if we are doing to test locally.

--> dbt run --select tag:customers_load --vars {""dag_id"": ""customers_load""}",1
6759,"multiple-sources-into-one-data-vault-hub","
## The problem I'm having
I'm trying to insert records into one data vault hub from multiple models (based on different ERPs each with their own extract/refresh timeline). I know a union all is possible, but that means maintaining the code every time something new is added, making the code more complex. I would rather that I could just have the equivalent of a single insert statement (however that is done in dbt), but dbt doesn't allow multiple models to insert into one table and raises an error on compilation. 

## The context of why I'm trying to do this
I don't know eventually how many ERP data sets I need to bring in

## What I've already tried 
I've used both the scalefree and dbtvault packages. 

## Some example code or error messages
```Compilation Error
  dbt found two resources with the database representation ""FOO.BAR.hub_contacts"".
  dbt cannot create two resources with identical database representations. To fix this,
  change the configuration of one of these resources:
  - model.ingest.foo_hub_party (models/raw_vault/foo_hub_party.sql)
  - model.ingest.foo_hub_contacts (models/raw_vault/foo_hub_contacts.sql)```",1
6764,"october-2021-dbt-update-metrics-and-hat-tricks","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-product-update-2021-october",1
6766,"how-to-handle-sql-exceptions-in-dml-statements","## The problem I'm having
As senior oracle database developer with +20 experience working in complex business supported in oracle and batch jobs, i kept finding most batch LT/TL of inbound files jobs would fail as sql dml hard exception due to errors in data file). 
Meanwhile I jumped into cloud data engineering and been very curious how the zillions of open sources handle a database sql exception.
I understand the basics of dbt macro/template idea and found it interesting, but i wonder whats recommended way loading file with good and bad records.

## The context of why I'm trying to do this
In batch job loading file with 1 million rows, imagine one record causes exception in a dml statement (e.g. INSERT/SELECT).

## What I've already tried 
Oracle databases support this amazing well by supplying a 2nd target table where rows causing exceptions will get inserted, just add LOG <exceptions_table> as part of an Oracle SQL statement like INSERT/SELECT ..LOG myExceptionsTable. But most databases not supporting this and given this is one of main causes of breaking pipelines when loading files - specially from external sources - So i wonder whats dbt pattern for this.",1
6766,"how-to-handle-sql-exceptions-in-dml-statements","yeah - I have a similar recent post in ""Can Dbt do this...?""  I cannot find where there is excepton handing in the Dbt model architecture.  When I write snowflake stored procedures, I can do anything.",2
6766,"how-to-handle-sql-exceptions-in-dml-statements","FYI sharing ChatGPT reply to my own question:

> Blockquote
Question: How to handle the problem of sql exception in a dml statement using dbt

Answer from ChatGPT: DBT (Data Build Tool) is a popular open-source tool for building, testing, and deploying data pipelines. Here are some common ways to handle exceptions in a DBT data pipeline:

- Use the on_error Configuration: The on_error configuration in a DBT model allows you to specify what action should be taken in the event of an error during the execution of a model. The available options are fail, warn, and ignore. For example, if you set on_error: fail, DBT will stop executing the pipeline if an error is encountered during the execution of a model.
- Write Custom Macros: Macros in DBT allow you to encapsulate common logic and reuse it across multiple models. You can write custom macros to handle specific exceptions in your pipeline and ensure that the pipeline continues to run smoothly.
- Use the {% exception %} Template Tag: The {% exception %} template tag allows you to handle exceptions within your DBT templates. For example, you can use the {% exception %} tag to catch any exceptions that occur during the execution of a template and handle them appropriately.
- Logging and Monitoring: DBT has built-in logging capabilities, which allow you to capture any error messages that occur during the execution of a pipeline. You can also use third-party tools to monitor your DBT pipelines and receive notifications in the event of an error.
- Test Your Models: Writing comprehensive tests for your DBT models can help you catch and resolve exceptions before they reach production. DBT includes built-in support for testing and allows you to write tests using SQL or Python.

By following these best practices, you can effectively handle exceptions in your DBT data pipeline and ensure that your pipeline continues to run smoothly.

> Blockquote",3
6767,"november-2021-dbt-update-v1-0-environment-variables-and-a-question-about-the-size-of-waves","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-product-update-2021-november",1
6776,"insert-to-a-table-using-loops","Hey All,
I have a use case that I need to iterate on an sql query and insert the results each iteration to an existing table.
for example I want to run it on a period of 3 days, so each iteration will have a different date 
so for example, {{loop_date}} will be 1.1.2023 on the first iteration,
then on the second iteration it will be 2.1.2023 (January 2nd)
and last, 3.1.2023 (January 3rd).
with this WHERE clause:
```
date_field BETWEEN '{{loop_date}}'  - INTERVAL '1 months' AND '{{loop_date}}' - 1
```
can someone share a similar code? and also how to send the timeframe for the loop? I tried with ""date_spine"" util and also something is not working fully.
I know there is ""insert_by_period"" but it is not supported for me.

thanks for your help",1
6776,"insert-to-a-table-using-loops","Hi, How are you able to insert in  existing tables? I am having problem in that bit.",2
6788,"dbt-serialization-errors","I'm having a lot of DBT serialization errors: ""Serializable isolation violation on table."" Sometimes instead the job just hangs indefinitely. We are using Redshift as the database.

Is there any way help alleviate this? We have tried implementing some incremental models, but it doesn't seem to speed up the write enough for the table locks to not form. I'm wondering if perhaps there is something in the config that is missing, or maybe we are using too many views and not enough tables? 

Any tips or tricks would be greatly appreciated, thanks!",1
6791,"dbt-with-snowflake-onboarding","Hi,

I'm looking for some guidance on the training needed to onboard both dbt cloud and Snowflake.
My team is made up of data analysts and data engineers who build and support our company data warehouse.  We have zero experience in these technologies at the moment as we are mostly SQL Server/SSIS experts.

Snowflake recommend their 4 days fundamentals course but I'm not sure of the value of this for all team members if they are spending most of their time in dbt.  Would it be better to invest more time in dbt training and maybe just train a couple of people on Snowflake training to support any back end specific issues that arise?

Can anyone with the dbt/Snowflake combination advise what they do?

Thanks in advance,  Jon",1
6792,"can-dbt-do-this-vs-using-a-stored-procedure","The below is an except from a snowflake stored procedure I wrote, which works great.  It backs up data, then deletes old pre-existing data and then inserts new data.  If an error occurs during delete/insert, it restores the data from backup, thus enabling idempotency.   Note:  The delete + insert is not a merge.  It might delete 100 rows and then insert 85.  Or visa versa.

If this were DBT-ized, instead of using a stored procedure, would I be able to acheive the same level of data protection?  If so, how do would this functionality working in DBT, with the backup and restore?

Thanks,

Dave

-- backup data in the table.  snowflake has unique commands for this...
drop table if exists FINANCE.CONFIGURATION_HIST_BAK;
create table FINANCE.CONFIGURATION_HIST_BAK clone FINANCE.CONFIGURATION_HIST;


-- delete old and then inert new (think replace not merge)incomming data.  Cannot use merge specifically
      BEGIN
                              -- delete -pre-existing data
            DELETE FROM FINANCE.CONFIGURATION_HIST  
            WHERE CLOSING_MONTH = :VAR_CLOSING_MONTH;
                              --insert new data
            INSERT INTO FINANCE.CONFIGURATION_HIST
            (COUNTRY_CODE, SUBSIDIARY, COMBO, ENTRY_NO, CLOSING_MONTH)
            SELECT COUNTRY_CODE, SUBSIDIARY, COMBO, ENTRY_NO, :VAR_CLOSING_MONTH
            FROM FINANCE.CONFIGURATION_TEMP;
      EXCEPTION
            WHEN OTHER THEN
-- restore table data on any error
                  ALTER TABLE FINANCE.CONFIGURATION_HIST SWAP WITH 
                                             FINANCE.CONFIGURATION_HIST_BAK;        
                  VAR_BUS_ERROR := sqlerrm || ': Error Loading 
                                              ENTRY_NO_CONFIGURATION_HIST';          
                  raise BUS_ERROR_EXC;
      END;",1
6792,"can-dbt-do-this-vs-using-a-stored-procedure","I received confrmation from DBT that DBT cannot do this.  There are no exception handlng capabilities in DBT.",2
6799,"september-2021-dbt-update-dag-in-the-ide-metadata-api-in-ga","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/dbt-product-update-2021-september",1
6802,"carry-forward-attributes-from-an-existing-record-in-destination-model","## The problem I'm having
I have multiple staging models that provide attributes for my destination model/table record. This destination model doesn't exactly fit into a dimension design, but that's another matter.
The situation is that, for the KEY columns in play, not every staging model will have a record.
Key columns = Natural Key + Start Effective Date + System Date

 When building a record joining all staging models, there are going to attributes that are null, if that model does not have a record for the KEY.
However, the ask is that we bring/carry forward the missing attributes from the record having the prior start effective date.

## What I've already tried 
I've explored snapshots, but the destination table is not set up to be a true type 2 dimension and thus having a snapshot between the staging /intermediate models and the final model *might* not work. 
The deviation from a true type 2 in this instance is cause the destination table is designed to hold multiple records for the same natural Key + Start Effective Date + End Effective Date, with the System Date providing the uniqueness on the record.
The End Effective Date however is determined solely by the appearance of a new Start Effective Date value.

## Example
#### Day 1: 

Destination Table data
Natural Key: ABC
Eff Date: 1/1/2023
System Date: 1/2/2023
Stage_1_attrbute: XYZ
Stage_2_attrbute: 123
End Eff Date: NULL

#### Day 2: 

Stage_1 : no records for ABC

Stage_2 record:
Natural Key: ABC
Eff Date: 1/3/2023
System Date: 1/4/2023
Stage_2_attrbute: 456

Intermediate model results using Stage 1 & 2:
Natural Key: ABC
 Eff Date: 1/3/2023
System Date: 1/4/2023
Stage_1_attrbute: NULL
Stage_2_attrbute: 456

Destination Table data (two records now)
Natural Key: ABC
Eff Date: 1/1/2023
System Date: 1/2/2023
Stage_1_attrbute: XYZ
Stage_2_attrbute: 123
End Eff Date: 1/2/2023 -- End date populated.

-- New record
Natural Key: ABC
Eff Date: 1/3/2023
System Date: 1/4/2023
Stage_1_attrbute: XYZ -- Carryforward from prior record
Stage_2_attrbute: 456
End Eff Date: NULL

TIA",1
6803,"tips-and-advice-to-study-for-and-pass-the-dbt-certification-exam","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/tips-for-the-dbt-certification-exam",1
6811,"corrupted-data-load-from-gcp-hosted-mysql-db","## The problem I'm having

Hey everybody! I have a dbt <-> BQ question. In a nutshell the problem is - dbt ingests data from a prod replica DB - we use `EXTERNAL_QUERY` command

1. Data gets loaded from our GCP-hosted MySQL DB
2. One or two rows is loaded incorrectly/corrupted, e.g. `is_bool_column` gets loaded as null instead of 0/1
3. We perform group by on the column within our dbt transformations
4. :x: we get a duplicated record for an order/retailer/... :x: 

## What I've already tried 

There is already a GH issue and I've included all detailed information here - https://github.com/dbt-labs/dbt-bigquery/issues/534

If there are any additional questions, please let me know, happy to provide as much context as possible!",1
6813,"surrogate-key-dbt-upgrade","I just did a DBT upgrade from v.1.0 -> v1.3 and also upgraded all dependencies, packages along with it as things were breaking.   I'm facing a recurring problem in ""dbt seed"" and macros in each run - where a surrogate key function is failing repeatedly

Need to fix this so upgrade goes smoothly

Error:- 
```
dbt seed
10:26:16  Running with dbt=1.3.2
10:26:16  Partial parse save file not found. Starting full parse.
Compilation Error in model website_sessionized (models/marts/marketing/attribution/website_sessionized.sql)
  
  Warning: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`. The new macro treats null values differently to empty strings. To restore the behaviour of the original macro, add a global variable in dbt_project.yml called `surrogate_key_treat_nulls_as_empty_strings` to your dbt_project.yml file with a value of True. The vauld_analytics.website_sessionized model triggered this warning. 
  ```

## What I've already tried 

Previous code :-
```
{{ dbt_utils.surrogate_key(['anonymous_id', 'session_number']) }} AS session_id 
```

New code :- 
```
{{ dbt_utils.generate_surrogate_key(['anonymous_id', 'session_number']) }} AS session_id # 
```

Also, tried adding the global variable in project.yml

```
#1.3 v fix
    #surrogate_key_treat_nulls_as_empty_strings: true - not working
```

Not sure what's the solution here to add the new command for surrogate key - any help is appreciated",1
6813,"surrogate-key-dbt-upgrade","I recommend you double-check if you have no leftover usages of the legacy `dbt_utils.surrogate_key`, as according to the warning, you have one in `website_sessionized`. 
Maybe this model is using a macro that uses the legacy function, I'd check it out too.

As for your latest question, you probably would like to add it for backward compatibility. Since the legacy function was treating null as if they were empty strings, in order to yield the same results as before, you would like to use this flag.",2
6813,"surrogate-key-dbt-upgrade","Thanks tomer, I think I found the main issue and fixed it - it was just replacing legacy with new **generate_surrogate_key** and with the macro definition in yml file clearly at top

However, getting a new error on another object that's causing an issue

`'dict object' has no attribute 'datediff'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with ""dbt deps"".`",3
6816,"schema-name-is-not-correct","We are running a model (eg: dbt run -m sales.sql"" ) having schema defined at properties.yml, , say it is mdel_schema. And profile too has the schema defined, say it is prof_schema.

When the job runs, sales view is created with schema name as prof_schema_mdel_schema. Schema name in the model is not getting overwritten, however it appends the names from profile and property.

Is there any option available to use the property schema alone?",1
6816,"schema-name-is-not-correct","[quote=""GaneshSubramani, post:1, topic:6816""]
dbt run -m sales.sql
[/quote]

Try using `dbt run -s sales` instead of `dbt run -m sales.sql`",2
6816,"schema-name-is-not-correct","Hi

Hi @GaneshSubramani,

You can override the default `generate_schema_name` macro
https://docs.getdbt.com/docs/build/custom-schemas#how-does-dbt-generate-a-models-schema-name

So, instead of using the macro like this
```
{% macro generate_schema_name(custom_schema_name, node) -%}

    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}

        {{ default_schema }}

    {%- else -%}

        {{ default_schema }}_{{ custom_schema_name | trim }}

    {%- endif -%}

{%- endmacro %}
```
You can create your own macro like this
```
{% macro generate_schema_name(custom_schema_name, node) -%}

    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}

        {{ default_schema }}

    {%- else -%}

        {{ custom_schema_name | trim }}

    {%- endif -%}

{%- endmacro %}
```",3
6816,"schema-name-is-not-correct","Thanks for the help, this worked as expected.",4
6816,"schema-name-is-not-correct","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
6819,"custom-test-output-compiled-sql-which-has-something-wrong","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
Hi, I'm trying to use custom test. 
I put this code under `macros` and name `test_is_between.sql`.
```
{% macro test_is_between(model, column_name, bottom_number, top_number) %}

with validation as (
    select
        {{ column_name }} as field_to_test
    from {{ model }}
),
validation_errors as (
    select
        field_to_test
    from validation
    where field_to_test > {{ top_number }} or field_to_test < {{ bottom_number }}
)

select count(*)
from validation_errors

{% endmacro %}
```

Then I prepare yaml file like this.
```
version: 2
sources:
  - name: debug
    database: bq-hoge-project
    schema: bq-fuga-dataset
    tables:
      - name: hourly_check
        columns:
          - name:hoge_column
            tests:
              - is_between:
                  bottom_number: -180
                  top_number: 180
```
Finally when I run `dbt test`, it output compiled sql. But the output file name is `source_is_between_debug_hourly_09e34fb4230ff5d8c8d0a11364f72e47.sql`.

What's is going on ? I don't think it is normal. How can I get compiled file correctly?

## The context of why I'm trying to do this
I just want to check test results, after I run `dbt test --store-failures`.
But the file names has some problems so that I couldn't get correct name of the table in audit dataset.
 
Thank you",1
6819,"custom-test-output-compiled-sql-which-has-something-wrong","For starters, I think your test is defined incorrectly. It should be `test` instead of `macro` (details here: <https://docs.getdbt.com/guides/best-practices/writing-custom-generic-tests>)

And, slightly related, the test you're attempting to write already exists in the `dbt-expectations` package (along with _ma_ny other very useful tests). Check it out here: <https://github.com/calogica/dbt-expectations#expect_column_values_to_be_between>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1676558146060339?thread_ts=1676557465.572529&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6823,"dbt-cloud-cannot-edit-read-only-editor","Just singed up for the first time on dbt cloud and I'm trying to work on a project, there is one issue with permission that I have no idea what wrong!! 

Cannot edit on my dbt cloud editor, I tried few options but have no idea why is not letting me edit. 

I tried to edit my ""user"" under ""account settings"", which is a developer with these roles: ""owner"", ""member"", ""everyone"". I tried to edit user details, but the button is disabled! 

Anyone has any idea? 

Thank you in advance.",1
6823,"dbt-cloud-cannot-edit-read-only-editor","Same here. I have an old project I hadn't visited for a few months and now it is stuck in read only mode. I alos need help with tis issue. I am the only user on this account and project.",2
6823,"dbt-cloud-cannot-edit-read-only-editor","If you are on the `main` branch, you will need to create a new branch before being able to edit your project. dbt cloud doesn't let you edit code on the main branch to encourage good branching hygiene",3
6823,"dbt-cloud-cannot-edit-read-only-editor","My github integration had been revoked so I was not able to check in changes and create a new branch. I started a new account with a new repository and now I am able to work. Thanks.",4
6824,"dbt-compile-fails-when-ran-dbt-ran-saved-manifest-not-found","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
When I ran dbt ran or dbt compile, I got an error.
 
Encountered an error:
'dict' object is not callable
14:26:50  Traceback (most recent call last):
  * File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\main.py"", line 135, in main
*     results, succeeded = handle_and_check(args)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\main.py"", line 198, in handle_and_check
*     task, res = run_from_args(parsed)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\main.py"", line 245, in run_from_args
*     results = task.run()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\task\runnable.py"", line 454, in run
*     self._runtime_initialize()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\task\runnable.py"", line 165, in _runtime_initialize
*     super()._runtime_initialize()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\task\runnable.py"", line 94, in _runtime_initialize
*     self.load_manifest()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\task\runnable.py"", line 81, in load_manifest
*     self.manifest = ManifestLoader.get_full_manifest(self.config)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\manifest.py"", line 203, in get_full_manifest
*     manifest = loader.load()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\manifest.py"", line 339, in load
*     self.parse_project(
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\manifest.py"", line 467, in parse_project
*     parser.parse_file(block)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\base.py"", line 425, in parse_file
*     self.parse_node(file_block)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\base.py"", line 386, in parse_node
*     self.render_update(node, config)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\models.py"", line 362, in render_update
*     super().render_update(node, config)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\base.py"", line 362, in render_update
*     context = self.render_with_context(node, config)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\parser\base.py"", line 240, in render_with_context
*     get_rendered(parsed_node.raw_code, context, parsed_node, capture_macros=True)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\clients\jinja.py"", line 590, in get_rendered
*     return render_template(template, ctx, node)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\dbt\clients\jinja.py"", line 545, in render_template
*     return template.render(ctx)
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\jinja2\environment.py"", line 1301, in render
*     self.environment.handle_exception()
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\jinja2\environment.py"", line 936, in handle_exception
*     raise rewrite_traceback_stack(source=source)
*   File ""<template>"", line 3, in top-level template code
*   File ""C:\Users\xli\Anaconda3\envs\dbt-env\lib\site-packages\jinja2\sandbox.py"", line 393, in call
*     return __context.call(__obj, *args, **kwargs) ot callable

## The context of why I'm trying to do this
I was trying to run my models after installed the dbt-labs/metrics

## What I've already tried 
I try dbt clean, but it doesn't work.",1
6829,"implementing-threads","Hello,

I want to implement threads via a job so that I can run multiple models at the same time in parallel. So I have an orchestration created which will run some around 30 models. Do I just go ahead and mention threads as 4, so that it will run the parent model first and then 4 models at a time so forth.",1
6831,"unload-data-from-snowflake-to-s3","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I created a model that unloads data from Snowflake to S3 and saves the unload times in an incremental model. It works just fine, however, I would like to take it to the next level and make it production-ready. I would be happy to do it using solely the **Snowflake <> dbt** eco-system but I'm not sure how to do it, therefore, I'm seeking some advice. 

## The context of why I'm trying to do this
As mentioned above, the objective is to store incremental records of successful unloads only. 
I already got the model up and running, however, I would like to make it work as an ""atomic unit"", which means, that if any part of the process fails, the entire procedure should halt **without leaving any trace**.
### Here is what I have so far:

```
{{ config(
    materialized='incremental',
    pre_hook = ""copy into @SOME_STAGE/path/ from (select * from {{ ref('my_source_model') }}""
) }}
select max(ts) as unload_ts from {{ ref('my_source_model') }}
where 1=1
    {% if is_incremental() %}
        and ts > (select max(ts) from {{ this }})
    {% endif %}
```
(tz is the timestamp I'm basing the incremental upon and it's the only column I would like to store in my new model)

As you can see, the model unloads data to an S3 bucket using a `pre-hook` that runs a Snowflake `COPY INTO` command.

## The basic scenarios I would like to resolve:
1. Suppose a Snowflake `copy into ` `s3://bucket/path` query is initiated using a `pre-hook`, but it fails. How can I prevent the model incremental build from continuing?
2. Suppose the pre-hook runs a Snowflake `copy into ` `s3://bucket/path` query successfully, but the actual incremental model build fails. How can I prevent the file from being stored in S3? 

## What I've already tried 
I found [this section](https://docs.getdbt.com/reference/resource-configs/pre-hook-post-hook#transaction-behavior), but I'm not really sure how to use it for my needs as for example in case 2 (mentioned above) I need to actually clean up the file(s) that the `pre-hook ` wrote to S3.

## To sum-up
I would genuinely appreciate ideas for achieving the objective mentioned above and solving the scenarios I presented. Thank you in advance for your time and expertise.",1
6835,"scalable-yaml-code-generation","Our dbt project has been growing very quickly over the past year, as we migrate many legacy data pipelines from their old form to dbt. Our data architecture involves support for many customers that exist in single-tenant databases, but where the many identically structured canonical data models exist in each customer's database. One of our challenges has been how best to scale out management of the content of our dbt project, especially in terms of managing many yaml files. 

We are turning to greatly expanding out code generation capabilities to help support this. So, for instance, we can have code generation that inspects some metadata that indicates what customers should have what data models/data pipelines, and then generate the corresponding assets (models, seeds, and corresponding yaml) in the corresponding locations in the dbt project. For the generation of yaml in particular, this is a little daunting, and we are hoping that others in the dbt community might be able to share approaches they have taken. Our current approach is simplistic:

* python code gen builds dictionaries in memory, use pyyaml's dump to output to corresponding yaml files
* In some cases, we will read in existing yaml files with pyyaml, append to it, write it back out to the same yaml file

Programmatically building the correct models in python using basic data structures is so far pretty cumbersome. I'm curious if anybody out there has worked on similar code-gen support, and if so, has anybody gone down the path of building out python classes that map to the components of the dbt yaml schemas (similar to what you might do with an ORM), or is aware of any open source projects that aim to facilitate that type of thing. I am thinking we might end up doing something like that, but would love not to re-invent something that might already exist somewhere else.",1
6835,"scalable-yaml-code-generation","Just to clarify an example of what I'm hoping exists out there somewhere. An analgous project might be https://pypi.org/project/swagger-to/, which can be used to generate language-specific bindings (as classes) to a specific schema (in that case, a swaggar spec which details the schema used for an api). Looking for similar for dbt yaml support.",2
6843,"using-multiple-databases-within-a-single-project","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I want to run two models on separate databases within the same project, each with its own set of credentials for two different environments. Is it possible to configure this setup using dbt?",1
6853,"dbt-run-command-fails","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
dbt run command fails

## The context of why I'm trying to do this
I'm learning dbt with BigQuery connection and following the tutorial

## What I've already tried 
copying the data table ID from BiqQuery into the dbl query:

my_firstdbt_model_sql:
`select * from `woofstr-7a206.datasetOne.tableone` limit 1000`

my_second_dbt_model_sql:
`-- Use the `ref` function to select from other models

select *
from {{ ref('woofstr-7a206.datasetOne.tableone') }}
where id = 1`

## Some example code or error messages
`07:13:33  Began running node model.my_new_project.my_second_dbt_model
07:13:33  2 of 2 START sql view model dbt_drhodes.my_second_dbt_model .................... [RUN]
07:13:33  Acquiring new bigquery connection 'model.my_new_project.my_second_dbt_model'
07:13:33  Began compiling node model.my_new_project.my_second_dbt_model
07:13:33  Writing injected SQL for node ""model.my_new_project.my_second_dbt_model""
07:13:33  Timing info for model.my_new_project.my_second_dbt_model (compile): 2023-02-18 07:13:33.031174 => 2023-02-18 07:13:33.044240
07:13:33  Began executing node model.my_new_project.my_second_dbt_model
07:13:33  Writing runtime sql for node ""model.my_new_project.my_second_dbt_model""
07:13:33  Opening a new connection, currently in state init
07:13:33  On model.my_new_project.my_second_dbt_model: /* {""app"": ""dbt"", ""dbt_version"": ""1.4.1"", ""profile_name"": ""user"", ""target_name"": ""default"", ""node_id"": ""model.my_new_project.my_second_dbt_model""} */


  create or replace view `woofstr-7a206`.`dbt_drhodes`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `woofstr-7a206`.`dbt_drhodes`.`my_first_dbt_model`
where id = 1;

07:13:33  BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: id at [10:7]')
07:13:35  BigQuery adapter: https://console.cloud.google.com/bigquery?project=woofstr-7a206&j=bq:US:1f2e02c4-6540-43b2-9bf0-1cae9995eab1&page=queryresults
07:13:35  Timing info for model.my_new_project.my_second_dbt_model (execute): 2023-02-18 07:13:33.044667 => 2023-02-18 07:13:35.166480
07:13:35  Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  Unrecognized name: id at [10:7]
  compiled Code at target/run/my_new_project/models/example/my_second_dbt_model.sql
07:13:35  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7527181-a24f-4b70-b5ac-eae91acc03a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afe94fb50>]}
07:13:35  2 of 2 ERROR creating sql view model dbt_drhodes.my_second_dbt_model ........... [ERROR in 2.14s]
07:13:35  Finished running node model.my_new_project.my_second_dbt_model`
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
6856,"full-table-scans-with-incremental-tables-in-bigquery","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

BigQuery doesn't support subqueries (or any dynamic data) in partition filters:
https://cloud.google.com/bigquery/docs/querying-partitioned-tables#better_performance_with_pseudo-columns

I'm using the default __config for incremental tables and it's creating a subquery to get the most recent record that was created from the current table.

Is there another workaround for incremental tables with bigquery to avoid full table scans?",1
6856,"full-table-scans-with-incremental-tables-in-bigquery","Hi @ca136,

Can you show the generated SQL in target/run, so I can understand better the problem?",2
6856,"full-table-scans-with-incremental-tables-in-bigquery","`merge` is a default behavior of incremental model .in Bigquery. DBT loads the data incrementally from source but scans full table to check matching unique key to update corresponding rows.
If you want to avoid full table scans you should use `insert+overwrite` strategy (https://docs.getdbt.com/reference/resource-configs/bigquery-configs#the-insert_overwrite-strategy). This will not scan table but drops associated partitions and loads the data in-place. For this strategy you should have partitioned table.",3
6859,"merge-insert-columns","Hi everyone,
 is there anyway that I can exclude specific columns when I use materialisation = ""incremental"" and strategy= ""merge"" , I used the below Configs:
config(
    materialized = 'incremental',
    incremental_strategy = 'merge',
    unique_key = 'key',
    ignore_columns= ['key']
  )
when its come to filter the inserted columns and not the updated column,
If there is a configuration like merge_update_columns, for inserted column ""merge_insert_columns""",1
6866,"set-up-dbt-on-windows","
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having is trying to set up the dbt connection via windows and anaconda. I am taking a **udemy course the complete dbt from zero to hero** and we set up our aws account and them we install, go to worksheets and run the ddl to create and load a sample database. Then we setup dbt. The instructions we are given is to use the address of the worksheets. They say it s/b in this format xxxxxx.us-east-2.aws. But my snowflake url is /us-east-2.aws/xxxxxxxx so of course it doesn't accept that and it doesn't accept xxxxxx.us-east-2.aws. When I run dbt debug I get the error

> Failed to connect to DB:Incorrect username or password was specified

So my question is what credentials do I use to setup dbt

## The context of why I'm trying to do this

## I have tried setting it up like this us-east-2.aws/xxxxxxx too but that doesn't work either. 


## Some example code or error messages
```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
6867,"does-dbt-have-exception-handling-for-dml-ddl-code","yeah - I have a similar recent post in ""Can Dbt do this...""  I cannot find where there is exception handing built into the Dbt model architecture, that would allow you to trigger the corresponding exception handling code (DML or DDL code).  When I write snowflake stored procedures, I can do anything in them, including proper exception handling.  I am thinking that Dbt should have a ""when_exception"" hook to deal with this.",1
6867,"does-dbt-have-exception-handling-for-dml-ddl-code","Hey @ddumas, 

When dbt hits a DML/DDL error, it stops execution of that model, reports it as a failure and will continue on to the rest of the project (although it will skip building downstream nodes). 

If you would like to suggest adding hooks to handle these exceptions, the best thing to do would be to [open an issue on the Core repo](https://github.com/dbt-labs/dbt-core/issues).",2
6867,"does-dbt-have-exception-handling-for-dml-ddl-code","Hey Joel,

Thanks - I just added that as a request, as you suggested.

Dave",3
6867,"does-dbt-have-exception-handling-for-dml-ddl-code","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6873,"multiple-developers-on-same-model-naming-tables-during-development","Say you have multiple developers working on a model, I'm assuming they are all needing to run their data to look at it to see how its working. How do most people handle this? I dont have a schema for each user so the best I can think of would be that if you are working on a new feature or general development, you would need to name the tables with your username? 

When it moves up to say ""preprod"" for testing, then I would expect a pipeline/svc account to execute it and this becomes a non-issue, same with ""prod.""

How do others handle this development work to test out their model is working and code is generating correctly?

Here is a sample structure I came up with, does this make sense or would you recommend something different?
ample Structure for Prod (POC likely slightly different):

Branch Name: Austins New Feature

* Deployment location: Workspace Schema (Shared by everyone)
* Automation â€“ none
* Table Name Examples:
  * {Username}_{ModelName}_stgClaims
  * {Username}_{ModelName}_intSumAndCount
  * {Username}_{ModelName}_fctDatamartName

Branch Name: Development

* Deployment location: PreProd Schema
* Automation â€“ On Commit AND/OR Schedule Defined
* Table Name Examples:
  * {PreProd}_{ModelName}_stgClaims
  * {PreProd}_{ModelName}_intSumAndCount
  * {PreProd}_{ModelName}_fctDatamartName

Branch Name: Master/Main

* Deployment location: Production Schema
* Automation â€“ On Commit AND/OR Schedule Defined
* Table Name Examples:
  * {ModelName}_stgClaims
  * {ModelName}_intSumAndCount
  * {ModelName}_fctDatamartName

How would I go about renaming the tables in this structure? I've read through custom aliases but haven't quite figured out how to do it.",1
6880,"how-to-configure-select-grants-on-tables-after-creation","I am new to dbt and I am trying to grant my tables select privileges after they have been created in my dbt code. So I am trying to create a SHARE between my snowflake account(my_SF) and another snowflake account(A_SF). when I create dbt models and materialize them as tables, they are created successfully. My objective is to share these tables in my snowflake account with another snowflake account. I created a SHARE(123) manually, which is configured to use my database (abc)and Schema(def). The code below is used to grant access to a table (deposit) in the schema

Grant select on table abc.def.deposit to share 123
All these seem to work. and the table was easily shared with the second snowflake account I added to the SHARE.

The problem is, when I run dbt run, the tables are created in my Snowflake account but not visible to the second snowflake account because the SELECT GRANT to the table has been deleted.

Is there a way I can GRANT SELECT privilege to all tables after creation in DBT? I use dbt version 1.0

I tried to use this post-hook but it didn't work


```
post-hook: |
  {% set schema = abc.def%}
  {% set share = '123' %}

  {% for table in adapter.list_relations(schema=schema, materialized='table') %}
    GRANT SELECT ON {{ schema }}.{{ table }} TO SHARE {{ share }};
  {% endfor %}
```",1
6880,"how-to-configure-select-grants-on-tables-after-creation","Hi @abbelylee, have a look at this article

https://discourse.getdbt.com/t/using-dbt-snowflake-shares/1110

let me know if it helps or if you are trying something else",2
6885,"error-when-same-variable-is-used-in-target-of-2-different-models","Hello folks, I am facing a weird issue. I'm trying to set the target table via a var. So my model has something like this:


```
{% set target_schema = var('target_schema') %}
{% set target_table = var('target_table') %}

config(
schema = target_schema,
alias = target_table
)
```

I passed the variables via command line. It worked fine when there was only one model. As soon as I replicated the same code to another model, DBT started giving me the below error:


```
Compilation Error
  dbt found two resources with the database representation ""`project_id`.`dataset_id`.`table_name`"".
  dbt cannot create two resources with identical database representations. To fix this,
  change the configuration of one of these resources:
```

I am passing different table names via var to both models, but still DBT is giving me this error. I believe this should not occur as same variable name across models doesn't mean same target name. Is there any solution to this?",1
6885,"error-when-same-variable-is-used-in-target-of-2-different-models","If I understand correctly, this is happening because when dbt compiles your project, the same variable value is fed into two different models, so when it compiles the information about your project as a whole it finds two database representations that are the same.

The critical information here is that even if you `dbt run -s model_one`, dbt compiles your whole project and so even though you're intending to pass info to one model at a time, dbt feeds the same variable to every model with that `var()`

It might help to step back and understand what you're trying to do and why, and then we can see if there might be a more dbtonic solution to your goals.",2
6885,"error-when-same-variable-is-used-in-target-of-2-different-models","@jaypeedevlin Thanks for the reply.

I am trying to setup airflow dags to be used with DBT. For each DBT model, there will be a dag in airflow which would trigger the model as per the schedule requirements. I am keeping variables like source_schema, source_table, target_schema, target_table in airflow variables, so that changes don't require editing the file which would also warrant version control.

I made a list config var in airflow with json object for each model. Of course, the key names are the same for each model. But due to this limitation, I am not able to keep the key names same which is where I am stuck.",3
6885,"error-when-same-variable-is-used-in-target-of-2-different-models","I would think your only solution given this setup would be to give each model it's own set of vars.

> so that changes donâ€™t require editing the file which would also warrant version control

It's worth keeping in mind that by thinking this way you're trying to circumvent something that's at the core of dbt's philosophy (analytics should be version controlled), so it's not _totally_ surprising that it's cumbersome to do in the tool.",4
6887,"github-actions-failing-immediately-after-connecting-to-warehouse","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

I'm trying to run a github actions workflow to publish my dbt doc to publish dbt docs to github pages

The it gets the github action run 'dbt docs generate' it fails.

I've tried changing the command to 'dbt build' and get the same error.  The command run fine locally and in dbt cloud.

The github action is 
```
 - name: generate documentatios
   run: dbt docs generate --target docs_prod
```

The error I get is:
```
Running with dbt=1.4.1

Unable to do partial parsing because saved manifest not found. Starting full parse.

Found 1 model, 2 tests, 0 snapshots, 0 analyses, 304 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics

Encountered an error:

Database Error

SQL compilation error:

syntax error line 1 at position 22 unexpected '.'.

Error: Process completed with exit code 2.
```",1
6893,"datediff-dict-object-errors-dbt-upgrades","Recently, upgraded my DBT to v1.3 and facing some issues with a few macros but a new compilation error keeps coming for datediff or datespine , which is not working out 

Error message:
```
09:29:41  Compilation Error in model android_track_sessionized
09:29:41    'dict object' has no attribute 'datediff'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with ""dbt deps"".
```

-- Same error shows up for datespine too, when changed

## The context of why I'm trying to do this
I'm trying to fix this so that my sessionization code works out
Source: https://github.com/dbt-labs/segment/blob/main/models/sessionization/segment_web_page_views__sessionized.sql

My packages look like this :
packages:
    - package: dbt-labs/dbt_utils
      version: ""1.0.0""

## What I've already tried 
Tested a few new macros like datespine, but the same error shows up. Also the macro does exist in dbt-utils too - so not sure what's causing it.
https://github.com/dbt-labs/dbt-utils/blob/main/macros/sql/date_spine.sql 


Code part that causes errors: - 
```
diffed AS (
    --calculates `period_of_inactivity`.
    SELECT
        *,
        {{ dbt_utils.datediff(
            'previous_tstamp',
            'timestamp',
            'second'
        ) }} AS period_of_inactivity
    FROM
        lagged
```

Would appreciate any directional help here, curious if I should upgrade to v1.4 dbt or add a new macro definition",1
6893,"datediff-dict-object-errors-dbt-upgrades","Found the answer 

just change

> dbt_utils --> dbt

here",2
6893,"datediff-dict-object-errors-dbt-upgrades","Thanks for coming back and posting your solution! This is because we moved a lot of cross-database macros out of dbt utils and into dbt Core for implementation by the database adapters. 

You might want to check out https://docs.getdbt.com/guides/migration/versions/upgrading-to-dbt-utils-v1.0 to check if there are any other things you should review",3
6893,"datediff-dict-object-errors-dbt-upgrades","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
6894,"add-location-to-create-database-schema-statement-in-databricks-to-enable-creation-of-managed-tables-on-external-storage-accounts","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I want to store my tables as managed tables on an external location like adls or blob.
By default databricks stores managed tables in: dbfs:/user/hive/warehouse, it is possible to change this default location by creating the database / schema and definining the location.

Example SQL statement:
CREATE DATABASE test_db LOCATION 'abfss://testdata@storage-account.dfs.core.windows.net/test_db'
or:
CREATE SCHEMA test_db LOCATION 'abfss://testdata@storage-account.dfs.core.windows.net/test_db'

(database / schema are equivalent in databricks)

This way all tables created in this database/schema will be managed tables and the data as well as the metadata will be deleted if a table is dropped. I can't find a way how to add the location to the create schema statement in DBT.

## The context of why I'm trying to do this

## What I've already tried 
Consulted dbt documentation

Question:
How can I add the location to the CREATE DATABASE / SCHEMA statement in DBT in such tables within this database / schema will become managed tables and data is stored on an external storage account?",1
6896,"authorized-views-always-selected-by-state-modified","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
We have added some authorized views models in our dbt project which is always triggered by state:modified together with defer for each env. This causes the selector with downstream or upstream dependencies to trigger a whole lot more models than anticipated. 

## The context of why I'm trying to do this
We want to use state:modified with down or upstream dependencies triggered in some CI jobs, but we only want to trigger those models that have been modified in that specific feature-branch / MR.

## What I've already tried 
One possible but ugly solution was to tag every authorized view model and exclude them in the runs. But then we don't check that they work as intended when they have been modified..",1
6898,"need-help-invalid-load-key-error","![image|519x500](upload://pcRxKqJbHuPApLeM4kdD5xOcRtE.png)
unable to connect database ,previously it was working fine",1
6900,"multiple-build-deploys-and-ci-searching-for-best-practice","Hi there

I've searched through documentation, this forum and Google(lol), but haven't really found the answer. Hope that someone can help here.

**Context**
We are having 3 models, and they are all deployed by the same job.  All 3 models come from the same repository.
We've set up a CI job that Defers to that deploy run.

**The challenge**
I'm considering splitting the deploy job into 3; 1 for each model. 

How would that affect the CI job?
Should I make 3 CI jobs or what is the best practice here?

Hope that it makes sense.",1
6909,"change-dbt-test-audit-schema-when-using-store-failures","## The problem I'm having
The docs for for store_failures says:

> When true, `store_failures` save all the record(s) that failed the test only if [limit](https://docs.getdbt.com/reference/resource-configs/limit) is not set or if there are fewer records than the limit. `store_failures` are saved in a new table with the name of the test. By default, `store_failures` use a schema named `dbt_test__audit` , but, you can configure the schema to a different value.

But doesn't say how to do so

## The context of why I'm trying to do this
I'm unable to save to the dbt_test__audit schema due to IT constraints. 

## What I've already tried 
I can't think what to try, there's no schema to override.  I tried looking for dbt_test__audit in the github but it's in a few places and isn't readily apparent where I'd override it.",1
6909,"change-dbt-test-audit-schema-when-using-store-failures","Hi @VDFaller, check this doc page here

https://docs.getdbt.com/reference/resource-configs/schema#tests

You can change the schema like this:

```yaml
tests:
  +store_failures: true
  +schema: the_island_of_misfit_tests
```",2
6909,"change-dbt-test-audit-schema-when-using-store-failures","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
6922,"do-we-have-a-way-to-check-model-definitions-against-the-actual-models","Hello,

Well, as the title sais. I'm having a project with a lot of models which have yml definitions and it's becoming painfull to keep them up to date as sometime I add a new column or remove it and forget to reflect that on the yml file, so I was wondering if there's any comand I missed or tool which I can run to see that all the columns are referenced on the yml and there are no extras? As a side note, I'm using dbt-core.",1
6922,"do-we-have-a-way-to-check-model-definitions-against-the-actual-models","You might be interested in dbt-checkpoint: <https://github.com/dbt-checkpoint/dbt-checkpoint>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1677080077997129?thread_ts=1677079861.682639&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
6926,"runtime-error-when-running-dbt-build-in-deployment-environment-databricks","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
```
Runtime Error
  Runtime Error
    Catalog 'npschafer@constantiambio.com' plugin class not found: spark.sql.catalog.npschafer@constantiambio.com is not defined
```
## The context of why I'm trying to do this
Running a `dbt build` command as a job in my Databricks deployment environment in dbt-cloud. The same code works in the development environment.

## What I've already tried 
Searching the forum and the internet for similar issues.

## Some example code or error messages
```
[0m23:44:11  Running with dbt=1.4.1
[0m23:44:11  Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:44:12  Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m23:44:12  
[0m23:44:13  
[0m23:44:13  Finished running  in 0 hours 0 minutes and 0.84 seconds (0.84s).
Runtime Error
  Runtime Error
    Catalog 'npschafer@constantiambio.com' plugin class not found: spark.sql.catalog.npschafer@constantiambio.com is not defined

```",1
6929,"can-i-create-an-scd-type-2-model-in-dbt-without-using-snapshot","Hi Experts!

I am super new to dbt and have a use case to build an SCD type 2 model which would have valid_to and valid_from (and maybe a active flag) columns. This seems easy to do using a snapshot but I've read that one should not put snapshots on datamart models. 

I'm uncertain why putting a snapshot on a datamart model is a bad idea if we just want to be able to see the history of values that the datamart model had at different points in time?

Can I build a SCD Type 2 model including valid_to and valid_from dates manually in dbt and is there a template I can look at how to do this if it exists?

I've looked through different posts on here but cannot find anything that will give me the valid_to, valid_from columns other than using snapshot. 

I'd like to use snapshot, because it looks very easy and appears to be working but am hesistant due to the warnings suggesting best practice is to only apply this to source tables.

Thank you!",1
6929,"can-i-create-an-scd-type-2-model-in-dbt-without-using-snapshot","Hi @jackiesbauer, I think there is no problem to create a snapshot just to monitor your model if you are aware of the risks.

It is a best practice to use snapshots in sources because from the source you are able to recreate any model at that point in time since you have the raw data and you didn't lose any information through transformations, casting, renaming, etc.

It is not a good practice to use snapshots in models because they are much more mutable than sources, and if you change the way the model is transformed, maybe you can not easily compare current data with historical data.

If you are aware that your snapshot can easily become a mess if you do not take care, I see no problem with using it in your models. It makes no sense to create an SCD type 2 other way since you will have the same problems.",2
6929,"can-i-create-an-scd-type-2-model-in-dbt-without-using-snapshot","Just keep in mind that you
- cannot recreate columns
- depending on the complexity of your model, will not have a reliable updated_at
- If you apply business logic in a snapshot query, and this logic changes in the future, it can be impossible (or, at least, very difficult) to apply the change in logic to your snapshots.

source: https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices",3
6929,"can-i-create-an-scd-type-2-model-in-dbt-without-using-snapshot","Thank you for your answer @brunoszdl!
It feels more difficult to put the snapshots on the source models (three different ones) and then figure out how to join those snapshots downstream to create the datamart model so I think I will try to implement this way (knowing the possible drawbacks you ponted out).  
I appreciate your time and answer!",4
6929,"can-i-create-an-scd-type-2-model-in-dbt-without-using-snapshot","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
6930,"as-scalar-isnt-working","Hi I'm trying to store a scalar value in a variable and using in select statement.

## What I've already tried 

## Some example code or error messages
```
set etl_batch_id_var = 
    run_query(
        ""select nvl(max(etl_batch_id) + 1, 1000) as etl_batch_id from {{ this }}""
     | as_scalar , %}
```
It gives me a compilation error that says as_scalar is not a filter. Is there is any alternate to it?",1
6930,"as-scalar-isnt-working","Hi @rjames 

`run_query` returns a table, you have to specify the column and row you want.

To do that you can return the column as a list, have a look at this example:

```
{% set payment_methods_query %}
select distinct payment_method from app_data.payments
order by 1
{% endset %}

{% set results = run_query(payment_methods_query) %}

{% if execute %}
{# Return the first column #}
{% set results_list = results.columns[0].values() %}
{% else %}
{% set results_list = [] %}
{% endif %}

select
order_id,
{% for payment_method in results_list %}
sum(case when payment_method = '{{ payment_method }}' then amount end) as {{ payment_method }}_amount,
{% endfor %}
sum(amount) as total_amount
from {{ ref('raw_payments') }}
group by 1
```

source: https://docs.getdbt.com/reference/dbt-jinja-functions/run_query

But instead of selecting the whole column you can select the first item `results_list[0]`.

Additionally, I have not found the `as_scalar` filter, did you mean `as_number`?

https://docs.getdbt.com/reference/dbt-jinja-functions/as_number",2
6930,"as-scalar-isnt-working","Thank you @brunoszdl My query returns a scalar value. like max of date. Do i still need to follow these steps. Is there is a way to get the resultset stored directly into a variable as it a scalar resultset?",3
6930,"as-scalar-isnt-working","@rjames , even if your select returns a scalar, the `run_query` **function returns a table**, with one column and one row.

https://docs.getdbt.com/reference/dbt-jinja-functions/run_query

So, yes, the only way I know is by doing all of these steps, don't know a workaround.

If you want to keep it all **in one line** you try something like

```
{% set your_var = run_query(""select nvl(max(etl_batch_id) + 1, 1000) as etl_batch_id from {{ this }}"").columns[0].values()[0] %}
```
Maybe you have to put it inside a {% if execute() %} condition like
```
{% if execute %}
{% set your_var = run_query(""select nvl(max(etl_batch_id) + 1, 1000) as etl_batch_id from {{ this }}"").columns[0].values()[0] %}
{% endif %}
```
But try without it first.

**I did not test it, so maybe you need to adjust something.**

**I would recommend separating it into multiple lines, so it is easier to debug.**

```
{% set your_query %}
select nvl(max(etl_batch_id) + 1, 1000) as etl_batch_id from {{ this }}
{% endset %}

{% set results = run_query(your_query) %}

{% if execute %}
{% set results_list = results.columns[0].values() %}
{% endif %}

{% set result = results_list[0] %}
```",4
6930,"as-scalar-isnt-working","@rjames I think you're trying to use an Ansible filter - they have extended Jinja beyond what comes out of the box. 

check out [get_single_value from dbt utils](https://github.com/dbt-labs/dbt-utils/tree/1.0.0/#get_single_value-source), it will abstract all of this away for you",5
6930,"as-scalar-isnt-working","@joellabes, didn't know this macro, much better than returning a table to get the single value :sweat_smile:, thank you!!",6
6930,"as-scalar-isnt-working","Thank you @joellabes  and @brunoszdl",7
6930,"as-scalar-isnt-working","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",8
6931,"trying-to-use-a-non-additive-metric-to-filter-on-a-new-metric","Hi!

I had a strange use case with metrics and Lightdash, and wanted to see if others had run into the same problem:

I have a summarised data source, and from it I've create a metric called `metric_one_percentage` (which is just `sum(COL_A)`/ `sum(COL_B)`), and the dimension is ENTITY_A.

So an example output for a monthly time grain would be:
![example|690x285](upload://6AdGHplavgfkwAW4e7GwIbFF4EA.png)


I want to create a second metric which shows the percentage of entities, of all the entities, where the `metric_one_percentage` exceeds a certain threshold, x. 

So in essence the formula would be:
`Number of entities with a metric_one_percentage  >=  x`  **/**   `Number of entities`

However I would also like this metric to have the flexibility with the time series that `metric_one_percentage` would have, that is, I don't want to pre-define the time period for `metric_one_percentage` to be able to calculate the second metric. I would like the user to define the time grain when working in Lightdash. 

Is this possible?",1
6934,"cant-access-dbt-courses-section","Hello, I live in Brazil and a friend of mine recommended dbt courses to learn more about Data Warehouses, but I can't access the https://courses.getdbt.com/. It throws me and 403 forbidden error on screen.

Anyone can help me with that? I appreciate.",1
6934,"cant-access-dbt-courses-section","Hi @JGsouzaa , I also live in Brazil, and it is working fine for me. Which browser are you using?  Have you tried to clean your cache/cookies?",2
6949,"read-custom-tables-in-elementary","We are using elementary to show the results out of dbt test. Is it possible to read metrics from custom table to display in elementary?",1
6964,"the-problem-im-having-is-pyarrow-error-when-installing-pip-install-dbt-snowflake","I am trying to install dbt-snowflake it gives me pyarrow error
i tried to install pyarrow seperately and  it is installed but when I try to re install dbt-snowflake again having the error with pyarrow. 

PS: pip version is the latest 
![pyarrow error|690x280](upload://eWryldlUFkGoaRBDvWxoqBP9XFU.png)",1
6964,"the-problem-im-having-is-pyarrow-error-when-installing-pip-install-dbt-snowflake","I'm having the same issue. The latest version of dbt-snowflake (1.4.1) relies on an old version of pyarrow (8.0.0) that is unable to be built from wheel.",2
6971,"does-dbt-cloud-work-with-duckdb","## The problem I'm having
I want to configure my dbt-cloud profile to work with a local DuckDB instance (and maybe eventually an instance on an EC2 machine), but right now I can't find any documentation on how to get started.

## The context of why I'm trying to do this
Just dipping my toes in the dbt pool right now.  I've completed the Fundamentals course using a trial SnowFlake account, but now I'd like test it out using DuckDB.  I've found a demo online, which is super helpful, but it's only focused on using dbt-core.  Question is, does the dbt-cloud platform work with DuckDB given that it's not cloud-based?  The GUI/IDE is super nifty and I want to take advantage of it as much as possible, but right now it's not really clear to me how to get started.

## What I've already tried 
Modifying my project details for Partner Connect Trial.

## Some example code or error messages
No error messages so far.",1
6972,"redshift-unload-macro-help","The problem I am having is the model keeps failing when trying to run the Redshift Package Macro Unload.  I have tried reaching out to support and they say ""lies in the structure or syntax of your dbt projectâ€™s code "" but will not help unless I upgrade to an Enterprise plan. 

## Some example code or error messages
My Code
```
{{
  config({ 
    ""post-hook"": [
      ""{{ redshift.unload_table('dbt_etl_dev',
                            'rpt_assurant_for_collection',
                            s3_path='s3://xxxxx/xxxxx',
                            iam_role='arn:aws:iam::xxxxxxxxxx:role/xxxxxxx',
                            header=True,
                            delimiter=',',
                            overwrite=true) }}""
    ]
  })
}}
```
Error Message:
```
15:02:47  Began running node model.dw_domuso.unload_assurant_for_collection
15:02:47  1 of 1 START view model dbt_etl_dev.unload_assurant_for_collection ............. [RUN]
15:02:47  Acquiring new redshift connection ""model.dw_domuso.unload_assurant_for_collection""
15:02:47  Began compiling node model.dw_domuso.unload_assurant_for_collection
15:02:47  Compiling model.dw_domuso.unload_assurant_for_collection
15:02:47  Writing injected SQL for node ""model.dw_domuso.unload_assurant_for_collection""
15:02:47  finished collecting timing info
15:02:47  Began executing node model.dw_domuso.unload_assurant_for_collection
15:02:47  Writing runtime SQL for node ""model.dw_domuso.unload_assurant_for_collection""
15:02:47  Using redshift connection ""model.dw_domuso.unload_assurant_for_collection""
15:02:47  On model.dw_domuso.unload_assurant_for_collection: BEGIN
15:02:47  Opening a new connection, currently in state closed
15:02:47  Redshift adapter: Connecting to Redshift using 'database' credentials
15:02:47  SQL status: BEGIN in 0.03 seconds
15:02:47  Using redshift connection ""model.dw_domuso.unload_assurant_for_collection""
15:02:47  On model.dw_domuso.unload_assurant_for_collection: /* {""app"": ""dbt"", ""dbt_version"": ""1.2.4"", ""profile_name"": ""user"", ""target_name"": ""default"", ""node_id"": ""model.dw_domuso.unload_assurant_for_collection""} */


  create view ""domusodatawarehouse"".""dbt_etl_dev"".""unload_assurant_for_collection__dbt_tmp"" as (
    
  ) ;
15:02:47  Postgres adapter: Postgres error: syntax error at or near "")""
LINE 6:   ) ;
          ^
15:02:47  On model.dw_domuso.unload_assurant_for_collection: ROLLBACK
15:02:47  finished collecting timing info
15:02:47  On model.dw_domuso.unload_assurant_for_collection: Close
15:02:47  Database Error in model unload_assurant_for_collection (models/unload/assurant/unload_assurant_for_collection.sql)
  syntax error at or near "")""
  LINE 6:   ) ;
            ^
  compiled SQL at target/run/dw_domuso/models/unload/assurant/unload_assurant_for_collection.sql
15:02:47  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23e1f2e3-7565-41d0-9b6c-2398cd19749c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3cd939a00>]}
15:02:47  1 of 1 ERROR creating view model dbt_etl_dev.unload_assurant_for_collection .... [ERROR in 0.14s]
15:02:47  Finished running node model.dw_domuso.unload_assurant_for_collection

```",1
6976,"database-error-could-not-deserialize-key-data-the-data-may-be-in-an-incorrect-format-or-it-may-be-encrypted-with-an-unsupported-algorithm","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

Running into this error: 
Database Error: Could not deserialize key data. The data may be in an incorrect format or it may be encrypted with an unsupported algorithm.

## The context of why I'm trying to do this

The DBT Cloud execution runtime is at 300s and I wanted to increase the range to 2000s. 
However when it said to enter the private key. I entered the private key ID instead. Which resulted in that error. Not sure how to revert this. 

## What I've already tried 
Stackoverflow + DBT docs


## Some example code or error messages",1
6976,"database-error-could-not-deserialize-key-data-the-data-may-be-in-an-incorrect-format-or-it-may-be-encrypted-with-an-unsupported-algorithm","Turns out needed the Database Private Key Access.",2
6980,"concurrent-dbt-jobs","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

I'm not sure how concurrent jobs work on dbt core and dbt Cloud. I am running dbt core in airflow for a snowflake data warehouse. Some models update incrementally hourly, but also do full refreshes daily/weekly. My pool size is 1 on Airflow, but I would like to increase it if possible. Will increasing pool size and running jobs at the same time mess up my data? 

How does this work on dbt cloud (are their safeguards in place somehow to allow for concurrent jobs without mucking up the data?)

https://stackoverflow.com/questions/75293215/can-dbt-run-with-a-pool-size-greater-than-1-in-airflow",1
6987,"how-to-exclude-a-model-seed-in-dag-if-tests-fail-instead-of-failing-the-whole-dag-build","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I have configured TESTs for a SEED I'm working with.  In order for the TEST to execute, the SEED must be created as a TABLE first into Snowflake.  At this point, even if the TESTs fail and tell me, the damage is done as the SEED is now in Snowflake.

## The context of why I'm trying to do this
I'd like to prevent wrong data from entering Snowflake in the first place... hence the TEST.  How do I do this?

## What I've already tried 
I've configured the tests and it runs correctly.  But I need to stop the SEED from being materialised into Snowflake if the TESTS fail - or ROLL BACK if it does fail....

## Some example code or error messages
No error messages as the TESTS are running as they should be.  It tells me when the tests fail but it's too late then... the seed has been created in Snowflake as a table by then.",1
6990,"looping-through-a-large-table-to-batch-insert-mssql","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
exploring dbt and have a question. We're on sql-server and we have some code that bathes loading into the destination table. 

staging table may have 30 million records (or more). We don't want to merge this into final table with one insert (for a few diff reasons) so we batch the insert in increments of 1 million. 
See example below - other procs are merging, insert and update. 


```
DECLARE @EndID INT = (SELECT MAX(RowID) FROM stagetable);
DECLARE @CurrentEndID INT


WHILE @StartID <= @EndID
BEGIN

       SET @CurrentEndID = @StartID + @BatchSize -1

               insert into destTable (<cols>)
		SELECT 
                           <cols>
		FROM stagetable a
		WHERE RowID >= @StartID 
			AND RowID <= @CurrentEndID
                       and  not exists (select * from desttable  t where t.key  = a.key)

SET @StartID += @BatchSize
end
```",1
6998,"abort-a-dbt-job-without-triggering-a-failure","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
In my Azure based Data Platform, I have DataDog monitoring the Azure Data Factory pipelines which trigger / orchestrate my dbt jobs.  When ADF triggers a dbt job, it waits until it completes & catches if it fails.  When a job fails, DataDog calls me out via PagerDuty for investigation.
This works well HOWEVER, sometimes I need dbt to abort a job (i.e. freshness / tests) and not execute subsequent models (to save credits) but I don't want it to trigger a call out.  Is there a way to **abort** a dbt job without triggering a **failure** ?

## The context of why I'm trying to do this
see above

## What I've already tried 
I can't see an option to stop a job other than a model failing...

## Some example code or error messages
N/A",1
6998,"abort-a-dbt-job-without-triggering-a-failure","A couple of thoughts that are worth looking into: 

## Prevent models running when tests fail
If you use dbt build, any models whose tests fail will prevent downstream nodes from being built, which will save you those credits 

## Only run fresh models 
Check out the docs on [source_status:fresher](https://docs.getdbt.com/docs/deploy/cloud-ci-job#fresh-rebuilds) which will only select nodes where there is fresh data compared to the last production run",2
6998,"abort-a-dbt-job-without-triggering-a-failure","hey Joel.  Thanks for your response.

I get that failed jobs will prevent downstream nodes from being run.  That's what I want too... but is there a way to prevent downstream nodes from being run WITHOUT triggering a FAILURE?  ie. ABORT or EXIT type of operation... this way, the pipeline isn't seen as a failure(because it didn't fail...)",3
6998,"abort-a-dbt-job-without-triggering-a-failure","Hmm not inside of dbt Core, but [outbound webhooks in dbt Cloud](https://docs.getdbt.com/docs/deploy/webhooks) will enter general availability tomorrow. Perhaps you could listen for the run complete webhook, then download the artifacts from the Cloud API and check those to see why the error occurred, only raising a PagerDuty alarm if something else is the cause of the issue?",4
7002,"is-it-possible-to-write-macros-in-python-instead-of-jinja-templates","Is it possible to write macros in Python instead of Jinja templates  And run with  run-operation command

Due to the fact that certain operations cannot be imported into the dbt jinja template, including `datetime.strptime`

`{% import ""python"" as py %}`  is not working in the macro 

    `py.datetime.strptime('20211', '%Y%m')`",1
7002,"is-it-possible-to-write-macros-in-python-instead-of-jinja-templates","It's not possible to write arbitrary macros in Python, but the datetime module is exposed to you here: https://docs.getdbt.com/reference/dbt-jinja-functions/modules#datetime",2
7002,"is-it-possible-to-write-macros-in-python-instead-of-jinja-templates","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
7008,"how-to-capture-dbt-expectations-tests-on-big-query","Hi All,

I am planning to use dbt_expectations package to do data quality checks in Big query using dbt.
After defining data quality checks in schema.yaml , when I run DBT TEST, itâ€™s showing failures on console.  

1. Is there any way to call  dbt_expectations macros in dbt models? (Like normal function calls in each model instead of defining in schema. Yaml) 
2. If I use dbt test , how to capture passed and failures report on Big query for each column level? 

Basically I am looking for some framework which I can use in models to capture data quality checks on Big query.

Thanks!
Chandra",1
7008,"how-to-capture-dbt-expectations-tests-on-big-query","Hi @vennapuc ,

1. I didn't understand why it is bad for you to define the tests in the `.yml` and why would it be better to call a macro in the `.sql`. Anyway, as they are tests, I think you can only define them in the `.yml`.

2. I am not sure this is what you want, but have you tried `store_failures`? https://docs.getdbt.com/reference/resource-configs/store_failures",2
7008,"how-to-capture-dbt-expectations-tests-on-big-query","Hi @brunoszdl,
We have Raw | Staging | curation layers. So I need to apply controls in raw to staging layer.
If any records failed, so need to filter out and execute the curation layer for only for passed records. So I was looking for approach  to call dbt_expectations macros and add audit columns in schema layer like status, reason for failed etc.. 

I have tried using store_failures- option but its only storing test and reason but not all columns from table. 

So is there way to use dbt_expectations macros in .sql so that I can apply at column level and will add status and reason etc ... to each schema table after calling macros so. 

Thanks!.",3
7009,"using-vargs-in-custom-tests","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I'd like to extend the current 'unique' test to be able to take a variable list of parameters so it can test if the concatenation of the list of columns is unique i.e. col1 || col2 || col3

## The context of why I'm trying to do this
I have tables that need to have this assertion tested.

## What I've already tried 
looking through the docs, it seems custom tests must take named arguements atm.

## Some example code or error messages
N/A",1
7009,"using-vargs-in-custom-tests","This already exists in the dbt-utils package: <https://github.com/dbt-labs/dbt-utils#unique_combination_of_columns-source>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1677504542423839?thread_ts=1677497366.808539&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",3
7009,"using-vargs-in-custom-tests","Thanks <@U0291GP2AQ0>, I was doing it in a more complex way :sweat_smile:

<sub>Note: `@Bruno de Lima` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1677504997968509?thread_ts=1677497366.808539&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",4
7009,"using-vargs-in-custom-tests","oh nice! I didn't realise that!!  below is what I tried but am now getting a compilation error complaining about the macro takes no keyword argument 'column_name' which I can't control... ideas?

      - name: METRIC_NAME
        tests:
          - dbt_utils.unique_combination_of_columns:
              combination_of_columns:
                - GROUP_NAME
                - METRIC_NAME
              quote_columns: false


Compilation Error in test dbt_utils_unique_combination_of_columns_SEED_METRIC_LIST_CNMS_METRIC_NAME__GROUP_NAME__METRIC_NAME__False (seeds/Metadata/metadata.yml)
  macro 'dbt_macro__test_unique_combination_of_columns' takes no keyword argument 'column_name'",5
7009,"using-vargs-in-custom-tests","Try to define the test below the name of the seed, not below the name of the column",6
7009,"using-vargs-in-custom-tests","Like this:

```
version: 2

seeds:
  - name: your_seed
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - GROUP_NAME
            - METRIC_NAME
    columns:
      - name: METRIC_NAME
```

just check the indentation.

Just explaining a little bit more:
- When you put the test below the model/seed name, dbt assumes you are passing to the test an arg called ""model"" with the name of your model/seed.
- When you put the test below the column name, dbt assumes you are passing to the test two args, model like before, and column_name, which is the name of your column.

This specific test does not accept a column_name arg, because it is not designed for a specific column, but a group of them. This group of columns is passed by a custom arg called ""combination_of_columns"" that is why you have to specify it in the .yml.",7
7009,"using-vargs-in-custom-tests","thank you so much!! much appreciated.",8
7009,"using-vargs-in-custom-tests","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
7015,"rollback-on-snapshots","Hello,

In terms of snapshots and models that are built on top of them, how can we revert a dbt snapshot to its previous state, in case that an extraction was not correct and needs to be reprocessed?

Thank you in advance.",1
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I have a very large complicated DAG that I am working through.  I want to identify the left most sources and/or models.

## The context of why I'm trying to do this
I'm doing this to better unravel how the existing data warehouse on which it was based works.

## What I've already tried 
I've tried reading the documentation to see if there is an argument I can give the --select section of the DAG or a way to modify the ls command to get what I need.

## Some example code or error messages
```
  No code examples, sorry :(
```",1
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","Have you tried to see the sources/models in the lineage graph of dbt docs? I think is the easiest way to check how your models are connected.",2
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","Yeah, that 's part of the problem.  Nearly a thousand tables with cycles that we had to break using dot notation.   It's a mess.",3
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","hmmm okay, but you can visualize small parts of the project using the `tags`, `select` or `exclude` options

https://imgur.com/a/2Sij1ur

e.g. you want to visualize only the part of the project related to 'my_model''

you can put inside select '+my_model', or '+my_model+'.  If there are yet too many models you can exclude some of them in the same way in `exclude`.

I did not understand if you already tried it, do you think this can be of some help?",4
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","In the docs viewer (or dbt ls), does this work? Writing it on mobile so haven't checked:

`dbt ls --select source:*+1`

`source:*` would be [any source](https://docs.getdbt.com/reference/node-selection/methods#the-source-method), the `+1` means the [first-order children](https://docs.getdbt.com/reference/node-selection/graph-operators#the-n-plus-operator)",5
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","Thanks for the thought.  I have researched those suggestions but without success because the diagram is fragmented.  Meaning, there are many orphaned  processes where two or more tables connect to each other but not to a source and are floating out on the DAG. The datawarehouse has a lot of technical debt that I'm trying to work through as I try to migrate to the cloud.",6
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","Hmmm sounds messy! So you're saying that not all of the leftmost models are derived from sources, they have hardcoded references to a table? 

Could you look at the manifest.json and find nodes which don't have their depends_on array populated? I think those would be your leftmost nodes.",7
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","Ah, that did it.  Thanks joellabes!

Yes, to answer your question.  We had to have hardcoded references to some fact/dim tables to break circular references as we weren't at the rewrite stage and were trying to build out a diagram to have some semblance of what was going on.",8
7022,"is-there-a-way-to-show-only-the-most-left-handed-sources-models-in-the-dag","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
7024,"dbt-semantic-layer-metrics-layer-redshift","Hi,

As dbt semantic layer is currently enabled only Snowflake, whats the best approach for Redshift.

Can we use dbt metrics + Redshift + transform as an interim solution.",1
7031,"create-models-dynamically","Hello,
I need to create a job that generates views based on data from Snowflake information schema.
First step is to pull all relevant tables for the process based information_schema tables.
Second step is to iterate over the results from the 1st step, and for each table, create a view over the table in another data base.
What is the best why to implement this in DBT?

example: 
1st step
select table_name from old_db.information_schema.tables 
where table_schema = 'FINANCE'

2nd step
for each table in tables:
   run query : create or replace view new_db.finance.table as select * from old_db.finance.table",1
7031,"create-models-dynamically","You should avoid writing custom DDL to create tables - this is the sort of thing that dbt abstracts away for you and ensures you don't have to worry about things like running nodes in the right order. 

Further, you won't be able to ref these different models if they're not created as individual dbt model files as dbt believes in a 1 model file = 1 database object paradigm. 

If you need help bootstrapping a lot of these models, you could consider using the codegen package which will help generate your staging models: https://github.com/dbt-labs/dbt-codegen",4
7031,"create-models-dynamically","Thanks @joellabes, I will remove my answer in order not to induce bad practices!

Always learning",5
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","Is there a way to have two different models coexist that is writing to same table?

The reason for this is because I am trying to find a way to run a copy of my main model using a different dbt job.

Here are the solutions that I've tried so far:
1. Put the model copy into a different folder
     - throws an error that models have the same name as the original model
2. Renamed the model copy
     - throws dbt cannot create two resources with identical database representations
3. Tried using --exclude to exclude the folder containing the original model
     - same error as #2

Any thoughts on this is very much appreciated. Thank you.",1
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","[quote=""ericballadares, post:1, topic:7034""]
I am trying to find a way to run a copy of my main model using a different dbt job
[/quote]

How are these variants of the model different from one another? Why do they need to be two different queries going into the same table? 

The standard way to do this in dbt would be to have each model do a single thing, and then have another model which is the union of those two tables. Even that might not be necessary though depending on what your goal is - maybe you could do it all in one model, and have an additional column on the end describing its origin

[quote=""brunoszdl, post:2, topic:7034""]
You can write the two SQL codes in the same file and use an `if` block. Then you can choose the model based on a var you can pass in the run command.
[/quote]

Unless it's built as an incremental model, this would overwrite the table's contents every time.",3
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","[quote=""joellabes, post:3, topic:7034, full:true""]
[quote=""ericballadares, post:1, topic:7034""]
I am trying to find a way to run a copy of my main model using a different dbt job
[/quote]

How are these variants of the model different from one another? Why do they need to be two different queries going into the same table? 

The standard way to do this in dbt would be to have each model do a single thing, and then have another model which is the union of those two tables. Even that might not be necessary though depending on what your goal is - maybe you could do it all in one model, and have an additional column on the end describing its origin

[/quote]

There's no difference in terms of what each of the models does but they differ in terms of when they are executed. The original model/job runs on schedule while the copy runs when needed. I'm trying to have a generic job that can execute the models in a specific folder. But that's the error that I'm getting.",4
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","If it's the same code, you should use the dbt model selection syntax to pick the specific models, not make a copy of them. For example you could add a tag and then run `dbt run --select tag:my_tag` as an adhoc job that is different to your regularly scheduled `dbt run` command.

Put another way, your proposed approach conflates the orchestration logic and the business logic of your files. You should build the models that you need to see in your database, and then attach extra metadata to them to make them runnable on the schedule you need, not have your project structure driven by your orchestration logic.",5
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","I am assuming, the models differ in terms of where clause then. I had similar requirements. The way I modelled it is by creating a variable e.g. **is_daily_run** in the project.yml file and the default value is set to true which will be the case for regular run. When the variable is false, it will run as copy mode.

Now in the model, you will play with the where condition checking the value of the **is_daily_run** variable. 
```
{% if var(""is_daily_run"") == 'True' %}

          AND load_date>= last_run_date

{% else %}
        AND 1=1 -- basically run for full data or insert your rules for copy
        
{% endif %}
```
You can also play with the variable to switch the source if the source is different for daily vs full load jobs etc.",6
7034,"is-there-a-way-to-have-two-different-models-coexist-that-is-writing-to-same-table","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",7
7044,"connect-self-hosted-gitlab-to-dbt-cloud","Is there a way to connect self-hosted GitLab account to DBT cloud?",1
7047,"parameterized-folder-model-name","I want to create a generic job that can execute a specific model that is located in a specific folder.

I already tried this to run all the models inside my_folder:

` dbt run -s ""{{var('my_folder')}}"".*+`

and `my_folder` was defined in `dbt_project.yml`.

However, this doesn't work as `""{{var('my_folder')}}"".*+` were treated as a model name.

Is there a way to implement this or something similar?

Thanks.",1
7047,"parameterized-folder-model-name","I don't think you can use the vars defined in your `dbt_project.yml` to substitute arguments in your terminal command.

Is it possible to you to use environment variables?

If so, you could do it like

```
export my_folder='staging'
```
to define the variable, and then

```
dbt run -s $my_folder
```

Then it will run

```
dbt run -s staging
```

for example",2
7047,"parameterized-folder-model-name","Where should I put `export my_folder='staging'`?

But this is helpful. Thank you.",3
7047,"parameterized-folder-model-name","Do you know which shell are you using? 

bash, zsh, ...?

If you are in doubt just run in your terminal

```
echo $SHELL
```",4
7047,"parameterized-folder-model-name","I'm afraid I can't define this in shell. Is there somewhere I can define this programmatically within my dbt project?",5
7047,"parameterized-folder-model-name","You can create a file such as `.env` inside your project root path.

Inside the file you can write `export my_folder='staging'`.

You just have to run

```
source .env
```

before you run your `dbt run`.

You just have to use the `source .env` once. But if you close your terminal and open it again you have to do it again.",6
7047,"parameterized-folder-model-name","If you're using dbt cloud, you can define environment variables here: https://docs.getdbt.com/docs/build/environment-variables",7
7047,"parameterized-folder-model-name","[quote=""joellabes, post:7, topic:7047, full:true""]
If you're using dbt cloud, you can define environment variables here: https://docs.getdbt.com/docs/build/environment-variables
[/quote]

Any other way other than the environment variables in dbt cloud? Nothing that I can do within my dbt project?",8
7047,"parameterized-folder-model-name","You can't use variables defined inside of your dbt project outside of your dbt project - the CLI doesn't understand Jinja. 

Can you say more about your use case? Why do you want to define the list of models you want to run inside of a variable? Doesn't that mean that if you want to change which models are run, you have to make a change to a source-controlled file instead of just specifying a different selector in the dbt Cloud config UI?",9
7057,"send-customized-emails-with-attachments-using-models","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having - Trying to send email using smtp in models. 

## The context of why I'm trying to do this - We have a requirement of sending the final load details to other teams.

## What I've already tried - I didn't find any options till now.

## Some example code or error messages
```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
7057,"send-customized-emails-with-attachments-using-models","@prvishnukumarece dbt is not designed to send emails, it can only do things that you can already do in sql. You should use a BI tool or similar to schedule a query to run and then send the results to your stakeholders",2
7057,"send-customized-emails-with-attachments-using-models","Thank you for the information.",3
7057,"send-customized-emails-with-attachments-using-models","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
7070,"why-does-dbt-create-temporary-table-for-incremental-updates","The problem i'm having is that my incremental updates are not very performant. I am using snowflake and trying to understand why DBT creates a temporary table from the incremental update and then merges it in to the destination in a subsequent query. Why not do this all in one query? Why is it necessary to materialize the incremental update as a table before merge?

```
create or replace temporary table DATABASE_NAME.SCHEMA_NAME.table_name__dbt_tmp  as
      (select * from(
```",1
7075,"python-incremental-model","Hello! I'm really excited to be able to use Python models, but I've been running into issues with incremental ones. 

If I have a working model with `materialized = ""table""` and I change it to `materialized = ""incremental""`, shouldn't that work fine even before adding in the correct `if dbt.is_incremental:` logic?

For some reason I get this error when I just change table -> incremental for a working model:
```  
File ""/tmp/d87435d6-edb3-4afa-84e7-04dae648adcf/query_consistency.py"", line 5
    create or replace table graph-mainnet.internal_metrics.query_consistency__dbt_tmp
    ^
IndentationError: unexpected indent
```

Any thoughts here? Has anyone else had issues with python incremental models? I'm on bigquery.

Code which works fine below (and does not work fine when going from table to incremental):
``` python
import requests
import pandas as pd
import json
import time

def model(dbt, session):
    dbt.config(materialized = ""table"")

    # ENTER THE SCHEMA TYPE YOU WANT TO GET ALL DATA FOR
    schema_type = 'dex-amm'
    
    # fetch the data from the deployment file
    response = requests.get('https://raw.githubusercontent.com/messari/subgraphs/master/deployment/deployment.json')
    subgraphs = response.json()
    
    # create query
    query = '''{
      financialsDailySnapshots(orderBy: timestamp, orderDirection: desc, first: 365) {
        cumulativeVolumeUSD
        dailyProtocolSideRevenueUSD
        totalValueLockedUSD
        cumulativeTotalRevenueUSD
        dailyTotalRevenueUSD
        dailyVolumeUSD
        timestamp
      }
    }'''
    
    base_url = 'https://api.thegraph.com/subgraphs/name/messari/'
    data = []
    for project in subgraphs:
        for deployment in subgraphs[project]['deployments']:
            schema = subgraphs[project]['schema']
            status = subgraphs[project]['deployments'][deployment]['status']
            if status != 'prod' or schema != schema_type:
                continue
            if len(data) >= 4:  # check if we've reached the subgraphs limit
                break
            try: # need this because not all have hosted-service field
                slug = subgraphs[project]['deployments'][deployment]['services']['hosted-service']['slug']
            except KeyError:
                print(f""KeyError: unable to extract data from '{slug}' for '{project}'"")   
            response = requests.post(base_url + slug, json={'query': query})
            time.sleep(1)
            if response.ok:
                response_json = response.json()
                headers = response.headers
                timestamp_query = headers.get('Date')
                try:
                    data.append((project, deployment, timestamp_query, response_json['data']['financialsDailySnapshots']))
                    print(f""Got data for: {slug}"")
                except KeyError:
                    print(f""KeyError: unable to extract data from '{slug}' for '{project}'"")
            else:
                print(f'Request failed for {base_url + slug} with status {response.status_code}')
            if len(data) >= 4:  # check if we've reached the subgraphs limit
                break
    
    # create dataframe
    df = pd.DataFrame(data, columns=['project', 'deployment', 'timestamp_query', 'data'])
    # adjust df
    df = df.explode('data')
    df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)
    # convert timestamp
    df['timestamp_query'] = df['timestamp_query'].drop_duplicates().reset_index(drop=True)
    df['timestamp_query'] = pd.to_datetime(df['timestamp_query'], format='%a, %d %b %Y %H:%M:%S %Z')
    # identifier
    df['product'] = 'hosted_service'
    # return result
    return df
```",1
7077,"dateadd-is-not-working-properly","## dateadd occasionally fails 

The screenshot 1 failed but screenshot 2 succeed for the incremental update. cam.date is already a timestamp without timezone. It only work when I cast it into text. If I keep it as timestamp, it says

function date_add(text, bigint, timestamp without time zone) does not exist

It is really weird to me, how come the cam.date can determine whether the macro (screenshot 3) work or not? The screenshot 3&4 are the utils function I wrote for my project. In addition, this macro actually works in other models in my project where ::timestamp is completely ok. I got really confused on why it sometimes works and sometimes does not.

![image|690x402](upload://xFEl96DexYnMA4Pt9bLjmvEUc1U.jpeg)",1
7077,"dateadd-is-not-working-properly","The screenshot 1 and 2 are almost exactly same expect in where clause
cam.date::timestamp > â€¦
vs
cam.date::text > â€¦
```
{{ config(
    materialized='incremental',
    unique_key=['channel','country_code','date','campaign_id','ad_group_id']
) }}

{{ simple_cte([
    ('amazonads_dsp_campaigns', 'source_ads__amazon_dsp_report_campaign_performance'),
]) }},

amazonads_dsp_matrics as (

    select
        'DSP' as channel,
        null::bigint as profile_id,
        cam.advertiser_country as country_code,
        cam.date::timestamp as ""date"",
        cam.order_id::text as campaign_id,
        ""order"" as campaign_name,
        '' as ad_group_id,
        '' as ad_group_name,
        sum(cam.click_throughs) as clicks,
        sum(cam.total_cost) as cost,
        sum(cam.impressions) as impressions,
        sum(cam.total_sales_usd) as sales14d,
        sum(cam.total_purchases_clicks) as conversions14d
    from
        amazonads_dsp_campaigns cam
    {% if is_incremental() %}
    where
        cam.date::text > {{ get_inc_key_timestamp_with_lookback(this, 'date', '', 48) }}
    {% endif %}
    {{ dbt_utils.group_by(8) }}
)

select * from amazonads_dsp_matrics

```",3
7084,"issues-with-dbt-sequential-insert-does-it-follow-order-of-queries","## We have a situation where we need multiple (4-10) inserts to a target table & target table is also serving as a source. The sequence in which queries are executed is critical for final results. This is incremental model

Each query is independent unit of work and you have dependencies between them to run sequentially within  BR table (ABC) model.
 Query 1 - Source 1 --> BR table (ABC)
 Query 2 - BR table (ABC) & source 2 --> BR table (ABC)
 Query 3 - BR table (ABC) & Source 3 --> BR table (ABC)
 Query 4 - BR table (ABC) & Source 4 --> BR table (ABC)


## We have tried the UNION all approach suggested in  https://docs.getdbt.com/guides/migration/tools/migrating-from-stored-procedures/2-inserts. However the results are not as expected, not sure if the order of queries is followed in above scenario. 

## code as below

{{config(
    materialized = 'incremental',
    full_refresh = false,
    schema = 'schema name',
    incremental_strategy = 'insert',
    )
}}
With  base_data AS(
select  * from {{ref('Query 1')}}
UNION
select  * from {{ref('Query  2')}}
UNION
select  * from {{ref('Query  3')}}
UNION
select  * from {{ref('Query 4')}}
)
select
*   
FROM base_data


## Is there any other alternative to execute this, could we stage the intermediate results or could this be handled via macro",1
7087,"dbt-run-fail-fast-gets-stuck-on-cancel-snowflake","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I'm using dbt-core==1.4.1 and dbt-snowflake==1.4.1

I'm running with dbt run --fail-fast 


```
dbt run -x
13:26:08  Running with dbt=1.4.1
13:26:08  Found 2 models, 0 tests, 0 snapshots, 0 analyses, 304 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
13:26:08
13:26:10  Concurrency: 1 threads (target='dev')
13:26:10
13:26:10  1 of 2 START sql table model xxx.model1  [RUN]
13:26:11  1 of 2 ERROR creating sql table model xxx.model1  [ERROR in 0.80s]
13:26:11  2 of 2 START sql table model xxx.model2 ... [RUN]
13:26:11  CANCEL query model.myproject.model1 ......... [CANCEL]

```


step 1  fails (the model container a syntax error `Snowflake adapter: Snowflake error: 001003 (42000): SQL compilation error: syntax error line 40`) , 

then it starts step 2 (model2) , which surprises me because I though that the --fail-fast meant that it won't continue at all. 

Then it gets stuck on CANCEL  (or maybe it's waiting for the model2 to complete, hard to tell)

Is there any way to make it fail fast for real? not starting model2 at all?",1
7087,"dbt-run-fail-fast-gets-stuck-on-cancel-snowflake","I waited  and it's not stuck forever. Instead of failing fast it just waits for the other model to complete

I reported it as as bug 
https://github.com/dbt-labs/dbt-core/issues/7110",2
7087,"dbt-run-fail-fast-gets-stuck-on-cancel-snowflake","Well, I realized that `--fail-fast` is a global options so  `dbt run --fail-fast` is incorrect. The actual right syntax is

`dbt --fail-fast run`

or 

`dbt -x run` 

but still it's   remarkable that `dbt run --fail-fast` does not produce any error. It should have errored with `dbt: error: unrecognized arguments: --fail-fast` if it's not supported there.",3
7087,"dbt-run-fail-fast-gets-stuck-on-cancel-snowflake","@ecerulm Wow, I never noticed that!

Although the docs show the correct usage, I thought that it was `dbt run --fail-fast`, it was automatic to think about the flags after `run`.

Thanks for pointing that out.",4
7087,"dbt-run-fail-fast-gets-stuck-on-cancel-snowflake","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
7096,"astonomer-cosmos-select","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## I'm trying the new astonomer-cosmos to orchestrate dbt with airflow. Anyone knows how to run only a specific model (i.e. dbt run --select my_model)? I managed only to run the entire project through the DbtTaskGroup:

```
with DAG(
    dag_id=""dbt_test"",
    start_date=datetime(2022, 11, 27),
    doc_md=__doc__,
    catchup=False,
    default_args={""owner"":""test""},
) as dag:

    # run the entire project
    dbt_run = DbtTaskGroup(
        dbt_project_name=""dbt_test"",
        conn_id=""redshift"",
        dbt_args={
            ""schema"": ""test"",
            ""dbt_executable_path"": ""/usr/local/airflow/dbt_venv/bin/dbt"",
        }
    )
```",1
7101,"surfacing-data-quality-issues","Right now we have several very complex transformations and we continually get questions from end users on why a particular row of source data didn't make it through all the transformation steps into the final database tables. This is sometimes easy to answer (""some source data field was invalid"") and sometimes difficult (""at some intermediate transformation step a particular join condition failed""). 

Our goal is to empower users to be able to see these problems themselves and not have to ask the engineering team.

We want to move our pipelines to dbt, but I am struggling to see how to implement this. tests came to mind first as you can save test failures in the database. We could build a UI on top of these tables so users could view problems. However, the downside is it seems rows that fail tests still make it through transformations. Ideally we'd be able to write some validation logic that would both record the problem in a db table for the UI as well as prevent the row from being included in the transformation.

I did see this post that seems to discuss the same issue, but it seems heavy weight for a seemingly common issue. I can't really imagine doing this for dozens of transformations.

https://blog.redpillanalytics.com/detecting-data-anomalies-via-an-inspection-layer-using-dbt-core-66a1a0787cfa

Has anyone faced this issue and found a simpler solution?",1
7103,"found-a-cycle-exception","I am trying to execute Merge action in DBT on my databricks. Getting 
**Found a cycle: model.myproj.transtoexpire --> model.myproj.trans** error.

I need to update certain columns in the existing record and append new record. Is there option to perform using DBT",1
7103,"found-a-cycle-exception","By using same table under ref and source, able to perform merge without any issue. Thanks.",2
7106,"transformation-postgres-existing-data","Hi everybody.

I've pulled data from Hubspot to Postgres DB using AirByte and want to transform next areas:
 - convert IDs from string to bigint
 - create and synchronize association tables (deals<->contacts, companies<->contacts)

I've seen that AirByte supports DBT. So I believe it will run my project after data pull step. I tried sample models in DBT. 

Is DBT fits for this purpose? What kind of models should I use to implement features described above?",1
7108,"how-to-pass-target-name-or-target-project-values-as-an-input-to-macro-while-calling-from-model","{%- if target.name == ""development"" -%} {% set event_date = get_max_value( '{{ target.dataset }}', '{{ target.table }}', 'session_date') %}

Now string is only accepted as macro input while calling the macro. but when i complie the target.dataset or target.table are not getting populated when its invoked from model.",1
7108,"how-to-pass-target-name-or-target-project-values-as-an-input-to-macro-while-calling-from-model","Hi @karrajan,

`target` variable belongs to the global scope, so you should be able to reference it without declaring it in macro.

Check this example of how to use it within jinja
https://docs.getdbt.com/reference/dbt-jinja-functions/target#use-targetname-to-change-your-source-database

The problem could be that you use double curly brackets.",2
7128,"dbt-fivetran-continuous-data-pipeline","Hi,
I'm working on a small project and I need to build a continuous data pipeline with fivetran and dbt. Here is a high level detail on the project.

Get the data from SFTP location to load it to snowflake. Use dbt for transforming the SFTP file and post the curated data in snowflake tables.

I understand Fivetran is used for Extract and Load and dbt is used for transformation. We have license for dbt cloud and fivetran. 

Question 1 : What is the difference between using dbt transformation in fivetran and having the project in dbt cloud. How are are they different and what are the benefits and use cases?. We don't have a whole lot of documentation related to this.

Question 2 : How do we build a continuous data pipeline without having any data orchestration tools. is there is a way to call fivetran connectors from dbt or call dbt jobs immediately after fivetran data sync?  

Question 3 : Using Fivetran and dbt is more of a ELT model. Is there is a way to design a ETL model with these tools? what i'm trying to understand is to extract data using fivetran and transform the data using dbt without loading the data to destination and then load the curated data to destination from dbt.

Any help is much appreciated. Thanks.",1
7135,"how-to-generate-tables-and-its-columns-schema-yaml-reading-from-snowflake-information-schema-columns","Background:
Instead of  manually typing all tables and columns and descriptions in schema.yml

Step 1: Add dbt-labs/codegen package in package.yml
```
packages:
  - package: dbt-labs/codegen
    version: 0.9.0

  - package: calogica/dbt_expectations
    version: 0.8.2
```

Step 2: Create file `macros/generate_source.sql`
```
{% macro generate_source(database_name, schema_name, source_name) %}

{% set sql %}
    with ""columns"" as (
        select '- name: ' || lower(column_name) || '\n            description: ""'|| lower(column_name) || ' (snowflake data type: '|| lower(DATA_TYPE) || ')""'
            as column_statement,
            table_name,
            column_name
        from {{ database_name }}.information_schema.columns
        where table_schema = '{{ schema_name | upper }}' and table_name not in ('FIVETRAN_AUDIT', 'SCHEMA_MIGRATIONS')
            and lower(column_name) not in ('_fivetran_deleted', '_fivetran_synced')
    ),
    tables as (
        select table_name,
        '\n      - name: ' || lower(table_name) || '\n        columns:' || listagg('\n          ' || column_statement || '\n') within group ( order by column_name ) as table_desc
        from ""columns""
        group by table_name
    )

    select listagg(table_desc) within group ( order by table_name )
    from tables;
{% endset %}

{%- call statement('generator', fetch_result=True) -%}
{{ sql }}
{%- endcall -%}

{%- set states=load_result('generator') -%}
{%- set states_data=states['data'] -%}
{%- set states_status=states['response'] -%}

{% set sources_yaml=[] %}
{% do sources_yaml.append('version: 2') %}
{% do sources_yaml.append('') %}
{% do sources_yaml.append('sources:') %}
{% do sources_yaml.append('  - name: ' ~ source_name | lower) %}
{% do sources_yaml.append('    description: """"' ) %}
{% do sources_yaml.append('    database: ' ~ database_name | lower) %}
{% do sources_yaml.append('    schema: ' ~ schema_name | lower) %}
{% do sources_yaml.append('    loader: fivetran') %}
{% do sources_yaml.append('    loaded_at_field: _FIVETRAN_SYNCED') %}
{% do sources_yaml.append('    meta:') %}
{% do sources_yaml.append('      owner: """"') %}
{% do sources_yaml.append('      tags: [""""]') %}
{% do sources_yaml.append('      subscribers: [""@data-team""]') %}

{% do sources_yaml.append('    tables:' ~ states_data[0][0] ) %}

{% if execute %}

{% set joined = sources_yaml | join ('\n') %}
{{ log(joined, info=True) }}
{% do return(joined) %}

{% endif %}

{% endmacro %}
```

Step 3: Create macro arguments docs in `macros/properties.yml`
```
version: 2

macros:
  - name: generate_source
    description: A macro to generate source tables and columns from database_name.information_schema.columns
    arguments:
      - name: database_name
        type: string
        description: The database name
      - name: schema_name
        type: string
        description: The schema name
      - name: source_name
        type: string
        description: The name of the source. For example, `raw_accounting`
```

Step 4: Run the following in your dbtCLI
```
dbt clean
dbt deps

# this will generate yml formatted, copy and paste it to your desired file location
dbt run-operation generate_source --args '{""database_name"": ""your_snowflake_database_name"", ""schema_name"": ""your_snowflake_schema_name"", ""source_name"": ""source_name_you_want_to_name_with_your_source""}'
```
**This can be improve by exporting the generated source as yml file",1
7137,"dbt-cli-on-gcp-cloudshell","## The problem I'm having
I've installed dbt cli bigquery on GCP cloud shell and cloned my github repo. Now when I'm trying to run a dbt model it throws error as - 

```
Encountered an error:
Database Error
  expected str, bytes or os.PathLike object, not NoneType
```

I've created profile.yml in ~/.dbt folder with the following structure -

```
default:
  target: dev
  outputs:
    dev:
      dataset: my_dataset
      job_execution_timeout_seconds: 1000
      job_retries: 1
      location: EU
      method: service-account
      priority: interactive
      project: my_project
      threads: 4
      type: bigquery
      keyfile_json:
        type: service_account
        project_id: XXXXXXX
        private_key_id: vasljasgjaspogjsapodgjasaklgjsagjs
        private_key: -----BEGIN PRIVATE KEY-----\nYYYYYYY\n-----END PRIVATE KEY-----\n
        client_email: XXXXXXXXX.iam.gserviceaccount.com
        client_id: 9577623947510934752394752
        auth_uri: https://accounts.google.com/o/oauth2/auth
        token_uri: https://oauth2.googleapis.com/token
        auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs
        client_x509_cert_url: https://www.googleapis.com/EWPOTEOIGH;LSGJJ;LJgcpsecuritykey-gbl-ww-pd.iam.gserviceaccount.com
```
The dbt_profile.yml file -


```
# Name your project!
name: 'my_new_project'
version: '1.0.0'
config-version: 2



# This setting configures which ""profile"" dbt uses for this project.
profile: 'default'


# These configurations specify where dbt should look for different types of files.
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  
.......
......
.......
```

Run command -> dbt run --select model_name

Can anyone please help me point what I'm doing wrong?",1
7137,"dbt-cli-on-gcp-cloudshell","When you run `dbt debug` is everything fine? Just to check if there is really some problem with the profiles and the project file.",2
7137,"dbt-cli-on-gcp-cloudshell","Yes brunoszdl, you are right the dbt debug gives the error message - 


```
Running with dbt=1.4.4
dbt version: 1.4.4
python version: 3.9.2
python path: /home/dbt_dive/dbt/dbt/bin/python
os info: Linux-5.15.65+-x86_64-with-glibc2.31
Using profiles.yml file at /home/dbt_dive/.dbt/profiles.yml
Using dbt_project.yml file at /home/dbt_dive/git_repo/dbt_project.yml

Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]

Required dependencies:
 - git [OK found]

Connection:
  method: service-account
  database: gcp-project-name
  schema: dbt_dive
  location: EU
  priority: interactive
  timeout_seconds: 1000
  maximum_bytes_billed: None
  execution_project: gcp-project-name
  job_retry_deadline_seconds: None
  job_retries: 1
  job_creation_timeout_seconds: None
  job_execution_timeout_seconds: 1000
  gcs_bucket: None
  Connection test: [ERROR]

1 check failed:
dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  expected str, bytes or os.PathLike object, not NoneType

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile
```
I don't know what's the problem here? The  database &  execution_project are same as the GCP poject name ""gcp-project-name"". Can anyone spot something wrong here? Please help.",3
7137,"dbt-cli-on-gcp-cloudshell","The '`method`' key is incorrectly indented or it was just a copy-and-paste problem?

And the only thing your profiles differ from mine is that `client_id` and `private_key` are between single quotes `' '`on my side.

Try that and see if it works.",4
7137,"dbt-cli-on-gcp-cloudshell","Hi brunoszdl, thanks for your quick response! 

'method' key was a copy & paste error, I've corrected that. 
I tried wrapping `client_id` and `private_key` in ' ' single quotes but still it didn't work. I really don't know what I'm doing wrong?",5
7137,"dbt-cli-on-gcp-cloudshell","So, let's take a step back to better analyze where the problem is. Configure your profile with just the keys needed, without the optional ones

and then try dbt debug. Then we can see if the credentials problem is in the optional ones or not.


```
my-bigquery-db:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account-json
      project: [GCP project id]
      dataset: [the name of your dbt dataset]
      threads: [1 or more]

      # These fields come from the service account json keyfile
      keyfile_json:
        type: xxx
        project_id: xxx
        private_key_id: xxx
        private_key: xxx
        client_email: xxx
        client_id: xxx
        auth_uri: xxx
        token_uri: xxx
        auth_provider_x509_cert_url: xxx
        client_x509_cert_url: xxx
```",6
7137,"dbt-cli-on-gcp-cloudshell","No, I think the problem is in my credentials as it's still giving me the same error. But the same credentials works for dbt cloud!",7
7137,"dbt-cli-on-gcp-cloudshell","Let me see just one more thing, can you paste here the log from `log/dbt.log`  related to the debt debug?
It will be at the bottom",8
7137,"dbt-cli-on-gcp-cloudshell","Hi brunoszdl,

This is the logs generated, I've pasted the last few lines 

```
05:16:00.543072 [debug] [MainThread]: Acquiring new bigquery connection 'master'
05:16:00.545125 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_'
05:16:00.545828 [debug] [ThreadPool]: Opening a new connection, currently in state init
05:16:00.546292 [debug] [ThreadPool]: BigQuery adapter: Got an error when attempting to create a bigquery client: 'expected str, bytes or os.PathLike object, not NoneType'
05:16:00.547996 [debug] [MainThread]: Connection 'master' was properly closed.
05:16:00.548340 [debug] [MainThread]: Connection 'list_' was properly closed.
05:16:00.548646 [info ] [MainThread]:
05:16:00.549211 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
05:16:00.550158 [error] [MainThread]: Encountered an error:
Database Error
  expected str, bytes or os.PathLike object, not NoneType
05:16:00.551455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xxxxx0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xxxxx0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0xxxxx0>]}
05:16:00.552013 [debug] [MainThread]: Flushing usage events
```
Output of dbt --version

```
Core:
  - installed: 1.4.4
  - latest:    1.4.4 - Up to date!

Plugins:
  - bigquery: 1.4.1 - Up to date!
```",9
7137,"dbt-cli-on-gcp-cloudshell","There one more thing I've discovered, if I write a simple python program to execute a query it works - 


```
>>> from google.cloud import bigquery
>>> client = bigquery.Client ()
>>> QUERY = (""select col1,col2 from project.dataset.table"")
>>> query_job = client.query(QUERY)
```
I checked the python path ->


```
dbt_dive@cloudshell:~ (gcp-project)$ which python
/usr/bin/python
```

```
(dbt)dbt_dive@cloudshell:~$ which python
/home/dbt_dive/dbt/bin/python
```
The python paths are different. Could this be the reason? Please help in correcting this.",10
7137,"dbt-cli-on-gcp-cloudshell","Ok, so the problem is a python error while creating the BigQuery client.

Sorry, I don't know how to proceed now :frowning: , maybe @joellabes or another one who knows the dbt-bigquery connection well may help.",11
7163,"changing-the-logic-of-column-in-incremental-model","Hello,

I have a doubt regarding incremental model the config I have for this model is below:
![image|516x158](upload://qw7ZkMBlxFTlwo6MrwMDnsNZWHM.png)

I have changed the logic for one of the existing column(and it is not unique key) to different field name from what it was before so just wondering if I need to do the full refresh for syncing those changes as I can't see the updated value for that particular column.
Please guide.

Thanks,
Arti",1
7163,"changing-the-logic-of-column-in-incremental-model","Hi @arti.aga16 

The incremental materialization has a setting called `on_schema_change`.
https://docs.getdbt.com/docs/build/incremental-models#what-if-the-columns-of-my-incremental-model-change

From the docs:
"" The possible values for `on_schema_change` are:

* `ignore`: Default behavior (see below).
* `fail`: Triggers an error message when the source and target schemas diverge
* `append_new_columns`: Append new columns to the existing table. Note that this setting does *not* remove columns from the existing table that are not present in the new data.
* `sync_all_columns`: Adds any new columns to the existing table, and removes any columns that are now missing. Note that this is *inclusive* of data type changes. On BigQuery, changing column types requires a full [table](https://docs.getdbt.com/terms/table) scan; be mindful of the trade-offs when implementing.""""

So if you want to see your new column you can use `append_new_columns` or `sync_all_columns`.

But notice this will not backfill the data for your new column. If you want to populate your new column with historical data you need to run the model in` full-refresh` mode",2
7163,"changing-the-logic-of-column-in-incremental-model","Hi brunoszdl,
Yeah I know about this feature but currently avoiding to use that so just trying to understand if i have changed the logic of existing field do i need to do the full refresh ?

Thanks,
Arti",3
7163,"changing-the-logic-of-column-in-incremental-model","If you want to apply this logic to historical data then yes,

 if you are ok with just recent data having this logic  and historical data a different logic then no",4
7163,"changing-the-logic-of-column-in-incremental-model","cool thanks so much :)",5
7164,"dynamic-schema-model-generation-based-on-a-metadata-table-in-snowflake","I am a beginner to DBT and am looking to explore it to build an incremental load from ""SourceDB.source"" to ""DestinationDB.schema1, DestinationDB.schema2, DestinationDB.schema3"" using company_id. The ""source"" schema is incremental data from the source. As a first step, I have 25 model files to generate the model using a unique id (company_id - hardcoded using the ""where"" clause and replicated the source data in DestinationDB.schema1).

## The problem I'm having 
I need a generalized solution to replicate the same with other company_id's.

## The context of why I'm trying to do this
Create data isolation for each company by creating individual schema using company id.

## What I've already tried 
I tried setting schema dynamically using for loop. I am getting two ""select"" statement syntax errors.

## Some example code or error messages
```
{%- call statement('companies_mapping_table', fetch_result=True) -%}
  select
    company_id,
    unique_key,
    target_schema
  from companies_mapping_table
{%- endcall -%}

{%- set result = load_result('companies_mapping_table') -%}
{%- set mappings = result['data'] -%}

{% for each in mappings %}
  {{ print(""Running Macro: "" ~ each[0] ~ "", "" ~ each[1] ~ "", "" ~ each[2]) }}

  -- Trying to create schema dynamically here
  {{
    config(
      database='destination_dev',
      schema='{{each[2]}}',
      materialized='incremental',
      unique_key='{{each[1]}}',
      incremental_strategy='merge'
    )
  }}

  select
    *
  from source_dev.raw_data.all_companies_data 
  where company_id = {{each[0]}}
  
{% endfor %}

```",1
7177,"usage-of-this-inside-model","SELECT 
* 
FROM {{ this }}

compiled into

select * from `project`.`dataset`.`result` but the expected outcome is `project`.`dataset`.`table_a`.

similar issue SO thread: https://stackoverflow.com/questions/72635337/dbt-using-this-function-has-different-results-in-compile-preview-vs-dbt-run",1
7177,"usage-of-this-inside-model","dbt-rpc (which powers the dbt Cloud IDE) doesn't correctly resolve references to `this`. Its replacement, dbt-server, will be able to do this in the future but we don't have a date to share on when that will be available in the IDE sorry!

<sub>Note: `@joellabes (dbt Labs)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678770263111799?thread_ts=1678259881.071419&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7179,"redshift-view-deadlocks","Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.

I am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.

Every single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.

By that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt's process is manually killed.

It seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.

What I've tried so far:
* decreasing number of threads to 1
* running during a time when there is no more DB activity (this works without a problem)

Any ideas? It's driving me crazy!

Thanks",1
7179,"redshift-view-deadlocks","I am also experiencing this issue in Redshift.  It is occurring for a view on a pretty simple and small table and the error occurrences include early morning schedules when there should be no activity on that table.  

It is intermittent though.  Not all scheduled runs have the issue.

I would love others thoughts on this.",2
7179,"redshift-view-deadlocks","@hags setting the late binding property for the views appears to have helped, but I don't really understand why",3
7179,"redshift-view-deadlocks","Here's an explanation of what late-binding views are, and why they are helpful: 
https://www.getdbt.com/blog/using-redshift-s-late-binding-views-with-dbt/",4
7195,"unable-to-view-the-enlarged-dag-for-models","Hi,

I am trying to generate the DAG for models but when I go and click the lineage graph it shows very small view which is hard to understand:
![image|213x500](upload://f0D08dl0fUDXXZwaj4mTjtK0DrQ.png)
and if I click the full screen it gives me every single model in the Project which I really don't want to see.I tried applying the filters but that dint work as well:

Is there any that i can export this whole model dependency structure as the server is reallly slow for us and it takes my of of time to see the dependencies.
Thanks,
Arti",1
7197,"how-to-change-default-transient-materialization-with-snowflake","Hi Team, 

It looks like DBT is creating a 'Transient' table by default when connected with Snowflake. 'Transient' table in Snowflake will only have a data retention period of 1 day. When i see final target sql queries generated for each model starts with ""create or replace TRANSIENT TABLE"". 

Instead of 'Transient', I would like to create a 'Permanent' table in Snowflake which will have a data retention period of 90 days.  Please help how can i change this default behavior. What I want DBT to generate SQLs is with  ""create or replace TABLE"" which will create a 'Permanent' table by default in Snowflake.

Thanks in advance for all your suggestions.",1
7197,"how-to-change-default-transient-materialization-with-snowflake","See this section of the documentation: <https://docs.getdbt.com/reference/resource-configs/snowflake-configs#transient-tables>

<sub>Note: `@Josh Devlin (he)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678325711707449?thread_ts=1678325486.946219&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7197,"how-to-change-default-transient-materialization-with-snowflake","Thank you. It helped.",3
7204,"what-can-i-do-if-destination-mysql-database-has-a-gtid-constraint-on","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
The destination Mysql replication has GTID on and  'create table as select ...'  not work for me.

## The context of why I'm trying to do this
I try to write data to cloud data base with dbt.",1
7216,"compilation-error-in-incremental-model","Hi community,
I am getting this compilation error with the following incremental model using dbt-sqlserver (DBT version 1.4.1).

Error message:
Compilation Error in model Stg_Usz_DRG_Diagnosis_v1 (models\staging\Stg_Usz_DRG_Diagnosis_v1.sql)
 macro 'dbt_macro__sqlserver__get_delete_insert_merge_sql' takes not more than 4 argument(s)

If I remove the unique_key DBT runs fine. Any suggestions would be much appreciated.

MyModel.sql:
```
{{
    config(
        materialized='incremental',
        unique_key='Diagnosis_ID'
    )
}}

select * from {{source('EntityView', 'Usz_DRG_Diagnosis') }}

{% if is_incremental() %}

  -- this filter will only be applied on an incremental run
  where LastUpdateDateTime >= (select max(LastUpdateDateTime) from {{ this }})

{% endif %}
```",1
7216,"compilation-error-in-incremental-model","I am not sure if this is the problem as I do not use dbt-sqlserver, but try to pass the column inside a list `unique_key=['Diagnosis_ID']`.

Maybe dbt is interpreting the string as a list of chars",2
7216,"compilation-error-in-incremental-model","Thanks for your suggestion, unfortunately the same error occurs.",3
7216,"compilation-error-in-incremental-model","It seems like the `get_delete_insert_merge_sql` macro in dbt-core is called with five arguments

![CleanShot 2023-03-09 at 16.36.14@2x|690x51](upload://4yBsAJvWuuehTyoW0QJfZsuXJP6.png)

And the macro in dbt-sqlserver accepts only four

![CleanShot 2023-03-09 at 16.37.48@2x|690x126](upload://ux7Amm8dySfULt83ffhpfVqhKcy.png)


Maybe that is the problem? Can you show the versions of dbt and dbt-sqlserver you are using?",4
7216,"compilation-error-in-incremental-model","If you are using dbt version 1.4, then I think that is the problem, because dbt-sqlserver latest version is 1.3.

Then I would suggest downgrading dbt version to 1.3 and raising an issue in the dbt-sqlserver

Edit: there is already an issue raised https://github.com/dbt-msft/dbt-sqlserver/issues/333",5
7216,"compilation-error-in-incremental-model","Thanks Bruno, I will test this asap (I am having other proxy issues) but I would find it surprising that installing dbt-sqlserver includes an incompatible dbt-core version.
Best",6
7216,"compilation-error-in-incremental-model","Confirmed, installing dbt-core==1.3 fixed the compilation error.
Thanks again.",7
7216,"compilation-error-in-incremental-model","Nice to hear that @Arnau, can you mark the answer as Solution? thanks :smiley:",8
7216,"compilation-error-in-incremental-model","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",9
7218,"export-dbt-documentations","could you give me some help in dbt docs I'd appreciate. doc website doesn't provide an answer and neither did the search function. How I can extract my dbt documentation as a file (pdf, excel, word, ... etc) /or something handy to my end user ?",1
7218,"export-dbt-documentations","I don't think the `dbt docs`  was considered as a static copy.  It is more interactive.  Click here, you go there.  Narrow down what you are looking at, etc.

I think your best bet is to take the documentation folder created when you issue `dbt docs generate`  and copy the entire hierarchy to some webserver where you can then send folks a URL to.

<sub>Note: `@boyer` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678383331416449?thread_ts=1678382120.942249&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7218,"export-dbt-documentations","Heads up dbt this would be a valuable function to provide - clients are requesting access to a data catalog / data dictionary but we have no way of providing it. Please correct me if there's an alternative.",3
7218,"export-dbt-documentations","You can provide a rich, interactive data dictionary by hosting the docs site somewhere (see [deploying the documentation site](https://docs.getdbt.com/docs/collaborate/documentation#deploying-the-documentation-site)), but it can't be a static word document etc.",4
7226,"unable-to-perform-refresh-git-state","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having 
I am unable to perform ""Refresh git state"" on the dbt Cloud IDE

## The context of why I'm trying to do this
Because I want to continue to work on my project.",1
7231,"speedup-merge-strategy-with-custom-incremental-predicate","Hi everyone,

I would like to share the solution to speedup the merge operation, highly inspired by [this thread](https://github.com/dbt-labs/dbt-core/issues/6658). The solution is to customize the `incremental_predicates` when using `incremental_strategy='merge'`. The idea is to pre-filter the partitions of the destination table before merging based dynamically on the values of the update. In my case, I noticed a 30% speedup when merging 100k rows into 1.4B rows (this number depends on the actual distribution of your data).


Now the nitty gritty details:

1/ in it's [native form with v1.4](https://docs.getdbt.com/docs/build/incremental-models#about-incremental_predicates), you can add the option `incremental_predicates` to filter the destination table in the config block. However, you can only add a static value. In other words, the filter cannot depend dynamically on the actual values of your update. Subqueries are not allowed, nor using variables, nor any jinja templating. In other words, the SQL command is not evaluated before runtime.

2/ to customize the behavior of `incremental_predicates`, you need to define the `get_merge_sql` macro which reads the values in your update, then add a predicate filter on the partition keys. The latter point is important since it will force dbt to skip the partitions. Otherwise, it will still scan the whole table.

caveat: I have not found a solution yet to make the macro adaptive to any situation. At the moment, I have defined an if statement each time I want to use a predicate for a different model.

Please find below a reproducible code. Let's assume that the source `mysource.yml` contains the following:

```yaml
- name: mytable
  columns:
    - name: id #primary key on which to perform the merge operation.
    - name: day #the partition key
    - name: myvalue #some data
    - name: _processing_time #how to filter between incremental updates
```

The model `mymodel.sql` using the custom `incremental_predicates`:

```
{{ 
    config (
        materialized=""incremental"",
        incremental_strategy='merge',
        file_format='delta',
        unique_key='id',
        partition_by=[""day""],
        incremental_predicates=[""custom_day""]
    )
}}

SELECT
    id, 
    day, 
    myvalue, 
   _processing_time
FROM {{ source('mysource', 'mytable') }} AS s
{% if is_incremental() %}
    WHERE s._processing_time > (select MAX(_processing_time) from {{ this }})
{% endif %}
```

The `get_merge_sql` macro:

```
{% macro get_merge_sql(target, source, unique_key, dest_columns, incremental_predicates) -%}
    {% set predicate_override = """" %}
      {% if incremental_predicates[0] == ""custom_day"" %}
        {% set get_limits_query %}
            SELECT
                MIN(day) AS min_day,
                MAX(day) AS max_day
            FROM {{ source }}
        {% endset %}
        {% set limits = run_query(get_limits_query)[0] %}
        {% set min_day, max_day = limits[0], limits[1] %}

        {% set predicate_override %}
            DBT_INTERNAL_DEST.day >= '{{ min_day }}'
            AND DBT_INTERNAL_DEST.day <= '{{ max_day }}'
        {% endset %}
    {% endif %}
    {% set predicates = [predicate_override] if predicate_override else incremental_predicates %}
    -- standard merge from here
    {% set merge_sql = dbt.get_merge_sql(target, source, unique_key, dest_columns, predicates) %}
    {{ return(merge_sql) }}

{% endmacro %}
```

Here are the steps to reproduce the behavior:
1/ create mytable with the SQL script (databricks):

```sql
CREATE TABLE mytable (id int, day date, myvalue int, _processing_time timestamp);

INSERT INTO mytable VALUES (1, '2023-01-01', 10, current_timestamp());
INSERT INTO mytable VALUES (2, '2023-01-01', 20, current_timestamp());
INSERT INTO mytable VALUES (3, '2023-01-02', 30, current_timestamp());
INSERT INTO mytable VALUES (4, '2023-01-03', 40, current_timestamp());
```
2/ `$ dbt run --full-refresh --select mymodel`
3/ add a new row to the source table:

```sql
INSERT INTO mytable VALUES (5, '2023-01-04', 50, current_timestamp());
```
4/ simulate the upsert with `$ dbt run --select mymodel`. If you have used a SQL warehouse, you should get in the query profile 'files pruned = 3', ie the merge should have skipped the existing partitions.",1
7231,"speedup-merge-strategy-with-custom-incremental-predicate","Yees ! Thanks a lot, that is exactly what I need for my project :star_struck:",2
7236,"nps-calculation-with-metrics","The problem I'm having:
I'm trying to make a metric for NPS calculation but I can't describe the expression correctly because the results don't compile.
For example, the expression can't handle an operation like this: A/B + C/D, because the parenthesis don't close. Also, everything that goes after the division / is left out from the parenthesis.
I had to make a workaround multiplying from the left side in order to make it work.

The context of why I'm trying to do this:
An NPS calculation which is an operation like this (A+B)/C * 100. This should return the result of the division * 100. But the 100 is calculated before the division occurs, so it returns (A+B) / (100 C).

What I've already tried:
100 * (A+B)/C. This worked.

Some example code or error messages

Here are some examples of derived metrics and their compiled code results:
1:
```
calculation_method: derived
    expression: ""({{ metric('nps_promoters') }} - {{ metric('nps_detractors') }}) / {{ metric('nps_surveys') }} * 100""
```
Compiled result 1:
```
select
        first_join_metrics.*
        , ((nps_promoters  / nullif( nps_surveys - nps_detractors) , 0) / nullif( nps_surveys * 100, 0)) as nps_app_bankaya
    from first_join_metrics
```
1 compiles but it does not return the expected result


2:  Of the Type (A/B - C/B) * 100
```
calculation_method: derived
    expression: ""({{ metric('nps_promoters') }} / {{ metric('nps_surveys') }} - {{ metric('nps_detractors') }} / {{ metric('nps_surveys') }}) * 100""
```

Compiled result 2
```
select
        first_join_metrics.*
        , ((nps_promoters  / nullif( nps_surveys - nps_detractors , 0) / nullif( nps_surveys) * 100, 0)) as nps_app_bankaya
    from first_join_metrics
```

A/(B - C) / (B * 100)",1
7237,"bigquery-ingestion-time-partitioning-and-partition-copy-with-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/bigquery-ingestion-time-partitioning-and-partition-copy-with-dbt",1
7242,"dbt-example-project-that-shows-dimensional-modeling-metrics-and-other-techniques","I put this project together a while ago to bring together various enterprise-level Business Intelligence techniques:
https://github.com/flexanalytics/dbt-business-intelligence

Here are some highlights:
* shows some introductory dimensional modeling techniques
* uses the dbt-labs/metrics package, showing a best-practice of how to build a wide table from your dimensional models
* uses the calogica/dbt_date package to build a robust Date Dimension
* uses the Snowflake-Labs/dbt_constraints (not just Snowflake) package to create primary and foreign key constraints, which can be used for joins/discovery even if the cloud data warehouse doesn't use them
* tested with Postgres, BigQuery, Snowflake, SQL Server, and Redshift adapters
* shows how to use dbt as a semantic layer (not ""the semantic layer"" that works on Snowflake only) to define metrics, dimensions, aggregations, calculations, data relationships, business-friendly names and descriptions, synonyms, formatting and other data catalog attributes

It would be awesome to have people contribute and build out more complex dimensional modeling and other enterprise BI features.",1
7242,"dbt-example-project-that-shows-dimensional-modeling-metrics-and-other-techniques","This is very cool! Making a note to dig into it further ðŸ‘€",2
7244,"incremental-merge-to-update-existing-value","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I am having a hard time wrapping my head around incremental strategies that involve an `UPDATE` like SQL syntax for existing values.

## The context of why I'm trying to do this

In SQL, I would normally have done an `UPDATE` like 

```sql
UPDATE products
SET quantity = quantity + 3
```

where the Product table is:

```
Products
    Quantity
    5
```

and the result is 

```
Quantity
8
```

I've been reading the DBT document around incremental strategies, and I can't quick get the answer I am looking for. I've manually tested it, and it works the way I would think it does, but would love a confirmation that I am not misunderstanding

## What I've already tried 
My test table. I started with the single Timestamp '2022-01-01', created my intermediate table with the incremental strategy, then added new row > ran my incremental model again, etc.. for each of the other timestamp for ProductID = 1

```sql
{{ config(materialized='table') }}

SELECT
    product_id,
    event_timestamp,
    quantity
FROM VALUES
  (1, TIMESTAMP '2022-01-01', 10),
  (2, TIMESTAMP '2022-01-01', 1),
  (3, TIMESTAMP '2022-01-01', 1),
  (1, TIMESTAMP '2022-01-02', 1),
  (1, TIMESTAMP '2022-01-03', 10)
AS 
  t(product_id, event_timestamp, quantity)
```

My aggregation / incremental table
```sql
{{ config(
        materialized = 'incremental',
        unique_key = 'product_id',
        incremental_strategy='merge',
   ) 
}}

WITH new_events AS (

    SELECT
        product_id,
        MAX(event_timestamp) AS last_updated,
        SUM(quantity) AS quantity
    FROM
        {{ ref(""base__test_products"") }}
    {% if incremental %}
    WHERE
        event_timestamp > (SELECT MAX(last_updated) FROM {{ this }})
    {% endif %}
    GROUP BY
        product_id
)

SELECT
    product_id,
    last_updated,
    quantity
FROM
  new_events
```

Thanks in advance!!",1
7245,"triggering-power-bi-refresh-from-a-python-model","I've been contemplating the best approach for triggering Power BI refreshes, and in particularly ensuring that dashboards are triggered according to when their underlying dependencies are updated.  It occured to me that rather than representing a dashboard in dbt as an exposure, it might be possible to represent them as a Python model.  I haven't tested this theory, but hypothetically, could you...

1. Create a python model that correlates to a Power BI dashboard
2. Within the model, make an API call to the Power BI service to refresh the dashboard
3. Make some kind of dummy references to dependencies, so these are captured in the DAG
4. Any response from the API could be captured and returned as the model output -- effectively acting as a log

Has anyone tried anything similar?",1
7251,"pros-cons-of-using-dbt-incremental-as-an-ingestion-tool-from-cloud-sql","hi dbt community,

a question on using dbt as data ingestion tool (with the incremental model)

i am trying to find out the pros & cons using 1) dbt incremental model to bring data from or production database under GCP cloud SQL into BigQuery vs 2) using GCP datastream to bring data directly from the production database into BigQuery

if the production database is hosted under GCP cloud SQL, BigQuery can query the production db directly via the 'EXTERNAL_QUERY()' feature. as such, one in theory can build an incremental model in dbt with syntax 'select * from external_query(<hosting area>, 'select * from production db table name') to bring incremental changes from the prod db.

what is the benefit and disadvantage over using GCP datastream?",1
7258,"tightening-up-security-when-using-dbt-core-with-github-actions","## The problem I'm having
We want to use GitHub actions to schedule dbt (i.e., dbt run ) but we don't want to allow  other (non dbt) steps in the action to access our database.

## What I've already tried 
We've implemented this using the action ""mwhitaker/dbt-action@master"", which requires the secrets to be held in GitHub and published into the environment of the runner. 


## The problem I'm trying to overcome
This approach works  fine but it allows developer to  add other steps that can  use these crednetial to access the database and extract data.

Is there another approach where only dbt can access the credentials.",1
7258,"tightening-up-security-when-using-dbt-core-with-github-actions","[quote=""kevin.mcisaac, post:1, topic:7258""]
add other steps that can use these crednetial to access the database and extract data.
[/quote]

The readme notes that:
> you can call this action with a specific [release](https://github.com/mwhitaker/dbt-action/releases), eg `mwhitaker/dbt-action@v0.21.0` or `mwhitaker/dbt-action@v0.19.1`

If you did this, you would also know that the release only contains the steps you see today. 

[quote=""kevin.mcisaac, post:1, topic:7258""]
Is there another approach where only dbt can access the credentials.
[/quote]
If you're using the action, you implicitly have to trust that the developer isn't going to misuse those credentials. You could instead use dbt Cloud for your orchestration, and either use its built-in scheduler (no GHA at all) or trigger a DBT Cloud run from a GHA instead (you would still need to provide a dbt Cloud API key to GitHub, but it wouldn't be your database credentials which might be preferable). 

Or you could fork the action and use your own copy of it?",2
7264,"how-get-curent-date-in-dbt-cli","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
Trying to build model and have dynamic variable - today date as string (YYYY-MM-DD)

## The context of why I'm trying to do this
Need to have models with historical date filter and current date filters.

## What I've already tried 
```dbt run --select +model --vars '{""key"": ""value"", ""current_date"": ""now().date()""}'```
```dbt run --select +model --vars '{""key"": ""value"", ""current_date"": ""{{ macros.date() }}'```

## Works manually
```dbt run --select +model --vars '{""key"": ""value"", ""current_date"": ""2023-03-13""}'```

## Solution
In BigQuery you can use ```current_date()``` function to get YYYY-MM-DD date.
In SQL you can use ```now().date()```

SETUP:
dbt_project.yml var:
```current_date: current_date: ""'2022-12-31'""```

macro:
```where transaction_date <= {{current_date}}```

model sql:
```
with model as ( 
    {{ macro_with_filter(current_date=var('current_date')) }}
)

select * from model
```

This setup will for for CLI's:
```
dbt run --select model_name 
dbt run --select model_name --vars '{""key"": ""value"", ""current_date"": ""current_date()""}'
```",1
7264,"how-get-curent-date-in-dbt-cli","Hi @Vytlksn, how are you using this var in your model?",2
7264,"how-get-curent-date-in-dbt-cli","Hi,

And have it in dbt_project.yml as as VAR : ```current_date: '2020-10-01'```

in Macro : ```  where transaction_date <= '{{current_date}}'  ```

and in model: 
```
with name as ( 
    {{ macro_with_filter(current_date=var('current_date')) }}
)

select * from name 
```",3
7264,"how-get-curent-date-in-dbt-cli","Have you tried in the macro
`where transaction_date <= {{current_date}}`
without quotes?",4
7264,"how-get-curent-date-in-dbt-cli","Yes, model takes ```var('current_date')``` and generate model with it. No problem with variable from dbt_project.yml.
CLI works also  like: ```dbt run --select +model --vars '{""key"": ""value"", ""current_date"": ""2023-03-13""}'```

But how instead of '2023-03-13 'get dynamic variable of today date in CLI? :/",5
7264,"how-get-curent-date-in-dbt-cli","You should be able to do this you wrote

`dbt run --select +model --vars '{""current_date"": ""now().date()""}'`

But I wanted to check if the problem is with the quotes here
`where transaction_date <= '{{current_date}}'`

I did not understand if you tried without the quotes, like this:
`where transaction_date <= {{current_date}}`

I think in the first option you are passing a string, and not the date function.",6
7264,"how-get-curent-date-in-dbt-cli","If I leave in macro: ```where transaction_date <= {{current_date}}```

model: 
```
with model as ( 
    {{ macro_with_filter(current_date=var('current_date')) }}
)

select * from model
```

And run CLI: 
```
dbt run --select +model --vars '{""key"": ""value"", ""current_date"": ""now().date()""}'
```

Got Syntax error: Function call cannot be applied to this expression. Function calls require a path, e.g. a.b.c()
I am using BigQuery",7
7264,"how-get-curent-date-in-dbt-cli","In bigquery you can use the current_date() function instead of now().date()",8
7264,"how-get-curent-date-in-dbt-cli","Oh, really! It worked :slight_smile :slight_smile: 
But now if run simple ```dbt run --select model``` 
got error
``` No matching signature for operator <= for argument types: DATE, INT64. Supported signature: ANY <= ANY```
But probably it's another topic.",9
7264,"how-get-curent-date-in-dbt-cli","can you show the generated .sql file in target/compile?",11
7264,"how-get-curent-date-in-dbt-cli","Generated SQL

```
with payouts as ( 
    
-- config

with payouts as ( 
    select 
        *
    from table_a
    where transaction_date <= 2022-12-31
    group by 
        column_b
)

select * from payouts

)

select * from payouts
```

I am misisng quotes '' to compile correctly, but with ```current_date()``` works fine.",12
7264,"how-get-curent-date-in-dbt-cli","I added  in dbt_project var: ```current_date: ""'2022-12-31'""```
And now works with VAR and with curent_date()
Thanks, good luck!",13
7264,"how-get-curent-date-in-dbt-cli","I look up current date from date dimension.
If you use a function like getdate(), you wont get benefits of cached results. Which is important for me to keep costs down.",14
7264,"how-get-curent-date-in-dbt-cli","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",15
7269,"how-to-get-dynamic-column-list-for-a-source-table","is there a way to get the actual column list to use in select statement from a source table? I used select * from source table for now.",1
7269,"how-to-get-dynamic-column-list-for-a-source-table","Not exactly sure what youâ€™re trying to do, but assuming you have a source and you want to create a model based on those columns, using the <https://github.com/dbt-labs/dbt-codegen|codegen package> will probably be helpful?

<sub>Note: `@lisa fang` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678897669672189?thread_ts=1678714524.330359&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7282,"cannot-disable-debug-mode","Somehow I've got stuck with debug mode on, and I can't seem to switch it off, I keep getting output like;

```
dbt debug
20:30:07  Running with dbt=1.4.5
INFO:stdout_log:20:30:07  Running with dbt=1.4.5
INFO:file_log:

============================== 2023-03-13 20:30:07.286430 | 9bf66394-948e-4e19-adbd-162eaafb697b ==============================
20:30:07.286430 [info ] [MainThread]: Running with dbt=1.4.5
DEBUG:file_log:20:30:07.289374 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/najmead/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
DEBUG:file_log:20:30:07.289831 [debug] [MainThread]: Tracking: tracking
DEBUG:file_log:20:30:07.295986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56461ca3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56461ca250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56461cafd0>]}
dbt version: 1.4.5
python version: 3.9.2
python path: /usr/bin/python3
os info: Linux-5.10.0-20-amd64-x86_64-with-glibc2.31

```
I've checked profiles.xml and dbt_project.xml, and can't see any particular settings I've switched on.  I've even tried to init an entirely new project, and still get the same result.

I saw [this post](https://discourse.getdbt.com/t/how-can-i-disable-debug-logs/6261) with a similar issue, but their resolution was related to VS Code -- I've tried running both from within and outside VS Code and I'm getting the same result.

What have I done???",1
7285,"doc-generation-problem","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
what to do if a model expects a parameter and the docs fail because of that? trying to generate docs but failing because of this.",1
7292,"create-view-with-two-part-identifiers-instead-of-3","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
VIEWS (materialization) are created with 3 part identifiers.
Example of a dbt compiled code sql:

```
create view ""schema_name"".""view_name"" as 
select ......... from ""**database_name**"".""schema_name"".""table_name""
```

## The context of why I'm trying to do this
we would need to skip the ""database_name"" from the view defintion. as have a process that copies definitions and moves across databases. is it possible?

what we need is without ""database_name"",

```
create view ""schema_name"".""view_name"" as 
select ......... from .""schema_name"".""table_name""
```",1
7298,"how-to-identify-models-not-tagged-and-or-not-documented-in-yaml","I am wondering if there is a more efficient way (besides manually scrolling through all folders and yamls) to identify models that may be slipping through the cracks  during scheduled runs because they are not tagged properly as well as models that are not currently documented in yaml files.",1
7298,"how-to-identify-models-not-tagged-and-or-not-documented-in-yaml","Check out [dbt-checkpoint](https://github.com/dbt-checkpoint/dbt-checkpoint), which is a dbt package with tests for models without documentation or tags, amongst other things!",2
7302,"rebuild-an-incremental-model-while-keeping-the-older-version","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I know we could use the `--full-refresh` option for `dbt run` to rebuild the data table from the beginning of time  and wipe out all the records from the previous model runs. Is it possible to rebuild from scratch while still keep the previous table records such that I could query both model versions? Thanks!

## The context of why I'm trying to do this
We are dealing with an evolving business logic for the data modeling work. Whenever there is a newer version of business logic, we would need to rebuild the data model. We'd like to maintain a stack of different model versions such that we could always revisit older version logic for the purpose of debug and making comparisons of model output.",1
7302,"rebuild-an-incremental-model-while-keeping-the-older-version","Right now, this isn't possible within core dbt constructs but will be soon <https://github.com/dbt-labs/dbt-core/discussions/6736>

The only way right now is to manually name your models and do it that way.

<sub>Note: `@Josh Devlin (he)` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678857285145329?thread_ts=1678855220.982159&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7304,"dbt-project-evaluator","What is refresh process on the int_all_dag_relationships in dbt-project evaluator package? Is the data update dynamically or executes it a scheduled time. How can we see the schedule in the second case",1
7307,"15-second-gap-in-cloud-run","Hello-

There appears to be a ~15 second gap/pause at the beginning of our DBT cloud runs. The attached screenshot shows the gap in model timing.  Below is an excerpt from the run log. In this example, we have 3 threads, and the log shows 3 files successfully run, then 18 seconds until the next 3 files are run.

Is this expected behavior? If not, are there any suggestions on how to close the gap?

DBT version is 1.4.5 and warehouse is Snowflake. The files on either side of the gap are base staging with ephemeral materialization. Thank you.

![image|322x290](upload://1Imw0pGJm3tmvQwdttBYMRE5hNf.png)

```
06:30:48  6 of 91 OK created sql view model BUSINESS.stg_[...] .............. [SUCCESS 1 in 1.50s]
06:30:49  5 of 91 OK created sql view model BUSINESS.stg_[...]  [SUCCESS 1 in 2.30s]
06:30:50  7 of 91 OK created sql view model BUSINESS.stg_[...] ......... [SUCCESS 1 in 1.62s]
06:31:08  8 of 91 START sql view model BUSINESS.stg_[...] ................ [RUN]
06:31:08  9 of 91 START sql view model BUSINESS.stg_[...] ........................... [RUN]
06:31:09  10 of 91 START sql view model BUSINESS.stg_[...] .......................... [RUN]
```",1
7316,"setting-custom-schema-in-dbt-project-yml","This is probably a really daft question, but I am trying to follow the instructions at (https://docs.getdbt.com/docs/build/custom-schemas) to be able to set custom schema so that the models in one folder go to a different schema (which I am happy to be target_custom in the normal way).

If I put in an individual model, the following code creates a table in default_openair (which is what I want):
{{ config(schema='openair') }}

But I'd like to be able to do this on batch for a number of files in the folder (yet to be created). I've created a dbt_project.yml which looks like this:
![image|690x463](upload://6PDERjNSrfnswUbKqbqKaBH887j.png)

* The arrow points to the file I am testing - this keeps showing up in just my 'target' schema (which is 'default').

* 1 points to the project name - that was created by default, and when I try and change it causes an error. 
* 2 shows what I've tried - but I also have tried it without the 'my_new_project' bit of code.

In-case its helpful the code from dbt_project.yml is here:
```

# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: 'my_new_project'
version: '1.0.0'
config-version: 2

# This setting configures which ""profile"" dbt uses for this project.
profile: 'default'

# These configurations specify where dbt should look for different types of files.
# The `source-paths` config, for example, states that models in this project can be
# found in the ""models/"" directory. You probably won't need to change these!
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  # directory which will store compiled SQL files
clean-targets:         # directories to be removed by `dbt clean`
  - ""target""
  - ""dbt_packages""


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/ directory
# as tables. These settings can be overridden in the individual model files
# using the `{{ config(...) }}` macro.
models:
  my_new_project:
    openair:
      staging:
        +schema: openair
  
  my_new_project:
    # Applies to all files under models/example/
    example:
      materialized: view
```

Apologies - I have done a search on the forum and couldn't find the magic answer - but I'm sure I'm going to be kicking myself in a moment... 

Thanks,

James",1
7316,"setting-custom-schema-in-dbt-project-yml","As an answer to my own question, if anyone else is looking in the future, it was because their were two sets of 'my_new_project' at the bottom of the dbt_projects.yml file (under model).",2
7316,"setting-custom-schema-in-dbt-project-yml","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
7320,"dbt-cloud-saved-run-results-artifacts-package","Hi

Is there a way in DBT cloud for us to see past run results from the beginning of DBT cloud usage? For context, my team has been deployed on DBT Cloud since the beginning of January and only have our dbt_artifacts package dating back to the beginning of February. I want to be able to understand our past run results and model execution times since the beginning of our dbt cloud usage. Is this possible? Has anyone explored this?",1
7320,"dbt-cloud-saved-run-results-artifacts-package","Yes you can access all artifcats using the [getArtifactsByRunId API](https://docs.getdbt.com/dbt-cloud/api-v2#tag/Runs/operation/getArtifactsByRunId)

<sub>Note: `@ryan.hauldren` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678911252799759?thread_ts=1678905223.666199&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7320,"dbt-cloud-saved-run-results-artifacts-package",":exploding_head:

<sub>Note: `@Keolina In` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678917633968999?thread_ts=1678905223.666199&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",3
7325,"execute-models-a-max-of-n-times-per-day","Hello, my dbt project is starting get large and models are getting used in multiple data flows. It's great to re-use existing models but scheduling jobs efficiently is becoming a challenge. Currently, we have jobs to build ""core"" models  like this: dbt run --select +mart_1+.

The problem is that a dependency for mart_1 can also be a dependency for mart_2. Mart_1 and mart_2 only really have 1 common table. If we schedule mart_2 as dbt run --select +mart_2+ then both lineage paths get run entirely. I really only want to run each model distinctly once, but in order of their dependencies.

My thought was to indicate to dbt that I want to only run models N times per day (once in this case). If a job triggers that model to run a second time, skip it and move to the next model. Is there a mechanism to do this or a more elegant way to solve the problem?

I could put the only dependency in as a source but that feels like it could create circular logic as time passes and the project gets more and more nested.",1
7340,"my-hive-incremental-not-working","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I would like to use incremental on hive, but it does not work based on my configuration.

## The context of why I'm trying to do this
I want to improve my sql, use 'inset into'.
## What I've already tried 
I has use 'incremental_strategy='inert_overwrite'and incremental_strategy='append'',but
the result is:
create  table xxx.test0302_1
    as
select * from xxx.test0302
errorï¼š
  Unable to establish connection to Hive server: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: xxx.test0302_1



## Some example code or error messages

this is my jinja:
{{ config(materialized='incremental', incremental_strategy='insert_overwrite',unique_key='') }}

 
select * from {{ ref('test0302') }} 
 
 {% if is_incremental() %}
   where event_time >=  (select max(event_time) from misphq.test0302_1) 
 
 {% endif %}




but:




```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
7345,"is-it-ok-to-join-marts-tables","Let's suppose I have on my marts layer the following tables:
- dim_customers
- dim_accounts
- fct_transactions

I want to create a report that joins all these tables. Therefore my query will contain the 3 tables. 

BUT I do not want to put the query in my visualisation tool. Instead I would like to create a table/view that joins these tables from the marts folder and my visualisation tool would simply do a select * from ""joined_table"".

Should I create that ""joined_table"" in the marts layer by joining the dim and fct tables ?

How would you approach this scenario and what is your current approach ?

Thanks in advance",1
7345,"is-it-ok-to-join-marts-tables","We do lots of this on my team. We have a separate directory called /datasets/ which is downstream of /marts/ where we put our joined and pre-aggregated datasets for our BI users to benefit from. This works well with our BI tool, Sigma, which has a presentation-layer concept of ""Datasets"" where we do the friendly naming, set column formats, add descriptions, etc. We materialize our datasets as tables in dbt. 95% of the time our BI tool is querying a flat table for a given viz/chart. We find it to be extremely fast, and our non-data expert end users get a lot of value. Happy joining!

<sub>Note: `@Joe Bloom` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1678977217763549?thread_ts=1678972173.803999&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7349,"redshift-connection-error-port-16455","I am getting an error when connection to Redshift about port 16455? Not sure what that port is needed for?

I was able to connect DBT Cloud through SSH to Redshift yesterday and execute queries. 
But, today, I am getting the following error: 

```
Opening a new connection, currently in state init
Redshift adapter: Connecting to Redshift using 'database' credentials
Postgres adapter: Got a retryable error when attempting to open a redshift connection.
Postgres adapter: Error running SQL: with
Postgres adapter: Rolling back transaction.
Timing info for rpc.warehouse.request (execute): 2023-03-16 15:53:01.286054 => 2023-03-16 15:53:01.287892
On rpc.warehouse.request: No close available on handle
Got an exception: Database Error
  could not connect to server: Connection refused
  	Is the server running on host ""127.0.0.1"" and accepting
  	TCP/IP connections on port 16455?
```

Not sure why DBT is trying to connect to port 16455?",1
7349,"redshift-connection-error-port-16455","Restarting the DBT Browser IDE fixed the issue. But curious to know what this port is for or why I was seeing this error?",2
7354,"publishing-metadata-into-a-data-catalogue","Has anybody been able to publish the table and column descriptions, and other meta data, from dbt-core into a Data Catalogue, particularly Alation",1
7359,"source-freshness-selecting-target-database","## dbt source freshness for a specific database
Hey, trying to check the freshness of the source but not sure how to select the target database.
I'm running this first

> `dbt source freshness --select source:DATABASENAME`

as I have other sources in yml file and then

> `dbt run  --target first`

but during the freshness check dbt randomly selects the target database. 
Does anyone know how to solve it? 
Can I use  `--target`  together with` dbt source freshness` command?

## This is my code
```
sources:
  - name: DATABASENAME
    schema: STAGE
    database: |
      {%- if  target.name == ""first"" -%} STAGING_FIRST
      {%- elif target.name == ""second""  -%} STAGING_SECOND
      {%- endif -%}

    freshness: # default freshness
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
    loaded_at_field: insert_date

    tables:
      - name: orders
        freshness: 
          warn_after: {count: 6, period: hour}
          error_after: {count: 12, period: hour}
```",1
7359,"source-freshness-selecting-target-database","I am not 100% sure, but I think you can use `--target` in `dbt source freshness`, have you tried it?",2
7359,"source-freshness-selecting-target-database","Yeah, it worked actually :slight_smile:  Thanks!",3
7359,"source-freshness-selecting-target-database","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
7362,"change-dbt-sql-wih-macros","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I have dbt macro with SQ model and need to take var from dbt_project.yml and use it in CLI.
Macro example
```
{% macro model_with_filter(factor_sql_filter) %}

select  *
    from tableA
    where 
        {{ factor_sql_filter }}

{% endmacro %}
```
```factor_sql_filter``` is  var in YML:
```""service_type in ('INTEREST_FEE', 'COMMISSION_FEE', 'LATE_FEE') and type != 'CREDIT_MEMO'""```

## The context of why I'm trying to do this
I don't want duplicate models with one different row, so I am trying with one macro create  multiple models.

## What I've already tried 
```
dbt run --select model_name --vars '{""factor_sql_filter"": ""factoring_sql_filter_ci""}'  
dbt run --select model_name --vars '{""factor_sql_filter"": var(""factoring_sql_filter_ci"")}' 
```
got error: dbt: error: unrecognized arguments on different tries

Any ideas how to solve it?",1
7362,"change-dbt-sql-wih-macros","I didn't understand what you want to do. Do you want to create several models and each one of them will have this macro with a different filter?

If so, and your model is just a select star with a single filter, I would rather use the SQL code in the models, as it makes them more readable, and you don't need to worry about vars.

But it is a personal preference, if you want do it your way, you can do it like this:

Your macro is ok.

In your model, you should call the macro this way: 
```python
{{ model_with_filter(var('factor_sql_filter')) }}
```

And you can either put the var in your` dbt_project.yml`
```yaml
vars:
  factor_sql_filter: ""service_type in ('INTEREST_FEE', 'COMMISSION_FEE', 'LATE_FEE') and type != 'CREDIT_MEMO'""
```
or in the CLI like
```
dbt run -s my_model --vars ""factor_sql_filter: service_type in ('INTEREST_FEE', 'COMMISSION_FEE', 'LATE_FEE') and type != 'CREDIT_MEMO""
```",2
7362,"change-dbt-sql-wih-macros","When you do something like
```
dbt run --select model_name --vars '{""factor_sql_filter"": ""factoring_sql_filter_ci""}'  
dbt run --select model_name --vars '{""factor_sql_filter"": var(""factoring_sql_filter_ci"")}'
```

You are overwriting your project variables with the value you are passing in the CLI, you can't call project variables from the CLI.",3
7362,"change-dbt-sql-wih-macros","Thanks!
After some building, decided to build two different models for simplicity instead of creating two models from one with CLI.",4
7362,"change-dbt-sql-wih-macros","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",5
7382,"using-dbt-withsynapse-spark","Morning all

I'd like to find out if anyone has managed to get dbt working with against Synapse Spark?

I've manged to create a test project which I have configured against Databricks and Synapse Dedicated pool, but I'm wondering if it is even possible to use Synapse Spark as a transformation engine using dbt.

I have created a database with external tables over my parquet\delta sources, but it seems to me that:

(1) the SQL functionality of Synapse's Spark implementation is too limited
(2) it may not be possible to configure dbt to access Synapse Spark as an end-point.

Any thoughts?

Thanks!",1
7395,"identifying-root-nodes-using-manifest-json","Hi, this is a follow up post for https://discourse.getdbt.com/t/is-there-a-way-either-or-show-only-the-most-left-handed-sources-models-in-the-dag/7022.  I cannot reply to that post anymore as it is closed.  However, thanks for the suggestion to use the manifest.json to find out the root node of a complex dbt dag.  I went further and wrote the code for the same using python.  Wanted to leave a note here so that if someone in need of it, can reference it.


```
import json
f = open('/Users/Humapathy/Desktop/manifest.json') # <location of the manifest.json file along with the path>
d=json.load(f)

node_to_analyze = 'model.openfit.dim_platform' # <The node in the dbt document dag for which the root notes (with no predecessors) to be found> 

# Fetch all the immediate 1st order parents of node in question.
for i in d['nodes']:
    if i==node_to_analyze:
        j=d['nodes'][i]
        pred_nodes = j['depends_on']['nodes']
print (pred_nodes)
# Initializing variables
pred_nodes1 = []
pred_nodes1_removed = []
root_parent_node =[]
x = True

#Loop through the 1st order parent nodes, if they in turn have first order parents, add them to the loop (keep looping), if they DO not have first order parents, then note them as root parents and remove them from loop.  

while x:
    for k in pred_nodes:
        l = d['nodes'][k]
        if (l['depends_on']['nodes'] ==[]):
            pred_nodes1_removed.append(k)
            if k in pred_nodes1:
                pred_nodes1.remove(k)
            root_parent_node.append(k)
        else:
            if pred_nodes1_removed:
                for m in l['depends_on']['nodes']:
                    if m not in pred_nodes1_removed: 
                        if m not in pred_nodes1:
                            pred_nodes1.append(m)
            else:
                for m in l['depends_on']['nodes']:
                    if m not in pred_nodes1:
                        pred_nodes1.append(m)
            pred_nodes1_removed.append(k)
            if k in pred_nodes1:
                pred_nodes1.remove(k)
    pred_nodes = pred_nodes1
    if pred_nodes1 == []:
        x = False
        break

print('Root Parent Nodes For ', node_to_analyze)
print('\n'.join(str(el) for el in root_parent_node ))
```",1
7395,"identifying-root-nodes-using-manifest-json","Here's Snowflake SQL to produce a table of model-descendent pairs (including self for ease-of-use).

The harder part is probably getting the manifest's `node_models`, `child_map`, and `run_results` in Snowflake in the first place.

We join this with Snowflake query history to identify models that are un-unsed in terms of directly accessing queries AND indirectly accessing queries (of downstream models).

```
{{
  config(
    materialized='table'
  )
}}


with run_nodes as (
  select * from {{ ref('stg_dbt_manifest__nodes_models') }}
)

, run_node_children as (
  select * from {{ ref('stg_dbt_manifest__child_map') }}
)

, nodes as (

  select
      run_nodes.node
    , run_nodes.run_id
    , run_nodes.table_sk
    
  from run_nodes
  
  where run_nodes.run_id = (select max(run_id) from run_nodes)
    and run_nodes.resource_type = 'model'
    and run_nodes.database = 'dbt_prod'
    and run_nodes.schema in  ('core', 'analytics_engineering', 'staging', 'intermediate')
    and run_nodes.run_last_update > current_date - 5

)

, node_children as (

  select 
      run_node_children.node
    , run_node_children.child as descendant
      
  from run_node_children
  -- to apply node filters from above
  inner join nodes as node
    on run_node_children.node = node.node
  inner join nodes as child
    on run_node_children.child = child.node

  where run_node_children.run_id = (select max(run_id) from run_node_children)

)

, node_descendants_recursive (node, descendant, depth, path_array) as (

  select
      node
    , descendant
    , 1 as depth
    , array_construct(node, descendant) as path_array

  from node_children

    union all
  
  select 
      node_descendants_recursive.node
    , node_children.descendant
    , node_descendants_recursive.depth + 1 as depth
    , array_cat(node_descendants_recursive.path_array, to_array(node_children.descendant)) as path_array

  from node_descendants_recursive
  inner join node_children
    on node_descendants_recursive.descendant = node_children.node

  where node_descendants_recursive.depth < 50 -- recursion hard-stop, current DAG depth is ~25

)

, node_descendants_and_self as (

  select 
    node
    , descendant
    , depth
    , path_array

  from node_descendants_recursive
  
  union all 

  select
    node
    , node as descendant
    , 0 as depth
    , to_array(node) as path_array 

  from nodes

)

, node_descendants as (

  select 
      node
    , descendant
    , count(*) as count_paths
    , min(depth) as min_depth
    , max(depth) as max_depth
    , array_agg(path_array) as path_arrays

  from node_descendants_and_self

  group by 1,2

)

, node_descendants_path as (

  select 
      node
    , descendant
    , path_array
    , row_number() over (partition by node, descendant order by depth asc, path_array asc) = 1 as is_shortest_path
    , row_number() over (partition by node, descendant order by depth desc, path_array asc) = 1 as is_longest_path

  from node_descendants_and_self

)

, final as (
  
  select 
      node_descendants.node
    , node_descendants.descendant
    , concat(node_descendants.node, '-', node_descendants.descendant) as node_descendant_sk
    , node.table_sk
    , descendant.table_sk as descendant_table_sk
    , concat(node.table_sk, '-', descendant.table_sk) as node_descendant_table_sk

    , node_descendants.min_depth
    , node_descendants.max_depth
    , node_descendants.count_paths
    , node_descendants.path_arrays
    , shortest_path.path_array as shortest_path_array
    , longest_path.path_array as longest_path_array

  from node_descendants
  inner join nodes as node
    on node_descendants.node = node.node
  inner join nodes as descendant
    on node_descendants.descendant = descendant.node
  inner join node_descendants_path as shortest_path
    on node_descendants.node = shortest_path.node
    and node_descendants.descendant = shortest_path.descendant
    and shortest_path.is_shortest_path
  inner join node_descendants_path as longest_path
    on node_descenda
```",2
7399,"transform-a-changelog-in-a-daily-changelog-with-an-incremental-model","<p>I would like to make an incremental DBT model of a daily changelog from a changelog that looks like the following:</p>
<pre><code>    ID  TIMESTAMP        DATA     OPERATION
    id1 2023-01-01 13:40 data1_v1 create
    id1 2023-01-01 15:00 data1_v2 update
    id1 2023-01-03 00:02 data1_v3 update
    id1 2023-01-04 05:04 data1_v3 delete
    id2 2023-01-01 XX:XX data2_v1 create
    id2 2023-01-02 XX:XX data2_v2 update
    id2 2023-01-04 XX:XX data2_v3 update
</code></pre>
<p>I would like to accomplish a daily picture of the data of each id (filtering deletes each day), so we get a daily changelog:</p>

```csv
    ID  DATE       DATA    
    id1 2023-01-01 data1_v2
    id1 2023-01-02 data1_v2
    id1 2023-01-03 data1_v3
    id2 2023-01-01 data2_v1
    id2 2023-01-02 data2_v2
    id2 2023-01-03 data2_v2
    id2 2023-01-04 data2_v3
```
<p>Mentioning:</p>
<ul>
<li>For id1 in date 2023-01-01, data1_v2 is the data we want because it
is the latest that day</li>
<li>id1 does not figure in 2023-01-04 because it
was deleted</li>
<li>id2 in date 2023-01-03 is data2_v2 since the it didn't
change that day</li>
</ul>
<p>We use BigQuery as our warehouse connected to DBT.</p>
<p>My current approach is the following:</p>

```sql
with
    days as (
        select day
        from unnest(generate_date_array(date_sub(current_date(), interval 6 month), current_date())) day
    ),
    changelog as (
        select date, timestamp, document_id, data
        from {{ ref(&quot;base_firestore_export__event_raw_changelog&quot;) }}
    ),
    daily_changelog as (
        select
            date,
            document_id,
            last_value(data ignore nulls) over (order by timestamp) as data
        from days d
        left join changelog c on d.day = c.date
        {% if is_incremental() %}
            -- this filter will only be applied on an incremental run
            where date &gt; (select max(date) from {{ this }}) 
        {% endif %}
    )
select *
from daily_changelog
```
<p>but I get a memory error:</p>

```
Database Error in rpc request (from remote system) Resources exceeded during query execution: The query could not be executed in the allotted memory. Peak usage: 122% of limit. Top memory consumer(s): sort operations used for analytic OVER() clauses: 98% other/unattributed: 2%
```
<p>Any help would be much appreciated! Thanks!</p>",1
7408,"dbt-packages-error-cannot-access-the-file","The problem I'm having is I tried to update my packages using dbt deps and I got an error stating: The process cannot access the file because it is being used by another process: 'dbt_packages\\dbt-expectations-0.8.2\\integration_tests'   and I have no idea how to fix this?

I am using dbt-sqlserver and my IDE is VsCode.  I have tried dbt clean but it doesnt get rid of two of the folders. To delete those I have found that I need to exit vscode then re-open in which I can delete them.

I have put my packages yaml down below.

```yaml
packages:

  - package: dbt-labs/codegen
    version: 0.9.0

  - package: calogica/dbt_date
    version: 0.7.2
    
  - package: calogica/dbt_expectations
    version: 0.8.2
    
  - package: dbt-labs/metrics
    version: 1.4.0

  - package: Datavault-UK/dbtvault
    version: 0.9.3
```",1
7408,"dbt-packages-error-cannot-access-the-file","Does the error message say which process is accessing the file? What extensions do you have installed in VS Code? It sounds like something else is getting its hooks into the files which is causing problems when dbt deps tries to overwrite the folders with updated files.",2
7408,"dbt-packages-error-cannot-access-the-file","So I was able to get out of the error by deleting my repo restarting my computer then cloning my repo again then dbt deps worked.  This is the second time that this has happened to me, and I think you're on the right train of thought with it being something to do with the extensions.

This an error I also  received when shifting around the packages and always seems to have to do with integration tests:
 [WinError 32] The process cannot access the file because it is being used by another process: 'dbt_packages\dbt_metrics-1.4.0\integration_tests' 15:03:05 Traceback (most recent call last): File ""C:\Users\jb_admin\AppData\Local\Programs\Python\Python310\lib\shutil.py"", line 816, in move os.rename(src, real_dst) PermissionError: [WinError 5] Access is denied: 'dbt_packages\dbt_metrics-1.4.0' -> 'dbt_packages\metrics'. 

Here is the list of extensions I am using: 
Better Jinja
C/C++
C/C++ Extension Pack
Code Spell Checker
Data Workspace
dbt Power User
Github Theme
Gitlens
indent rainbow
YAML
IntelliCode
IntelliCode API Usage
isort
jupyter
jupyter Cell Tags
Jupyter Keymap
Jupyter Notebook renderers
jupyter slide show
markdown preview Github sytling
markdown lint
material icon theme
prettier
pylance
python
python indent
search lights 
sql bindings
sql database projects
sql server
vscode-dbt
vscode icons",3
7408,"dbt-packages-error-cannot-access-the-file","From [this issue on the VS Code repo](https://github.com/microsoft/vscode/issues/128418), it looks like [this article](https://helpcenter.gsx.com/hc/en-us/articles/115015880627-How-to-Identify-which-Windows-Process-is-Locking-a-File-or-Folder) might help (haven't tried it myself). It looks like it uses [Process Explorer](https://learn.microsoft.com/en-gb/sysinternals/downloads/process-explorer) to identify which process is holding the file lock, which might get down to the extension level",4
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","## The problem I'm having
We're hitting our database (Snowflake) far harder now that we've switched to dbt, resulting in extreme costs.
By default, Fivetran puts the `+` operator to the left of our model names so as to run upstream models as well. We need this in order to ensure dependencies are refreshed first.

This creates two problems:
1. We're re-running heavy models that are commonly hit. For example, we may have a heavy sales model that combines data from three tables into one denormalized table. This should only be run daily. But because our hourly models depend on it, they end up triggering it to be re-run every hour.
2. We're needlessly re-creating 1000+ views in our staging layer repeatedly throughout the day. Because every model at some point relies on a staging model, the staging model gets re-run many many times throughout the day. These staging models are simply views with very light transformations, so each one takes very little time. That said, due to the number of them, it becomes a problem. It creates no value for us to recreate these views, because their definitions are always the same (and since they're views, their data is also always the same).

## The context of why I'm trying to do this
We use the `+` operator in front of our models because most of our models require at least 2-3 other models before finally coming together in a ""final"" or ""output"" model.

## What I've already tried 
The only thing I can really think to do is to change from `ref()` to `source()` wherever we're hitting something we don't want to re-run frequently. But that feels inconsistent with dbt's purpose and we therefore lose a lot of advantages of dbt by so doing.

I considered trying to put the `+` operator after more upstream models, but that won't necessarily guarantee everything necessary gets run.

How do others do this with Fivetran? Even if we ignore the first part, how do others have a staging layer that isn't constantly getting re-created for no reason?

I've tried looking at yaml selectors and applying a default property, but the docs state  that they only apply on an ""unqualified"" command and will be ignored if any selectors are embedded in the command (which Fivetran always does).

For further context, Fivetran's logs show that it always executes a command like the following:
`dbt run --models +model_1 +model_2 +model_7`",1
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","Hey @ptrn, just to clarify, are you using Fivetran's [Transformations for dbt Core](https://fivetran.com/docs/transformations/dbt#scheduledinfivetran) product?

I haven't used it (obviously we use dbt Cloud internally as well as at my last company, which has full control over the DAG selection when you configure a job), but at a guess: are you able to control the selectors at all, e.g. by managing the job as code instead?",2
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","Hey, Joel! I really appreciate your taking a look at this.

Yes, we're using Fivetran's Transformations for dbt Core (specifically, the ""Scheduled in Fivetran"" version) product.

We can switch to their ""Scheduled in Code"" version, which I'm going to try today. That's the only way we can control the selectors.

I'm curious, though, what the best practice is to prevent re-running our staging layer, for example, every time even using selectors. Would we just always append something like --exclude staging on every single command?",3
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","Your best bet for now is to switch to ""Scheduled in Code"". You lose the benefit of coordinating between syncs and dbt models, though. Our long-term goal is that the ""Integrated Schedule"" mode can handle all cases, but we're not there yet.",4
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","Hi @ptrn! As you were already advised to gain full control over selectors you might switch to ""Scheduled in Code"" transformations, but you also have an option to run your intermediate models with ""Partially integrated"" schedule. In this case it is possible to skip the unnecessary model runs while keeping your output models in ""Fully integrated"" mode running after upstream connectors. In any case I advise you to create a Fivetran support ticket and ask to route it to me, so the team could check your use case in depth.",5
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","Thanks for the idea.
Before I create a ticket to pursue this, I want to clarify: You're saying that if I put an intermediate model on a ""Partially Integrated"" schedule, that other models in ""Fully Integrated"" mode which are dependent on this intermediate model will skip it when they execute?",6
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","I'm trying to work out the best way to do this using YAML Selectors.

I notice in [the docs that it says](https://docs.getdbt.com/reference/node-selection/yaml-selectors#exclude):
> exclude ... is always applied last within its scope.

Let's say I have the following lineage for an output model used by a Tableau report:
```
inventory --------------------->
int_products_enhanced --------->
                                 --> rpt_tableau
sales ------------------------->
```

And the following lineage used by my sales model:
```
int_products_enhanced ----->
stg_brick_mortar_sales ---->
                             ---> sales
stg_ecommerce_sales ------->
```

You'll notice that `int_products_enhanced` is used in both transformations.
The `sales` model, along with all it's parents, is the heavy model I want to exclude (which I run daily).

That said, because the lineage for the `rpt_tableau` model also directly references `int_products_enhanced`, I do want `int_products_enhanced` to be re-run when `rpt_tableau` and it's parents are run.

Given that exclude selectors apply last and the following selectors.yml, `int_products_enhanced` doesn't get re-run. Suggestions on how I could alter it so that it does get re-run due to it's being directly referenced by `rpt_tableau`?


```
selectors:
    - name: reporting_layer_refresh_excluding_daily_and_staging
      definition:
        union:
          - method: fqn
            value: rpt_tableau
            parents: true
            children: false
          - exclude:
                - method: fqn
                  value: sales
                  parents: true
                  children: false
```",7
7412,"fivetran-how-to-run-models-frequently-enough-but-not-too-frequently","@ptrn here is more detailed [description of Partially Integrated mode](https://fivetran.com/docs/transformations/dbt#partiallyintegratedscheduling).",8
7416,"how-to-skip-downstream-models-following-test-failures-on-sources","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having:
I would like to configure DBT tests on my sources like this for [example](https://docs.getdbt.com/reference/configs-and-properties#example) and use DBT build to skip downstream (reliant) models using the native [skip on failure](https://docs.getdbt.com/reference/commands/build#details) from DBT build. The skip on failure works great for tests on models and it's downstream models, but I haven't been able to consistently reproduce the same effect with tests on sources. 

## What I've already tried 
I've tried setting up a selector.yml like this and it doesn't seem to work for fqn:* but i have had some limited success when selecting specific models.
```
selectors:
  - name: daily_run
    description: ""daily dbt run""
    definition:
      union:
        # check freshness + test all sources
        - method: source
          value: '*'

        # select all nodes
        - method: fqn
          value: '*'

        # exclude tagged models
        - exclude:
            - method: tag
              value: exclude
```

Has anyone been able to successfully implement this type of solution? Would appreciate any insight on this.",1
7416,"how-to-skip-downstream-models-following-test-failures-on-sources","hey Jeff, in my experience, if a test on my source data fails `dbt build` will correctly skip all the downstream models.

can you provide anymore info on why you havenâ€™t been able to â€œ_consistently reproduce the same effect with tests on sources_â€?

`dbt build` is defined such that if _any_ upstream resource fails (including sources), all the downstream resources are skipped

<sub>Note: `@Sean Meehan` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1679451417867939?thread_ts=1679441697.227639&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7416,"how-to-skip-downstream-models-following-test-failures-on-sources","one other thing I just thought ofâ€¦..are you making sure to call on your source in the downstream model via:
```from {{ source('jaffle_shop', 'orders') }}```
if you donâ€™t do this, dbt wonâ€™t know that your model is downstream of the source

<sub>Note: `@Sean Meehan` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1679451540778579?thread_ts=1679441697.227639&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",3
7422,"call-a-dataset-persistent-function-with-a-ref","I'm migrating a view from bigquery and it uses a couple persistent functions written in javascript. Since they aren't written in sql, I cannot use a macro to reproduce them, so I probably have to call them in my dbt model. Since I have to write my project_id and dataset_id in order to call the function, is there a way to add these as source and call the function using a ref, so the model respects modularity of sources?

Thank you!",1
7422,"call-a-dataset-persistent-function-with-a-ref","[quote=""dreinon, post:1, topic:7422""]
[/quote]

@dreinon

1. Define your function as source
2. Use source() function in your dbt model, it compiles to the full object name in the database.

src_view.yml
```
version: 2

sources:
  - name: function_source_name# this is the source_name
    database: function_project_id 
    schema: function_dataset_id
    tables:
      - name: function_name # this is the your function_name
```
model.sql
```
SELECT
col1,
col2,
{{source('function_source_name','function_name ')}}(parm1,parm2) # Compiles to the full object name in the database

FROM table

```",2
7422,"call-a-dataset-persistent-function-with-a-ref","Thanks for answering! Good to know that method works, but isn't it a bit confusing to have functions and tables both in the same place treated as the same type of thing?",3
7422,"call-a-dataset-persistent-function-with-a-ref","to avoid confusion create a separate source only for functions in your yml file",4
7422,"call-a-dataset-persistent-function-with-a-ref","I'm not familiar with BQ JS functions, but could you not also create a macro that just returns the name of the macro and the arguments, which would then be called as normal inside of your project? Something like this: 

```
{% macro js_function(arg1, arg2) %}
    {{ this.database }}.{{this.schema}}.js_function({{ arg1 }}, {{ arg2 }})
{% endmacro %}
```

When you call it like this: 
```
select col_1, 
{{ js_function('col_2', 'col_3') }}
from {{ ref('some_model') }}
```
it will render as

```
select col_1,
ANALYTICS_DEV.dbt_jlabes.js_function(col_2, col_3)
from ANALYTICS_DEV.dbt_jlabes.some_model
```",5
7426,"how-to-get-upstream-documentation-to-flow-downstream-without-manually-updating","Hi, I'm trying to create column-level documentation for staging models and would like to see that documentation populated downstream in later models when those columns are brought in using the ref() function with a documented staging model. I don't see an automatic feature that does that currently within DBT.  
ex: stg_model_a is fully documented. int_model_abc brings in stg_model_a. When I create documentation for int_model_abc, I would like any documentation from stg_model_a to be brought into int_model_abc without any manual effort.

I know that I could use the Codegen macro with the upstream_documentation configuration but that would still require me to populate that, copy that information, and update the YAML file. This is quite manual, and if upstream columns are changed, this would need to be done for all downstream models. 

Is there a way to have DBT do the heavy lifting for this?",1
7426,"how-to-get-upstream-documentation-to-flow-downstream-without-manually-updating","I'm using [dbt-osmosis](https://github.com/z3z1ma/dbt-osmosis) for this",2
7447,"yaml-selectors-how-to-refresh-intermediate-models-touched-by-multiple-output-models","I notice in [the docs that it says](https://docs.getdbt.com/reference/node-selection/yaml-selectors#exclude):
> exclude ... is always applied last within its scope.

Let's say I have the following lineage for an output model used by a Tableau report:
```
inventory --------------------->
int_products_enhanced --------->
                                 --> rpt_tableau
sales ------------------------->
```

And the following lineage used by my sales model:
```
int_products_enhanced ----->
stg_brick_mortar_sales ---->
                             ---> sales
stg_ecommerce_sales ------->
```

You'll notice that `int_products_enhanced` is used in both transformations.
The `sales` model, along with all it's parents, is a heavy model I want to exclude from hourly runs (because I run the `sales` model daily).

That said, because the lineage for the `rpt_tableau` model also directly references `int_products_enhanced`, I do want `int_products_enhanced` to be re-run when `rpt_tableau` and it's parents are run.

Given that exclude selectors apply last and the following selectors.yml, `int_products_enhanced` doesn't get re-run. Suggestions on how I could alter it so that it does get re-run due to it's being directly referenced by `rpt_tableau`?


```
selectors:
    - name: reporting_layer_refresh_excluding_daily_and_staging
      definition:
        union:
          - method: fqn
            value: rpt_tableau
            parents: true
            children: false
          - exclude:
                - method: fqn
                  value: sales
                  parents: true
                  children: false
```",1
7450,"dbt-run-fails-when-deploying-on-certain-schemas","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I'm using dbt over a VPN connection to deploy models in snowflake. I have a schema called trans where I deploy my views. These run fine without issue. However, when I try to run an incremental model in another schema.
## The context of why I'm trying to do this

## What I've already tried 

## Some example code or error messages
```
OSError('Tunnel connection failed: 407 Proxy Authentication Required'
```",1
7464,"audit-helper-in-dbt-bringing-data-auditing-to-a-higher-level","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/audit-helper-for-migration",1
7468,"binarization-with-sql-macros","I'm not sure if this is the appropriate place to ask; please feel free to direct me somewhere else! 
### The problem I'm having
I have been struggling to convert a categorical column (containing a varying number of comma separated strings) into binary columns for use in classification algorhythms.

### The context of why I'm trying to do this
This column has roughly 5000 unique values and I may be doing this to other columns as well, so I am seeking a dynamic approach

### What I've already tried 
Chat GPT and I have tried a number of various macros and this is the iteration I'm currently on:

```
{% macro binarize_column(column, prefix) %}

WITH split_data AS (
  SELECT
    LOWER({{ column }}) AS {{ column }},
    REGEXP_SPLIT_TO_TABLE(LOWER({{ column }}), ',') AS split_column
  FROM {{ ref('my_table') }}
),

unique_data AS (
  SELECT DISTINCT split_column
  FROM split_data
),

binned_data AS (
  SELECT
    {{ column }},
    {% for row in unique_data %}
      CASE
        WHEN {{ row.split_column }} IS NOT NULL THEN 1
        ELSE 0
      END AS {{ prefix }} || '_' || {{ row.split_column }}
      {% if not loop.last %},{% endif %}
    {% endfor %}
  FROM split_data
)

SELECT *
FROM binned_data;

{% endmacro %}
```
and calling it via:
```
{{ config(materialized='table') }}

{{ binarize_column('my_column', 'my_prefix') }}
```

This is the resulting error message when trying to ```dbt run```
```
22:34:35.609155 [error] [MainThread]: Database Error in model test (models/intermediate/my_model.sql)
22:34:35.609412 [error] [MainThread]:   syntax error at or near ""FROM""
22:34:35.609717 [error] [MainThread]:   LINE 29:   FROM split_data
```",1
7473,"run-results-json-is-not-logging-models-executed-from-dbt-run","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

Actually, we have a job scheduler ""ActiveBatch"" which runs following dbt run command to execute the models:

![image|690x133](upload://7iyl33b03qQPHy1sN1g7CsXPxkY.png)


Now, when this process finishes the models information should be logged in run_results.json file which is one of the dbt artifacts but for some reason the models information is not being logged.

I have tested to execute models in my local environment by simply running ""dbt run -s model_name"" and it is bein logged on every execution


So, please guide me through this as this is very important issue for me!

Actively looking forward your suggestions.

Thank you!",1
7473,"run-results-json-is-not-logging-models-executed-from-dbt-run","Hi, 

I have identified the issue. Actually I have to look into the specific folder that is mentioned in the command rather than the standard project folder. From there I can see the expected results in dbt artifacts.",2
7473,"run-results-json-is-not-logging-models-executed-from-dbt-run","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",3
7477,"how-can-i-call-a-test-like-a-macro","hi, everyone:

I'm using dbt in my work, it's great. But I have no idea on something: I defined some test, and I want to call them after a model running to make sure that the result of model if correct, or the data quality is good. If I run `dbt test`, it will run only after all the models generated, and will cause some problem if some model's quality is bad.
So i want call the test from post-hook like calling a macro. How can i do that?

Thanks a lot for any suggestion!",1
7477,"how-can-i-call-a-test-like-a-macro","[quote=""cntwelve, post:1, topic:7477""]
dbt test
[/quote]

You can also select your model in the test command  using `dbt test --select model_name`.
if you wanna run the test macro in post hook , add the post-hook to the config function in your model.sql file as shown below 
```
{{ config(
  post_hook = "" model_test_macro(arg1,arg2)""
) }}
```",2
7477,"how-can-i-call-a-test-like-a-macro","Thanks a lot, I will try it later!",3
7477,"how-can-i-call-a-test-like-a-macro","Running a test in a post-hook will still mean that the model has been built and cause any issues you were concerned about. 

Running a test macro in a post-hook is not a supported configuration - I think it will *run* the test sql, but it won't do anything with the results if the test fails. 

The recommended way to avoid deploying invalid datasets is to run a [blue-green deployment](https://discourse.getdbt.com/t/performing-a-blue-green-deploy-of-your-dbt-project-on-snowflake/1349), where you build everything but only switch out the tables if the tests have passed.",4
7477,"how-can-i-call-a-test-like-a-macro","I test it, but it fails.

I write a model , and add these code:
`{{ config( post_hook=""{{ unique('t1', 'id') }}"") }}`
When I do `dbt run`, I get:
`
00:30:43  Running with dbt=1.4.5
00:30:43  Found 1 model, 0 tests, 0 snapshots, 0 analyses, 292 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0
metrics
00:30:43
00:30:44  Concurrency: 1 threads (target='dev')
00:30:44
00:30:44  1 of 1 START sql view model main.t2 ............................................ [RUN]
00:30:44  1 of 1 ERROR creating sql view model main.t2 ................................... [ERROR in 0.04s]
00:30:44
00:30:44  Finished running 1 view model in 0 hours 0 minutes and 0.11 seconds (0.11s).
00:30:44
00:30:44  Completed with 1 error and 0 warnings:
00:30:44
00:30:44  Compilation Error in model t2 (models\example\t2.sql)
00:30:44    'unique' is undefined
00:30:44
00:30:44    > in macro run_hooks (macros\materializations\hooks.sql)
00:30:44    > called by macro materialization_view_sqlite (macros\materializations\view\view.sql)
00:30:44    > called by model t2 (models\example\t2.sql). This can happen when calling a macro that does not exist. Check for
 typos and/or install package dependencies with ""dbt deps"".
00:30:44
00:30:44  Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
`

I think the ""unique"" test can not be treated as a macro.",5
7477,"how-can-i-call-a-test-like-a-macro","Thank for your replay!

I want do a quality check when a model was built. In some cases, it's important to have a check during a dbt job. For example, I can make sure that all the data flowing to the next node are unique, if something wrong, I can take an action.
So it's a check during a run.

I can make a call to macro, but I want to use the test defined in th project.

Thanks a lot!",6
7477,"how-can-i-call-a-test-like-a-macro","you can write a macro and call the macro in model pre-hook.
the macro should have your test logic, 
if your the test logic passes then return none else raise an exception.

@joellabes  i hope this would work",7
7477,"how-can-i-call-a-test-like-a-macro","It can work. But using the pre-defined test logic can provide a unified method, and all the test in dbt_utils, dbt_expectations(and some other packages) can be used. :slight_smile:",8
7477,"how-can-i-call-a-test-like-a-macro","[quote=""cntwelve, post:6, topic:7477""]
I want do a quality check when a model was built. In some cases, itâ€™s important to have a check during a dbt job. For example, I can make sure that all the data flowing to the next node are unique, if something wrong, I can take an action.
[/quote]

If you use `dbt build`, it will skip any downstream nodes when a test fails, which will prevent data which doesn't meet your expectations from flowing further",9
7477,"how-can-i-call-a-test-like-a-macro","So what I want to do is doing test just after the model was built. If the build job was finished with no error, but the data produced has some error(espcially logic error), the whole job will not fire an error event. Here, we can do some check by specifying the test marco.

BTW, I think the perfect way is to include some(certern) test in DAG.

Thanks",10
7477,"how-can-i-call-a-test-like-a-macro","just like ""post_test"" or ""pre_test"" :grinning:",11
7477,"how-can-i-call-a-test-like-a-macro","[quote=""cntwelve, post:10, topic:7477""]
So what I want to do is doing test just after the model was built.
[/quote]

This is what `dbt build` does. When the model is built, it runs any tests attached to it. 

[quote=""cntwelve, post:10, topic:7477""]
If the build job was finished with no error, but the data produced has some error(espcially logic error), the whole job will not fire an error event.
[/quote]
Are you saying this is what you think happens, or what you *want* to happen? If a logic error is detected by a model's test, then models that depend on the first model will not run, and the whole job *will* have an error event (when everything else has completed). 

If you want the job to not fire an error event when it encounters an error, that is not possible.

You might want to read/weigh in on this discussion: https://github.com/dbt-labs/dbt-core/discussions/5687",12
7477,"how-can-i-call-a-test-like-a-macro","Yes! The discussion is what I want, and give all the details! 

Thanks again for your help!

A suggestion: the discussion  can be published as a document or a blog post. :slight_smile:",13
7485,"apply-row-limits-in-model-runs-in-ci-run","Hello I've implemented Slim CI with dbt-core. I have been running modified models whne PR is created or updated with following command.

`dbt run --defer --models state:modified+1 --state state_file_path`

I want to limit no of records in model runs to speed up CI. I have done it through custom source and ref macros. For example : https://github.com/dbt-labs/dbt-core/issues/4201#issuecomment-1133630131

Some of my models use dbt_utils methods where it expects Relation object. 
Question : How would I apply limit in this case? Currently I pass relation parameter in my custom macro to return Relation object but I can't limit the records.

Thanks!",1
7485,"apply-row-limits-in-model-runs-in-ci-run","[quote=""chintanpatelTH, post:1, topic:7485""]
case
[/quote]

can you send us the **dbt_utils** methods you are using in your model files and are causing issues",2
7485,"apply-row-limits-in-model-runs-in-ci-run","Hello @Surya,

For example : dbt_utils.deduplicate https://github.com/dbt-labs/dbt-utils/blob/main/macros/sql/deduplicate.sql


```
dbt_utils.deduplicate(
            relation=source('schema', 'table'),
            group_by=""col1"",
            order_by=""col2 desc"",
        )
```",3
7487,"access-model-name-with-jinja-inside-the-model","I would like to use my model name in its query using jinja. Is this possible?

Example: 
My model is named: `stg_source__model_name.sql`

I would like something like:
`{{ model.name }}`
that returns
`stg_source__model_name`

Thanks!",1
7487,"access-model-name-with-jinja-inside-the-model","Would `{{ this.table }}` meet your needs? <https://docs.getdbt.com/reference/dbt-jinja-functions/this>

<sub>Note: `@Owen` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1679920017302959?thread_ts=1679919838.252609&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7487,"access-model-name-with-jinja-inside-the-model","It doesn't, since in an incremental model, this compiles to `request`",3
7487,"access-model-name-with-jinja-inside-the-model","[model.name should work!](https://docs.getdbt.com/reference/dbt-jinja-functions/model)

It looks like you might be using the dbt cloud IDE - the rpc server that powers it will sometimes return `request` instead of the node name (normally when in an unsaved file).",4
7487,"access-model-name-with-jinja-inside-the-model","can u try using` {{ this.name }}`

`{{ this.name }}`  ->  current_model name

`{{ this.database }}` -> current model database name

`{{ this.schema }}` -> current model schema name

`{{ this.identifier }}` -> cureent_model name

`{{ this }} -> {{ this.database }}.{{ this.schema }}. {{ this.name }}.`",5
7487,"access-model-name-with-jinja-inside-the-model","Thanks for answering @joellabes @Surya!
Right, since I am using the cloud IDE, it always returns `request` instead of the model name when using preview and compile, but does return the correct model name on build.

Is there a way to make it work for preview too? Otherwise I cannot debug it and preview it while developing.

Thanks!",6
7487,"access-model-name-with-jinja-inside-the-model","[quote=""dreinon, post:6, topic:7487""]
Is there a way to make it work for preview too? Otherwise I cannot debug it and preview it while developing.
[/quote]

Not at the moment unfortunately - it's a limitation of the old dbt-rpc project. The good news is that we are actively working to replace dbt-rpc with a new system that will be able to do this correctly - this doesn't help you right now but is very front of mind for us.",7
7487,"access-model-name-with-jinja-inside-the-model","Nice @joellabes!
Is there any issue or PR that describes this new dbt-rpc so I can follow it?

Thanks!

miÃ©rcoles, 29 marzo 2023, 00:46a. m. +02:00 de Joel Labes via dbt Community Forum [notifications@getdbt.discoursemail.com](mailto:notifications@getdbt.discoursemail.com):",8
7487,"access-model-name-with-jinja-inside-the-model","Yep! Check out https://github.com/dbt-labs/dbt-server

We're expecting this to land in the IDE somewhere around the middle of this year :crossed_fingers:",9
7490,"python-model-injecting-sql","Welcome everybody! :slight_smile: 

## The problem I'm having
I just started using python models and right on the first one I am experiencing some weird behaviour. My model bases on an SQL model (which I think is valid given the documentation [here](https://docs.getdbt.com/docs/build/python-models#referencing-other-models)). However, when generating the .py file, it injects pure SQL into the code

### Input (model.py)
```import pandas as pd

def model(dbt, session):
    dbt.config(materialized = ""table"")
    df = dbt.ref(""my-sql-model"")

    dx = df
    ...
```

### Output (.py file on GCS)
```
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set(""viewsEnabled"",""true"")
spark.conf.set(""temporaryGcsBucket"",""my-bucket-name"")

with __dbt__cte__dictionary__xxx as (
select
    something1 as column1,
    something2 as column2
from
    `my-project`.`my-dataset`.`my-table`
where
    xxx
)def model(dbt, session):
    dbt.config(materialized = ""table"")
    import pandas as pd
    df = dbt.ref(""my-sql-model"")

    dx = df
```
### Setup
BigQuery, GCS, DBT Cloud v1.3

There is not much I was able to do here as I really don't understand where it is coming from. Did you ever face that issue before?",1
7490,"python-model-injecting-sql","I spent some more time with it and I noticed that that the base model being set to ephemeral is causing this issue. Probably, I should've caught it much earlier cause the SQL is pretty distinct.

Still, isn't it somewhat of a bug? For example, ephemeral model could still be passed as a python query or there could be a validation that would prevent from using such models in python models.",2
7490,"python-model-injecting-sql","You're correct that your issue is because you're ref-ing an ephemeral model. I would open an issue on the core repo for this - I suspect this is a bug as opposed to an intentional decision to not support accessing ephemeral models. If it is intentional then it should have a better error message at least!",3
7490,"python-model-injecting-sql","This topic was automatically closed 7 days after the last reply. New replies are no longer allowed.",4
7499,"how-can-i-pass-vars-between-models","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

Im trying to call a macro that takes from one table the max data. 
then the macro should use this value in some models, and in some models this value need to 
be calculate with other values.

How can I allow declaration of vars in schema.yml for each model and not in dbt_project.yml? 
And i want this vars will be a result from a query cause i dont want to access the DB each time. 
Thank you",1
7499,"how-can-i-pass-vars-between-models","Each model is run independently, there is no shared context between them. This also means that it's not possible to have a shared variable based on a code calculation. It's also worth noting that variables can only be declared in dbt_project.yml and have to be set project-wide for a dbt invocation. 

How complex is the thing you're hitting the database for? One workaround for a complex calculation is to write the result to a separate table so you only need to evaluate it once.

This use case comes up from time to time; it would be worth opening a GitHub issue if one doesn't already exist: https://github.com/dbt-labs/dbt-core/issues",2
7504,"why-are-ci-jobs-state-method-building-more-less-models-nodes-than-expected","Starting a topic to discuss scenarios where using the `state:modified` method (and it's variation) are resulting dbt running more or less models (nodes) than one might expect.

* https://docs.getdbt.com/reference/node-selection/methods#the-state-method
* https://docs.getdbt.com/reference/node-selection/defer
* https://docs.getdbt.com/reference/node-selection/state-comparison-caveats",1
7504,"why-are-ci-jobs-state-method-building-more-less-models-nodes-than-expected","### Source properties are different

Assuming we have a dbt project (`profiles.yml`) setup with 2 different target names:

```yaml
# ~/.dbt/profiles.yml
snowflake:
  target: prod
  outputs:
    prod: &sf-creds
      type: snowflake
      ...
    ci: *sf-creds

# dbt_project.yml
name: my_dbt_project
profile: snowflake
config-version: 2
version: 1.0

models:
  my_dbt_project:
    +materialized: table

# models/sources.yml
# There is the same `users` source table here but two of it exists - one in each database.
version: 2
sources:
  - name: company_foo
    database: '{{ ""development"" if target.name == ""prod"" else ""development_jyeo"" }}'
    tables:
      - name: users
```

```sql
-- models/foo_1.sql
select * from {{ source('company_foo', 'users') }}

-- models/foo_2.sql
select * from {{ ref('foo_1') }}

-- models/bar_1.sql
select 1 as id

-- models/bar_2.sql
select * from {{ ref('bar_2') }}
```

> Note that dbt Cloud also has target names configurable per dbt Cloud job. The job shown here has a `target.name == 'ci'`:
> 
> ![image|595x500](upload://8oGfw4TWxjDKOD3l2gCGYKKJX5G.png)


1. First lets generate a `manifest.json` of a production run - which will be deferred to later in our subsequent ci run. Be sure to move the generate artifacts to it's own folder as well.

```sh
$ dbt ls --target prod
my_dbt_project.bar_1
my_dbt_project.bar_2
my_dbt_project.foo_1
my_dbt_project.foo_2
source:my_dbt_project.company_foo.users
$ mv target target_old
```

> Some users do `dbt compile` instead which works too. Most commonly, this would be `dbt run` as production jobs are meant to be building models. In special circumstances do users want the job being deferred to to be an `ls` or `compile` job.

2. Now let's modify our model `bar_1.sql` but not anything else.

```sql
-- models/bar_1.sql
select 2 as id
```

3. Let's do a run with state deferral.

```sh
$ dbt run -s state:modified --defer --state target_old --target ci
22:51:46  Running with dbt=1.4.5
22:51:47  Found 4 models, 0 tests, 0 snapshots, 0 analyses, 307 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
22:51:47  
22:51:52  Concurrency: 1 threads (target='ci')
22:51:52  
22:51:52  1 of 1 START sql table model dbt_jyeo.bar_1 .................................... [RUN]
22:51:57  1 of 1 OK created sql table model dbt_jyeo.bar_1 ............................... [SUCCESS 1 in 4.65s]
22:51:57  
22:51:57  Finished running 1 table model in 0 hours 0 minutes and 10.18 seconds (10.18s).
22:51:57  
22:51:57  Completed successfully
22:51:57  
22:51:57  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
```

> Note that dbt Cloud automatically does deferral to the job selected - we don't need `--state --defer` with dbt Cloud. You also don't need to specify `--target` since that would be set in the dbt Cloud job UI.

Here - there is no confusion - we modified model `bar_1` and indeed it was the only one that was changed. 

4. Most of the time though, we also want to run models that are downstream since changes to `bar_1`, could also affect `bar_2` - and to do this, we add a plus (`+`) to our selector.  Let's try that:

```sh
$ dbt run -s state:modified+ --defer --state target_old --target ci
22:56:37  Running with dbt=1.4.5
22:56:38  Found 4 models, 0 tests, 0 snapshots, 0 analyses, 307 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
22:56:38  
22:56:44  Concurrency: 1 threads (target='ci')
22:56:44  
22:56:44  1 of 4 START sql table model dbt_jyeo.bar_1 .................................... [RUN]
22:56:49  1 of 4 OK created sql table model dbt_jyeo.bar_1 ............................... [SUCCESS 1 in 4.54s]
22:56:49  2 of 4 START sql table model dbt_jyeo.foo_1 .................................... [RUN]
22:56:53  2 of 4 OK created sql table model dbt_jyeo.foo_1 ............................... [SUCCESS 1 in 4.13s]
22:56:53  3 of 4 START sql table model dbt_jyeo.bar_2 .................................... [RUN]
22:56:57  3 of 4 OK created sql table model dbt_jyeo.bar_2 ............................... [SUCCESS 1 in 4.23s]
22:56:57  4 of 4 START sql table model dbt_jyeo.foo_2 .................................... [RUN]
22:57:02  4 of 4 OK created sql table model dbt_jyeo.foo_2 ............................... [SUCCESS 1 in 4.72s]
22:57:02  
22:57:02  Finished running 4 table models in 0 hours 0 minutes and 23.71 seconds (23.71s).
22:57:02  
22:57:02  Completed successfully
22:57:02  
22:57:02  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
```

> Note we're still deferring to the `manifest.json` generated earlier with `dbt ls` in step (1) and not the `manifest.json` generated in step (3).

Now, many more models have been executed. For `bar_2` - it's not surprising since `bar_2` is directly downstream of `bar_1`. But why `foo_1` (and subsequently `foo_2`)? Well, that's due to how we've defined our source which `foo_1` uses - recall:

```yaml
# models/sources.yml
version: 2
sources:
  - name: company_foo
    database: '{{ ""development"" if target.name == ""prod"" else ""development_jyeo"" }}'
    tables:
      - name: users
```

In our production job (target == 'prod'), the source evaluated to `development.company_foo.users` but in our ci job (target == 'ci'), the source is instead `development_jyeo.company_foo.users` - this means that YES, the source is detected as being modified. But of course, even if a source is modified, `run -s state:modifed` doesn't actually do ANYTHING to sources (since sources are not run). But as soon as we do `run -s state:modified+` - we would be ""running"" things that are downstream of the changed source - which is of course `foo_1` (and `foo_2` by extension).

![image|690x369](upload://z61u1sJMi0rFnWHdjRzOjSAP442.png)",2
7505,"incremental-model-merge-without-update","Hi,
Is there a way to use merge incremental strategy without data getting updated if the Unique id is already available in the destination. The merge incremental strategy inserts records if there is no matching id in destination and updates the columns if the ID is already present in the destination. I want to ignore the incoming records if the unique ID is already available in the destination and insert only new records.",1
7505,"incremental-model-merge-without-update","dbt will only insert records that are returned by your query. Inside of your `is_incremental` block, if you use a `not exists` clause to ignore any records whose unique ID is already in the destination, then it will only return the new records which means they're the only ones that will be inserted.",2
7505,"incremental-model-merge-without-update","create your own incremental strategy and configure the strategy to ur model using config function
you create the incremental strategy macro and write your merge logic.
dbt identifies the macro with the name `get_incremental_{strategy}_sql` so you have to create macro with the above name.
dbt internally passes the below arguments to the user defined incremental strategy macro

`{'target_relation': target_relation, 'temp_relation': tmp_relation, 'unique_key': unique_key, 'dest_columns': dest_columns, 'predicates': incremental_predicates }` 

you have to return a merge sql from the macro, dbt runs the sql on the configured database

user defined incremental strategy:

```
{% macro get_incremental_{strategy}_sql(arg_dict) %}
 merge logic
  {{ return (merge_sql_query) }}

{% endmacro %}
```",3
7505,"incremental-model-merge-without-update","[quote=""Surya, post:3, topic:7505""]
create your own incremental strategy
[/quote]

This is true, but is probably overkill given that the default merge strategy will work out of the box if it's given the correct rows to work with.",4
7507,"how-to-grant-privilege-to-groups-using-grant-redshift","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

Hi, I'm reading through this link: [grants | dbt Developer Hub (getdbt.com)](https://docs.getdbt.com/reference/resource-configs/grants) 
And I cannot find a way to define if you're granting privilege to a GROUP.

Would be cool if we don't need to do macros for this and just use the new feature.

I already tried this:

```
models:
  my_schema:
    schema: my_schema
    description: ""My schema""
    grant:
      select:
        - group: my_group
```
Didn't get much luck.

Thanks!",1
7507,"how-to-grant-privilege-to-groups-using-grant-redshift","Have you tried 


```
models:
  my_schema:
    schema: my_schema
    description: ""My schema""
    grant:
      select:
        - my_group
```
?

You should only need to provide the group/user name, as redshift doesn't require you to know the type of entity (individual user vs group) that is being granted access, just its name.",2
7510,"custom-test-isnt-being-inserted-into-dbt-internal-test-subquery","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

The issue I am having is, whenever I build my custom tests, I get an error. After inspecting the target code, it looks like the FROM is never being populated.

I even tried using code from the documentation and that fails to build as well. I am curious if this is a bug or if am I missing something obvious.

Here is the test code:

```
{% test Currency(model, column_name) %}

    SELECT *
    FROM {{ model }}
    WHERE NOT REGEXP_LIKE({{ column_name }}, '^[0-9]+(\.[0-9]{1,2})?$')

{% endtest %}
```
All it does is figure out if the value is a valid currency.

The compiled code looks like this:
```
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
      
    ) dbt_internal_test
```

```
The error message is:
Database Error in test Currency (tests\Currency.sql)
 001003 (42000): SQL compilation error:
syntax error line 8 at position 4 unexpected ')'.
compiled Code at target\run\ea_data_analytics\tests\Currency.sql
```

Makes sense why it fails, but, why does it refuse to replace the {{model}} . Also, where has my logic gone? It makes sense that it should appear in the FROM as an embedded query but does not. Is this a bug? Or did I miss something.",1
7510,"custom-test-isnt-being-inserted-into-dbt-internal-test-subquery","Can you post the YAML where you're attaching the test to a model?",2
7510,"custom-test-isnt-being-inserted-into-dbt-internal-test-subquery","sure:

```

version: 2 
      
sources:
  - name: test_staging
    database: <database>(exists but did not want to share it)
    schema: common
    tables:
      - name: test_test
```",3
7510,"custom-test-isnt-being-inserted-into-dbt-internal-test-subquery","OK, it doesn't look like the test is actually linked to your source. It should look something like this: 

```
version: 2 
      
sources:
  - name: test_staging
    database: <database>
    schema: common
    tables:
      - name: test_test
        columns: 
          - name: currency_col
            tests:
              - Currency
```

Check out the docs on tests for more examples: https://docs.getdbt.com/docs/build/tests#generic-tests",4
7524,"manipulating-dbt-valid-to-timestamp-in-dbt-snapshot","Hi everyone,  a question on snapshots:
 In my process I need to subtract a millisecond from the subsequent record's ""valid_from"" timestamp and this value should be updated as ""valid_to timestamp"" of the current record ( rather than simply  updating  to the  ""valid_from"" of the next row as is..)
How can I configure this manipulation within the snapshot mechanism?
Thanks in advance for all the help!",1
7524,"manipulating-dbt-valid-to-timestamp-in-dbt-snapshot","Two options: 
- you could build a view on top of your snapshot which applies the transformations you need. This is best in alignment with our recommendation of [treating snapshots as sources](https://docs.getdbt.com/docs/build/snapshots#snapshot-source-data).
- you could build a custom implementation of the snapshot macro which applies the time offset. This would be more complex, and I haven't done it myself so don't have any specific guidance beyond pointing you to the macros where it's implemented: https://github.com/dbt-labs/dbt-core/tree/main/core/dbt/include/global_project/macros/materializations/snapshots",2
7524,"manipulating-dbt-valid-to-timestamp-in-dbt-snapshot","Thank you so much  for the response @joellabes !",3
7527,"dbt-cli-messed-up","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
My DBT stopped working in VS Code all of a sudden. This is the error message I'm getting

![image|689x403](upload://kFhfgWCYfxNrfD7eW3kwpfMyFxo.png)


## The context of why I'm trying to do this

## What I've already tried 

## Some example code or error messages
```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
7527,"dbt-cli-messed-up","This doesn't look like the full error/stack trace - the bottom line is cut off at least. Please post the whole error as text inside of backticks, not as an image",2
7527,"dbt-cli-messed-up","```
PS C:\Users\xxxxx\source\Snowflake\dbt\ccbcc_dbt> dbt --version
Traceback (most recent call last):
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\Scripts\dbt.exe\__main__.py"", line 4, in <module>
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\main.py"", line 23, in <module>
    import dbt.task.build as build_task
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\task\build.py"", line 1, in <module>
    from .run import RunTask, ModelRunner as run_model_runner
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\task\run.py"", line 8, in <module>
    from .compile import CompileRunner, CompileTask
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\task\compile.py"", line 3, in <module>
    from .runnable import GraphRunnableTask
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\task\runnable.py"", line 17, in <module>
    from dbt.task.base import ConfiguredTask
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\task\base.py"", line 47, in <module>
    from dbt.adapters.factory import register_adapter
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\adapters\factory.py"", line 14, in <module>
    from dbt.adapters.protocol import (
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\adapters\protocol.py"", line 27, in <module>
    from dbt.graph import Graph
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\graph\__init__.py"", line 1, in <module>
    from .selector_spec import (  # noqa: F401
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\graph\selector_spec.py"", line 8, in <module>
    from .graph import UniqueId
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\dbt\graph\graph.py"", line 3, in <module>
    import networkx as nx  # type: ignore
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\networkx\__init__.py"", line 115, in <module>
    import networkx.readwrite
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\networkx\readwrite\__init__.py"", line 15, in <module>
    from networkx.readwrite.graphml import *
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\networkx\readwrite\graphml.py"", line 314, in <module>
    class GraphML(object):
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\networkx\readwrite\graphml.py"", line 346, in GraphML
    (np.int, ""int""), (np.int8, ""int""),
  File ""C:\Users\xxxxx\AppData\Local\Programs\Python\Python38\lib\site-packages\numpy\__init__.py"", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'int'.
`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```",3
7527,"dbt-cli-messed-up","Thank you! It's odd that it's a numpy error - just to be sure, you're not trying to create a dbt Python model, right? That's the only reason I could think that user-generated code would lead to a numpy error, and even that shouldn't interfere with `dbt --version`. 

Do you have dbt installed into a virtual environment? It doesn't look like it, and I'm wondering whether you've installed a new version of numpy for a different project, and that's led to conflicts. 

I would encourage you to create a new virtual environment (aka venv) and install dbt into that, and use it when developing dbt code. 

For help with this, check out https://docs.getdbt.com/faqs/core/install-pip-best-practices.md",4
7527,"dbt-cli-messed-up","I initially created a virtual environment that uses Python 1.3 to run the python models, since my default dbt version was 1.1 (dbt cloud is also using version 1.1), that uses dbt non python models. However, I deleted the virtual environment and when I try to run the .sql files in my default environment i.e., Version 1.1 I'm getting an error as shared in my earlier message. Is there a way to fix my default version without even creating a virtual environment with 1.1 version?",5
7530,"using-html-in-dbt-docs","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having

I'd like to be able to implement some html functionality within the generated dbt docs. I found [this post on the dbt website](https://docs.getdbt.com/styles), but couldn't get much/any of the elements to work (I'm assuming it is restricted to dbt cloud?).

## The context of why I'm trying to do this

I'm working on filling out a lot of our documentation, and I'm looking to add/utilize some functionality that would be helpful for our analysts that use the data daily, such as:
- Wide table with a few rows as a data sample
  - Right now, if there are too many columns, the table extends past the `About` block in the docs
- Include tabs with SQL queries that would help users with some simple EDA

## What I've already tried 

- Directly including html tags --> these get parsed directly as plain text",1
7530,"using-html-in-dbt-docs","Hey @rmcilrai, 

[quote=""rmcilrai, post:1, topic:7530""]
I found [this post on the dbt website](https://docs.getdbt.com/styles), but couldnâ€™t get much/any of the elements to work
[/quote]
These are components for use in the docs.getdbt.com website (i.e. the documentation on how to develop with dbt), not the static dbt-docs project (the documentation about your database and models). We know it's a confusing name overlap :persevere:

We removed a lot of raw HTML rendering around dbt Core v1.0, to avoid some XSS security issues: https://github.com/dbt-labs/dbt-docs/pull/227

Because of this, I don't think there's anything you can do to style tables etc outside of what's exposed in markdown.",2
7532,"export-from-mysql-to-bigquery","I'm not able to read data from MySQL and write them to a BigQuery DB

I'm stuck at the very beginning because seems that the 2 connetors `dbt-bigQuery` and `dbt-mysql` depend on different versions of dbt-code

```
dbt-bigquery 1.4.3 requires dbt-core~=1.4.0, but you have dbt-core 1.1.4 which is incompatible.
```

`dbt-core 1.1.4` is installed by dbt-mysql

 can someone suggests a way to make dbt to work together with BigQ and MySQL ?",1
7532,"export-from-mysql-to-bigquery","dbt is not a data movement tool (the Extract/Load in ELT), it is only for transforming data once it lands in a warehouse. 

To move data from one warehouse to another, you'll want to look into tools like Fivetran, Stitch, Meltano, Airbyte etc.",2
7532,"export-from-mysql-to-bigquery","thanks, I'm aware of the scope of dbt, and indeed we're using it on BigQ for data trasnformation. It happens now that one raw datasource is not on BigQ itself, but rather on a different db (MySQL) and seem it should be possible to just read from MySQL and write to BigQ.
Do you say that is not possibile at all or that dbt is just not the best tool for that ?",3
7532,"export-from-mysql-to-bigquery","[quote=""FrancoC, post:3, topic:7532""]
Do you say that is not possibile at all or that dbt is just not the best tool for that ?
[/quote]

dbt can only do things that your warehouse can already do in SQL. So you can do something like [unload a CSV to a GS bucket](https://cloud.google.com/bigquery/docs/reference/standard-sql/other-statements#export_data_statement), but unless there is some native way to connect to MySQL from BQ then no it can't do it at all.",4
7535,"incremental-model-default-behavior-when-not-specifying-unique-key-snowflake","I'm in the process of updating the logic for one of our incremental models for our CDC pipeline (the incremental model is run daily) and wanted to check the default behaviour in Snowflake when not specifying a unique key. 

From the documentation, it appears that MERGE is the default incremental mode. When not specifying a Unique Key does this do a merge over the entire row if all columns are the same?",1
7535,"incremental-model-default-behavior-when-not-specifying-unique-key-snowflake","Without a unique key, you effectively create an append only model since there's nothing to key the merge from.",2
7540,"handling-bigquery-incremental-dbt-tmp-tables","Recently started using dbt, loving it so far,  however I noticed something kind of weird with the incremental models.

I am using incremental models to create the ""intermediate"" level of my data, as the raw data is JSON based, I want to extract the keys that I need, and populate a table that will be relational.
To avoid going over the entire dataset every time, we decided to use Incremental models, with merge strategy, and a ""on_schema_change=append_new_columns"" approach.

Once we started using the models, we noticed that dbt creates ""temporary"" tables for the data that needs to be merged, with a lifetime of 12 hours. However, as we run our transformations about every 30 minutes, these tables are always present in our BigQuery project.

We do want to keep using the incremental strategy, and appending new columns, but according to [this git issue](https://github.com/dbt-labs/dbt-bigquery/issues/184), it seems that we might be able to resolve it if we change the on_schema_change to a different value.

Since the data is mostly filtered down and is not too large, we don't mind about the storage costs - but we do mind that there are x2 more tables in our project, since for every incremental model we made, we now have the output table X and also the temporary table X__dbt_tmp

Is there a way we can get dbt to self-clean those temporary tables? It seems that it should've done so naturally, but it doesn't.

Thanks!",1
7540,"handling-bigquery-incremental-dbt-tmp-tables","I managed to currently solve this issue by using a post-hook macro call for all the models that I want to include:

The macro:
```
{% macro incremental_tmp_table_dropper(bigQueryRelationObject) %}
    {% set tmpTableName %}
        {{ bigQueryRelationObject.database + '.' + bigQueryRelationObject.schema + '.' + bigQueryRelationObject.identifier + '__dbt_tmp'}}
    {% endset %}
    {% set query %}
        drop table if exists {{tmpTableName}};
    {% endset %}

    {{ return(query) }}
{% endmacro %}
```

Adding it in the dbt_project.yml config file:

```
        +post-hook:
          - ""{{incremental_tmp_table_dropper(this)}}""
```

## My current question:

Is there any risk in using this? can it somehow affect the incremental models? I tried to test & look in the documentation as much as I could, and I can't see any reason not to use this implementation.",2
7543,"dbt-macro-output-enclosed-in-double-quotes-causes-bigquery-illegal-input-character-error","dbt macro outputs SQL enclosed with double quotes (â€œâ€), which raises an error in BigQuery: ""Syntax error: Illegal input character ""\342"" at [2:1]"".

```
/* {""app"": ""dbt"", ""dbt_version"": ""1.3.0"", ""profile_name"": ""jaffle_shop"", ""target_name"": ""dev"", ""connection_name"": ""master""} */
â€œ

  create or replace table dbt_cli.test as (
      <...>
  );

â€
```

Enclosing quotes are added automatically by dbt, nothing done in the macro itself.

Environment: dbt 1.3.0 CLI (Docker image)
**Update:**  same with dbt v1.4.4 / bigquery plugin v1.4.1

Has anyone faced such an issue before or has an idea how to control (get rid of) enclosing quotes?",1
7547,"dbt-with-snowflake-connection","connection error - dbt with snowflake through sso

I tired to connect dbt with snowflake through sso ,all my checks are passed ,but browser is not opened up.",1
7548,"how-to-pass-the-variables-in-the-json-file-to-vars-argument","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

I have a lot of variables defined in a json file, i wanna pass them to --vars argument. how can i do that?

I have tried the below command but its giving me an error
` dbt run --vars $(cat folder/json_file.json)`",1
7548,"how-to-pass-the-variables-in-the-json-file-to-vars-argument","Try debug what happens is you pass that to a simple echo, and see if it shows what you expect. any sorts of unexpected characters can break the flow, so this could very be a data input problem, and the command is just fine

<sub>Note: `@willem.hendriks` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1680099974577789?thread_ts=1680099124.177309&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7551,"what-are-the-expected-data-types-in-schema-yml-fille","When creating a `schema.yml` file for a data model, I'm always confused about the expected `data_type` values for the format like below. Do we have a list of suggested types, and does it vary on different query engines? Thanks!

```yaml
    columns:
      - name: event_timestamp
        data_type: TIMESTAMP
        description: """"
```",1
7554,"schema-yaml-for-macro-complaining-two-macros-with-the-same-name","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
When adding two entries of macros in `property.yml`, `dbt compile` has the following complain:
```sh
02:17:51  Encountered an error:
Compilation Error
  dbt found two macros named ""macro2"" in the project ""xxx"".
   To fix this error, rename or remove one of the following macros:
      - macros/utils/a_file_with_2_macros_defined.sql
      - macros/utils/a_file_with_2_macros_defined.sql
```

## The context of why I'm trying to do this
Initially there was only 1 macro `macro1` defined in `macros/utils/a_file_with_2_macros_defined.sql` and the `property.yml` is like below and everything works just fine.
```yaml
version: 2

macros:
  - name: macro1
    description: """"
    arguments: """"
```
Then I added an additional macro `macro2` to the same `sql` file and the `property.yml` file is changed to something below. But `dbt compile` starts to pop up the complains. If I remove the entry from `property.yml` file, everything works just fine and the data model consuming the new macro also works fine. 

Anyone has any clue on what's going on here? I'm pretty sure I don't have two macros with the same name in my project. 
```yaml
version: 2

macros:
  - name: macro1
    description: """"
    arguments: """"
  - name: macro2
    description: """"
    arguments: """"
```",1
7555,"multiple-run-results-json-and-manifest-json-files","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I need multiple run_results.json and manifest.json files under different directories in dbt. So, is that possible?

## The context of why I'm trying to do this
I have different parallel executions of dbt commands so I need these files to be separated from each other in order to avoid data loss/overlapping.",1
7555,"multiple-run-results-json-and-manifest-json-files","I think we cannot have multiple json artifacts files in dbt. I got the idea from following

https://github.com/dbt-labs/dbt-core/issues/4096",2
7555,"multiple-run-results-json-and-manifest-json-files","@adilrehman you could set a different [target-path](https://docs.getdbt.com/reference/project-configs/target-path) on each run, e.g. taking including the current timestamp in your command: `dbt --target-path 1680136368 run` (that's the current unix timestamp, you could do whatever you wanted)

Alternatively, if you're using dbt Cloud, you can use the API to access each individual step's artifacts [using the `step` parameter](https://docs.getdbt.com/dbt-cloud/api-v2#tag/Runs/operation/getArtifactsByRunId).",3
7555,"multiple-run-results-json-and-manifest-json-files","Thank you @joellabes 
This works for me!",4
7557,"the-missing-guide-to-debug-in-dbt","This is a companion discussion topic for the original entry at https://docs.getdbt.com/blog/guide-to-jinja-debug",1
7557,"the-missing-guide-to-debug-in-dbt","Thanks @bper! I've never used `debug()` but am now keen to give it a try myself. There's definitely times I would have found it useful in the past :eyes: 

One question: you said in the post 
>if [...] while in debug mode you press `c`, the debugger will stop at each of your breakpoints

Does this mean that a `{{ debug() }}` call very early on in your file would be almost impossible to trigger? I can understand how enough time would pass to trigger the breakpoint if you're trying to capture something that happens in an `on-run-end` hook, but what about if you were debugging something in `on-run-start`? 

Or are you also allowed to press `c` while dbt is doing the initial parse work etc, which means that you don't need the quickest draw in the west to make it work?",2
7557,"the-missing-guide-to-debug-in-dbt","",3
7557,"the-missing-guide-to-debug-in-dbt","Great question @joellabes 

`c` stands for `continue`, e.g. continue from the current breakpoint until the next one (or until the end of the program).

So you don't need to type `c` to trigger the first breakpoint, it will happen automatically (and you wont't need the quickest draw in the west). If you have multiple breakpoints though, after you enter the first one you might want to type `c` to execute all the code until the next one.",4
7570,"how-to-set-mode-for-the-column-in-the-schema-yml-from-dbt-cloud-for-bigquery","how to set mode for the column in the schema.yml from dbt cloud for bigquery.

I need to set mode for few columns as REQUIRED instead of NULLABLE. Because all the columns of the model created exhibits NULLABLE property along with REPEATED column in those scenarios. So how to set mode as REQUIRED.",1
7572,"output-custom-schema-name-in-model","I am using Dbt cloud IDE. Suppose I have a custom schema in a .yaml file like below:

```
models:
    -name: example_model
     config:
         schema: some_schema_name
```

How do I output the schema name in example_model.sql model ? 

I have tried different things like {{ model.schema }}, {{ target.schema }}, {{ this.schema }}, {{ config.schema }}, but nothing gives me the desired template output which is 'some_schema_name' in this case.",1
7572,"output-custom-schema-name-in-model","[quote=""Tahseen0354, post:1, topic:7572""]
`some_schema_name`
[/quote]

u have given schema under config block, can u try this `{{ config.get('schema') }}`",2
7572,"output-custom-schema-name-in-model","Hi, this does not work. I think it takes input from user and so it will output None if user does not provide any input.",3
7572,"output-custom-schema-name-in-model","isn't present in .yml file?",4
7572,"output-custom-schema-name-in-model","Yes, the value is already present in .yml file. So user does not need to provide the value as input. But {{ config.get('schema') }} outpus None.",5
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem  
I have a table where it consists of two columns with customer phone numbers and Country . I need to convert it to E.164 format . 

## The context of why I'm trying to do this

I am trying to solve with the dbt utils package and not able to find phone macro in it . So that I can use the macro in a Select statement.

 Try to use Python to return only that transformed column and use it in Sql model as a macro. 

Is there any way to do like this . Let me know which way it can be solved.

## What I've already tried 

I am trying to solve with the dbt utils  package and not able to find phone macro in or UDF. 

## Some example code or error messages
```
Put code inside backticks
  to preserve indentation
    which is especially important 
      for Python and YAML! 
```",1
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Hi @sivareddy3244, you can create your own custom macro!

If you provide some examples of your columns values I can give some help",2
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Hello bruno, I would say that for an example I have two columns named with phone_no and Country . I will state the similar values that I am dealing with.  

004917642024148       Germany 
017642025128.              Germany
1762390345                    France
+331762390345.            France 
 Goes on 

I have seen a similar problem dealt with Python code on net using library phonenumbers. 

I just want to use this columns and create a transformed column with E.164 format . 


Thank. You in advance.",3
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Do you have an example of a python library to show? We can convert that logic to SQL and use a jinja macro to compile this SQL logic",4
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Hello Bruno.   

Here is an example in Python . I need to implement in dbt

import phonenumbers

my_number = phonenumbers.parse(""0721234567"", ""RO"")
e164_f=phonenumbers.format_number(my_number, phonenumbers.PhoneNumberFormat.E164)
print(e164_f)",5
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Ok, this is what I thought, you can give your opinion about that:

**Assumptions:**
- Your columns are called  `phone_number` and `country`
- The columns are of type `string`
- The steps to transform a string into a e.164 phone number format are:
1 - Remove all non-numeric characters from `phone_number`
2 - Remove leading zeros from `phone_number`
3 - Check if the country code is included, if it is not, include it
4 - Include a plus sign at the end

**My solution:**

Create a macro in `macros/` folder with this SQL code:

```sql
{% macro country_code(country) %}

    case
        when trim(lower({{ country }})) = ""germany"" then ""49""
        when trim(lower({{ country }})) = ""france"" then ""33""
    end

{% endmacro %}


{% macro e164(phone_number, country) %}

  case
      when
         left(
              ltrim(
                regexp_replace(
                    {{ phone_number }}, '[^0-9]+', ''
                ), '0'
              )
              , length(
                  {{ country_code(country) }}
              )
          ) = {{ country_code(country) }}
      then
          concat(
              '+'
              , ltrim(
                  regexp_replace(
                      {{ phone_number }}, '[^0-9]+', ''
                  ), '0'
              )
          )
      else
          concat(
              '+'
              , {{ country_code(country) }}
              , ltrim(
                  regexp_replace(
                      {{ phone_number }}, '[^0-9]+', ''
                  )
                  , '0'
              )
          )
      end

{% endmacro %}
```

Then in your model you can do something like

```sql
select
    some_id
    , some_column
    ,  {{ e164('phone_number', 'country') }} as e164_phone_number
from your_model
```

**What you have to know**
- These functions work for **BigQuery**, if you are using another DB, then maybe you will have to adapt the functions
- I didn't care for performance, just wrote some idea, so it is **not optimized**, but you can work on that
- In this solution, you must **manually input the countries' codes in a case when statement**. Maybe using a dictionary is better suited? Don't know
- Sorry if the indentation is ugly, you can change that the way you prefer",6
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Thanks a lot bruno. I will work on it and write to you back how it goes .",7
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Sure, let me know if you have any problems! And if it works I would appreciate if you could mark it as the solution :smiley:",8
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","For sure . If have any problem I will definitely write to you back. If it works it would be my solution.",9
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","It is possible to run Python workloads on BigQuery via Dataproc: https://docs.getdbt.com/docs/build/python-models#specific-data-platforms 

If you use a Dataproc Cluster (as opposed to Dataproc serverless), you can install arbitrary packages from pip or conda, which would presumably include the `phonenumbers` library.

In a past life I used the .net port of the phonenumbers library and would definitely recommend using that instead of trying to normalise in SQL if you can!",10
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","I am using with snowflake",11
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Joel's suggestion works in Snowflake as well, via Snowpark! You can check it out in the same link he showed",12
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","Ahh my mistake! I saw BQ mentioned above and went with it. Bruno's right, phonenumbers is available on Snowpark via anaconda: https://repo.anaconda.com/pkgs/snowflake/",13
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","I am so used to SQL models that I went straight to the SQL solution :laughing:

But I agree this is a very good case for python models, and if you are not familiarized with that, it is a good first problem solve while learning",14
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","But can we use this Python model as a ref or macro as used above in the Sql after installing the phonenumbers library. 

I see that we canâ€™t use the Python models in another models as an import or a macro. 

Do you have the better suggestion or correct me . Thanks",15
7586,"i-am-trying-to-convert-phone-numbers-to-e-164-format","You canâ€™t create a Python macro that you call from SQL, but you can create a Python model that returns the transformed data as a normal table (can't be a view), and then you can `ref` it in downstream models, whether those models are written in SQL or Python.

The [Overview](https://docs.getdbt.com/docs/build/python-models#overview) section of the Python models docs does a good job of explaining their capabilities and limitations",16
7591,"please-disregard","Please disregard 
[I would deleted this post, but don't see that as an option]",1
7591,"please-disregard","not sure ... do you mean the dbt website forum?

<sub>Note: `@jarrell_dunson` originally [posted this reply in Slack](https://getdbt.slack.com/archives/CBSQTAPLG/p1680199742916319?thread_ts=1680199131.449379&cid=CBSQTAPLG). It might not have transferred perfectly.</sub>",2
7602,"dbtcloud-cant-connect-to-snowflake","I thought this problem was because of an upgrade from 1.3 to 1.4 but my transformations are still not running 

```
 Database Error
    250001 (08001): Failed to connect to DB: skxxxxx.eu-central-1.snowflakecomputing.com:443. User temporarily locked. Contact your local system administrator or please create a case with Snowflake Support or reach us on our support line: 
```

It's weird because when I use DataGrip to connect to Snowflake, also using my account, then there's no problem, the user is not locked. 

Running the same instance of dbt locally using dbt-core also works fine, it's only dbtCloud that fails to connect..

Anybody having similar problems?",1
7602,"dbtcloud-cant-connect-to-snowflake","I am new to dbt cli as well as cloud. Currently learning how to run a dbt cloud project with snowflake platform but the connection between dbt cloud and snowflake fails. I don't know whether you have had a similar situation? 
The error message occurs when i am initialising a dbt cloud project and it is as follows:

*ERROR
Something went wrong! Check the logs below for more information and contact support if you need additional assistance.*",2
7602,"dbtcloud-cant-connect-to-snowflake","Even worse is that the running the transformations in Develop works fine but scheduling doesn't",3
7602,"dbtcloud-cant-connect-to-snowflake","Have you checked the logs? Usually some problems come from permissions or connections been blocked.

Can you post the log here?",4
7602,"dbtcloud-cant-connect-to-snowflake","Not sure why but dbtCloud started working when I removed the role from connection. It's weird because if I connect as my user with that role directly via Datagrip it also works perfectly fine..",5
7604,"problem-querying-duckdb-database","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## I'm having trouble querying my duckdb database. I'm quite new to dbt so it could be a dumb mistake but all help is greately appreciated.

this is my models/example/schema.yml file

```
version: 2

models:
  - name: energy_sellers
    description: this dataset holds the data of the captured solar and wind energy also the price and the datetime this data was captured, this dataset has data for every 15 minuts
    columns:
      - name: id
        description: ""The primary key for this table""
        tests:
          - unique
          - not_null
      - name: date_time
        description: The datetime the data was captured
        tests:
          - unique
          - not_null
      - name: solar_measured
        description: The amount of solar energy produced in the whole of belgium at the given timestamp
        tests:
          - not_null
      - name: wind_measured
        description: The amount of wind energy produced in Belgium at the given timestamp
        tests:
          - not_null
      - name: solar_price
        description: The calculated price for solar energy at that given timestamp
        tests:
          - not_null
      - name: wind_price
        description: The calculated price for wind energy at that given timestamp
        tests:
          - not_null
```

this is my sources/sources.yml file (I was not sure if this file was mandatory to make)
```
version: 2

sources:
  name: energy_sellers
  database: energy_sellers.duckdb
  schema: energy_sellers
  tables:
    - name: energy
      columns:
      - name: id
        description: ""The primary key for this table""
        tests:
          - unique
          - not_null
      - name: date_time
        description: The datetime the data was captured
        tests:
          - unique
          - not_null
      - name: solar_measured
        description: The amount of solar energy produced in the whole of belgium at the given timestamp
        tests:
          - not_null
      - name: wind_measured
        description: The amount of wind energy produced in Belgium at the given timestamp
        tests:
          - not_null
      - name: solar_price
        description: The calculated price for solar energy at that given timestamp
        tests:
          - not_null
      - name: wind_price
        description: The calculated price for wind energy at that given timestamp
        tests:
          - not_null
```
This is my dbt_project.yml file
```
# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: transform_dbt
version: '1.0.0'
config-version: 2
vars:
  db_name: energy_sellers.duckdb

# This setting configures which ""profile"" dbt uses for this project.
profile: 'transform_dbt'

# These configurations specify where dbt should look for different types of files.
# The `model-paths` config, for example, states that models in this project can be
# found in the ""models/"" directory. You probably won't need to change these!
model-paths: [""models""]
analysis-paths: [""analyses""]
test-paths: [""tests""]
seed-paths: [""seeds""]
macro-paths: [""macros""]
snapshot-paths: [""snapshots""]

target-path: ""target""  # directory which will store compiled SQL files
clean-targets:         # directories to be removed by `dbt clean`
  - ""target""
  - ""dbt_packages""


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/
# directory as views. These settings can be overridden in the individual model
# files using the `{{ config(...) }}` macro.
models:
  transform_dbt:
    example:
      materialized: table
```
and this is my profiles.yml file
 ```
transform_dbt:
  target: dev
  outputs:
    dev:
      type: duckdb
      database: ./sources/energy_sellers.duckdb
      schema: energy_sellers
      path: './out/energy_sellers.duckdb'
      extensions:
        - httpfs
        - parquet
      settings:
        s3_region: eu-central-1
        s3_access_key_id: ${S3_ACCESS_KEY_ID}
        s3_secret_access_key: ${S3_SECRET_ACCESS_KEY}
```
My duckdb file is located in sources/energ_sellers.duckdb

the sql file I'm trying to run looks like this.

```
select id, date_time, solar_measured, wind_measured, solar_price, wind_price

from {{ source('energy_sellers', 'energy') }}
```

The database structure: 
Schema: energy_sellers
table: energy

but when I try to query it through a IDE I have to do ""Select * from ""energy_sellers.energy_sellers.energy""
I think it has something to do with Catalog.


When i run `dbt run` I receive the following error:
```
Compilation Error
  Model 'model.transform_dbt.energy_sellers' (models\example\energy_sellers.sql) depends on a source named 'energy_sellers.energy' which was not found
```

As I said all help is greatly appreciated and if you need any more information you can ask. Thanks in advance.",1
7604,"problem-querying-duckdb-database","[quote=""davidbackx, post:1, topic:7604""]
sources.yml
[/quote]

can you place the` sources.yml` under `models` folder and try running the model
move **sources/sources.yml** to **models/sources.yml**",2
7604,"problem-querying-duckdb-database","Thank you for looking into my problem @Surya . Unfortunately this change did not fix my problem. It still gives the same error after it first said `The schema file at models\example\sources.yml is invalid because the value of 'sources' is not a list` but when I changed it from â€˜sourcesâ€™ to â€˜sourceâ€™ it produced the same error again `Model 'model.transform_dbt.energy_sellers' (models\example\energy_sellers.sql) depends on a source named 'energy_sellers.energy' which was not found`",4
7604,"problem-querying-duckdb-database","[quote=""davidbackx, post:1, topic:7604""]
```
sources:
  name: energy_sellers
```
[/quote]

@davidbackx source configuration is incorrect in your sources.yml file
incorrect syntax:-
```
sources:
  name: energy_sellers
```
correct syntax:-

```
sources:
  - name: energy_sellers
```
place sources.yml file under models directory",5
7604,"problem-querying-duckdb-database","Thank you again @Surya for looking into the problem, indeed there was an error in my sources.yml file. Sadly this did not fix my error. this is my current sources.yml
```
version: 2

sources:
  - name: energy_sellers
    database: energy_sellers.duckdb
    schema: energy_sellers
    tables:
    - name: energy
      columns:
      - name: id
        description: ""The primary key for this table""
        tests:
          - unique
          - not_null
      - name: date_time
        description: The datetime the data was captured
        tests:
          - unique
          - not_null
      - name: solar_measured
        description: The amount of solar energy produced in the whole of belgium at the given timestamp
        tests:
          - not_null
      - name: wind_measured
        description: The amount of wind energy produced in Belgium at the given timestamp
        tests:
          - not_null
      - name: solar_price
        description: The calculated price for solar energy at that given timestamp
        tests:
          - not_null
      - name: wind_price
        description: The calculated price for wind energy at that given timestamp
        tests:
          - not_null
```
And this is my error I received.

```
09:36:58  Running with dbt=1.4.5
09:36:58  Unable to do partial parsing because profile has changed
09:37:01  Found 1 model, 16 tests, 0 snapshots, 0 analyses, 297 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
09:37:01  
09:37:02  Concurrency: 1 threads (target='dev')
09:37:02  
09:37:02  1 of 1 START sql table model energy_sellers.energy_sellers ..................... [RUN]
09:37:03  1 of 1 ERROR creating sql table model energy_sellers.energy_sellers ............ [ERROR in 0.37s]
09:37:03  
09:37:03  Finished running 1 table model in 0 hours 0 minutes and 2.45 seconds (2.45s).
09:37:03  
09:37:03  Completed with 1 error and 0 warnings:
09:37:03
09:37:03  Runtime Error in model energy_sellers (models\example\energy_sellers.sql)
09:37:03    Binder Error: Catalog ""energy_sellers.duckdb"" does not exist!
09:37:03
09:37:03  Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
```
Again thank you for looking into my problem.",6
7604,"problem-querying-duckdb-database","I think I fixed the problem, I hardcoded the FROM statement and my errors are gone. Thank you though for looking at my problem. For now it is fixed.",7
7604,"problem-querying-duckdb-database","[quote=""davidbackx, post:1, topic:7604""]
`energy_sellers`
[/quote]

if you don't mind, can u send the updated model files here? and what you have hardcoded",8
7604,"problem-querying-duckdb-database","This is my updated energy_sellers.sql file.
```
SELECT id, date_time, solar_measured, wind_measured, solar_price, wind_price FROM energy_sellers.energy_sellers.energy
```

This is the only file which I changed.",9
7604,"problem-querying-duckdb-database","The FROM statement is what is hardcode at first it was
```FROM {{ source(""energy_sellers"", ""energy"") }}```
and it changed to 
```FROM energy_sellers.energy_sellers.energy ```",10
7604,"problem-querying-duckdb-database","hardcoding is not a good practice
you please update  `database: energy_sellers.duckdb` to `database: energy_sellers` in your `sources.yml` file and try using  `{{ source('energy_sellers', 'energy') }}` in model file",11
7604,"problem-querying-duckdb-database","Hey @Surya that indeed fixed my problem. Thank you very much for helping me and looking at my problem.",12
7604,"problem-querying-duckdb-database","you're welcome. Happy to help :blush:",13
7610,"dbt-bigquery-table-creation-code-generation-issue","Hi experts,

I have been using DBT for BigQuery table creation and transformation but it has been constantly giving me query errors that it generates.

Here is what the model looks like, this is to imitate BQ's WRITE_TRUNCATE on partition column `submit_date`.

```
{{
    config(
        materialized='incremental',
        incremental_strategy='insert_overwrite',
        on_schema_change='sync_all_columns',
        partition_by = {
            'field': 'submit_date',
            'data_type': 'date',
            'granularity': 'day'
        },
        require_partition_filter = true,
        database='db',
        schema='dataset',
        alias='my_table_forge'
    )
}}
-- An ordinary SELECT
```

DBT then somehow generates two queries, one is to create a temp table and the other is to create the real table. The first query fires off perfectly fine with no error, but what is really weird is the second query:

```
create or replace table `db`.`dataset`.`my_table_forge`
partition by submit_date

OPTIONS(
require_partition_filter=True
)
as (

select col1, col2, col3...
from `db`.`dataset`.`my_table_forge`);
```

This hits an error:
```
Cannot query over table 'db.dataset.my_table_forge' without a filter over column(s) 'submit_date' that can be used for partition elimination
```

Why would dbt use the table to create itself? Shouldn't it query from the temp table and dump data into the real table? And even if it makes sense, somehow, it is ignoring the partition_field `submit_date`.

Any idea how to fix it?",1
7610,"dbt-bigquery-table-creation-code-generation-issue","This seems a bug from dbt 1.4,  which version are you using?

It is already fixed from what I know.

You can try upgrading your dbt-bigquery to `dbt-bigquery==1.4.3` or downgrading to `dbt-bigquery==1.3.2
`",2
7610,"dbt-bigquery-table-creation-code-generation-issue","Thanks @brunoszdl for the quick reply. I'll speak to the team and get back to you.",3
7610,"dbt-bigquery-table-creation-code-generation-issue","@brunoszdl We are using dbt-bigquery 1.3.0, do you think it could be another issue?",4
7610,"dbt-bigquery-table-creation-code-generation-issue","Hmmm that's weird.

It seems pretty much this issue here:
https://github.com/dbt-labs/dbt-bigquery/issues/483

That was corrected here:
https://github.com/dbt-labs/dbt-bigquery/releases/tag/v1.4.2

Can you just make a quick test and try to run your model using `dbt-bigquery==1.4.2` or `dbt-bigquery==1.4.3`?


---

If it does not work I am not sure what the problem is, but you can try to use a solution a community member (@johanndw) has come up with.

I will paste his message here:

""Related note, BQ does not perform partition elimination when using a subquery in the where clause. We use a macro to derive the highwater mark and pass that to the where clause as a literal value. This will probably also have solved your issue which requires a literal value, bonus is that BQ can perform partition elimination as well
```
{% macro get_incremental_filter(model, column, filter) %}

{% set max_query %}
    select coalesce(max(timestamp( {{column}} )), timestamp('1900-01-01')) as max_timevalue 
    from {{ model }}
    {% if filter is defined %}
        where {{ filter }}
    {% endif %}
{% endset %}

{% set max_query_results = run_query(max_query) %}

{% if execute %}
    {% set max_timestamp = max_query_results.rows[0].values()[0] %}
    {% if max_timestamp is none %}
        {{ exceptions.raise_compiler_error('max_timestamp returned from ""get_incremental_filter"" macro is None') }}
    {% endif %}
    {% set max_timestamp = max_timestamp | string %}
{% else %}
    {% set max_timestamp = '1901-01-01' %}
{% endif %}

{{ log('Incremental filter for ' ~ model ~ ' is ' ~ max_timestamp) }} {# log incremental filter #}
{{ return( ""'"" ~ max_timestamp ~ ""'"") }}

{% endmacro %}
```

And then in the models

```
from 
   bills

    --Prod incremental behaviour
    {% if target.name == 'prod' and is_incremental() %}      
        where bills.row_ingested_at > {{get_incremental_filter(this ,'row_ingested_at')}}
    {% endif %}
```
""",5
7610,"dbt-bigquery-table-creation-code-generation-issue","And also kudos to Christophe Oudar (didn't find his profile here) for creating the issue and the PR",6
7610,"dbt-bigquery-table-creation-code-generation-issue","Thanks a lot! Looks like we will need to upgrade to a more recent version and see what happens. I'm currently writing a script to generate dbt config block so prefer to not use the alternative solution as it looks complicated. Have a great weekend!",7
7610,"dbt-bigquery-table-creation-code-generation-issue","When you test it, let me know if it worked! :smiley:",8
7616,"how-can-i-get-the-source-configuration-via-jinja","
I have several source.yml configuration files which indicates the key-value for each table. We have one source.yml file for each customer and the tables set for each customer would be different; then, the models would be different for each customers e.g. I have the following source.yml file for a customer:
```
- name: channels
  tables:
     - name: table1
       identifier: identifier1
```

I need a jinja function call to verify that the table1 is existed in the source.yml file or not. I tried and searched a lot: First, I tried source(""channels"",""table1"") but it gives me the error when the table is not existed e.g. source(""channels"",""table1""). Second,  I verified the config() function call but it doesn't have any information around the source.yml.

In a word, I need a function such as ```config.has_key('sources.channels.table3')``` to verify that table1 is existed and table2 is not existed.  Would you please give me any hints to check the configurations is existed in the yml files or not?",1
7616,"how-can-i-get-the-source-configuration-via-jinja","can u send us the error ur getting?
i think ur source config syntax might be wrong
it should be like below

```
- name: channels
  tables:
    - name: table1
      identifier: identifier1
```",2
7616,"how-can-i-get-the-source-configuration-via-jinja","I did the same if you mean the dash behind the tables tag. I had a simple typo and now correct the question. In a simple word, I would like to find a function that give me true for table1 e.g. ```config.has_key('channels','table1')``` and give me false for table2 which is not existed in the configuration e.g. ```config.has_key('channels','table2')```.",3
7616,"how-can-i-get-the-source-configuration-via-jinja","You can try a macro like this:

```python
{% macro check_source(source_name, table_name) %}
    {% if execute %}

        {% for source in graph.sources.values() %}
    
            {% if source.source_name == source_name and source.name == table_name %}
    
                {{ return(true) }}
    
            {% endif %}

        {% endfor %}
    
        {{ return(false) }}
    
    {% endif %}
{% endmacro %}
```

Then call this macro like 

```python
{% if check_source('channels', 'table_1') %}
# your code here
{% endif %}
```

Just keep in mind this macro only works at run-time, not parse-time. So I am not sure it will do what you want.",4
7616,"how-can-i-get-the-source-configuration-via-jinja","Thanks @brunoszdl. It works with slight modifications.

```
{% macro check_source(source_name, table_name) %}

{%- if execute -%}
    {% for source in graph.sources.values() %}
        {%- if source.source_name == source_name and source.name == table_name -%}

            {{ return(true) }}

        {%- endif -%}
    {%- endfor -%}

    {{ return(false) }}

{%- endif -%}

{% endmacro %}
```",5
7623,"how-to-run-dbt-with-cockroach-db","I am building DBT models using multiple database connectors that my organization uses for many cases. We also use CRDB which is the only database not supported by DBT in my organization. 
My problem is when trying to create a view or a table using a CRDB connection I get the following error:

I tried searching on Google which led me to this open GitHub issue:
https://github.com/cockroachdb/cockroach/issues/57715

But I did not understand how other people's solutions can be implemented correctly in a heavy-data organization.

Our CRDB is the center of the organization. We already made full migration to DBT with Airflow. This is our last milestone.

```
Encountered an error:
Database Error
there is already a transaction in progress
```",1
7625,"how-to-make-good-sense-of-how-to-connect-to-and-reference-dbt-sources-for-example-referencing-dbt-tutorial-following-dbt-bigquery-set-up-guidelines"," WHERE IS THE 'dbt-tutorial' NAME REFERENCED (AS A SOURCE NAME) IN BiqQuery OR  IN DBT?

A major deficiency for dbt is managing data sources, starting with connecting to and referencing data sources.

To set up a BigQuery source for DBT, the guide https://docs.getdbt.com/docs/quickstarts/dbt-cloud/bigquery covers the following:

A.

Create BigQuery datasets[â€‹](https://docs.getdbt.com/docs/quickstarts/dbt-cloud/bigquery#create-bigquery-datasets)

2. Verify that you can run SQL queries. Copy and paste these queries into the Query Editor:

```
select * from `dbt-tutorial.jaffle_shop.customers`;select * from `dbt-tutorial.jaffle_shop.orders`;select * from `dbt-tutorial.stripe.payment`;
```

However, searching BigQuery Explorer for 'dbt-tutorial' finds 0 results. Where should it be found?


B.

Recommended dbt sql code includes 'select * from `dbt-tutorial.jaffle_shop.customers`

However, where is naming reference for 'dbt-tutorial' found?",1
7628,"dbt-python-model-to-configure-as-view","The problem I'm having is I can't configure dbt python model as view. 

 ```
import snowflake.snowpark.functions as F

def model(dbt, session):
    dbt.config(materialized=""view"")
    df = dbt.ref(""sample"")

    return df
```
Facing Error when ran the model

```
 Materialization ""materialization_view_snowflake"" only supports languages ['sql']; got ""python""
```

So my understanding is we can't  materialize model as **view**  using dbt-python model",1
7628,"dbt-python-model-to-configure-as-view","Currently you canâ€™t materialize dbt python models as views, only table and incremental

https://docs.getdbt.com/docs/build/python-models#materializations",2
7635,"what-documentation-summarizes-folder-and-naming-conventions-for-deciphering-their-usage-in-the-dbt-fundamentals-course","After completing the dbt fundamentals course and browsing dbt reference documentation, I'm still unable to decipher the basic working practice/meaning of dbt folder structure and file naming.

The course works through a number of scenarios, both implying and conflating how files should be named and where they should be located, but failing to clearly specify how they should be named and located, their sequencing of processing, etc.  Points 3 to 5 below file naming are demonstrated as part of the dbt fundamentals course segments but their relevance is not specified, so consequently, their significance are fuzzy and/or conflated.

Some related queries are:

1. What is the order of file processing related to file names and folder placement?

2. What is the relevance of what file naming and folder placement related to file processing sequencing and layering OR does somehow the internal code irrespective of file naming and folder location determine layering, processing, etc.?

3. What (if any) meaning does the 'stg_' file name prefix and/or models staging folder have? If no meaning, how is staging code recognized and sequenced as staging code?

4. What (if any) meaning does the 'fct_' file name prefix and/or models staging folder have? If no meaning, how is staging code recognized and sequenced as staging code?

5. What (if any) meaning does the 'dim_' file name prefix and/or models staging folder have? If no meaning, how is staging code recognized and sequenced as staging code?",1
7635,"what-documentation-summarizes-folder-and-naming-conventions-for-deciphering-their-usage-in-the-dbt-fundamentals-course","I am not sure if I understood your question, so let me know if this is not what you are asking.

When you execute a command such as  `dbt run`,  the models will be run in the DAGs order, and dbt knows this order because of the `{{ ref() }}` and `{{ source() }}` jinja functions. The name of the files and folders will not impact the order of processing, the resources are related through ref and source. (you other particularities with tests and exposures)

The name and placing conventions are used to make your project more understandable and consistent. And also to allow you to easily select a group of models in your command using selector syntax.

> As such, it's especially important to establish a deep and broad set of patterns to ensure as many people as possible are empowered to leverage their particular expertise in a positive way, and to ensure that the project remains approachable and maintainable as your organization scales.
https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview

> **Folders.** Folder structure is extremely important in dbt. Not only do we need a consistent structure to find our way around the codebase, as with any software project, but our folder structure is also one of the key interfaces for understanding the knowledge graph encoded in our project (alongside the DAG and the data output into our warehouse). It should reflect how the data flows, step-by-step, from a wide variety of source-conformed models into fewer, richer business-conformed models. Moreover, we can use our folder structure as a means of selection in dbt [selector syntax](https://docs.getdbt.com/reference/node-selection/syntax).
https://docs.getdbt.com/guides/best-practices/how-we-structure/2-staging

> **File names.** Creating a consistent pattern of file naming is [crucial in dbt](https://docs.getdbt.com/blog/on-the-importance-of-naming). File names must be unique and correspond to the name of the model when selected and created in the warehouse. We recommend putting as much clear information into the file name as possible, including a prefix for the layer the model exists in, important grouping information, and specific information about the entity or transformation in the model.
https://docs.getdbt.com/guides/best-practices/how-we-structure/2-staging

You can find more information in the links in the citations.",2
7635,"what-documentation-summarizes-folder-and-naming-conventions-for-deciphering-their-usage-in-the-dbt-fundamentals-course","Thanks.

Your explicit reply regarding 
1. the order of processing with regard to the `{{ ref() }}` and `{{ source() }}` jinja functions,
2. the impact of folder structuring and file naming,
is helpful.

I agree that logical folder structuring and naming conventions are important for good project management. Also important is distinguishing between how data/files are processed from logical file organization, otherwise, an important fundamental understanding of dbt is missing.

For further clarification, and to confirm your reply and citation links: 

A.
i. What about the folder structure at the level of models and tests? 
Do those folder naming conventions impact processing?
ii. If I created and used a folder named 'design' instead of 'models', would processing still be the same?
Is that determined by the 'model-paths: [""models""]' setting in dbt_project.yml?
iii. If I included staging files in the tests folder, would they still process properly for staging as long as the appropriate `{{ ref() }}` and `{{ source() }}` jinja functions were included?
Is that related to the 'test-paths: [""tests""]' setting in dbt_project.yml?

B. 
i. Would naming a folder in models to 'stg' instead of 'staging' and prefixing the files inside with 'stage_' instead of 'stg_' work the same as long as the `{{ ref() }}` and `{{ source() }}` jinja functions are appropriately the same?

C.
How to decipher the reference to 'dbt-tutorial' from ""`dbt-tutorial`.jaffle_shop.customers"" from [Quickstart for dbt Cloud and BigQuery | dbt Developer Hub](https://docs.getdbt.com/docs/quickstarts/dbt-cloud/bigquery)?

Where is 'dbt-tutorial' specified as a data warehouse and/or source in dbt or BigQuery?",3
7635,"what-documentation-summarizes-folder-and-naming-conventions-for-deciphering-their-usage-in-the-dbt-fundamentals-course","A.

dbt will try to find each type of resource in their respective folders, according to the paths you specify in dbt_project.yml. If you do not specify anything it will search for the default ones.

Your models must be inside the `models` folder unless you specify another folder in `model-paths` as you said. If you want them to be inside a `design` folder you can specify it in the paths.

I have never tried to pass the tests folder in the `model-paths` and put a model inside it. It might work, not sure. For learning purposes, you can try it, just have in mind that it was not how it was supposed to be structured.

https://docs.getdbt.com/reference/project-configs/model-paths.


B.

No problem doing that. You can name the subfolders inside your `models` folder, and the models inside as you want. It will work the same.

C.

`dbt-tutorial` is a public project in BigQuery, you can define it as a source in dbt as

sources.yml (inside models)
```yml
version: 2

sources:
  - name: jaffle_shop
    database: dbt-tutorial  
    schema: jaffle_shop  
    tables:
      - name: orders
      - name: customers
...
```

And call from your model as

```sql
select *
from {{ source('jaffle_shop', 'orders') }}
```

I didn't see the tutorial, so I am not sure if you need to do it, but you can.",4
7642,"dbt-metrics-time-grains","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
I would like to introduce a metric that is calculated on both: beginning of the week, beginning of the month, end of the week, end of the month time grains.

## The context of why I'm trying to do this
I'm trying to do this because our internal definition of the metric can vary depending on the team that is using it. Finance teams are only interested in headcount on the finale day of each period (week, month, quarter) while other teams require taking the whole month into consideration.

## What I've already tried 
I've built the mart model with the following fields: 
date, is_active, user_id, day_is_first_of_week, day_is_first_of_month,  day_is_last_of_week, day_is_last_of_month

## Some example code or error messages
Here I'm not sure how to define grain properly as I assume it will calculate count of distinct user_id within the week, month but I would actually need it to consider the end dates of each period as well. 

Should I be creating separate metrics based on multiple mart models or can my requirements still be achieved through one metric?",1
7650,"training-saving-and-running-machine-learning-workloads-with-dbt-snowflake","TLDR: For those of you deploying data science models using dbt & Snowflake, which parts of the model do you run in dbt vs snowflake? 

---

Hi,

We're working on a project that involves training a predictive machine learning model in Snowflake. We create our training dataset by transforming raw data with SQL in dbt, and then want to 

1) train an ML model on this dataset and save the model, e.g. as a pickle file
2) separately, load the model and make predictions 

We know it's possible to run Python with dbt, and we've successfully trained the model in dbt. In terms of storing objects, we can think of using 

- Snowflake internal stages
- External stages, such as S3 

We'd be eager to understand whether there are any best practices for running SQL + Python workloads in Snowflake involving saving and then loading in machine learning models?

Thanks!",1
7651,"table-values-showing-as-null","(topic deleted by author)",1
7653,"dbt-cloud-git-clone-with-private-gitlab-repo","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem i am having is that i use dbt cloud with private gitlab repo. during project setup i use git clone for my gitlab repo and added the deploy keys into my gitlab ssh keys and it connect successfully. now when i try to commit and changes into the DBT project, it is not commiting those changes in my gitlab repo. please guide me",1
7661,"running-backfills-in-incremental-models-obsolete-records-may-persist","<!-- https://docs.getdbt.com/guides/legacy/getting-help 
You don't need to keep the headers, but try to hit these points in your question: -->

## The problem I'm having
Weâ€™ve seen in the â€œincremental modelsâ€ where the date fieldâ€™s degree of detail drops to lower levels like minutes or seconds, a data problem may occur where incorrect data persists in production on future reloads/backfills from the same period of time. To make it simpler to describe what I am observing, I will do so using the following example at the day level.
We have the current model:

```
{{config(
  materialized='incremental',
  incremental_strategy='delete+insert',
  unique_key='date_day',
)}}

select date('2023-01-01') as date_day
UNION 
select date('2023-01-02') as date_day
UNION 
select date('2023-01-03') as date_day
UNION 
select date('2023-01-04') as date_day
UNION 
select date('2023-01-05') as date_day
UNION 
select date('2023-01-06') as date_day
```

I executed it the first time, and the table was created. If I execute it more times, what the model is doing is:

```
CREATE TEMP_TABLE AS CTE;

DELETE FROM FINAL_TABLE WHERE date_day in (SELECT date_day from TEMP_TABLE);

INSERT INTO FINAL_TABLE SELECT * FROM TEMP_TABLE;
```

Letâ€™s imagine that we are doing a backfill of the data and this `data_day = 2023-01-03` we donâ€™t have data. Then after executing the following code:
```
{{config(
  materialized='incremental',
  incremental_strategy='delete+insert',
  unique_key='date_day'
)}}

select date('2023-01-01') as date_day
UNION
select date('2023-01-02') as date_day
UNION
select date('2023-01-04') as date_day
UNION
select date('2023-01-05') as date_day
UNION
select date('2023-01-06') as date_day
```
We will find that `2023-01-03` persist in the final table.
The problem comes from the `DELETE` command that is not built as a range of dates.
```
DELETE FROM FINAL_TABLE
WHERE date_day >= (SELECT min(date_date) FROM TEMP_TABLE)
  and date_day <= (SELECT max(date_date) FROM TEMP_TABLE)
```
## The context of why I'm trying to do this
We have some cases where date_key is hours or minutes and this is a case that can happen with some data
## What I've already tried 
I look for a solution of incremental materialization and delete_strategy equal delete+insert, but I did not find anything.The only solution was to aggregate a pre_hook, but it means that we need to know before hand the `min` an `max` date that is going to be load in the final table.
```
{{config(
  materialized='incremental',
  incremental_strategy='delete+insert',
  unique_key='date_day',
  pre_hook=""
  {% if is_incremental() %}
    delete from {{ this }} where date_day between date('2023-01-02') and date('2023-01-06')
  {% endif %}""
)}}
```
The same happens with incremental_strategy=â€˜mergeâ€™
## Some example code or error messages
NA",1
7666,"dbt-core-python-model-issue-ssl-certificates","## The problem I'm having
I'm unable to use Python models with dbt-core. I run into a a SSL certificate verify failure when communicating with my Databricks instance. SQL models work fine.

## What I've already tried 
- Creating simple Python models (doesn't work)
- Creating  SQL models (does work)
- Disabling ssl_verify on my conda configuration file

## Some example code or error messages
HTTPSConnectionPool(host='<databricks_domain>', port=443): Max retries exceeded with url: /api/1.2/contexts/create (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",1
